<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Decision by Supervised Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization</title>
<link>https://arxiv.org/abs/2503.13544</link>
<guid>https://arxiv.org/abs/2503.13544</guid>
<content:encoded><![CDATA[
<div> Framework, Portfolio optimization, Supervised learning, Deep ensemble, Backtesting

Summary:<br>
The Decision by Supervised Learning (DSL) framework introduces a novel approach to robust portfolio optimization by treating it as a supervised learning problem. By training models to predict optimal portfolio weights using cross-entropy loss and maximizing ratios, such as Sharpe or Sortino, DSL achieves superior performance compared to traditional and machine learning-based methods. By employing Deep Ensemble methods, DSL significantly reduces variance in portfolio allocations, enhancing stability and reliability. Through comprehensive backtesting across various market universes and neural architectures, DSL demonstrates higher median returns and more stable risk-adjusted performance with increased ensemble size. The code for DSL is available on GitHub, facilitating accessibility and implementation for researchers and practitioners in the field.<br><br>Summary: <div>
arXiv:2503.13544v4 Announce Type: replace 
Abstract: We propose Decision by Supervised Learning (DSL), a practical framework for robust portfolio optimization. DSL reframes portfolio construction as a supervised learning problem: models are trained to predict optimal portfolio weights, using cross-entropy loss and portfolios constructed by maximizing the Sharpe or Sortino ratio. To further enhance stability and reliability, DSL employs Deep Ensemble methods, substantially reducing variance in portfolio allocations. Through comprehensive backtesting across diverse market universes and neural architectures, shows superior performance compared to both traditional strategies and leading machine learning-based methods, including Prediction-Focused Learning and End-to-End Learning. We show that increasing the ensemble size leads to higher median returns and more stable risk-adjusted performance. The code is available at https://github.com/DSLwDE/DSLwDE.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2507.18376</link>
<guid>https://arxiv.org/abs/2507.18376</guid>
<content:encoded><![CDATA[
<div> Keywords: smart agriculture, precision agriculture, artificial intelligence, deep learning models, diffusion models

Summary:<br><br>Smart agriculture and precision agriculture are key directions for the future of agricultural development, as the global population grows and arable land becomes scarce. Artificial intelligence technologies, particularly deep learning models, are being increasingly utilized in areas such as crop monitoring and pest detection. Diffusion models, as a new generative model, have shown great promise in agricultural image processing, data augmentation, and remote sensing, offering superior training stability and generation quality compared to traditional GANs. The advancements in diffusion models in agriculture focus on crop pest and disease detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Experimental results demonstrate improved model accuracy and robustness in data augmentation, image generation, and denoising, especially in complex environments. Despite challenges in computational efficiency and generalization capabilities, diffusion models are expected to have a significant impact on smart and precision agriculture, supporting the sustainable development of global agriculture. 

Summary: <div>
arXiv:2507.18376v2 Announce Type: replace 
Abstract: With the global population growing and arable land resources becoming increasingly scarce,smart agriculture and precision agriculture have emerged as key directions for the future ofagricultural development.Artificial intelligence (AI) technologies, particularly deep learning models, have found widespread applications in areas such as crop monitoring and pest detection. As an emerging generative model, diffusion models have shown significant promise in tasks like agricultural image processing, data augmentation, and remote sensing. Compared to traditional generative adversarial networks (GANs), diffusion models offer superior training stability and generation quality, effectively addressing challenges such as limited agricultural data and imbalanced image samples. This paper reviews the latest advancements in the application of diffusion models in agriculture, focusing on their potential in crop pest and disease detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Experimental results demonstrate that diffusion models significantly improve model accuracy and robustness in data augmentation, image generation, and denoising, especially in complex environments. Despite challenges related to computational efficiency and generalization capabilities, diffusion models are expected to play an increasingly important role in smart and precision agriculture as technology advances, providing substantial support for the sustainable development of global agriculture.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks</title>
<link>https://arxiv.org/abs/2507.18675</link>
<guid>https://arxiv.org/abs/2507.18675</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, action recognition, healthcare, masking strategies, generalizability<br>
Summary:<br>
- Traditional models like CNNs and RNNs struggle to generalize across diverse actions in healthcare and medicine.<br>
- CLIP, a vision-language model, shows promise for action recognition but exhibits inconsistent behavior and misclassifications when important visual cues are obscured.<br>
- Three masking strategies are evaluated on the UCF-101 dataset, revealing limitations in CLIP's performance.<br>
- To improve accuracy and reduce bias, a custom loss function is proposed to incorporate class-specific noise for reinforcing attention to key features.<br>
- Challenges in applying such models in clinical domains are discussed, and future work directions for enhancing generalizability in healthcare scenarios are outlined. <br> 
Summary: <div>
arXiv:2507.18675v2 Announce Type: replace-cross 
Abstract: Human action recognition plays a critical role in healthcare and medicine, supporting applications such as patient behavior monitoring, fall detection, surgical robot supervision, and procedural skill assessment. While traditional models like CNNs and RNNs have achieved moderate success, they often struggle to generalize across diverse and complex actions. Recent advancements in vision-language models, especially the transformer-based CLIP model, offer promising capabilities for generalizing action recognition from video data. In this work, we evaluate CLIP on the UCF-101 dataset and systematically analyze its performance under three masking strategies: (1) percentage-based and shape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to suppress bias-inducing elements, and (3) isolation masking that retains only class-specific regions. Our results reveal that CLIP exhibits inconsistent behavior and frequent misclassifications, particularly when essential visual cues are obscured. To overcome these limitations, we propose incorporating class-specific noise, learned via a custom loss function, to reinforce attention to class-defining features. This enhancement improves classification accuracy and model confidence while reducing bias. We conclude with a discussion on the challenges of applying such models in clinical domains and outline directions for future work to improve generalizability across domain-independent healthcare scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PurpCode: Reasoning for Safer Code Generation</title>
<link>https://arxiv.org/abs/2507.19060</link>
<guid>https://arxiv.org/abs/2507.19060</guid>
<content:encoded><![CDATA[
<div> Keywords: PurpCode, safe code reasoning models, cyberactivities, reinforcement learning, cybersafety

Summary:
PurpCode introduces a novel approach for training safe code reasoning models to generate secure code and defend against malicious cyberactivities. The training process consists of two stages: Rule Learning, where the model learns to reference cybersafety rules to generate vulnerability-free code, and Reinforcement Learning, which optimizes model safety and utility through diverse reward mechanisms. The training data is enriched through comprehensive prompts created by internal red-teaming exercises simulating real-world cyber threats. The resulting model, PurpCode-32B, showcases superior cybersafety performance compared to existing models. Additionally, an alignment method is employed to reduce overrefusal rates while maintaining model utility in code generation and security knowledge. This innovative approach holds promise for enhancing the security and reliability of code generation processes in the cybersecurity domain.<br><br>Summary: <div>
arXiv:2507.19060v2 Announce Type: replace-cross 
Abstract: We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Neural Autoregressive Modeling of Brain Aging</title>
<link>https://arxiv.org/abs/2507.22954</link>
<guid>https://arxiv.org/abs/2507.22954</guid>
<content:encoded><![CDATA[
<div> Keywords: Brain aging synthesis, NeuroAR, generative autoregressive transformers, MRI scan, subject-specific patterns <br />
<br />
Summary: 
The study introduces NeuroAR, a novel brain aging simulation model that addresses the challenges of high-dimensionality data, subtle structural changes, and subject-specific patterns in synthesizing the aging brain. NeuroAR utilizes generative autoregressive transformers to predict future brain structures from earlier MRI scans, incorporating subject-specific information and age guidance. It outperforms existing models like latent diffusion models and generative adversarial networks in image fidelity for both elderly and adolescent populations. The model's effectiveness is validated through a pre-trained age predictor, confirming the realism and consistency of the synthesized images with expected aging patterns. NeuroAR demonstrates superior performance in modeling subject-specific brain aging trajectories, showcasing its potential in clinical and computational neuroscience applications. <br /><br /> <div>
arXiv:2507.22954v1 Announce Type: new 
Abstract: Brain aging synthesis is a critical task with broad applications in clinical and computational neuroscience. The ability to predict the future structural evolution of a subject's brain from an earlier MRI scan provides valuable insights into aging trajectories. Yet, the high-dimensionality of data, subtle changes of structure across ages, and subject-specific patterns constitute challenges in the synthesis of the aging brain. To overcome these challenges, we propose NeuroAR, a novel brain aging simulation model based on generative autoregressive transformers. NeuroAR synthesizes the aging brain by autoregressively estimating the discrete token maps of a future scan from a convenient space of concatenated token embeddings of a previous and future scan. To guide the generation, it concatenates into each scale the subject's previous scan, and uses its acquisition age and the target age at each block via cross-attention. We evaluate our approach on both the elderly population and adolescent subjects, demonstrating superior performance over state-of-the-art generative models, including latent diffusion models (LDM) and generative adversarial networks, in terms of image fidelity. Furthermore, we employ a pre-trained age predictor to further validate the consistency and realism of the synthesized images with respect to expected aging patterns. NeuroAR significantly outperforms key models, including LDM, demonstrating its ability to model subject-specific brain aging trajectories with high fidelity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Cheating Detection in Korean Language via Keystrokes</title>
<link>https://arxiv.org/abs/2507.22956</link>
<guid>https://arxiv.org/abs/2507.22956</guid>
<content:encoded><![CDATA[
<div> keywords: keystroke-based framework, LLM-assisted cheating, Korean, cognitive processes, classification

Summary:
This paper introduces a framework for detecting cheating with the assistance of large language models (LLMs) in Korean writing tasks. The study involved 69 participants completing tasks under different conditions, ranging from genuine writing to paraphrasing and transcribing responses generated by a language model. The tasks were designed to cover various cognitive processes according to Bloom's Taxonomy. The analysis focused on extracting temporal and rhythmic features and evaluating different classifiers in Cognition-Aware and Cognition-Unaware settings. Results showed that temporal features were effective in scenarios where cognition was taken into account, while rhythmic features performed better across different cognitive demands. Detecting genuine and transcribed responses was more straightforward compared to detecting paraphrased ones, and the models consistently outperformed human evaluators. This study highlights the potential of keystroke dynamics in effectively identifying LLM-assisted cheating in writing tasks of varying complexities and strategies. 

<br /><br />Summary: <div>
arXiv:2507.22956v1 Announce Type: new 
Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted cheating in Korean, addressing key gaps in prior research regarding language coverage, cognitive context, and the granularity of LLM involvement. Our proposed dataset includes 69 participants who completed writing tasks under three conditions: Bona fide writing, paraphrasing ChatGPT responses, and transcribing ChatGPT responses. Each task spans six cognitive processes defined in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and create). We extract interpretable temporal and rhythmic features and evaluate multiple classifiers under both Cognition-Aware and Cognition-Unaware settings. Temporal features perform well under Cognition-Aware evaluation scenarios, while rhythmic features generalize better under cross-cognition scenarios. Moreover, detecting bona fide and transcribed responses was easier than paraphrased ones for both the proposed models and human evaluators, with the models significantly outperforming the humans. Our findings affirm that keystroke dynamics facilitate reliable detection of LLM-assisted writing across varying cognitive demands and writing strategies, including paraphrasing and transcribing LLM-generated responses.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Machine Learning with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2507.22959</link>
<guid>https://arxiv.org/abs/2507.22959</guid>
<content:encoded><![CDATA[
<div> machine learning, Kolmogorov-Arnold Networks, data encoding, interpretability, physics-informed modeling

Summary:<br />
- Scientific machine learning is shifting towards Kolmogorov-Arnold Networks (KANs) for data encoding, overcoming the limitations of multilayer perceptrons (MLPs).
- KANs provide enhanced interpretability, flexibility, and efficiency in modeling complex nonlinear interactions compared to MLPs.
- Recent progress in KAN-based models is categorized into data-driven learning, physics-informed modeling, and deep operator learning perspectives.
- KANs show consistent improvements in accuracy, convergence, and spectral representation when benchmarked against MLPs.
- Challenges in KAN development include computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity.
- Future research directions aim to enhance the robustness, scalability, and physical consistency of KAN-based frameworks.

<br /><br />Summary: <div>
arXiv:2507.22959v1 Announce Type: new 
Abstract: The field of scientific machine learning, which originally utilized multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold Networks (KANs) for data encoding. This shift is driven by the limitations of MLPs, including poor interpretability, fixed activation functions, and difficulty capturing localized or high-frequency features. KANs address these issues with enhanced interpretability and flexibility, enabling more efficient modeling of complex nonlinear interactions and effectively overcoming the constraints associated with conventional MLP architectures. This review categorizes recent progress in KAN-based models across three distinct perspectives: (i) data-driven learning, (ii) physics-informed modeling, and (iii) deep operator learning. Each perspective is examined through the lens of architectural design, training strategies, application efficacy, and comparative evaluation against MLP-based counterparts. By benchmarking KANs against MLPs, we highlight consistent improvements in accuracy, convergence, and spectral representation, clarifying KANs' advantages in capturing complex dynamics while learning more effectively. Finally, this review identifies critical challenges and open research questions in KAN development, particularly regarding computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. We also outline future research directions aimed at improving the robustness, scalability, and physical consistency of KAN-based frameworks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations</title>
<link>https://arxiv.org/abs/2507.22962</link>
<guid>https://arxiv.org/abs/2507.22962</guid>
<content:encoded><![CDATA[
<div> Keywords: climate extremes, agriculture, early warning systems, deep learning, XAI

Summary: 
- The paper addresses the need for reliable multi-hazard early warning systems in agriculture due to escalating climate risks.
- Traditional single-hazard forecasting methods are insufficient in capturing complex interactions among concurrent climatic events.
- The authors propose a multi-hazard forecasting framework for agriculture using deep learning models and Explainable Artificial Intelligence (XAI) techniques.
- The framework integrates attention mechanisms with TimeSHAP to provide comprehensive temporal explanations of influential climatic features and their impacts.
- Experimental results with meteorological data from four agricultural regions in the United States show high predictive accuracy, particularly with the BiLSTM architecture, and the system's capacity to inform proactive risk management strategies.

<br /><br />Summary: <div>
arXiv:2507.22962v1 Announce Type: new 
Abstract: Climate extremes present escalating risks to agriculture intensifying the need for reliable multi-hazard early warning systems (EWS). The situation is evolving due to climate change and hence such systems should have the intelligent to continue to learn from recent climate behaviours. However, traditional single-hazard forecasting methods fall short in capturing complex interactions among concurrent climatic events. To address this deficiency, in this paper, we combine sequential deep learning models and advanced Explainable Artificial Intelligence (XAI) techniques to introduce a multi-hazard forecasting framework for agriculture. In our experiments, we utilize meteorological data from four prominent agricultural regions in the United States (between 2010 and 2023) to validate the predictive accuracy of our framework on multiple severe event types, which are extreme cold, floods, frost, hail, heatwaves, and heavy rainfall, with tailored models for each area. The framework uniquely integrates attention mechanisms with TimeSHAP (a recurrent XAI explainer for time series) to provide comprehensive temporal explanations revealing not only which climatic features are influential but precisely when their impacts occur. Our results demonstrate strong predictive accuracy, particularly with the BiLSTM architecture, and highlight the system's capacity to inform nuanced, proactive risk management strategies. This research significantly advances the explainability and applicability of multi-hazard EWS, fostering interdisciplinary trust and effective decision-making process for climate risk management in the agricultural industry.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization</title>
<link>https://arxiv.org/abs/2507.22963</link>
<guid>https://arxiv.org/abs/2507.22963</guid>
<content:encoded><![CDATA[
<div> federated learning, cardiovascular disease, risk prediction, Random Forest, XGBoost <br />
Summary:<br />
- FedCVD++ is a privacy-preserving predictive system for cardiovascular disease risk prediction using federated learning.
- The framework integrates both parametric and non-parametric models, achieving state-of-the-art results on the Framingham dataset.
- FedCVD++ introduces tree-subset sampling, XGBoost-based feature extraction, and federated SMOTE synchronization to address communication challenges and class imbalance.
- Results show that FedCVD++ outperforms existing FL frameworks with higher F1-scores and improved scalability for multi-institutional deployment.
- The framework reduces bandwidth consumption by 3.2X while maintaining 95% accuracy, demonstrating its practicality in real-world clinical settings. <br /> <div>
arXiv:2507.22963v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVD) cause over 17 million deaths annually worldwide, highlighting the urgent need for privacy-preserving predictive systems. We introduce FedCVD++, an enhanced federated learning (FL) framework that integrates both parametric models (logistic regression, SVM, neural networks) and non-parametric models (Random Forest, XGBoost) for coronary heart disease risk prediction. To address key FL challenges, we propose: (1) tree-subset sampling that reduces Random Forest communication overhead by 70%, (2) XGBoost-based feature extraction enabling lightweight federated ensembles, and (3) federated SMOTE synchronization for resolving cross-institutional class imbalance.
  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves state-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its centralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81) matches non-federated performance. Additionally, our communication-efficient strategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.
  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher F1-scores and superior scalability for multi-institutional deployment. This work represents the first practical integration of non-parametric models into federated healthcare systems, providing a privacy-preserving solution validated under real-world clinical constraints.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation</title>
<link>https://arxiv.org/abs/2507.23000</link>
<guid>https://arxiv.org/abs/2507.23000</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme heat events, urban heat stress, deep learning, UTCI prediction, urban climate adaptation<br />
Summary: 
The study introduces a new deep learning framework, GSM-UTCI, to predict daytime average Universal Thermal Climate Index (UTCI) at hyperlocal resolution by integrating surface morphology, land cover data, and meteorological conditions. The model shows high accuracy and significantly reduces computation time, making it suitable for city-wide planning. Application of the framework in Philadelphia demonstrates the effectiveness of landscape transformation scenarios in reducing heat stress, with tree canopy conversion showing the most significant cooling effects. The study emphasizes the tool's utility for decision-making in urban climate adaptation, allowing for scenario-based evaluation of greening strategies and highlighting the connection between land cover types and thermal reduction potential.<br /><br />Summary: <div>
arXiv:2507.23000v1 Announce Type: new 
Abstract: As extreme heat events intensify due to climate change and urbanization, cities face increasing challenges in mitigating outdoor heat stress. While traditional physical models such as SOLWEIG and ENVI-met provide detailed assessments of human-perceived heat exposure, their computational demands limit scalability for city-wide planning. In this study, we propose GSM-UTCI, a multimodal deep learning framework designed to predict daytime average Universal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The model fuses surface morphology (nDSM), high-resolution land cover data, and hourly meteorological conditions using a feature-wise linear modulation (FiLM) architecture that dynamically conditions spatial features on atmospheric context. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical accuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\deg}C, while reducing inference time from hours to under five minutes for an entire city. To demonstrate its planning relevance, we apply GSM-UTCI to simulate systematic landscape transformation scenarios in Philadelphia, replacing bare earth, grass, and impervious surfaces with tree canopy. Results show spatially heterogeneous but consistently strong cooling effects, with impervious-to-tree conversion producing the highest aggregated benefit (-4.18{\deg}C average change in UTCI across 270.7 km2). Tract-level bivariate analysis further reveals strong alignment between thermal reduction potential and land cover proportions. These findings underscore the utility of GSM-UTCI as a scalable, fine-grained decision support tool for urban climate adaptation, enabling scenario-based evaluation of greening strategies across diverse urban environments.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead</title>
<link>https://arxiv.org/abs/2507.23009</link>
<guid>https://arxiv.org/abs/2507.23009</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Psychological tests, AI performance, Validation, AI-specific evaluation

Summary: 
The paper argues against interpreting the results of Large Language Models (LLMs) on standardized human cognitive and psychological tests as evidence of human-like characteristics, labeling it an ontological error. Human tests are designed for a specific population and applying them to AI without validation risks misinterpretation. There are concerns about the validity, data contamination, cultural bias, and sensitivity of AI benchmark performance as measurements of human-like traits like intelligence. The paper advocates for the development of principled, AI-specific evaluation frameworks tailored to AI systems, instead of relying on human tests. These frameworks could be based on existing psychometrics tests or newly created to suit the unique context of AI. 

<br /><br />Summary: <div>
arXiv:2507.23009v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable results on a range of standardized tests originally designed to assess human cognitive and psychological traits, such as intelligence and personality. While these results are often interpreted as strong evidence of human-like characteristics in LLMs, this paper argues that such interpretations constitute an ontological error. Human psychological and educational tests are theory-driven measurement instruments, calibrated to a specific human population. Applying these tests to non-human subjects without empirical validation, risks mischaracterizing what is being measured. Furthermore, a growing trend frames AI performance on benchmarks as measurements of traits such as ``intelligence'', despite known issues with validity, data contamination, cultural bias and sensitivity to superficial prompt changes. We argue that interpreting benchmark performance as measurements of human-like traits, lacks sufficient theoretical and empirical justification. This leads to our position: Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead. We call for the development of principled, AI-specific evaluation frameworks tailored to AI systems. Such frameworks might build on existing frameworks for constructing and validating psychometrics tests, or could be created entirely from scratch to fit the unique context of AI.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods</title>
<link>https://arxiv.org/abs/2507.23010</link>
<guid>https://arxiv.org/abs/2507.23010</guid>
<content:encoded><![CDATA[
<div> latent spaces, multimodal, inverse mappings, optimization, AI<br />
<br />
Summary: 
This paper explores the potential of multimodal latent spaces in task-specific AI models for inverse mappings. The authors propose an optimization-based framework to infer input characteristics from desired outputs across Text-Image and Text-Audio modalities. The experiments show that while models can be guided towards inverse tasks through optimization, the resulting mappings lack semantic meaning and perceptual coherence. The study reveals that while models can align textually with targets, the perceptual quality of the inversions is chaotic. Additionally, when inferring original semantic input from generative models, the reconstructed latent space embeddings often lack interpretability. This highlights a critical limitation in current multimodal latent spaces, which are optimized for forward tasks and do not support robust and interpretable inverse mappings. Further research is needed to develop truly semantically rich and invertible multimodal latent spaces. <br /><br /> <div>
arXiv:2507.23010v1 Announce Type: new 
Abstract: This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.
  Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.
  These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KLLM: Fast LLM Inference with K-Means Quantization</title>
<link>https://arxiv.org/abs/2507.23035</link>
<guid>https://arxiv.org/abs/2507.23035</guid>
<content:encoded><![CDATA[
<div> Quantization, Large language model, K-Means, Outlier detection, Hardware-software co-design<br />
<br />
Summary: <br />
The article introduces KLLM, a hardware-software co-design framework aimed at optimizing Large Language Model (LLM) inference through Weight and Activation Quantization (WAQ) with K-Means quantization. Traditional WAQ designs face challenges like accuracy degradation at low precision and activation outliers. KLLM addresses these by implementing an index-based computation scheme for efficient execution, reducing dequantization and full-precision computations. It also includes a novel outlier detection engine, Orizuru, for efficient identification of outlier elements during online inference. Experimental results demonstrate substantial speedups and energy efficiency improvements compared to existing GPU and Atom systems, showcasing the effectiveness of the KLLM framework in enhancing LLM inference performance. <br /> <div>
arXiv:2507.23035v1 Announce Type: new 
Abstract: Large language model (LLM) inference poses significant challenges due to its intensive memory and computation demands. Weight and activation quantization (WAQ) offers a promising solution by reducing both memory footprint and arithmetic complexity. However, two key challenges remain in the existing WAQ designs. (1) Traditional WAQ designs rely on uniform integer-based quantization for hardware efficiency, but this often results in significant accuracy degradation at low precision. K-Means-based quantization, a non-uniform quantization technique, achieves higher accuracy by matching the Gaussian-like distributions of weights and activations in LLMs. However, its non-uniform nature prevents direct execution on low-precision compute units, requiring dequantization and floating-point matrix multiplications (MatMuls) during inference. (2) Activation outliers further hinder effective low-precision WAQ. Offline thresholding methods for outlier detection can lead to significant model performance degradation, while existing online detection techniques introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of WAQ with K-Means quantization for LLM inference, in this paper, we propose KLLM, a hardware-software co-design framework. KLLM features an index-based computation scheme for efficient execution of MatMuls and nonlinear operations on K-Means-quantized data, which avoids most of the dequantization and full-precision computations. Moreover, KLLM incorporates a novel outlier detection engine, Orizuru, that efficiently identifies the top-$k$ largest and smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x, 7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the A100 GPU and Atom, respectively.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Actor Behavior to Process Performance Over Time</title>
<link>https://arxiv.org/abs/2507.23037</link>
<guid>https://arxiv.org/abs/2507.23037</guid>
<content:encoded><![CDATA[
<div> actor behavior, process mining, Granger causality, time series data, throughput time

Summary:
This study focuses on the relationship between actor behavior and process outcomes in process mining. Traditional approaches often overlook the temporal and causal dynamics of individual actor behavior, limiting the understanding of real-world processes. By integrating actor behavior analysis with Granger causality, the study identifies correlating links in time series data. Using Group Lasso for lag selection, a small set of lags that capture causal influence on process performance, particularly throughput time, are identified. The findings suggest that actor behavior has direct and measurable impacts on process efficiency. This actor-centric, time series-based approach offers a more nuanced understanding of how individual behaviors influence overall process outcomes. <div>
arXiv:2507.23037v1 Announce Type: new 
Abstract: Understanding how actor behavior influences process outcomes is a critical aspect of process mining. Traditional approaches often use aggregate and static process data, overlooking the temporal and causal dynamics that arise from individual actor behavior. This limits the ability to accurately capture the complexity of real-world processes, where individual actor behavior and interactions between actors significantly shape performance. In this work, we address this gap by integrating actor behavior analysis with Granger causality to identify correlating links in time series data. We apply this approach to realworld event logs, constructing time series for actor interactions, i.e. continuation, interruption, and handovers, and process outcomes. Using Group Lasso for lag selection, we identify a small but consistently influential set of lags that capture the majority of causal influence, revealing that actor behavior has direct and measurable impacts on process performance, particularly throughput time. These findings demonstrate the potential of actor-centric, time series-based methods for uncovering the temporal dependencies that drive process outcomes, offering a more nuanced understanding of how individual behaviors impact overall process efficiency.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost</title>
<link>https://arxiv.org/abs/2507.23043</link>
<guid>https://arxiv.org/abs/2507.23043</guid>
<content:encoded><![CDATA[
<div> phosphate, total bilirubin, magnesium, Charlson index, APSIII
Summary: 
- A machine learning model was developed to predict vancomycin-related creatinine elevation in ICU patients.
- The model analyzed data from 10,288 ICU patients who received vancomycin and had a kidney injury defined by KDIGO criteria.
- CatBoost algorithm performed the best with an AUROC of 0.818, sensitivity of 0.800, specificity of 0.681, and negative predictive value of 0.900.
- Key predictors included phosphate, total bilirubin, magnesium, Charlson index, and APSIII.
- SHAP analysis confirmed phosphate as a major risk factor, while ALE showed dose-response patterns.
- Bayesian analysis estimated a mean risk of 60.5% in high-risk cases.
Summary: <div>
arXiv:2507.23043v1 Announce Type: new 
Abstract: Background: Vancomycin, a key antibiotic for severe Gram-positive infections in ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in critically ill patients is challenging. This study aimed to develop a machine learning model to predict vancomycin-related creatinine elevation using routine ICU data.
  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV database who received vancomycin. Kidney injury was defined by KDIGO criteria (creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were selected via SelectKBest (top 30) and Random Forest ranking (final 15). Six algorithms were tested with 5-fold cross-validation. Interpretability was evaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior sampling.
  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation. CatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800, specificity 0.681, negative predictive value 0.900). Key predictors were phosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP confirmed phosphate as a major risk factor. ALE showed dose-response patterns. Bayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%) in high-risk cases.
  Conclusions: This machine learning model predicts vancomycin-associated creatinine elevation from routine ICU data with strong accuracy and interpretability, enabling early risk detection and supporting timely interventions in critical care.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Differentially Private Thresholding Bandits</title>
<link>https://arxiv.org/abs/2507.23073</link>
<guid>https://arxiv.org/abs/2507.23073</guid>
<content:encoded><![CDATA[
<div> differential privacy, bandit problem, thresholding, private responses, decision-making

Summary:
This study examines the effects of implementing local differential privacy in the thresholding bandit problem, considering fixed budget and fixed confidence scenarios. The proposed methods use private responses from a Bernoulli-based differentially private mechanism to identify arms with expected rewards surpassing a set threshold, ensuring strong privacy guarantees. Theoretical performance bounds for the algorithms are derived, along with general lower bounds on the additional loss incurred by any differentially private mechanism. The algorithms presented in the study match these lower bounds up to poly-logarithmic factors. The results offer valuable insights for privacy-preserving decision-making frameworks in bandit problems. 

<br /><br />Summary: <div>
arXiv:2507.23073v1 Announce Type: new 
Abstract: This work investigates the impact of ensuring local differential privacy in the thresholding bandit problem. We consider both the fixed budget and fixed confidence settings. We propose methods that utilize private responses, obtained through a Bernoulli-based differentially private mechanism, to identify arms with expected rewards exceeding a predefined threshold. We show that this procedure provides strong privacy guarantees and derive theoretical performance bounds on the proposed algorithms. Additionally, we present general lower bounds that characterize the additional loss incurred by any differentially private mechanism, and show that the presented algorithms match these lower bounds up to poly-logarithmic factors. Our results provide valuable insights into privacy-preserving decision-making frameworks in bandit problems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Material Fracture Prediction</title>
<link>https://arxiv.org/abs/2507.23077</link>
<guid>https://arxiv.org/abs/2507.23077</guid>
<content:encoded><![CDATA[
<div> fracture prediction, materials, machine learning, simulation, transformer-based architecture

Summary:<br />
- Accurate prediction of material failure is essential for designing safe structures under stress.
- Current fracture modeling methods struggle to generalize across different materials and loading conditions.
- A new data-driven model using a transformer-based architecture is introduced for fracture prediction.
- The model can handle various materials and loading conditions, supporting structured and unstructured meshes.
- The model requires minimal data for training and can adapt to new materials with just a single sample, reducing data needs significantly compared to traditional ML models. This offers a scalable alternative to simulator-specific workflows. 

Summary: <br />
Accurately predicting material failure is crucial for safe design under stress, but current models have limitations. A new data-driven model using a transformer-based architecture addresses these issues, offering scalability and adaptability across different materials and loading conditions. This model requires minimal data for training and can generalize to unseen materials with minimal samples, providing a promising alternative to traditional fracture prediction methods. <div>
arXiv:2507.23077v1 Announce Type: new 
Abstract: Accurately predicting when and how materials fail is critical to designing safe, reliable structures, mechanical systems, and engineered components that operate under stress. Yet, fracture behavior remains difficult to model across the diversity of materials, geometries, and loading conditions in real-world applications. While machine learning (ML) methods show promise, most models are trained on narrow datasets, lack robustness, and struggle to generalize. Meanwhile, physics-based simulators offer high-fidelity predictions but are fragmented across specialized methods and require substantial high-performance computing resources to explore the input space. To address these limitations, we present a data-driven foundation model for fracture prediction, a transformer-based architecture that operates across simulators, a wide range of materials (including plastic-bonded explosives, steel, aluminum, shale, and tungsten), and diverse loading conditions. The model supports both structured and unstructured meshes, combining them with large language model embeddings of textual input decks specifying material properties, boundary conditions, and solver settings. This multimodal input design enables flexible adaptation across simulation scenarios without changes to the model architecture. The trained model can be fine-tuned with minimal data on diverse downstream tasks, including time-to-failure estimation, modeling fracture evolution, and adapting to combined finite-discrete element method simulations. It also generalizes to unseen materials such as titanium and concrete, requiring as few as a single sample, dramatically reducing data needs compared to standard ML. Our results show that fracture prediction can be unified under a single model architecture, offering a scalable, extensible alternative to simulator-specific workflows.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Sustainability of AI Inferences in the Edge</title>
<link>https://arxiv.org/abs/2507.23093</link>
<guid>https://arxiv.org/abs/2507.23093</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, AI-enabled applications, edge devices, performance, energy usage

Summary:
This study investigates the performance and energy usage of edge devices such as Raspberry Pi, Intel Neural Compute Stick, NVIDIA Jetson nano, and Google Coral USB in supporting AI inferences for IoT applications. The research focuses on trade-offs between model F1 score, inference time, power consumption, and memory usage. By analyzing traditional neural networks and large language models, the study aims to inform decision-making on device and model selection for efficient edge AI deployments. Hardware and framework optimization, along with external parameter tuning of AI models, are essential in balancing model performance with resource usage. This analysis provides valuable insights for achieving practical and sustainable edge AI implementations. 

Summary: <br /><br />This study examines the performance and energy usage of edge devices like Raspberry Pi and NVIDIA Jetson nano in supporting AI inferences for IoT applications. It analyzes trade-offs between model F1 score, inference time, power consumption, and memory usage to inform decision-making on device and model selection for efficient edge AI deployments. Hardware and framework optimization, along with external parameter tuning, are crucial for balancing model performance with resource usage. The research offers valuable insights for achieving practical and sustainable edge AI implementations. <div>
arXiv:2507.23093v1 Announce Type: new 
Abstract: The proliferation of the Internet of Things (IoT) and its cutting-edge AI-enabled applications (e.g., autonomous vehicles and smart industries) combine two paradigms: data-driven systems and their deployment on the edge. Usually, edge devices perform inferences to support latency-critical applications. In addition to the performance of these resource-constrained edge devices, their energy usage is a critical factor in adopting and deploying edge applications. Examples of such devices include Raspberry Pi (RPi), Intel Neural Compute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU). Despite their adoption in edge deployment for AI inferences, there is no study on their performance and energy usage for informed decision-making on the device and model selection to meet the demands of applications. This study fills the gap by rigorously characterizing the performance of traditional, neural networks, and large language models on the above-edge devices. Specifically, we analyze trade-offs among model F1 score, inference time, inference power, and memory usage. Hardware and framework optimization, along with external parameter tuning of AI models, can balance between model performance and resource usage to realize practical edge AI deployments.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Generative Modeling of Weighted Graphs</title>
<link>https://arxiv.org/abs/2507.23111</link>
<guid>https://arxiv.org/abs/2507.23111</guid>
<content:encoded><![CDATA[
<div> Graphs, Weighted, Generative models, Deep neural networks, BiGG-E <br />
Summary: BiGG-E is a novel autoregressive model designed to learn a joint distribution over weighted graphs, addressing the limitations of current generative models. It is capable of generating weighted graphs with efficiency, taking into account complex dependencies between edges and their weights. The model extends the BiGG framework and efficiently generates graphs with both topology and edge weights. By leveraging sparsity, BiGG-E can generate weighted graphs with n nodes and m edges in O((n + m)log n) time. Simulation studies and experiments on benchmark datasets show that BiGG-E outperforms existing models in capturing distributions over weighted graphs while remaining scalable and computationally efficient. The development of BiGG-E represents a significant advancement in the field of generative modeling for weighted graphs. <br /> <div>
arXiv:2507.23111v1 Announce Type: new 
Abstract: Weighted graphs are ubiquitous throughout biology, chemistry, and the social sciences, motivating the development of generative models for abstract weighted graph data using deep neural networks. However, most current deep generative models are either designed for unweighted graphs and are not easily extended to weighted topologies or incorporate edge weights without consideration of a joint distribution with topology. Furthermore, learning a distribution over weighted graphs must account for complex nonlocal dependencies between both the edges of the graph and corresponding weights of each edge. We develop an autoregressive model BiGG-E, a nontrivial extension of the BiGG model, that learns a joint distribution over weighted graphs while still exploiting sparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n + m)\log n)$ time. Simulation studies and experiments on a variety of benchmark datasets demonstrate that BiGG-E best captures distributions over weighted graphs while remaining scalable and computationally efficient.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLOSS: Federated Learning with Opt-Out and Straggler Support</title>
<link>https://arxiv.org/abs/2507.23115</link>
<guid>https://arxiv.org/abs/2507.23115</guid>
<content:encoded><![CDATA[
<div> Keywords: data privacy, federated learning, stragglers, user opt-out, missing data

Summary:
FLOSS is introduced as a system designed to address the challenges of missing data in federated learning systems, specifically in the presence of stragglers and user opt-out scenarios. The system aims to mitigate bias and improve model performance caused by missing data from various sources. While previous research on data privacy in federated learning has focused on preserving privacy for shared data, FLOSS considers the impact of users choosing to opt out of data sharing. By addressing the issue of missing data and stragglers, FLOSS provides a solution to improve the overall performance of federated learning systems. Empirical simulations demonstrated the effectiveness of FLOSS in handling missing data and mitigating bias in model training. Overall, FLOSS offers a novel approach to enhancing data privacy and model performance in federated learning settings. 

<br /><br />Summary: <div>
arXiv:2507.23115v1 Announce Type: new 
Abstract: Previous work on data privacy in federated learning systems focuses on privacy-preserving operations for data from users who have agreed to share their data for training. However, modern data privacy agreements also empower users to use the system while opting out of sharing their data as desired. When combined with stragglers that arise from heterogeneous device capabilities, the result is missing data from a variety of sources that introduces bias and degrades model performance. In this paper, we present FLOSS, a system that mitigates the impacts of such missing data on federated learning in the presence of stragglers and user opt-out, and empirically demonstrate its performance in simulations.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts</title>
<link>https://arxiv.org/abs/2507.23128</link>
<guid>https://arxiv.org/abs/2507.23128</guid>
<content:encoded><![CDATA[
<div> audio-based models, keyword classifiers, neural architectures, noise-aware training, generalization

Summary:
The study investigates the impact of training conditions and input features on the robustness and generalization abilities of spoken keyword classifiers in out-of-distribution (OOD) conditions. Various neural architectures are benchmarked using evaluation sets, with the effect of noise on generalization quantified using Fairness (F) and Robustness (R) metrics. The results indicate that noise-aware training can improve robustness in certain configurations, highlighting the importance of noise-based augmentation for generalization in speech models.<br /><br />Summary: <div>
arXiv:2507.23128v1 Announce Type: new 
Abstract: Although prior work in computer vision has shown strong correlations between in-distribution (ID) and out-of-distribution (OOD) accuracies, such relationships remain underexplored in audio-based models. In this study, we investigate how training conditions and input features affect the robustness and generalization abilities of spoken keyword classifiers under OOD conditions. We benchmark several neural architectures across a variety of evaluation sets. To quantify the impact of noise on generalization, we make use of two metrics: Fairness (F), which measures overall accuracy gains compared to a baseline model, and Robustness (R), which assesses the convergence between ID and OOD performance. Our results suggest that noise-aware training improves robustness in some configurations. These findings shed new light on the benefits and limitations of noise-based augmentation for generalization in speech models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observational Multiplicity</title>
<link>https://arxiv.org/abs/2507.23136</link>
<guid>https://arxiv.org/abs/2507.23136</guid>
<content:encoded><![CDATA[
<div> observational multiplicity, probabilistic classification tasks, regret, safety, data collection
Summary: 
The article discusses the phenomenon of observational multiplicity in probabilistic classification tasks, where multiple models can perform equally well, leading to conflicting predictions. To address this, the concept of regret is introduced as a measure of how predictions could change with different training labels. The authors propose a method to estimate regret in probabilistic classification tasks, showing that regret can be higher for certain groups in the dataset. The concept of regret can be used to promote safety in real-world applications by encouraging abstention and data collection. This approach highlights the importance of evaluating the arbitrariness of individual probability predictions and provides insights into improving interpretability and safety in machine learning models. 
Summary:  <div>
arXiv:2507.23136v1 Announce Type: new 
Abstract: Many prediction tasks can admit multiple models that can perform almost equally well. This phenomenon can can undermine interpretability and safety when competing models assign conflicting predictions to individuals. In this work, we study how arbitrariness can arise in probabilistic classification tasks as a result of an effect that we call \emph{observational multiplicity}. We discuss how this effect arises in a broad class of practical applications where we learn a classifier to predict probabilities $p_i \in [0,1]$ but are given a dataset of observations $y_i \in \{0,1\}$. We propose to evaluate the arbitrariness of individual probability predictions through the lens of \emph{regret}. We introduce a measure of regret for probabilistic classification tasks, which measures how the predictions of a model could change as a result of different training labels change. We present a general-purpose method to estimate the regret in a probabilistic classification task. We use our measure to show that regret is higher for certain groups in the dataset and discuss potential applications of regret. We demonstrate how estimating regret promote safety in real-world applications by abstention and data collection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver</title>
<link>https://arxiv.org/abs/2507.23141</link>
<guid>https://arxiv.org/abs/2507.23141</guid>
<content:encoded><![CDATA[
<div> Keywords: differential equations, artificial intelligence, high frequency components, scale-dilation operator, Transformer AI solver

Summary:<br /><br />
This article introduces a new paradigm for solving differential equations (DEs) using artificial intelligence (AI). Traditional AI solvers struggle with high frequency components in DEs due to limited data availability. The proposed methodology involves generating first-principles-consistent training datasets by deriving sources and initial/boundary conditions from solutions. A scale-dilation operator (SDO) is used to handle high frequency components by leveraging the Fourier transform of multiscale solutions. An attention-based Transformer AI solver is designed to solve DEs efficiently. The study proves that the SDO leads to a smoother loss landscape, improving training efficiency. Extensive testing shows that this AI paradigm outperforms existing methods in terms of accuracy. This approach makes AI solver of DEs applicable in various fields of nature and engineering. <div>
arXiv:2507.23141v1 Announce Type: new 
Abstract: Many problems are governed by differential equations (DEs). Artificial intelligence (AI) is a new path for solving DEs. However, data is very scarce and existing AI solvers struggle with approximation of high frequency components (AHFC). We propose an AI paradigm for solving diverse DEs, including DE-ruled first-principles data generation methodology and scale-dilation operator (SDO) AI solver. Using either prior knowledge or random fields, we generate solutions and then substitute them into the DEs to derive the sources and initial/boundary conditions through balancing DEs, thus producing arbitrarily vast amount of, first-principles-consistent training datasets at extremely low computational cost. We introduce a reversible SDO that leverages the Fourier transform of the multiscale solutions to fix AHFC, and design a spatiotemporally coupled, attention-based Transformer AI solver of DEs with SDO. An upper bound on the Hessian condition number of the loss function is proven to be proportional to the squared 2-norm of the solution gradient, revealing that SDO yields a smoother loss landscape, consequently fixing AHFC with efficient training. Extensive tests on diverse DEs demonstrate that our AI paradigm achieves consistently superior accuracy over state-of-the-art methods. This work makes AI solver of DEs to be truly usable in broad nature and engineering fields.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations</title>
<link>https://arxiv.org/abs/2507.23154</link>
<guid>https://arxiv.org/abs/2507.23154</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban heatwaves, droughts, land degradation, Land Surface Temperature, spatio-temporal information

Summary: 
FuseTen is a novel generative framework designed to produce daily Land Surface Temperature (LST) observations at a fine 10 m spatial resolution by combining data from Sentinel-2, Landsat 8, and Terra MODIS satellites. The framework uses a generative architecture trained with an averaging-based strategy and incorporates attention and normalization modules for enhanced accuracy. It also utilizes a PatchGAN discriminator to ensure realistic results. Experimental results demonstrate that FuseTen surpasses linear baselines, with significant improvements in both quantitative metrics and visual fidelity. This innovative approach marks the first non-linear method capable of generating daily LST estimates at such a high spatial resolution. The framework addresses the critical need for accurate spatio-temporal information on land surface conditions to better understand and assess challenges such as urban heatwaves, droughts, and land degradation in the context of climate change.<br /><br />Summary: <div>
arXiv:2507.23154v1 Announce Type: new 
Abstract: Urban heatwaves, droughts, and land degradation are pressing and growing challenges in the context of climate change. A valuable approach to studying them requires accurate spatio-temporal information on land surface conditions. One of the most important variables for assessing and understanding these phenomena is Land Surface Temperature (LST), which is derived from satellites and provides essential information about the thermal state of the Earth's surface. However, satellite platforms inherently face a trade-off between spatial and temporal resolutions. To bridge this gap, we propose FuseTen, a novel generative framework that produces daily LST observations at a fine 10 m spatial resolution by fusing spatio-temporal observations derived from Sentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative architecture trained using an averaging-based supervision strategy grounded in physical principles. It incorporates attention and normalization modules within the fusion process and uses a PatchGAN discriminator to enforce realism. Experiments across multiple dates show that FuseTen outperforms linear baselines, with an average 32.06% improvement in quantitative metrics and 31.42% in visual fidelity. To the best of our knowledge, this is the first non-linear method to generate daily LST estimates at such fine spatial resolution.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning</title>
<link>https://arxiv.org/abs/2507.23170</link>
<guid>https://arxiv.org/abs/2507.23170</guid>
<content:encoded><![CDATA[
<div> trade-off, LLM services, inference-time budget, factual authenticity, reasoning capacity
Summary:
The article discusses the trade-off in designing Language Model (LLM) services, focusing on three essential properties: inference-time budget, factual authenticity, and reasoning capacity. Through analysis, it is concluded that no model can optimize all three properties simultaneously. This trade-off is formally proven, leading to the proposal of a framework called The BAR Theorem for designing LLM applications. The framework aims to provide a principled approach in balancing these properties to meet the requirements of practitioners in the field. The notion of trade-offs in LLM design is highlighted, emphasizing the need for careful consideration and strategic decision-making to achieve the desired outcomes. <div>
arXiv:2507.23170v1 Announce Type: new 
Abstract: When designing LLM services, practitioners care about three key properties: inference-time budget, factual authenticity, and reasoning capacity. However, our analysis shows that no model can simultaneously optimize for all three. We formally prove this trade-off and propose a principled framework named The BAR Theorem for LLM-application design.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions</title>
<link>https://arxiv.org/abs/2507.23186</link>
<guid>https://arxiv.org/abs/2507.23186</guid>
<content:encoded><![CDATA[
<div> NaN-propagation, black-box functions, sparsity detection, gradient-based optimization, IEEE 754 compliance<br />
<br />
Summary: <br />
Sparsity detection in black-box functions for gradient-based optimization faces issues with false negatives due to coincidental zero gradients. NaN-propagation leverages IEEE 754 Not-a-Number values to trace dependencies in numerical computations, effectively eliminating false negatives and detecting missed dependencies. By contaminating inputs with NaN, the method reconstructs accurate sparsity patterns to enhance computational speedups. The approach, demonstrated on an aerospace wing weight model, achieves a 1.52x speedup and improves upon existing black-box sparsity detection methods with faster-than-linear time complexity. The technique is versatile across programming languages and math libraries, making it applicable without modifying existing codes. Advanced strategies, including NaN payload encoding, further enhance the efficiency of sparsity detection, offering significant improvements in optimization workflows. Additionally, practical algorithms are proposed to address challenges from branching code execution in engineering applications. <div>
arXiv:2507.23186v1 Announce Type: new 
Abstract: Sparsity detection in black-box functions enables significant computational speedups in gradient-based optimization through Jacobian compression, but existing finite-difference methods suffer from false negatives due to coincidental zero gradients. These false negatives can silently corrupt gradient calculations, leading to difficult-to-diagnose errors. We introduce NaN-propagation, which exploits the universal contamination property of IEEE 754 Not-a-Number floating-point values to trace input-output dependencies through floating-point numerical computations. By systematically contaminating inputs with NaN and observing which outputs become NaN, the method reconstructs conservative sparsity patterns that eliminate false negatives. We demonstrate the approach on an aerospace wing weight model, achieving a 1.52x speedup while detecting dozens of dependencies missed by conventional methods -- a significant improvement since gradient computation is the bottleneck in many optimization workflows. The technique leverages IEEE 754 compliance to work across programming languages and math libraries without modifying existing black-box codes. Advanced strategies including NaN payload encoding enable faster-than-linear time complexity, improving upon existing black-box sparsity detection methods. Practical algorithms are also proposed to mitigate challenges from branching code execution common in engineering applications.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23217</link>
<guid>https://arxiv.org/abs/2507.23217</guid>
<content:encoded><![CDATA[
<div> pseudo Table of Contents, hierarchical Retrieval-Augmented Generation, multimodal documents, large language models, document understanding<br />
Summary:<br />
- DocsRay is a document understanding system that does not require training and can process complex multimodal documents.
- It combines prompt-based interactions with Large Language Models to generate a hierarchical Table of Contents and analyze diverse document elements.
- DocsRay's retrieval system reduces complexity and improves query latency efficiency.
- Evaluated on lengthy documents, DocsRay reduced query latency by 45% and achieved high accuracy on benchmark tests.
- DocsRay-Pro surpassed previous state-of-the-art results in document understanding accuracy. 
<br />Summary: <div>
arXiv:2507.23217v1 Announce Type: new 
Abstract: Understanding complex multimodal documents remains challenging due to their structural inconsistencies and limited training data availability. We introduce \textit{DocsRay}, a training-free document understanding system that integrates pseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented Generation (RAG). Our approach leverages multimodal Large Language Models' (LLMs) native capabilities to seamlessly process documents containing diverse elements such as text, images, charts, and tables without requiring specialized models or additional training. DocsRay's framework synergistically combines three key techniques: (1) a semantic structuring module using prompt-based LLM interactions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal analysis that converts diverse document elements into unified, text-centric representations using the inherent capabilities of multimodal LLMs, and (3) an efficient two-stage hierarchical retrieval system that reduces retrieval complexity from $O(N)$ to $O(S + k_1 \cdot N_s)$. Evaluated on documents averaging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency from 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the MMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%, substantially surpassing previous state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations</title>
<link>https://arxiv.org/abs/2507.23221</link>
<guid>https://arxiv.org/abs/2507.23221</guid>
<content:encoded><![CDATA[
<div> Keywords: Contextual hallucinations, AI interpretability, linear probe, MLP sub-circuits, ContraTales benchmark

Summary: 
An AI study addresses the challenge of contextual hallucinations in text generation. The research introduces a generator-agnostic observer model that can detect hallucinations through a single forward pass and a linear probe on its residual stream. This probe isolates a transferable linear direction that distinguishes hallucinated text from accurate text, outperforming existing methods. The study identifies specific MLP sub-circuits associated with hallucination tracking using gradient-times-activation. It demonstrates that manipulation of this direction can causally control generator hallucination rates, highlighting its practical utility. The findings provide new insights into internal, low-dimensional hallucination tracking mechanisms in AI models and offer a benchmark called ContraTales for evaluating detection and mitigation strategies for hallucinations in text generation models. <div>
arXiv:2507.23221v1 Announce Type: new 
Abstract: Contextual hallucinations -- statements unsupported by given context -- remain a significant challenge in AI. We demonstrate a practical interpretability insight: a generator-agnostic observer model detects hallucinations via a single forward pass and a linear probe on its residual stream. This probe isolates a single, transferable linear direction separating hallucinated from faithful text, outperforming baselines by 5-27 points and showing robust mid-layer performance across Gemma-2 models (2B to 27B). Gradient-times-activation localises this signal to sparse, late-layer MLP activity. Critically, manipulating this direction causally steers generator hallucination rates, proving its actionability. Our results offer novel evidence of internal, low-dimensional hallucination tracking linked to specific MLP sub-circuits, exploitable for detection and mitigation. We release the 2000-example ContraTales benchmark for realistic assessment of such solutions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Machine Unlearning via Influence Approximation</title>
<link>https://arxiv.org/abs/2507.23257</link>
<guid>https://arxiv.org/abs/2507.23257</guid>
<content:encoded><![CDATA[
<div> machine unlearning, influence-based unlearning, forgetting, memorizing, incremental learning

Summary:
The paper introduces the Influence Approximation Unlearning (IAU) algorithm, aiming to address the issue of efficient machine unlearning by establishing a theoretical connection between forgetting and memorizing in the context of incremental learning. While existing influence-based unlearning methods suffer from computational overhead, IAU leverages gradient optimization techniques to achieve efficient unlearning without the need for time-consuming Hessian computations. The algorithm demonstrates superior removal guarantee, efficiency, and model utility compared to state-of-the-art methods across various datasets and model architectures. The theoretical link between forgetting and memorizing from cognitive science provides a foundation for IAU's approach, highlighting the potential of incremental learning in machine unlearning tasks. <div>
arXiv:2507.23257v1 Announce Type: new 
Abstract: Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System</title>
<link>https://arxiv.org/abs/2507.23261</link>
<guid>https://arxiv.org/abs/2507.23261</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent systems, DynaSwarm, reinforcement learning, dynamic graph structures, in-context learning

Summary:
DynaSwarm is a dynamic framework for enhancing multi-agent systems by optimizing graph structures using an actor-critic reinforcement learning mechanism. It also incorporates a dynamic graph selector to choose the best structure for each input sample through fine-tuning. This approach allows for the adaptation of graph architectures based on sample-specific characteristics, leading to improved performance and adaptability. The framework eliminates the need for rigid structures and leverages in-context learning to optimize agent networks. Extensive experiments on various tasks show that DynaSwarm outperforms existing baselines, emphasizing the importance of sample-aware structural flexibility in multi-agent system designs.<br /><br />Summary: <div>
arXiv:2507.23261v1 Announce Type: new 
Abstract: Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance. To address these limitations, we propose DynaSwarm, a dynamic framework that enhances LLM-based MAS through two key innovations: (1) an actor-critic reinforcement learning (A2C) mechanism to optimize graph structures with improved stability over prior RL methods, and (2) a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. (c) We propose to fine-tune the demonstration retriever to fully exploit the power of in-context learning (ICL). Extensive experiments on question answering, mathematical reasoning, and coding tasks demonstrate that DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones. Our findings highlight the importance of sample-aware structural flexibility in LLM MAS designs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Dynamics of Membership Privacy in Deep Learning</title>
<link>https://arxiv.org/abs/2507.23291</link>
<guid>https://arxiv.org/abs/2507.23291</guid>
<content:encoded><![CDATA[
<div> privacy, deep learning, membership inference attacks, training data, analytical framework

Summary:
This paper introduces a dynamic analytical framework to analyze and quantify privacy leakage dynamics in deep learning models at the individual sample level. By tracking a sample's vulnerability throughout training on an FPR-TPR plane, the framework assesses how dataset complexity, model architecture, and optimizer choice impact the rate and severity of privacy risks. The study reveals a strong correlation between a sample's intrinsic learning difficulty and its privacy risk in the final trained model, indicating that the vulnerability of samples is primarily determined early in the training process. These findings enhance our understanding of how privacy risks evolve during training and suggest the need for privacy-conscious model training strategies to mitigate membership inference attacks.<hr />Summary: <div>
arXiv:2507.23291v1 Announce Type: new 
Abstract: Membership inference attacks (MIAs) pose a critical threat to the privacy of training data in deep learning. Despite significant progress in attack methodologies, our understanding of when and how models encode membership information during training remains limited. This paper presents a dynamic analytical framework for dissecting and quantifying privacy leakage dynamics at the individual sample level. By tracking per-sample vulnerabilities on an FPR-TPR plane throughout training, our framework systematically measures how factors such as dataset complexity, model architecture, and optimizer choice influence the rate and severity at which samples become vulnerable. Crucially, we discover a robust correlation between a sample's intrinsic learning difficulty, and find that the privacy risk of samples highly vulnerable in the final trained model is largely determined early during training. Our results thus provide a deeper understanding of how privacy risks dynamically emerge during training, laying the groundwork for proactive, privacy-aware model training strategies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy</title>
<link>https://arxiv.org/abs/2507.23292</link>
<guid>https://arxiv.org/abs/2507.23292</guid>
<content:encoded><![CDATA[
<div> API, library, sequence modeling, neural network, state representation

Summary:
The article introduces a new neural network layer API and library for sequence modeling. This API is designed to easily create sequence models that can be executed both layer-by-layer and step-by-step, allowing for flexible training and sampling approaches. The layers in the API define an explicit representation of their state over time and a step method that evolves that state to ensure consistent results. This approach enables complex models to be streamable and reduces common bugs in streaming and parallel sequence processing. The SequenceLayers contract can be implemented in any deep learning library and offers a composable and declarative API with a variety of layers and combinators for building production-scale models. Implementations of SequenceLayers in JAX and TensorFlow 2 are available for use. The goal is to streamline the construction of models using simple, streamable components while maintaining strong correctness guarantees. <br /><br />Summary: <div>
arXiv:2507.23292v1 Announce Type: new 
Abstract: We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items</title>
<link>https://arxiv.org/abs/2507.23303</link>
<guid>https://arxiv.org/abs/2507.23303</guid>
<content:encoded><![CDATA[
<div> Keywords: supermarket, forgotten item prediction, interpretability, Next Basket Prediction, real-world dataset

Summary:
Our study addresses the issue of accurately identifying items that are forgotten during a supermarket visit, a problem that has not been extensively explored in the Next Basket Prediction (NBP) domain. Existing NBP methods primarily focus on predicting future purchases without considering unintentionally omitted items. We introduce the forgotten item prediction task and propose two new algorithms that are designed to be interpretable to provide clear explanations for recommendations. These algorithms outperform current NBP models by 10-15% across various evaluation metrics using real-world retail data. The scarcity of reliable datasets for estimating forgotten items and the reliance on black-box models for NBP have hindered progress in this area. Our research fills a significant gap in the field and demonstrates the importance of interpretability in making recommendations to end-users.<br /><br />Summary: Our study introduces the forgotten item prediction task, proposes interpretable algorithms, outperforms current NBP approaches, addresses the scarcity of real-world datasets, and emphasizes the importance of transparent recommendations. <div>
arXiv:2507.23303v1 Announce Type: new 
Abstract: Accurately identifying items forgotten during a supermarket visit and providing clear, interpretable explanations for recommending them remains an underexplored problem within the Next Basket Prediction (NBP) domain. Existing NBP approaches typically only focus on forecasting future purchases, without explicitly addressing the detection of unintentionally omitted items. This gap is partly due to the scarcity of real-world datasets that allow for the reliable estimation of forgotten items. Furthermore, most current NBP methods rely on black-box models, which lack transparency and limit the ability to justify recommendations to end users. In this paper, we formally introduce the forgotten item prediction task and propose two novel interpretable-by-design algorithms. These methods are tailored to identify forgotten items while offering intuitive, human-understandable explanations. Experiments on a real-world retail dataset show our algorithms outperform state-of-the-art NBP baselines by 10-15% across multiple evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner</title>
<link>https://arxiv.org/abs/2507.23317</link>
<guid>https://arxiv.org/abs/2507.23317</guid>
<content:encoded><![CDATA[
<div> RL, Large reasoning models, Process reward models, Intrinsic signals, Training acceleration  
Summary:  
Large reasoning models optimized with Reinforcement Learning (RL) face challenges due to sparse outcome-only rewards. This study introduces Process Reward Models (PRMs) to enhance RL training for Large Reasoning Models (LRMs). By using intrinsic signals at the thought level, PRMs evaluate steps' correctness and segment them into coherent units, improving credit assignment and reducing reward hacking. A capability-adaptive reward mechanism balances exploration and exploitation based on LRMs' proficiency, enhancing learning efficiency. The new off-policy RL algorithm, TP-GRPO, combines grouped proximal optimization with process-based rewards, achieving higher problem-solving accuracy with fewer training samples than baseline methods in math reasoning tasks. Experiments on LRMs with 1.5B and 7B parameters validate the efficacy of structured process rewards in accelerating LRM optimization. The code is available on GitHub for further exploration.  
<br /><br />Summary: <div>
arXiv:2507.23317v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have recently shown promise in solving complex math problems when optimized with Reinforcement Learning (RL). But conventional approaches rely on outcome-only rewards that provide sparse feedback, resulting in inefficient optimization process. In this work, we investigate the function of process reward models (PRMs) to accelerate the RL training for LRMs. We propose a novel intrinsic signal-driven generative process evaluation mechanism operating at the thought level to address major bottlenecks in RL-based training. Specifically, instead of requiring PRMs to know how to solve problems, our method uses intrinsic signals in solutions to judge stepwise correctness and aggregate contiguous correct/incorrect steps into coherent 'thought' units. This structured, thought-level rewards enable more reliable credit assignment by reducing ambiguity in step segmentation and alleviating reward hacking. We further introduce a capability-adaptive reward mechanism that dynamically balances exploration and exploitation based on the LRM's current proficiency, guiding learning without stifling creative trial-and-error. These innovations are integrated into a new off-policy RL algorithm, TP-GRPO, which extends grouped proximal optimization with process-based rewards and improves training efficiency. Experiments on 1.5B and 7B parameter LRMs demonstrate that our method achieves higher problem-solving accuracy with significantly fewer training samples than outcome-only reward baselines. The results validate that well-structured process rewards can substantially accelerate LRM optimization in math reasoning tasks. Code is available at https://github.com/cs-holder/tp_grpo.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions</title>
<link>https://arxiv.org/abs/2507.23335</link>
<guid>https://arxiv.org/abs/2507.23335</guid>
<content:encoded><![CDATA[
<div> voting, certification, deep learning, patch robustness, adversarial attacks

Summary:
CostCert is a novel approach for patch robustness certification in deep learning systems, specifically targeting adversarial patch attacks. Unlike existing techniques, CostCert does not rely on pairwise comparisons or exhaustive enumeration to certify the true label within the top k predictions. Instead, it considers the attack budget and the smallest additional votes needed to exclude the true label from the top k predictions. Experimental results demonstrate that CostCert outperforms the current state-of-the-art defender PatchGuard, achieving up to 57.3% certified accuracy retention for a patch size of 96, while PatchGuard drops to zero accuracy. CostCert provides a scalable and precise solution for defending against adversarial patch attacks with provable guarantees, enhancing the robustness of deep learning systems against such threats. 

<br /><br />Summary: <div>
arXiv:2507.23335v1 Announce Type: new 
Abstract: Patch robustness certification is an emerging verification approach for defending against adversarial patch attacks with provable guarantees for deep learning systems. Certified recovery techniques guarantee the prediction of the sole true label of a certified sample. However, existing techniques, if applicable to top-k predictions, commonly conduct pairwise comparisons on those votes between labels, failing to certify the sole true label within the top k prediction labels precisely due to the inflation on the number of votes controlled by the attacker (i.e., attack budget); yet enumerating all combinations of vote allocation suffers from the combinatorial explosion problem. We propose CostCert, a novel, scalable, and precise voting-based certified recovery defender. CostCert verifies the true label of a sample within the top k predictions without pairwise comparisons and combinatorial explosion through a novel design: whether the attack budget on the sample is infeasible to cover the smallest total additional votes on top of the votes uncontrollable by the attacker to exclude the true labels from the top k prediction labels. Experiments show that CostCert significantly outperforms the current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in certified accuracy when the patch size is 96, whereas PatchGuard has already dropped to zero.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation</title>
<link>https://arxiv.org/abs/2507.23344</link>
<guid>https://arxiv.org/abs/2507.23344</guid>
<content:encoded><![CDATA[
<div> dynamic pricing, bike-sharing systems, inventory management, agent-based simulation, user demand

Summary:
Dynamic pricing for bike-sharing systems is crucial for managing inventory imbalance caused by varying user demands. A novel approach using a differentiable agent-based simulation was developed to optimize dynamic pricing, considering the diverse backgrounds and probabilistic choices of users. Validation experiments showed significant improvements over conventional methods, achieving higher accuracy with faster convergence. The approach was further validated in a large-scale urban bike-sharing system scenario, demonstrating the ability to induce balanced inventory without manual relocation. The study also highlighted the importance of setting appropriate initial conditions to minimize the cost of discounts for achieving balanced inventory. The findings suggest that the developed dynamic pricing policies can effectively optimize bike-sharing system operations and reduce relocation costs. <br /><br />Summary: <div>
arXiv:2507.23344v1 Announce Type: new 
Abstract: Bike-sharing systems are emerging in various cities as a new ecofriendly transportation system. In these systems, spatiotemporally varying user demands lead to imbalanced inventory at bicycle stations, resulting in additional relocation costs. Therefore, it is essential to manage user demand through optimal dynamic pricing for the system. However, optimal pricing design for such a system is challenging because the system involves users with diverse backgrounds and their probabilistic choices. To address this problem, we develop a differentiable agent-based simulation to rapidly design dynamic pricing in bike-sharing systems, achieving balanced bicycle inventory despite spatiotemporally heterogeneous trips and probabilistic user decisions. We first validate our approach against conventional methods through numerical experiments involving 25 bicycle stations and five time slots, yielding 100 parameters. Compared to the conventional methods, our approach obtains a more accurate solution with a 73% to 78% reduction in loss while achieving more than a 100-fold increase in convergence speed. We further validate our approach on a large-scale urban bike-sharing system scenario involving 289 bicycle stations, resulting in a total of 1156 parameters. Through simulations using the obtained pricing policies, we confirm that these policies can naturally induce balanced inventory without any manual relocation. Additionally, we find that the cost of discounts to induce the balanced inventory can be minimized by setting appropriate initial conditions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Explanation of Concept Drift -- A Truly Actionable Approach</title>
<link>https://arxiv.org/abs/2507.23389</link>
<guid>https://arxiv.org/abs/2507.23389</guid>
<content:encoded><![CDATA[
<div> Keywords: concept drift, industrial manufacturing, critical infrastructure, model failures, causal explanations

Summary: 
Concept drift, the understanding of critical changes in systems, is crucial in the ever-evolving world. This work focuses on extending model-based drift explanations to causal explanations to enhance the actionability of provided insights. By isolating causally relevant features impacted by concept drift, targeted interventions can be implemented to prevent or correct model failures in industrial manufacturing and critical infrastructure. The framework developed in this study allows for practical applications in various use cases, showcasing its usefulness in identifying and addressing the root causes of system malfunctions and errors. By providing causal explanations for concept drift, this work enables a deeper understanding of system changes and facilitates proactive decision-making to mitigate negative impacts on operational performance. <div>
arXiv:2507.23389v1 Announce Type: new 
Abstract: In a world that constantly changes, it is crucial to understand how those changes impact different systems, such as industrial manufacturing or critical infrastructure. Explaining critical changes, referred to as concept drift in the field of machine learning, is the first step towards enabling targeted interventions to avoid or correct model failures, as well as malfunctions and errors in the physical world. Therefore, in this work, we extend model-based drift explanations towards causal explanations, which increases the actionability of the provided explanations. We evaluate our explanation strategy on a number of use cases, demonstrating the practical usefulness of our framework, which isolates the causally relevant features impacted by concept drift and, thus, allows for targeted intervention.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Learning from Large Vision-Language Model Feedback without Reward Modeling</title>
<link>https://arxiv.org/abs/2507.23391</link>
<guid>https://arxiv.org/abs/2507.23391</guid>
<content:encoded><![CDATA[
<div> Offline reinforcement learning, robotic agents, pre-collected datasets, safety-critical applications, PLARE <br />
Summary: 
PLARE is a new approach for offline reinforcement learning that utilizes large vision-language models to provide guidance signals for agent training. Unlike traditional methods, PLARE does not require manually designed reward functions, instead querying a VLM for preference labels on visual trajectory segments based on language task descriptions. The policy is trained directly from these preference labels using a supervised contrastive preference learning objective, eliminating the need for explicit reward models. Extensive experiments on robotic manipulation tasks show that PLARE achieves performance comparable to existing VLM-based methods, and real-world tasks with a physical robot further validate its practicality. <div>
arXiv:2507.23391v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) provides a powerful framework for training robotic agents using pre-collected, suboptimal datasets, eliminating the need for costly, time-consuming, and potentially hazardous online interactions. This is particularly useful in safety-critical real-world applications, where online data collection is expensive and impractical. However, existing offline RL algorithms typically require reward labeled data, which introduces an additional bottleneck: reward function design is itself costly, labor-intensive, and requires significant domain expertise. In this paper, we introduce PLARE, a novel approach that leverages large vision-language models (VLMs) to provide guidance signals for agent training. Instead of relying on manually designed reward functions, PLARE queries a VLM for preference labels on pairs of visual trajectory segments based on a language task description. The policy is then trained directly from these preference labels using a supervised contrastive preference learning objective, bypassing the need to learn explicit reward models. Through extensive experiments on robotic manipulation tasks from the MetaWorld, PLARE achieves performance on par with or surpassing existing state-of-the-art VLM-based reward generation methods. Furthermore, we demonstrate the effectiveness of PLARE in real-world manipulation tasks with a physical robot, further validating its practical applicability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles</title>
<link>https://arxiv.org/abs/2507.23412</link>
<guid>https://arxiv.org/abs/2507.23412</guid>
<content:encoded><![CDATA[
<div> ML-based system, honey adulteration, mineral element profiles, classification, random forest

Summary:
This paper presents a Machine Learning (ML) system for detecting honey adulteration using mineral element profiles. The system consists of preprocessing and classification phases. The preprocessing phase handles missing values and normalization. Three supervised ML models  logistic regression, decision tree, and random forest  are employed in the classification phase to distinguish between authentic and adulterated honey. The study utilizes a public dataset containing mineral element measurements of authentic honey, sugar syrups, and adulterated honey to assess model performance. Results indicate that mineral element content is a reliable indicator of honey adulteration, with the random forest classifier achieving the highest cross-validation accuracy of 98.37%. <div>
arXiv:2507.23412v1 Announce Type: new 
Abstract: This paper aims to develop a Machine Learning (ML)-based system for detecting honey adulteration utilizing honey mineral element profiles. The proposed system comprises two phases: preprocessing and classification. The preprocessing phase involves the treatment of missing-value attributes and normalization. In the classifica-tion phase, we use three supervised ML models: logistic regression, decision tree, and random forest, to dis-criminate between authentic and adulterated honey. To evaluate the performance of the ML models, we use a public dataset comprising measurements of mineral element content of authentic honey, sugar syrups, and adul-terated honey. Experimental findings show that mineral element content in honey provides robust discriminative information for detecting honey adulteration. Results also demonstrate that the random forest-based classifier outperforms other classifiers on this dataset, achieving the highest cross-validation accuracy of 98.37%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning</title>
<link>https://arxiv.org/abs/2507.23418</link>
<guid>https://arxiv.org/abs/2507.23418</guid>
<content:encoded><![CDATA[
<div> machine learning, coconut milk, infrared spectroscopy, adulteration, detection

Summary:<br />
This paper introduces a system for detecting adulteration in coconut milk using infrared spectroscopy. The system employs a machine learning approach with three phases: preprocessing, feature extraction, and classification. In the preprocessing phase, irrelevant data is removed from the spectral signals of coconut milk. The Linear Discriminant Analysis (LDA) algorithm is then used in the feature extraction phase to extract the most discriminative features. In the classification phase, the K-Nearest Neighbor (KNN) model is utilized to classify coconut milk samples as authentic or adulterated. The system's performance is evaluated using a dataset containing Fourier Transform Infrared (FTIR) spectral data of pure and contaminated coconut milk samples, achieving a cross-validation accuracy of 93.33%. 

<br /><br />Summary: <div>
arXiv:2507.23418v1 Announce Type: new 
Abstract: In this paper, we propose a system for detecting adulteration in coconut milk, utilizing infrared spectroscopy. The machine learning-based proposed system comprises three phases: preprocessing, feature extraction, and classification. The first phase involves removing irrelevant data from coconut milk spectral signals. In the second phase, we employ the Linear Discriminant Analysis (LDA) algorithm for extracting the most discriminating features. In the third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut milk samples into authentic or adulterated. We evaluate the performance of the proposed system using a public dataset comprising Fourier Transform Infrared (FTIR) spectral information of pure and contaminated coconut milk samples. Findings show that the proposed method successfully detects adulteration with a cross-validation accuracy of 93.33%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merging Memory and Space: A Spatiotemporal State Space Neural Operator</title>
<link>https://arxiv.org/abs/2507.23428</link>
<guid>https://arxiv.org/abs/2507.23428</guid>
<content:encoded><![CDATA[
<div> factorization, spatiotemporal dynamics, state-space models, neural operators, partial observability 

Summary:
The Spatiotemporal State Space Neural Operator (ST-SSM) is introduced as a compact architecture for learning solution operators of time-dependent partial differential equations (PDEs). ST-SSM factorizes the spatial and temporal dimensions using structured state-space models, allowing for independent modeling of temporal evolution and spatial interactions. The architecture shows improved performance compared to alternative schemes on various PDE benchmarks such as Burgers' equation and Navier-Stokes equations. It achieves competitive results with fewer parameters, highlighting the benefits of dimensionally factorized operator learning. A theoretical connection is established between state-space models and neural operators, and a universality theorem is proven for this class of architectures. The model also demonstrates the advantages of temporal memory and performs well under partial observability. This work solidifies the foundation of dimensionally factorized operator learning for efficient and generalizable PDE modeling. 

<br /><br />Summary: <div>
arXiv:2507.23428v1 Announce Type: new 
Abstract: We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact architecture for learning solution operators of time-dependent partial differential equations (PDEs). ST-SSM introduces a novel factorization of the spatial and temporal dimensions, using structured state-space models to independently model temporal evolution and spatial interactions. This design enables parameter efficiency and flexible modeling of long-range spatiotemporal dynamics. A theoretical connection is established between SSMs and neural operators, and a unified universality theorem is proved for the resulting class of architectures. Empirically, we demonstrate that our factorized formulation outperforms alternative schemes such as zigzag scanning and parallel independent processing on several PDE benchmarks, including 1D Burgers' equation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations under varying physical conditions. Our model performs competitively with existing baselines while using significantly fewer parameters. In addition, our results reinforce previous findings on the benefits of temporal memory by showing improved performance under partial observability. Our results highlight the advantages of dimensionally factorized operator learning for efficient and generalizable PDE modeling, and put this approach on a firm theoretical footing.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design</title>
<link>https://arxiv.org/abs/2507.23437</link>
<guid>https://arxiv.org/abs/2507.23437</guid>
<content:encoded><![CDATA[
<div> Sparse Gaussian Process, Multi-objective Bayesian optimization, Hardware-Aware Neural Architecture Search, Deep Neural Network accelerators, Energy efficiency <br />
Summary: <br />
The article introduces Coflex, a novel framework for Hardware-Aware Neural Architecture Search (HW-NAS) that combines Sparse Gaussian Process (SGP) with multi-objective Bayesian optimization. Coflex aims to co-optimize neural network performance and hardware energy efficiency, specifically targeting Deep Neural Network accelerators for edge devices. By utilizing sparse inducing points, Coflex reduces computational overhead and maintains high predictive accuracy in large-scale search spaces. Experimental results demonstrate that Coflex outperforms existing methods in terms of network accuracy and Energy-Delay-Product while achieving significant computational speed-ups of up to 9.5x. The framework provides a scalable solution for HW-NAS, addressing the challenges of high computational costs and extensive search spaces in optimizing hardware-efficient neural networks. <br /> <div>
arXiv:2507.23437v1 Announce Type: new 
Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach to automatically co-optimizing neural network performance and hardware energy efficiency, making it particularly useful for the development of Deep Neural Network accelerators on the edge. However, the extensive search space and high computational cost pose significant challenges to its practical adoption. To address these limitations, we propose Coflex, a novel HW-NAS framework that integrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian optimization. By leveraging sparse inducing points, Coflex reduces the GP kernel complexity from cubic to near-linear with respect to the number of training samples, without compromising optimization performance. This enables scalable approximation of large-scale search space, substantially decreasing computational overhead while preserving high predictive accuracy. We evaluate the efficacy of Coflex across various benchmarks, focusing on accelerator-specific architecture. Our experi- mental results show that Coflex outperforms state-of-the-art methods in terms of network accuracy and Energy-Delay-Product, while achieving a computational speed-up ranging from 1.9x to 9.5x.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifold-regularised Signature Kernel Large-Margin $\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.23449</link>
<guid>https://arxiv.org/abs/2507.23449</guid>
<content:encoded><![CDATA[
<div> Keywords: large-margin, $\ell_p$-SVDD, manifold regularisation, signature kernel, time series anomaly detection

Summary:
The article introduces a manifold-regularised extension of the large-margin $\ell_p$-SVDD method for time series anomaly detection. By incorporating label smoothness on the data manifold, the proposed approach aims to enhance detection performance by capturing structural information. Utilizing a Representer theorem, an optimization technique is provided for efficient implementation, further enhanced by the use of signature kernels to capture time series complexities. Theoretical analysis using Rademacher complexities assesses the generalization performance of the method. Experimental evaluations across diverse datasets compare its effectiveness against alternative approaches. The study demonstrates the potential of the manifold-regularised $\ell_p$-SVDD method in improving anomaly detection performance by leveraging data geometry and signature kernels. 

<br /><br />Summary: 
- Introduction of manifold-regularised $\ell_p$-SVDD for time series anomaly detection
- Emphasis on label smoothness and structural information capture
- Utilization of Representer theorem for optimization
- Integration of signature kernels for time series complexity representation
- Theoretical analysis using Rademacher complexities for generalization assessment
- Experimental evaluations across datasets for performance comparison <div>
arXiv:2507.23449v1 Announce Type: new 
Abstract: We generalise the recently introduced large-margin $\ell_p$-SVDD approach to exploit the geometry of data distribution via manifold regularising and a signature kernel representation for time series anomaly detection. Specifically, we formulate a manifold-regularised variant of the $\ell_p$-SVDD method to encourage label smoothness on the underlying manifold to capture structural information for improved detection performance. Drawing on an existing Representer theorem, we then provide an effective optimisation technique for the proposed method and show that it can benefit from the signature kernel to capture time series complexities for anomaly detection.
  We theoretically study the proposed approach using Rademacher complexities to analyse its generalisation performance and also provide an experimental assessment of the proposed method across various data sets to compare its performance against other methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus</title>
<link>https://arxiv.org/abs/2507.23491</link>
<guid>https://arxiv.org/abs/2507.23491</guid>
<content:encoded><![CDATA[
<div> patients, T2DM, mortality risk, machine learning, interpretability
Summary:<br /><br />
Type 2 diabetes mellitus (T2DM) is a prevalent chronic disease that reduces life expectancy. A study of 554 T2DM patients analyzed key survival-associated features and developed a machine learning model, the Extra Survival Trees (EST) model, to predict all-cause mortality risk. The model achieved a C-statistic of 0.776 and strong predictive performance for 5-, 10-, 15-, and 16.8-year all-cause mortality predictions. The Shapley additive explanations (SHAP) approach was used to interpret the model's decision-making processes. This model's interpretability allows for potential bedside application, aiding in the identification of high-risk patients and optimizing treatment strategies promptly. <div>
arXiv:2507.23491v1 Announce Type: new 
Abstract: Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent non-communicable chronic disease that substantially reduces life expectancy. Accurate estimation of all-cause mortality risk in T2DM patients is crucial for personalizing and optimizing treatment strategies. Research Design and Methods. This study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed T2DM over a maximum follow-up period of 16.8 years, during which 202 patients (36%) died. Key survival-associated features were identified, and multiple machine learning (ML) models were trained and validated to predict all-cause mortality risk. To improve model interpretability, Shapley additive explanations (SHAP) was applied to the best-performing model. Results. The extra survival trees (EST) model, incorporating ten key features, demonstrated the best predictive performance. The model achieved a C-statistic of 0.776, with the area under the receiver operating characteristic curve (AUC) values of 0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause mortality predictions, respectively. The SHAP approach was employed to interpret the model's individual decision-making processes. Conclusions. The developed model exhibited strong predictive performance for mortality risk assessment. Its clinically interpretable outputs enable potential bedside application, improving the identification of high-risk patients and supporting timely treatment optimization.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating structural uncertainty in causal decision making</title>
<link>https://arxiv.org/abs/2507.23495</link>
<guid>https://arxiv.org/abs/2507.23495</guid>
<content:encoded><![CDATA[
<div> Bayesian model averaging, causal effects, structural uncertainty, bivariate relationships, modern causal discovery methods <br />
<br />Summary:
Practitioners often overlook structural uncertainty when making decisions based on causal effects. This study examines the importance of considering structural uncertainty and suggests Bayesian model averaging as a methodological solution. The benefits of model averaging are evident when structural uncertainty is moderate to high, causal effects vary significantly between structures, and loss functions are sensitive to the size of the causal effect. The proposed approach is proven to be optimal under certain conditions. By utilizing modern causal discovery methods, practitioners can quantify the necessary information within limits. This framework complements existing robust causal inference approaches by addressing a unique source of uncertainty that is commonly neglected in practice. <div>
arXiv:2507.23495v1 Announce Type: new 
Abstract: Practitioners making decisions based on causal effects typically ignore structural uncertainty. We analyze when this uncertainty is consequential enough to warrant methodological solutions (Bayesian model averaging over competing causal structures). Focusing on bivariate relationships ($X \rightarrow Y$ vs. $X \leftarrow Y$), we establish that model averaging is beneficial when: (1) structural uncertainty is moderate to high, (2) causal effects differ substantially between structures, and (3) loss functions are sufficiently sensitive to the size of the causal effect. We prove optimality results of our suggested methodological solution under regularity conditions and demonstrate through simulations that modern causal discovery methods can provide, within limits, the necessary quantification. Our framework complements existing robust causal inference approaches by addressing a distinct source of uncertainty typically overlooked in practice.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Ensemble Aggregation for Actor-Critics</title>
<link>https://arxiv.org/abs/2507.23501</link>
<guid>https://arxiv.org/abs/2507.23501</guid>
<content:encoded><![CDATA[
<div> Ensemble, Off-policy reinforcement learning, Continuous control tasks, Directional Ensemble Aggregation, Adaptive aggregation <br />
<br />
Summary: 
The paper introduces Directional Ensemble Aggregation (DEA) as a method for adaptively combining Q-value estimates in off-policy reinforcement learning in continuous control tasks. DEA uses fully learnable directional parameters to adjust critic-side conservatism and actor-side policy exploration based on ensemble disagreement-weighted Bellman errors. This allows DEA to adapt aggregation to uncertainty levels and training phases. The effectiveness of DEA is demonstrated across various continuous control benchmarks and learning regimes, showing superior performance to static ensemble strategies. The method shows promise in improving Q-value estimation accuracy and reducing overestimation bias in off-policy reinforcement learning tasks. <div>
arXiv:2507.23501v1 Announce Type: new 
Abstract: Off-policy reinforcement learning in continuous control tasks depends critically on accurate $Q$-value estimates. Conservative aggregation over ensembles, such as taking the minimum, is commonly used to mitigate overestimation bias. However, these static rules are coarse, discard valuable information from the ensemble, and cannot adapt to task-specific needs or different learning regimes. We propose Directional Ensemble Aggregation (DEA), an aggregation method that adaptively combines $Q$-value estimates in actor-critic frameworks. DEA introduces two fully learnable directional parameters: one that modulates critic-side conservatism and another that guides actor-side policy exploration. Both parameters are learned using ensemble disagreement-weighted Bellman errors, which weight each sample solely by the direction of its Bellman error. This directional learning mechanism allows DEA to adjust conservatism and exploration in a data-driven way, adapting aggregation to both uncertainty levels and the phase of training. We evaluate DEA across continuous control benchmarks and learning regimes - from interactive to sample-efficient - and demonstrate its effectiveness over static ensemble strategies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Verifier Hierarchy</title>
<link>https://arxiv.org/abs/2507.23504</link>
<guid>https://arxiv.org/abs/2507.23504</guid>
<content:encoded><![CDATA[
<div> Trade-off, certificate length, verifier runtime, complexity classes, \(\np\), \(\exptime\) <br />
<br />
Summary: 
The article explores the relationship between certificate length and verifier runtime, proving a Verifier Trade-off Theorem that establishes a minimum certificate length based on the reduction in verification time for a given language. This theorem introduces a hierarchy of certificate complexity with implications for understanding separations between complexity classes like \(\np\) and \(\exptime\). It also applies this framework to analyze problems such as string periodicity and rotation detection. The discussion on the \(\p\) vs. \(\np\) problem suggests that the existence of sub-linear certificates could shed light on the resolution of this fundamental computational question. <div>
arXiv:2507.23504v1 Announce Type: new 
Abstract: We investigate the trade-off between certificate length and verifier runtime. We prove a Verifier Trade-off Theorem showing that reducing the inherent verification time of a language from \(f(n)\) to \(g(n)\), where \(f(n) \ge g(n)\), requires certificates of length at least \(\Omega(\log(f(n) / g(n)))\). This theorem induces a natural hierarchy based on certificate complexity. We demonstrate its applicability to analyzing conjectured separations between complexity classes (e.g., \(\np\) and \(\exptime\)) and to studying natural problems such as string periodicity and rotation detection. Additionally, we provide perspectives on the \(\p\) vs. \(\np\) problem by relating it to the existence of sub-linear certificates.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level</title>
<link>https://arxiv.org/abs/2507.23512</link>
<guid>https://arxiv.org/abs/2507.23512</guid>
<content:encoded><![CDATA[
<div> DP-Clipped-SGD, high-probability convergence, fixed clipping level, smooth optimization, heavy-tailed noise <br />
Summary: 
This article introduces a new analysis of gradient clipping in Deep Learning, focusing on DP-Clipped-SGD with a fixed clipping level for both convex and non-convex smooth optimization. The method is designed to handle heavy-tailed noise common in training large language models. The analysis shows that with a fixed clipping level, the method converges faster than existing ones to a neighborhood of the optimal solution. This neighborhood can be adjusted to balance convergence speed with privacy guarantees introduced by Differential Privacy mechanisms. The research provides a refined trade-off between convergence speed and privacy, solving the issue of incompatible clipping thresholds with standard DP mechanisms like the Gaussian mechanism. The results offer valuable insights for optimizing deep learning models and improving privacy protection in training processes. <br /> <div>
arXiv:2507.23512v1 Announce Type: new 
Abstract: Gradient clipping is a fundamental tool in Deep Learning, improving the high-probability convergence of stochastic first-order methods like SGD, AdaGrad, and Adam under heavy-tailed noise, which is common in training large language models. It is also a crucial component of Differential Privacy (DP) mechanisms. However, existing high-probability convergence analyses typically require the clipping threshold to increase with the number of optimization steps, which is incompatible with standard DP mechanisms like the Gaussian mechanism. In this work, we close this gap by providing the first high-probability convergence analysis for DP-Clipped-SGD with a fixed clipping level, applicable to both convex and non-convex smooth optimization under heavy-tailed noise, characterized by a bounded central $\alpha$-th moment assumption, $\alpha \in (1,2]$. Our results show that, with a fixed clipping level, the method converges to a neighborhood of the optimal solution with a faster rate than the existing ones. The neighborhood can be balanced against the noise introduced by DP, providing a refined trade-off between convergence speed and privacy guarantees.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning with Synthetic Boundary Experience Blending</title>
<link>https://arxiv.org/abs/2507.23534</link>
<guid>https://arxiv.org/abs/2507.23534</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual learning, synthetic data, experience blending, decision boundary, catastrophic forgetting

Summary: 
Continual learning (CL) is a method to address catastrophic forgetting in models trained on multiple tasks sequentially. Experience replay is commonly used but is limited by sparse stored key samples. This study introduces Synthetic Boundary Data (SBD) near decision boundaries during training to act as an implicit regularizer, improving boundary stability. The proposed training framework, Experience Blending, combines knowledge from stored key samples and SBD using a multivariate Differential Privacy noise mechanism to generate SBD and end-to-end training strategy. Experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet show that the method outperforms nine CL baselines, leading to accuracy improvements of 10%, 6%, and 13%, respectively. <div>
arXiv:2507.23534v1 Announce Type: new 
Abstract: Continual learning (CL) aims to address catastrophic forgetting in models trained sequentially on multiple tasks. While experience replay has shown promise, its effectiveness is often limited by the sparse distribution of stored key samples, leading to overly simplified decision boundaries. We hypothesize that introducing synthetic data near the decision boundary (Synthetic Boundary Data, or SBD) during training serves as an implicit regularizer, improving boundary stability and mitigating forgetting. To validate this hypothesis, we propose a novel training framework, {\bf Experience Blending}, which integrates knowledge from both stored key samples and synthetic, boundary-adjacent data. Experience blending consists of two core components: (1) a multivariate Differential Privacy (DP) noise mechanism that injects batch-wise noise into low-dimensional feature representations, generating SBD; and (2) an end-to-end training strategy that jointly leverages both stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet demonstrate that our method outperforms nine CL baselines, achieving accuracy improvements of 10%, 6%, and 13%, respectively.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent AI: The Case for Interpretability and Explainability</title>
<link>https://arxiv.org/abs/2507.23535</link>
<guid>https://arxiv.org/abs/2507.23535</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, interpretability, transparency, implementation guidance, design principle
<br />
<br />
Summary: 
As artificial intelligence systems play an increasingly significant role in critical decision-making processes, the necessity of transparency in AI implementation has become more prevalent. This paper, drawing on the expertise of a pioneering institute in the field of AI research, shares valuable insights and best practices for incorporating interpretability into AI applications across a range of industries. By highlighting practical applications and lessons learned, the paper aims to provide actionable strategies and guidance for organizations at different levels of AI maturity. Emphasizing the importance of integrating interpretability as a fundamental design aspect rather than an afterthought, the authors stress the significance of building transparency into AI systems from the outset. This approach ensures that interpretability remains a core principle in responsible and trustworthy AI development. <div>
arXiv:2507.23535v1 Announce Type: new 
Abstract: As artificial intelligence systems increasingly inform high-stakes decisions across sectors, transparency has become foundational to responsible and trustworthy AI implementation. Leveraging our role as a leading institute in advancing AI research and enabling industry adoption, we present key insights and lessons learned from practical interpretability applications across diverse domains. This paper offers actionable strategies and implementation guidance tailored to organizations at varying stages of AI maturity, emphasizing the integration of interpretability as a core design principle rather than a retrospective add-on.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices</title>
<link>https://arxiv.org/abs/2507.23536</link>
<guid>https://arxiv.org/abs/2507.23536</guid>
<content:encoded><![CDATA[
<div> Parameter-efficient fine-tuning, deep learning models, convolutional neural networks, edge devices, PyTorch profilers <br />
<br />
Summary: 
This paper explores the use of parameter-efficient fine-tuning (PEFT) methods on convolutional architectures commonly deployed on edge devices. The study evaluates the performance of popular PEFT methods such as LoRA, DoRA, and GaLore in handling distribution shifts and unseen classes in convolutional models. The research focuses on resource efficiency and compares PEFT methods with traditional fine-tuning approaches using PyTorch profilers. The results show that PEFT methods are not as memory-efficient when applied to depthwise-separable convolution architectures compared to large language models (LLMs). However, when targeting convolutional architectures optimized for edge deployment, adapter-based PEFT methods can significantly reduce floating-point operations (FLOPs) during model updates. These findings provide valuable insights for selecting PEFT methods based on hardware constraints, performance requirements, and specific application needs. The code used in the study is available online. <br /> <div>
arXiv:2507.23536v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs of updating deep learning models by minimizing the number of additional parameters used to adapt a model to a down- stream task. While extensively researched in large language models (LLMs), their application to smaller models used on edge devices, such as convolutional neural networks, remains underexplored. This paper benchmarks and analyzes popular PEFT methods on convolutional architectures typically deployed in resource-constrained edge environments. We evaluate LoRA, DoRA, and GaLore for updating standard and depthwise convolutional architectures to handle distribution shifts and accommodate unseen classes. We utilize recently proposed PyTorch profilers to compare the updated model performance and computational costs of these PEFT methods with traditional fine-tuning approaches. With resource efficiency in mind, we investigate their update behavior across different rank dimensions. We find that the evaluated PEFT methods are only half as memory-efficient when applied to depthwise-separable convolution architectures, compared to their efficiency with LLMs. Conversely, when targeting convolu- tional architectures optimized for edge deployment, adapter-based PEFT methods can reduce floating point operations (FLOPs) during model updates by up to 95%. These insights offer valuable guidance for selecting PEFT methods based on hardware constraints, performance requirements, and application needs. Our code is online.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions</title>
<link>https://arxiv.org/abs/2507.23539</link>
<guid>https://arxiv.org/abs/2507.23539</guid>
<content:encoded><![CDATA[
<div> Gaussian Kernel matrices, fast algorithms, matrix-vector products, attention computation, subquadratic time<br />
<br />
Summary: 
The study focuses on fast algorithms for computing matrix-vector products for asymmetric Gaussian Kernel matrices used in attention matrices. The columns of the matrices are indexed by keys, and rows by queries, with an entry formulation based on the Euclidean distance between the queries and keys. The goal is to output a vector with minimal error compared to the original vector in subquadratic time with linear dependence on the dimensionality of the vectors. The algorithms are designed under the assumption that the sum of matrix entries scales linearly with the number of rows, validated through experimental data. The research presents the first subquadratic-time algorithm under this assumption for unrestricted vectors, offering potential applications in fast attention computation for Large Language Models (LLMs). <div>
arXiv:2507.23539v1 Announce Type: new 
Abstract: Motivated by the problem of fast processing of attention matrices, we study fast algorithms for computing matrix-vector products for asymmetric Gaussian Kernel matrices $K\in \mathbb{R}^{n\times n}$. $K$'s columns are indexed by a set of $n$ keys $k_1,k_2\ldots, k_n\in \mathbb{R}^d$, rows by a set of $n$ queries $q_1,q_2,\ldots,q_n\in \mathbb{R}^d $, and its $i,j$ entry is $K_{ij} = e^{-\|q_i-k_j\|_2^2/2\sigma^2}$ for some bandwidth parameter $\sigma>0$. Given a vector $x\in \mathbb{R}^n$ and error parameter $\epsilon>0$, our task is to output a $y\in \mathbb{R}^n$ such that $\|Kx-y\|_2\leq \epsilon \|x\|_2$ in time subquadratic in $n$ and linear in $d$. Our algorithms rely on the following modelling assumption about the matrices $K$: the sum of the entries of $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We validate this assumption experimentally, for Gaussian kernel matrices encountered in various settings such as fast attention computation in LLMs. We obtain the first subquadratic-time algorithm that works under this assumption, for unrestricted vectors.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform</title>
<link>https://arxiv.org/abs/2507.23562</link>
<guid>https://arxiv.org/abs/2507.23562</guid>
<content:encoded><![CDATA[
<div> quantized SNNs, reinforcement learning, energy-efficient implementation, neuromorphic computing, SpiNNaker2

Summary:
The study focuses on implementing a reinforcement learning algorithm using quantized Spiking Neural Networks (SNNs) for energy-efficient solutions in robotic tasks. The SNN model is trained with the Q-learning algorithm, then fine-tuned and quantized to 8-bit precision for deployment on SpiNNaker2 neuromorphic chip. Performance evaluation against a GTX 1650 GPU baseline shows SpiNNaker2's potential for low-energy neuromorphic computing, achieving up to 32x reduction in energy consumption. Inference latency on SpiNNaker2 matches GPU-based execution, with improvements in specific task settings. The results highlight SpiNNaker2's capability for real-time neuromorphic control, making it a promising platform for efficient deep Q-learning applications. <div>
arXiv:2507.23562v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power consumption and low-latency inference on neuromorphic hardware for a wide range of robotic tasks. In this work, we present an energy-efficient implementation of a reinforcement learning (RL) algorithm using quantized SNNs to solve two classical control tasks. The network is trained using the Q-learning algorithm, then fine-tuned and quantized to low-bit (8-bit) precision for embedded deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative advantage of SpiNNaker2 over conventional computing platforms, we analyze inference latency, dynamic power consumption, and energy cost per inference for our SNN models, comparing performance against a GTX 1650 GPU baseline. Our results demonstrate SpiNNaker2's strong potential for scalable, low-energy neuromorphic computing, achieving up to 32x reduction in energy consumption. Inference latency remains on par with GPU-based execution, with improvements observed in certain task settings, reinforcing SpiNNaker2's viability for real-time neuromorphic control and making the neuromorphic approach a compelling direction for efficient deep Q-learning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimised Feature Subset Selection via Simulated Annealing</title>
<link>https://arxiv.org/abs/2507.23568</link>
<guid>https://arxiv.org/abs/2507.23568</guid>
<content:encoded><![CDATA[
<div> simulated annealing, feature selection, combinatorial optimization, Fisher discriminant ratio, high-dimensional settings <br />
<br />
Summary: SA-FDR is a new algorithm for feature selection that approaches the task as a combinatorial optimization problem. It uses simulated annealing to globally search for feature subsets, guided by the Fisher discriminant ratio to evaluate classification model quality efficiently. The algorithm's experiments on large datasets show it consistently selects more compact feature subsets with high predictive accuracy. SA-FDR excels in capturing inter-feature dependencies often overlooked by greedy optimization methods, making it flexible and effective in creating interpretable models in high-dimensional settings where sparsity, interpretability, and performance are key. <div>
arXiv:2507.23568v1 Announce Type: new 
Abstract: We introduce SA-FDR, a novel algorithm for $\ell_0$-norm feature selection that considers this task as a combinatorial optimisation problem and solves it by using simulated annealing to perform a global search over the space of feature subsets. The optimisation is guided by the Fisher discriminant ratio, which we use as a computationally efficient proxy for model quality in classification tasks. Our experiments, conducted on datasets with up to hundreds of thousands of samples and hundreds of features, demonstrate that SA-FDR consistently selects more compact feature subsets while achieving a high predictive accuracy. This ability to recover informative yet minimal sets of features stems from its capacity to capture inter-feature dependencies often missed by greedy optimisation approaches. As a result, SA-FDR provides a flexible and effective solution for designing interpretable models in high-dimensional settings, particularly when model sparsity, interpretability, and performance are crucial.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.23581</link>
<guid>https://arxiv.org/abs/2507.23581</guid>
<content:encoded><![CDATA[
arXiv:2507.23581v1 Announce Type: new 
Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness in enhancing the reasoning abilities of LLMs by leveraging graph structures for knowledge representation and modeling complex real-world relationships. However, existing GraphRAG methods still face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs. To address this problem, we propose GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. Our method can decompose complex problems, autonomously invoke retrieval tools to acquire necessary information, and perform effective reasoning. Specifically, we utilize a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability. Next, we design two process-constrained reward functions. To handle the shallow retrieval problem, we design a Progressive Retrieval Attenuation (PRA) reward to encourage essential retrievals. Then, to handle the over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the model performance with computational costs. We further design a phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards. Lastly, our method adopts a hybrid graph-textual retrieval to improve the reasoning capacity. Extensive experimental results demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, our framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[
arXiv:2507.23600v1 Announce Type: new 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed chemical signals into base patterns (components) and their concentrations, playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified component count, usually unknown in real data. As dataset size or component count increases, the scalability and reliability of MF-based MCR face significant challenges. This study reformulates MCR as a generative process (gMCR), and introduces an energy-based deep learning solver, EB-gMCR, that automatically discovers the smallest component set able to reconstruct the data faithfully. EB-gMCR starts from a large candidate pool (e.g., 1024 spectra) and employs a differentiable gating network to retain only active components while estimating their concentrations. On noisy synthetic datasets containing up to 256 latent sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near exact component estimation. Additional chemical priors, such as non-negativity or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to other instruments or domains without altering the core learning process. By uniting high-capacity generative modeling and hard component selection, EB-gMCR offers a practical route to large-scale signal unmixing analysis, including chemical library-driven scenarios. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.23604</link>
<guid>https://arxiv.org/abs/2507.23604</guid>
<content:encoded><![CDATA[
arXiv:2507.23604v1 Announce Type: new 
Abstract: Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for learning scalable multi-agent policies, but suffer from partial observability and induced non-stationarity. These challenges can be addressed by introducing mechanisms that facilitate coordination and high-level planning. Specifically, coordination and temporal abstraction can be achieved through communication (e.g., message passing) and Hierarchical Reinforcement Learning (HRL) approaches to decision-making. However, optimization issues limit the applicability of hierarchical policies to multi-agent systems. As such, the combination of these approaches has not been fully explored. To fill this void, we propose a novel and effective methodology for learning multi-agent hierarchies of message-passing policies. We adopt the feudal HRL framework and rely on a hierarchical graph structure for planning and coordination among agents. Agents at lower levels in the hierarchy receive goals from the upper levels and exchange messages with neighboring agents at the same level. To learn hierarchical multi-agent policies, we design a novel reward-assignment method based on training the lower-level policies to maximize the advantage function associated with the upper levels. Results on relevant benchmarks show that our method performs favorably compared to the state of the art.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates</title>
<link>https://arxiv.org/abs/2507.23607</link>
<guid>https://arxiv.org/abs/2507.23607</guid>
<content:encoded><![CDATA[
arXiv:2507.23607v1 Announce Type: new 
Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-GTA: Latent Generative Modeling for Time Series Augmentation</title>
<link>https://arxiv.org/abs/2507.23615</link>
<guid>https://arxiv.org/abs/2507.23615</guid>
<content:encoded><![CDATA[
arXiv:2507.23615v1 Announce Type: new 
Abstract: Data augmentation is gaining importance across various aspects of time series analysis, from forecasting to classification and anomaly detection tasks. We introduce the Latent Generative Transformer Augmentation (L-GTA) model, a generative approach using a transformer-based variational recurrent autoencoder. This model uses controlled transformations within the latent space of the model to generate new time series that preserve the intrinsic properties of the original dataset. L-GTA enables the application of diverse transformations, ranging from simple jittering to magnitude warping, and combining these basic transformations to generate more complex synthetic time series datasets. Our evaluation of several real-world datasets demonstrates the ability of L-GTA to produce more reliable, consistent, and controllable augmented data. This translates into significant improvements in predictive accuracy and similarity measures compared to direct transformation methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective</title>
<link>https://arxiv.org/abs/2507.23632</link>
<guid>https://arxiv.org/abs/2507.23632</guid>
<content:encoded><![CDATA[
arXiv:2507.23632v1 Announce Type: new 
Abstract: Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting</title>
<link>https://arxiv.org/abs/2507.23638</link>
<guid>https://arxiv.org/abs/2507.23638</guid>
<content:encoded><![CDATA[
arXiv:2507.23638v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed medical institutions while preserving patient privacy, but remains vulnerable to Byzantine attacks and statistical heterogeneity. We present OptiGradTrust, a comprehensive defense framework that evaluates gradient updates through a novel six-dimensional fingerprint including VAE reconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency ratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module for adaptive trust scoring. To address convergence challenges under data heterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch Normalization with proximal regularization for optimal accuracy-convergence trade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI datasets under various Byzantine attack scenarios demonstrates significant improvements over state-of-the-art defenses, achieving up to +1.6 percentage points over FLGuard under non-IID conditions while maintaining robust performance against diverse attack patterns through our adaptive learning approach.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAP-Guided Regularization in Machine Learning Models</title>
<link>https://arxiv.org/abs/2507.23665</link>
<guid>https://arxiv.org/abs/2507.23665</guid>
<content:encoded><![CDATA[
arXiv:2507.23665v1 Announce Type: new 
Abstract: Feature attribution methods such as SHapley Additive exPlanations (SHAP) have become instrumental in understanding machine learning models, but their role in guiding model optimization remains underexplored. In this paper, we propose a SHAP-guided regularization framework that incorporates feature importance constraints into model training to enhance both predictive performance and interpretability. Our approach applies entropy-based penalties to encourage sparse, concentrated feature attributions while promoting stability across samples. The framework is applicable to both regression and classification tasks. Our first exploration started with investigating a tree-based model regularization using TreeSHAP. Through extensive experiments on benchmark regression and classification datasets, we demonstrate that our method improves generalization performance while ensuring robust and interpretable feature attributions. The proposed technique offers a novel, explainability-driven regularization approach, making machine learning models both more accurate and more reliable.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses</title>
<link>https://arxiv.org/abs/2507.23674</link>
<guid>https://arxiv.org/abs/2507.23674</guid>
<content:encoded><![CDATA[
arXiv:2507.23674v1 Announce Type: new 
Abstract: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step Flow Policy Mirror Descent</title>
<link>https://arxiv.org/abs/2507.23675</link>
<guid>https://arxiv.org/abs/2507.23675</guid>
<content:encoded><![CDATA[
arXiv:2507.23675v1 Announce Type: new 
Abstract: Diffusion policies have achieved great success in online reinforcement learning (RL) due to their strong expressive capacity. However, the inference of diffusion policy models relies on a slow iterative sampling process, which limits their responsiveness. To overcome this limitation, we propose Flow Policy Mirror Descent (FPMD), an online RL algorithm that enables 1-step sampling during policy inference. Our approach exploits a theoretical connection between the distribution variance and the discretization error of single-step sampling in straight interpolation flow matching models, and requires no extra distillation or consistency training. We present two algorithm variants based on flow policy and MeanFlow policy parametrizations, respectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate that our algorithms show strong performance comparable to diffusion policy baselines while requiring hundreds of times fewer function evaluations during inference.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data</title>
<link>https://arxiv.org/abs/2507.23676</link>
<guid>https://arxiv.org/abs/2507.23676</guid>
<content:encoded><![CDATA[
arXiv:2507.23676v1 Announce Type: new 
Abstract: Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomalous Samples for Few-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.23712</link>
<guid>https://arxiv.org/abs/2507.23712</guid>
<content:encoded><![CDATA[
arXiv:2507.23712v1 Announce Type: new 
Abstract: Several anomaly detection and classification methods rely on large amounts of non-anomalous or "normal" samples under the assump- tion that anomalous data is typically harder to acquire. This hypothesis becomes questionable in Few-Shot settings, where as little as one anno- tated sample can make a significant difference. In this paper, we tackle the question of utilizing anomalous samples in training a model for bi- nary anomaly classification. We propose a methodology that incorporates anomalous samples in a multi-score anomaly detection score leveraging recent Zero-Shot and memory-based techniques. We compare the utility of anomalous samples to that of regular samples and study the benefits and limitations of each. In addition, we propose an augmentation-based validation technique to optimize the aggregation of the different anomaly scores and demonstrate its effectiveness on popular industrial anomaly detection datasets.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System</title>
<link>https://arxiv.org/abs/2507.23756</link>
<guid>https://arxiv.org/abs/2507.23756</guid>
<content:encoded><![CDATA[
arXiv:2507.23756v1 Announce Type: new 
Abstract: This study centers on overcoming the challenge of selecting the best annotators for each query in Active Learning (AL), with the objective of minimizing misclassifications. AL recognizes the challenges related to cost and time when acquiring labeled data, and decreases the number of labeled data needed. Nevertheless, there is still the necessity to reduce annotation errors, aiming to be as efficient as possible, to achieve the expected accuracy faster. Most strategies for query-annotator pairs do not consider internal factors that affect productivity, such as mood, attention, motivation, and fatigue levels. This work addresses this gap in the existing literature, by not only considering how the internal factors influence annotators (mood and fatigue levels) but also presenting a new query-annotator pair strategy, using a Knowledge-Based Recommendation System (RS). The RS ranks the available annotators, allowing to choose one or more to label the queried instance using their past accuracy values, and their mood and fatigue levels, as well as information about the instance queried. This work bases itself on existing literature on mood and fatigue influence on human performance, simulating annotators in a realistic manner, and predicting their performance with the RS. The results show that considering past accuracy values, as well as mood and fatigue levels reduces the number of annotation errors made by the annotators, and the uncertainty of the model through its training, when compared to not using internal factors. Accuracy and F1-score values were also better in the proposed approach, despite not being as substantial as the aforementioned. The methodologies and findings presented in this study begin to explore the open challenge of human cognitive factors affecting AL.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-Driven Active Model Selection</title>
<link>https://arxiv.org/abs/2507.23771</link>
<guid>https://arxiv.org/abs/2507.23771</guid>
<content:encoded><![CDATA[
arXiv:2507.23771v1 Announce Type: new 
Abstract: The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset -- a costly and time-intensive process. We propose a method for active model selection, using predictions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best candidate. Our method, CODA, performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected. We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios. CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. Code and data are available at https://github.com/justinkay/coda.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Typing Tensor Calculus in 2-Categories (I)</title>
<link>https://arxiv.org/abs/1908.01212</link>
<guid>https://arxiv.org/abs/1908.01212</guid>
<content:encoded><![CDATA[
arXiv:1908.01212v4 Announce Type: cross 
Abstract: To formalize calculations in linear algebra for the development of efficient algorithms and a framework suitable for functional programming languages and faster parallelized computations, we adopt an approach that treats elements of linear algebra, such as matrices, as morphisms in the category of matrices, $\mathbf{Mat_{k}}$. This framework is further extended by generalizing the results to arbitrary monoidal semiadditive categories. To enrich this perspective and accommodate higher-rank matrices (tensors), we define semiadditive 2-categories, where matrices $T_{ij}$ are represented as 1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This formalization provides an index-free, typed linear algebra framework that includes matrices and tensors with up to four indices. Furthermore, we extend the framework to monoidal semiadditive 2-categories and demonstrate detailed operations and vectorization within the 2-category of 2Vec introduced by Kapranov and Voevodsky.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver</title>
<link>https://arxiv.org/abs/2507.22906</link>
<guid>https://arxiv.org/abs/2507.22906</guid>
<content:encoded><![CDATA[
arXiv:2507.22906v1 Announce Type: cross 
Abstract: As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO architecture has been shown to own a great potential to replace the massive or extremely large-scale fully-digital MIMO in the future wireless networks to address the three challenging problems faced by the latter: high energy consumption, high circuit cost, and high complexity. However, how to intelligently sense the number and direction of multi-emitters via such a structure is still an open hard problem. To address this, we propose a two-stage sensing framework that jointly estimates the number and direction values of multiple targets. Specifically, three target number sensing methods are designed: an improved eigen-domain clustering (EDC) framework, an enhanced deep neural network (DNN) based on five key statistical features, and an improved one-dimensional convolutional neural network (1D-CNN) utilizing full eigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is achieved via the introduced online micro-clustering (OMC-DOA) method. Furthermore, we derive the Cram\'er-Rao lower bound (CRLB) for the H2AD under multiple-source conditions as a theoretical performance benchmark. Simulation results show that the developed three methods achieve 100\% number of targets sensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior under extremely-low SNR conditions. The introduced OMC-DOA outperforms existing clustering and fusion-based DOA methods in multi-source environments.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection</title>
<link>https://arxiv.org/abs/2507.22908</link>
<guid>https://arxiv.org/abs/2507.22908</guid>
<content:encoded><![CDATA[
arXiv:2507.22908v1 Announce Type: cross 
Abstract: Rapid growth of digital transactions has led to a surge in fraudulent activities, challenging traditional detection methods in the financial sector. To tackle this problem, we introduce a specialised federated learning framework that uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model with advanced privacy preserving techniques. By integrating quantum layers into the LSTM architecture, our approach adeptly captures complex cross-transactional patters, resulting in an approximate 5% performance improvement across key evaluation metrics compared to conventional models. Central to our framework is "FedRansel", a novel method designed to defend against poisoning and inference attacks, thereby reducing model degradation and inference accuracy by 4-8%, compared to standard differential privacy mechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances fraud detection accuracy and reinforces the security and confidentiality of sensitive financial data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms</title>
<link>https://arxiv.org/abs/2507.22912</link>
<guid>https://arxiv.org/abs/2507.22912</guid>
<content:encoded><![CDATA[
arXiv:2507.22912v1 Announce Type: cross 
Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the internet, including the deep and dark web, as well as platforms such as Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of illicit goods including drugs, weapons, and stolen credentials. Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources. This paper presents a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy to detect and classify illicit marketplace content across diverse platforms. We extract semantic representations using ModernBERT, a transformer model for long documents, finetuned on domain-specific data from deep and dark web pages, Telegram channels, Subreddits, and Pastebin pastes to capture specialized jargon and ambiguous linguistic patterns. In addition, we incorporate manually engineered features such as document structure, embedded patterns including Bitcoin addresses, emails, and IPs, and metadata, which complement language model embeddings. The classification pipeline operates in two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random Forest, and SVM with entropy-based weighted voting to detect sales-related documents. The second stage further classifies these into drug, weapon, or credential sales. Experiments on three datasets, including our multi-source corpus, DUTA, and CoDA, show that our model outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Convergence: Investigating Shared Representations Across Scaled LLMs</title>
<link>https://arxiv.org/abs/2507.22918</link>
<guid>https://arxiv.org/abs/2507.22918</guid>
<content:encoded><![CDATA[
arXiv:2507.22918v1 Announce Type: cross 
Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts. Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, and compare the matched feature spaces with SVCCA and RSA. Middle layers yield the strongest overlap, while early and late layers show far less similarity. Preliminary experiments extend the analysis from single tokens to multi-token subspaces, showing that semantically similar subspaces interact similarly with language models. These results strengthen the case that large language models carve the world into broadly similar, interpretable features despite size differences, reinforcing universality as a foundation for cross-model interpretability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology</title>
<link>https://arxiv.org/abs/2507.22941</link>
<guid>https://arxiv.org/abs/2507.22941</guid>
<content:encoded><![CDATA[
arXiv:2507.22941v1 Announce Type: cross 
Abstract: Electronic medical reports (EHR) contain a vast amount of information that can be leveraged for machine learning applications in healthcare. However, existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form. Here, we propose SigBERT, an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient. SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores. The model was trained and evaluated on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios</title>
<link>https://arxiv.org/abs/2507.22947</link>
<guid>https://arxiv.org/abs/2507.22947</guid>
<content:encoded><![CDATA[
arXiv:2507.22947v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \emph{https://github.com/sii-research/elmes.git}.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Post-hoc Explanations of Knowledge Graph Completions</title>
<link>https://arxiv.org/abs/2507.22951</link>
<guid>https://arxiv.org/abs/2507.22951</guid>
<content:encoded><![CDATA[
arXiv:2507.22951v1 Announce Type: cross 
Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks formalization and consistent evaluations, hindering reproducibility and cross-study comparisons. This paper argues for a unified approach to post-hoc explainability in KGC. First, we propose a general framework to characterize post-hoc explanations via multi-objective optimization, balancing their effectiveness and conciseness. This unifies existing post-hoc explainability algorithms in KGC and the explanations they produce. Next, we suggest and empirically support improved evaluation protocols using popular metrics like Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of interpretability as the ability of explanations to address queries meaningful to end-users. By unifying methods and refining evaluation standards, this work aims to make research in KGC explainability more reproducible and impactful.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Label Placement on Maps via Large Language Models</title>
<link>https://arxiv.org/abs/2507.22952</link>
<guid>https://arxiv.org/abs/2507.22952</guid>
<content:encoded><![CDATA[
arXiv:2507.22952v1 Announce Type: cross 
Abstract: Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at https://github.com/HarryShomer/MAPLE.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Between the Nodes: Community Discovery Beyond Vectors</title>
<link>https://arxiv.org/abs/2507.22955</link>
<guid>https://arxiv.org/abs/2507.22955</guid>
<content:encoded><![CDATA[
arXiv:2507.22955v1 Announce Type: cross 
Abstract: Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam</title>
<link>https://arxiv.org/abs/2507.22958</link>
<guid>https://arxiv.org/abs/2507.22958</guid>
<content:encoded><![CDATA[
arXiv:2507.22958v1 Announce Type: cross 
Abstract: This paper introduces a novel benchmark, EGE-Math Solutions Assessment Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to assess hand-written mathematical solutions. Unlike existing benchmarks that focus on problem solving, our approach centres on understanding student solutions, identifying mistakes, and assigning grades according to fixed criteria. We compile 122 scanned solutions from the Russian Unified State Exam (EGE) together with official expert grades, and evaluate seven modern VLMs from Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The results reveal current limitations in mathematical reasoning and human-rubric alignment, opening new research avenues in AI-assisted assessment. You can find code in https://github.com/Karifannaa/Auto-check-EGE-math
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Prune Branches in Modern Tree-Fruit Orchards</title>
<link>https://arxiv.org/abs/2507.23015</link>
<guid>https://arxiv.org/abs/2507.23015</guid>
<content:encoded><![CDATA[
arXiv:2507.23015v1 Announce Type: cross 
Abstract: Dormant tree pruning is labor-intensive but essential to maintaining modern highly-productive fruit orchards. In this work we present a closed-loop visuomotor controller for robotic pruning. The controller guides the cutter through a cluttered tree environment to reach a specified cut point and ensures the cutters are perpendicular to the branch. We train the controller using a novel orchard simulation that captures the geometric distribution of branches in a target apple orchard configuration. Unlike traditional methods requiring full 3D reconstruction, our controller uses just optical flow images from a wrist-mounted camera. We deploy our learned policy in simulation and the real-world for an example V-Trellis envy tree with zero-shot transfer, achieving a 30% success rate -- approximately half the performance of an oracle planner.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Smoothing Newton Method for Rank-one Matrix Recovery</title>
<link>https://arxiv.org/abs/2507.23017</link>
<guid>https://arxiv.org/abs/2507.23017</guid>
<content:encoded><![CDATA[
arXiv:2507.23017v1 Announce Type: cross 
Abstract: We consider the phase retrieval problem, which involves recovering a rank-one positive semidefinite matrix from rank-one measurements. A recently proposed algorithm based on Bures-Wasserstein gradient descent (BWGD) exhibits superlinear convergence, but it is unstable, and existing theory can only prove local linear convergence for higher rank matrix recovery. We resolve this gap by revealing that BWGD implements Newton's method with a nonsmooth and nonconvex objective. We develop a smoothing framework that regularizes the objective, enabling a stable method with rigorous superlinear convergence guarantees. Experiments on synthetic data demonstrate this superior stability while maintaining fast convergence.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Readiness for Scientific AI at Scale</title>
<link>https://arxiv.org/abs/2507.23018</link>
<guid>https://arxiv.org/abs/2507.23018</guid>
<content:encoded><![CDATA[
arXiv:2507.23018v1 Announce Type: cross 
Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models. We analyze archetypal workflows across four representative domains - climate, nuclear fusion, bio/health, and materials - to identify common preprocessing patterns and domain-specific constraints. We introduce a two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard), both tailored to high performance computing (HPC) environments. This framework outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. Together, these dimensions form a conceptual maturity matrix that characterizes scientific data readiness and guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving</title>
<link>https://arxiv.org/abs/2507.23042</link>
<guid>https://arxiv.org/abs/2507.23042</guid>
<content:encoded><![CDATA[
arXiv:2507.23042v1 Announce Type: cross 
Abstract: Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, &amp; Waypoints</title>
<link>https://arxiv.org/abs/2507.23064</link>
<guid>https://arxiv.org/abs/2507.23064</guid>
<content:encoded><![CDATA[
arXiv:2507.23064v1 Announce Type: cross 
Abstract: Autonomous cars need geometric accuracy and semantic understanding to navigate complex environments, yet most stacks handle them separately. We present XYZ-Drive, a single vision-language model that reads a front-camera frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs steering and speed. A lightweight goal-centered cross-attention layer lets waypoint tokens highlight relevant image and map patches, supporting both action and textual explanations, before the fused tokens enter a partially fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and 0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and halving collisions, all while significantly improving efficiency by using only a single branch. Sixteen ablations explain the gains. Removing any modality (vision, waypoint, map) drops success by up to 11%, confirming their complementary roles and rich connections. Replacing goal-centered attention with simple concatenation cuts 3% in performance, showing query-based fusion injects map knowledge more effectively. Keeping the transformer frozen loses 5%, showing the importance of fine-tuning when applying VLMs for specific tasks such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent and map layout enables accurate, transparent, real-time driving.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL</title>
<link>https://arxiv.org/abs/2507.23104</link>
<guid>https://arxiv.org/abs/2507.23104</guid>
<content:encoded><![CDATA[
arXiv:2507.23104v1 Announce Type: cross 
Abstract: Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Complexity of Finding Stationary Points in Nonconvex Simple Bilevel Optimization</title>
<link>https://arxiv.org/abs/2507.23155</link>
<guid>https://arxiv.org/abs/2507.23155</guid>
<content:encoded><![CDATA[
arXiv:2507.23155v1 Announce Type: cross 
Abstract: In this paper, we study the problem of solving a simple bilevel optimization problem, where the upper-level objective is minimized over the solution set of the lower-level problem. We focus on the general setting in which both the upper- and lower-level objectives are smooth but potentially nonconvex. Due to the absence of additional structural assumptions for the lower-level objective-such as convexity or the Polyak-{\L}ojasiewicz (PL) condition-guaranteeing global optimality is generally intractable. Instead, we introduce a suitable notion of stationarity for this class of problems and aim to design a first-order algorithm that finds such stationary points in polynomial time. Intuitively, stationarity in this setting means the upper-level objective cannot be substantially improved locally without causing a larger deterioration in the lower-level objective. To this end, we show that a simple and implementable variant of the dynamic barrier gradient descent (DBGD) framework can effectively solve the considered nonconvex simple bilevel problems up to stationarity. Specifically, to reach an $(\epsilon_f, \epsilon_g)$-stationary point-where $\epsilon_f$ and $\epsilon_g$ denote the target stationarity accuracies for the upper- and lower-level objectives, respectively-the considered method achieves a complexity of $\mathcal{O}\left(\max\left(\epsilon_f^{-\frac{3+p}{1+p}}, \epsilon_g^{-\frac{3+p}{2}}\right)\right)$, where $p \geq 0$ is an arbitrary constant balancing the terms. To the best of our knowledge, this is the first complexity result for a discrete-time algorithm that guarantees joint stationarity for both levels in general nonconvex simple bilevel problems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Factorization Machine Annealing for Rapid Discovery of Transparent Conducting Materials</title>
<link>https://arxiv.org/abs/2507.23160</link>
<guid>https://arxiv.org/abs/2507.23160</guid>
<content:encoded><![CDATA[
arXiv:2507.23160v1 Announce Type: cross 
Abstract: The development of novel transparent conducting materials (TCMs) is essential for enhancing the performance and reducing the cost of next-generation devices such as solar cells and displays. In this research, we focus on the (Al$_x$Ga$_y$In$_z$)$_2$O$_3$ system and extend the FMA framework, which combines a Factorization Machine (FM) and annealing, to search for optimal compositions and crystal structures with high accuracy and low cost. The proposed method introduces (i) the binarization of continuous variables, (ii) the utilization of good solutions using a Hopfield network, (iii) the activation of global search through adaptive random flips, and (iv) fine-tuning via a bit-string local search. Validation using the (Al$_x$Ga$_y$In$_z$)$_2$O$_3$ data from the Kaggle "Nomad2018 Predicting Transparent Conductors" competition demonstrated that our method achieves faster and more accurate searches than Bayesian optimization and genetic algorithms. Furthermore, its application to multi-objective optimization showed its capability in designing materials by simultaneously considering both the band gap and formation energy. These results suggest that applying our method to larger, more complex search problems and diverse material designs that reflect realistic experimental conditions is expected to contribute to the further advancement of materials informatics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration</title>
<link>https://arxiv.org/abs/2507.23167</link>
<guid>https://arxiv.org/abs/2507.23167</guid>
<content:encoded><![CDATA[
arXiv:2507.23167v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-based solution for mango classification in agricultural environments</title>
<link>https://arxiv.org/abs/2507.23174</link>
<guid>https://arxiv.org/abs/2507.23174</guid>
<content:encoded><![CDATA[
arXiv:2507.23174v1 Announce Type: cross 
Abstract: This article exemplifies the design of a fruit detection and classification system using Convolutional
  Neural Networks (CNN). The goal is to develop a system that automatically assesses fruit quality for
  farm inventory management. Specifically, a method for mango fruit classification was developed using
  image processing, ensuring both accuracy and efficiency. Resnet-18 was selected as the preliminary
  architecture for classification, while a cascade detector was used for detection, balancing execution speed
  and computational resource consumption. Detection and classification results were displayed through a
  graphical interface developed in MatLab App Designer, streamlining system interaction. The integration
  of convolutional neural networks and cascade detectors proffers a reliable solution for fruit classification
  and detection, with potential applications in agricultural quality control.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geak: Introducing Triton Kernel AI Agent &amp; Evaluation Benchmarks</title>
<link>https://arxiv.org/abs/2507.23194</link>
<guid>https://arxiv.org/abs/2507.23194</guid>
<content:encoded><![CDATA[
arXiv:2507.23194v1 Announce Type: cross 
Abstract: The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Recommenders Self-Aware? Label-Free Recommendation Performance Estimation via Model Uncertainty</title>
<link>https://arxiv.org/abs/2507.23208</link>
<guid>https://arxiv.org/abs/2507.23208</guid>
<content:encoded><![CDATA[
arXiv:2507.23208v1 Announce Type: cross 
Abstract: Can a recommendation model be self-aware? This paper investigates the recommender's self-awareness by quantifying its uncertainty, which provides a label-free estimation of its performance. Such self-assessment can enable more informed understanding and decision-making before the recommender engages with any users. To this end, we propose an intuitive and effective method, probability-based List Distribution uncertainty (LiDu). LiDu measures uncertainty by determining the probability that a recommender will generate a certain ranking list based on the prediction distributions of individual items. We validate LiDu's ability to represent model self-awareness in two settings: (1) with a matrix factorization model on a synthetic dataset, and (2) with popular recommendation algorithms on real-world datasets. Experimental results show that LiDu is more correlated with recommendation performance than a series of label-free performance estimators. Additionally, LiDu provides valuable insights into the dynamic inner states of models throughout training and inference. This work establishes an empirical connection between recommendation uncertainty and performance, framing it as a step towards more transparent and self-evaluating recommender systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Just What, But When: Integrating Irregular Intervals to LLM for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2507.23209</link>
<guid>https://arxiv.org/abs/2507.23209</guid>
<content:encoded><![CDATA[
arXiv:2507.23209v1 Announce Type: cross 
Abstract: Time intervals between purchasing items are a crucial factor in sequential recommendation tasks, whereas existing approaches focus on item sequences and often overlook by assuming the intervals between items are static. However, dynamic intervals serve as a dimension that describes user profiling on not only the history within a user but also different users with the same item history. In this work, we propose IntervalLLM, a novel framework that integrates interval information into LLM and incorporates the novel interval-infused attention to jointly consider information of items and intervals. Furthermore, unlike prior studies that address the cold-start scenario only from the perspectives of users and items, we introduce a new viewpoint: the interval perspective to serve as an additional metric for evaluating recommendation methods on the warm and cold scenarios. Extensive experiments on 3 benchmarks with both traditional- and LLM-based baselines demonstrate that our IntervalLLM achieves not only 4.4% improvements in average but also the best-performing warm and cold scenarios across all users, items, and the proposed interval perspectives. In addition, we observe that the cold scenario from the interval perspective experiences the most significant performance drop among all recommendation methods. This finding underscores the necessity of further research on interval-based cold challenges and our integration of interval information in the realm of sequential recommendation tasks. Our code is available here: https://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.23220</link>
<guid>https://arxiv.org/abs/2507.23220</guid>
<content:encoded><![CDATA[
arXiv:2507.23220v1 Announce Type: cross 
Abstract: Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs</title>
<link>https://arxiv.org/abs/2507.23227</link>
<guid>https://arxiv.org/abs/2507.23227</guid>
<content:encoded><![CDATA[
arXiv:2507.23227v1 Announce Type: cross 
Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents</title>
<link>https://arxiv.org/abs/2507.23242</link>
<guid>https://arxiv.org/abs/2507.23242</guid>
<content:encoded><![CDATA[
arXiv:2507.23242v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3 for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis</title>
<link>https://arxiv.org/abs/2507.23248</link>
<guid>https://arxiv.org/abs/2507.23248</guid>
<content:encoded><![CDATA[
arXiv:2507.23248v1 Announce Type: cross 
Abstract: Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at https://github.com/BengaliAI/bn-llm-benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-based inference for Precision Neutrino Physics through Neural Monte Carlo tuning</title>
<link>https://arxiv.org/abs/2507.23297</link>
<guid>https://arxiv.org/abs/2507.23297</guid>
<content:encoded><![CDATA[
arXiv:2507.23297v1 Announce Type: cross 
Abstract: Precise modeling of detector energy response is crucial for next-generation neutrino experiments which present computational challenges due to lack of analytical likelihoods. We propose a solution using neural likelihood estimation within the simulation-based inference framework. We develop two complementary neural density estimators that model likelihoods of calibration data: conditional normalizing flows and a transformer-based regressor. We adopt JUNO - a large neutrino experiment - as a case study. The energy response of JUNO depends on several parameters, all of which should be tuned, given their non-linear behavior and strong correlations in the calibration data. To this end, we integrate the modeled likelihoods with Bayesian nested sampling for parameter inference, achieving uncertainties limited only by statistics with near-zero systematic biases. The normalizing flows model enables unbinned likelihood analysis, while the transformer provides an efficient binned alternative. By providing both options, our framework offers flexibility to choose the most appropriate method for specific needs. Finally, our approach establishes a template for similar applications across experimental neutrino and broader particle physics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification</title>
<link>https://arxiv.org/abs/2507.23315</link>
<guid>https://arxiv.org/abs/2507.23315</guid>
<content:encoded><![CDATA[
arXiv:2507.23315v1 Announce Type: cross 
Abstract: Lightweight convolutional and transformer-based models have become vital for real-time image classification in resource-constrained applications, such as embedded systems and edge devices. This work analyzes the influence of hyperparameter adjustment on the accuracy and convergence behavior of seven efficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT v2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are trained on the ImageNet-1K dataset under consistent training settings, with an emphasis on real-time practicality. An comprehensive ablation study is undertaken to separate the effect of critical hyperparameters, including learning rate schedules, batch sizes, input resolution, data augmentation, regularization approaches, and optimizer choice. To assess appropriateness for real-time applications, each model is assessed not only in terms of Top-1 and Top-5 classification accuracy, but also in terms of inference time, parameter count, model size, and frames-per-second (FPS) on a GPU-accelerated edge deployment simulation. Results demonstrate that cosine learning rate decay and adjustable batch size may greatly boost both accuracy and convergence speed, while keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80% Top-1 accuracy with efficient inference performance, offering a compelling balance between accuracy and deployment cost for VGG-style models. The results give practical guidance for constructing resource-efficient deep learning models appropriate for real-time image processing pipelines. All code and training logs are publicly accessible at https://github.com/VineetKumarRakesh/lcnn-opt.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[
arXiv:2507.23334v1 Announce Type: cross 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23348</link>
<guid>https://arxiv.org/abs/2507.23348</guid>
<content:encoded><![CDATA[
arXiv:2507.23348v1 Announce Type: cross 
Abstract: Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized Treatment Rules</title>
<link>https://arxiv.org/abs/2507.23349</link>
<guid>https://arxiv.org/abs/2507.23349</guid>
<content:encoded><![CDATA[
arXiv:2507.23349v1 Announce Type: cross 
Abstract: Individualized treatment rules (ITRs) have gained significant attention due to their wide-ranging applications in fields such as precision medicine, ridesharing, and advertising recommendations. However, when ITRs are influenced by sensitive attributes such as race, gender, or age, they can lead to outcomes where certain groups are unfairly advantaged or disadvantaged. To address this gap, we propose a flexible approach based on the optimal transport theory, which is capable of transforming any optimal ITR into a fair ITR that ensures demographic parity. Recognizing the potential loss of value under fairness constraints, we introduce an ``improved trade-off ITR," designed to balance value optimization and fairness while accommodating varying levels of fairness through parameter adjustment. To maximize the value of the improved trade-off ITR under specific fairness levels, we propose a smoothed fairness constraint for estimating the adjustable parameter. Additionally, we establish a theoretical upper bound on the value loss for the improved trade-off ITR. We demonstrate performance of the proposed method through extensive simulation studies and application to the Next 36 entrepreneurial program dataset.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Exp: Experience-Driven Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23361</link>
<guid>https://arxiv.org/abs/2507.23361</guid>
<content:encoded><![CDATA[
arXiv:2507.23361v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGA: An adaptive group alignment framework for structured medical cross-modal representation learning</title>
<link>https://arxiv.org/abs/2507.23402</link>
<guid>https://arxiv.org/abs/2507.23402</guid>
<content:encoded><![CDATA[
arXiv:2507.23402v1 Announce Type: cross 
Abstract: Learning medical visual representations from paired images and reports is a promising direction in representation learning. However, current vision-language pretraining methods in the medical domain often simplify clinical reports into single entities or fragmented tokens, ignoring their inherent structure. In addition, contrastive learning frameworks typically depend on large quantities of hard negative samples, which is impractical for small-scale medical datasets. To tackle these challenges, we propose Adaptive Grouped Alignment (AGA), a new framework that captures structured semantics from paired medical images and reports. AGA introduces a bidirectional grouping mechanism based on a sparse similarity matrix. For each image-report pair, we compute fine-grained similarities between text tokens and image patches. Each token selects its top-matching patches to form a visual group, and each patch selects its most related tokens to form a language group. To enable adaptive grouping, we design two threshold gating modules, called Language Grouped Threshold Gate and Vision Grouped Threshold Gate, which learn grouping thresholds dynamically. Group representations are computed as weighted averages based on similarity scores. To align each token with its group representation, we introduce an Instance Aware Group Alignment loss that operates within each image-text pair, removing the need for external negatives. Finally, a Bidirectional Cross-modal Grouped Alignment module is applied to enhance fine-grained alignment between visual and linguistic group representations. Extensive experiments on public and private datasets show that our method achieves strong performance on image-text retrieval and classification tasks under both fine-tuning and zero-shot settings.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning</title>
<link>https://arxiv.org/abs/2507.23416</link>
<guid>https://arxiv.org/abs/2507.23416</guid>
<content:encoded><![CDATA[
arXiv:2507.23416v1 Announce Type: cross 
Abstract: This paper aims to develop a machine learning-based system for automatically detecting honey adulteration with sugar syrup, based on honey hyperspectral imaging data. First, the floral source of a honey sample is classified by a botanical origin identification subsystem. Then, the sugar syrup adulteration is identified, and its concentration is quantified by an adulteration detection subsystem. Both subsystems consist of two steps. The first step involves extracting relevant features from the honey sample using Linear Discriminant Analysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN) model to classify the honey botanical origin in the first subsystem and identify the adulteration level in the second subsystem. We assess the proposed system performance on a public honey hyperspectral image dataset. The result indicates that the proposed system can detect adulteration in honey with an overall cross-validation accuracy of 96.39%, making it an appropriate alternative to the current chemical-based detection methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models</title>
<link>https://arxiv.org/abs/2507.23443</link>
<guid>https://arxiv.org/abs/2507.23443</guid>
<content:encoded><![CDATA[
arXiv:2507.23443v1 Announce Type: cross 
Abstract: We introduce an adjoint-based aerodynamic shape optimization framework that integrates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes. This manifold is enforced as an equality constraint to the shape optimization problem. Central to our method is the computation of adjoint gradients of the design objectives (e.g., drag and lift) with respect to the manifold space. These gradients are derived by first computing shape derivatives with respect to conventional shape design parameters (e.g., Hicks-Henne parameters) and then backpropagating them through the diffusion model to its latent space via automatic differentiation. Our framework preserves mathematical rigor and can be integrated into existing adjoint-based design workflows with minimal modification. Demonstrated on extensive transonic RANS airfoil design cases using off-the-shelf and general-purpose nonlinear optimizers, our approach eliminates ad hoc parameter tuning and variable scaling, maintains robustness across initialization and optimizer choices, and achieves superior aerodynamic performance compared to conventional approaches. This work establishes how AI generated priors integrates effectively with adjoint methods to enable robust, high-fidelity aerodynamic shape optimization through automatic differentiation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning and machine learned prediction in chest X-ray images</title>
<link>https://arxiv.org/abs/2507.23455</link>
<guid>https://arxiv.org/abs/2507.23455</guid>
<content:encoded><![CDATA[
arXiv:2507.23455v1 Announce Type: cross 
Abstract: Machine learning and artificial intelligence are fast-growing fields of research in which data is used to train algorithms, learn patterns, and make predictions. This approach helps to solve seemingly intricate problems with significant accuracy without explicit programming by recognizing complex relationships in data. Taking an example of 5824 chest X-ray images, we implement two machine learning algorithms, namely, a baseline convolutional neural network (CNN) and a DenseNet-121, and present our analysis in making machine-learned predictions in predicting patients with ailments. Both baseline CNN and DenseNet-121 perform very well in the binary classification problem presented in this work. Gradient-weighted class activation mapping shows that DenseNet-121 correctly focuses on essential parts of the input chest X-ray images in its decision-making more than the baseline CNN.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>
<link>https://arxiv.org/abs/2507.23523</link>
<guid>https://arxiv.org/abs/2507.23523</guid>
<content:encoded><![CDATA[
arXiv:2507.23523v1 Announce Type: cross 
Abstract: Imitation learning for robotic manipulation faces a fundamental challenge: the scarcity of large-scale, high-quality robot demonstration data. Recent robotic foundation models often pre-train on cross-embodiment robot datasets to increase data scale, while they face significant limitations as the diverse morphologies and action spaces across different robot embodiments make unified training challenging. In this paper, we present H-RDT (Human to Robotics Diffusion Transformer), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities. Our key insight is that large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors that capture natural manipulation strategies and can benefit robotic policy learning. We introduce a two-stage training paradigm: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders. Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions. Extensive evaluations encompassing both simulation and real-world experiments, single-task and multitask scenarios, as well as few-shot learning and robustness assessments, demonstrate that H-RDT outperforms training from scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving significant improvements of 13.9% and 40.5% over training from scratch in simulation and real-world experiments, respectively. The results validate our core hypothesis that human manipulation data can serve as a powerful foundation for learning bimanual robotic manipulation policies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Point Matching</title>
<link>https://arxiv.org/abs/2507.23609</link>
<guid>https://arxiv.org/abs/2507.23609</guid>
<content:encoded><![CDATA[
arXiv:2507.23609v1 Announce Type: cross 
Abstract: This study demonstrates that incorporating a consistency heuristic into the point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness in matching anatomical locations across pairs of medical images. We validated our approach on diverse longitudinal internal and public datasets spanning CT and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep Lesion Tracking dataset. Additionally, we show that the method effectively addresses landmark localization. The algorithm operates efficiently on standard CPU hardware and allows configurable trade-offs between speed and robustness. The method enables high-precision navigation between medical images without requiring a machine learning model or training data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DivControl: Knowledge Diversion for Controllable Image Generation</title>
<link>https://arxiv.org/abs/2507.23620</link>
<guid>https://arxiv.org/abs/2507.23620</guid>
<content:encoded><![CDATA[
arXiv:2507.23620v1 Announce Type: cross 
Abstract: Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2507.23673</link>
<guid>https://arxiv.org/abs/2507.23673</guid>
<content:encoded><![CDATA[
arXiv:2507.23673v1 Announce Type: cross 
Abstract: Hyperspectral imaging (HSI) provides rich spectral information for medical imaging, yet encounters significant challenges due to data limitations and hardware variations. We introduce SAMSA, a novel interactive segmentation framework that combines an RGB foundation model with spectral analysis. SAMSA efficiently utilizes user clicks to guide both RGB segmentation and spectral similarity computations. The method addresses key limitations in HSI segmentation through a unique spectral feature fusion strategy that operates independently of spectral band count and resolution. Performance evaluation on publicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a neurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative porcine hyperspectral dataset. Experimental results demonstrate SAMSA's effectiveness in few-shot and zero-shot learning scenarios and using minimal training examples. Our approach enables seamless integration of datasets with different spectral characteristics, providing a flexible framework for hyperspectral medical image analysis.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2507.23682</link>
<guid>https://arxiv.org/abs/2507.23682</guid>
<content:encoded><![CDATA[
arXiv:2507.23682v1 Announce Type: cross 
Abstract: Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction</title>
<link>https://arxiv.org/abs/2507.23736</link>
<guid>https://arxiv.org/abs/2507.23736</guid>
<content:encoded><![CDATA[
arXiv:2507.23736v1 Announce Type: cross 
Abstract: Access to medical imaging and associated text data has the potential to drive major advances in healthcare research and patient outcomes. However, the presence of Protected Health Information (PHI) and Personally Identifiable Information (PII) in Digital Imaging and Communications in Medicine (DICOM) files presents a significant barrier to the ethical and secure sharing of imaging datasets. This paper presents a hybrid de-identification framework developed by Impact Business Information Solutions (IBIS) that combines rule-based and AI-driven techniques, and rigorous uncertainty quantification for comprehensive PHI/PII removal from both metadata and pixel data.
  Our approach begins with a two-tiered rule-based system targeting explicit and inferred metadata elements, further augmented by a large language model (LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of synthetic datasets simulating realistic clinical PHI/PII. For pixel data, we employ an uncertainty-aware Faster R-CNN model to localize embedded text, extract candidate PHI via Optical Character Recognition (OCR), and apply the NER pipeline for final redaction. Crucially, uncertainty quantification provides confidence measures for AI-based detections to enhance automation reliability and enable informed human-in-the-loop verification to manage residual risks.
  This uncertainty-aware deidentification framework achieves robust performance across benchmark datasets and regulatory standards, including DICOM, HIPAA, and TCIA compliance metrics. By combining scalable automation, uncertainty quantification, and rigorous quality assurance, our solution addresses critical challenges in medical data de-identification and supports the secure, ethical, and trustworthy release of imaging data for research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.23740</link>
<guid>https://arxiv.org/abs/2507.23740</guid>
<content:encoded><![CDATA[
arXiv:2507.23740v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaled Beta Models and Feature Dilution for Dynamic Ticket Pricing</title>
<link>https://arxiv.org/abs/2507.23767</link>
<guid>https://arxiv.org/abs/2507.23767</guid>
<content:encoded><![CDATA[
arXiv:2507.23767v1 Announce Type: cross 
Abstract: A novel approach is presented for identifying distinct signatures of performing acts in the secondary ticket resale market by analyzing dynamic pricing distributions. Using a newly curated, time series dataset from the SeatGeek API, we model ticket pricing distributions as scaled Beta distributions. This enables accurate parameter estimation from incomplete statistical data using a hybrid of quantile matching and the method of moments. Incorporating the estimated $\alpha$ and $\beta$ parameters into Random Forest classifiers significantly improves pairwise artist classification accuracy, demonstrating the unique economic signatures in event pricing data. Additionally, we provide theoretical and empirical evidence that incorporating zero-variance (constant-value) features into Random Forest models acts as an implicit regularizer, enhancing feature variety and robustness. This regularization promotes deeper, more varied trees in the ensemble, improving the bias-variance tradeoff and mitigating overfitting to dominant features. These findings are validated on both the new ticket pricing dataset and the standard UCI ML handwritten digits dataset.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Bayesian Transfer Learning via the Total Risk Prior</title>
<link>https://arxiv.org/abs/2507.23768</link>
<guid>https://arxiv.org/abs/2507.23768</guid>
<content:encoded><![CDATA[
arXiv:2507.23768v1 Announce Type: cross 
Abstract: In analyses with severe data-limitations, augmenting the target dataset with information from ancillary datasets in the application domain, called source datasets, can lead to significantly improved statistical procedures. However, existing methods for this transfer learning struggle to deal with situations where the source datasets are also limited and not guaranteed to be well-aligned with the target dataset. A typical strategy is to use the empirical loss minimizer on the source data as a prior mean for the target parameters, which places the estimation of source parameters outside of the Bayesian formalism. Our key conceptual contribution is to use a risk minimizer conditional on source parameters instead. This allows us to construct a single joint prior distribution for all parameters from the source datasets as well as the target dataset. As a consequence, we benefit from full Bayesian uncertainty quantification and can perform model averaging via Gibbs sampling over indicator variables governing the inclusion of each source dataset. We show how a particular instantiation of our prior leads to a Bayesian Lasso in a transformed coordinate system and discuss computational techniques to scale our approach to moderately sized datasets. We also demonstrate that recently proposed minimax-frequentist transfer learning techniques may be viewed as an approximate Maximum a Posteriori approach to our model. Finally, we demonstrate superior predictive performance relative to the frequentist baseline on a genetics application, especially when the source data are limited.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</title>
<link>https://arxiv.org/abs/2507.23773</link>
<guid>https://arxiv.org/abs/2507.23773</guid>
<content:encoded><![CDATA[
arXiv:2507.23773v1 Announce Type: cross 
Abstract: AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \modelname improves the success of flight search from 0\% to 32.2\%. World-model-based planning, in particular, shows consistent advantage of up to 124\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \modelname with pretrained LLMs, available as a research demo for public testing.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding</title>
<link>https://arxiv.org/abs/2507.23777</link>
<guid>https://arxiv.org/abs/2507.23777</guid>
<content:encoded><![CDATA[
arXiv:2507.23777v1 Announce Type: cross 
Abstract: Current auto-regressive models can generate high-quality, topologically precise meshes; however, they necessitate thousands-or even tens of thousands-of next-token predictions during inference, resulting in substantial latency. We introduce XSpecMesh, a quality-preserving acceleration method for auto-regressive mesh generation models. XSpecMesh employs a lightweight, multi-head speculative decoding scheme to predict multiple tokens in parallel within a single forward pass, thereby accelerating inference. We further propose a verification and resampling strategy: the backbone model verifies each predicted token and resamples any tokens that do not meet the quality criteria. In addition, we propose a distillation strategy that trains the lightweight decoding heads by distilling from the backbone model, encouraging their prediction distributions to align and improving the success rate of speculative predictions. Extensive experiments demonstrate that our method achieves a 1.7x speedup without sacrificing generation quality. Our code will be released.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions</title>
<link>https://arxiv.org/abs/2507.23784</link>
<guid>https://arxiv.org/abs/2507.23784</guid>
<content:encoded><![CDATA[
arXiv:2507.23784v1 Announce Type: cross 
Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disparate Conditional Prediction in Multiclass Classifiers</title>
<link>https://arxiv.org/abs/2206.03234</link>
<guid>https://arxiv.org/abs/2206.03234</guid>
<content:encoded><![CDATA[
arXiv:2206.03234v3 Announce Type: replace 
Abstract: We propose methods for auditing multiclass classifiers for fairness under multiclass equalized odds,by estimating the deviation from equalized odds when the classifier is not completely fair. We generalize to multiclass classifiers the measure of Disparate Conditional Prediction (DCP), originally suggested by Sabato & Yom-Tov (2020) for binary classifiers. DCP is defined as the fraction of the population for which the classifier predicts with conditional prediction probabilities that differ from the closest common baseline. We provide new local-optimization methods for estimating the multiclass DCPunder two different regimes,one in which the conditional confusion matrices for each protected sub-population are known, and one in which these cannot be estimated, for instance, because the classifier is inaccessible or because good-quality individual-level data is not available. These methods can be used to detect classifiers that likely treat a significant fraction of the population unfairly. Experiments demonstrate the accuracy of the methods. Code is provided at https://github.com/sivansabato/ DCPmulticlass.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights into Closed-form IPM-GAN Discriminator Guidance for Diffusion Modeling</title>
<link>https://arxiv.org/abs/2306.01654</link>
<guid>https://arxiv.org/abs/2306.01654</guid>
<content:encoded><![CDATA[
arXiv:2306.01654v2 Announce Type: replace 
Abstract: Diffusion models are a state-of-the-art generative modeling framework that transform noise to images via Langevin sampling, guided by the score, which is the gradient of the logarithm of the data distribution. Recent works have shown empirically that the generation quality can be improved when guided by classifier network, which is typically the discriminator trained in a generative adversarial network (GAN) setting. In this paper, we propose a theoretical framework to analyze the effect of the GAN discriminator on Langevin-based sampling, and show that the IPM-GAN optimization can be seen as one of smoothed score-matching, wherein the scores of the data and the generator distributions are convolved with the kernel function associated with the IPM. The proposed approach serves to unify score-based training and optimization of IPM-GANs. Based on these insights, we demonstrate that closed-form kernel-based discriminator guidance, results in improvements (in terms of CLIP-FID and KID metrics) when applied atop baseline diffusion models. We demonstrate these results on the denoising diffusion implicit model (DDIM) and latent diffusion model (LDM) settings on various standard datasets. We also show that the proposed approach can be combined with existing accelerated-diffusion techniques to improve latent-space image generation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal and Near-Optimal Adaptive Vector Quantization</title>
<link>https://arxiv.org/abs/2402.03158</link>
<guid>https://arxiv.org/abs/2402.03158</guid>
<content:encoded><![CDATA[
arXiv:2402.03158v2 Announce Type: replace 
Abstract: Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.
  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Molecule Graph Networks with Many-body Equivariant Interactions</title>
<link>https://arxiv.org/abs/2406.13265</link>
<guid>https://arxiv.org/abs/2406.13265</guid>
<content:encoded><![CDATA[
arXiv:2406.13265v3 Announce Type: replace 
Abstract: Message passing neural networks have demonstrated significant efficacy in predicting molecular interactions. Introducing equivariant vectorial representations augments expressivity by capturing geometric data symmetries, thereby improving model accuracy. However, two-body bond vectors in opposition may cancel each other out during message passing, leading to the loss of directional information on their shared node. In this study, we develop Equivariant N-body Interaction Networks (ENINet) that explicitly integrates l = 1 equivariant many-body interactions to enhance directional symmetric information in the message passing scheme. We provided a mathematical analysis demonstrating the necessity of incorporating many-body equivariant interactions and generalized the formulation to $N$-body interactions. Experiments indicate that integrating many-body equivariant representations enhances prediction accuracy across diverse scalar and tensorial quantum chemical properties.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering interventional dynamical causality from non-intervention complex systems</title>
<link>https://arxiv.org/abs/2407.01621</link>
<guid>https://arxiv.org/abs/2407.01621</guid>
<content:encoded><![CDATA[
arXiv:2407.01621v2 Announce Type: replace 
Abstract: Detecting and quantifying causality is a focal topic in the fields of science, engineering, and interdisciplinary studies. However, causal studies on non-intervention systems attract much attention but remain extremely challenging. Delay-embedding technique provides a promising approach. In this study, we propose a framework named Interventional Dynamical Causality (IntDC) in contrast to the traditional Constructive Dynamical Causality (ConDC). ConDC, including Granger causality, transfer entropy and convergence of cross-mapping, measures the causality by constructing a dynamical model without considering interventions. A computational criterion, Interventional Embedding Entropy (IEE), is proposed to measure causal strengths in an interventional manner. IEE is an intervened causal information flow but in the delay-embedding space. Further, the IEE theoretically and numerically enables the deciphering of IntDC solely from observational (non-interventional) time-series data, without requiring any knowledge of dynamical models or real interventions in the considered system. In particular, IEE can be applied to rank causal effects according to their importance and construct causal networks from data. We conducted numerical experiments to demonstrate that IEE can find causal edges accurately, eliminate effects of confounding, and quantify causal strength robustly over traditional indices. We also applied IEE to real-world tasks. IEE performed as an accurate and robust tool for causal analyses solely from the observational data. The IntDC framework and IEE algorithm provide an efficient approach to the study of causality from time series in diverse non-intervention complex systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios</title>
<link>https://arxiv.org/abs/2407.03080</link>
<guid>https://arxiv.org/abs/2407.03080</guid>
<content:encoded><![CDATA[
arXiv:2407.03080v2 Announce Type: replace 
Abstract: While synthetic tabular data generation using Deep Generative Models (DGMs) offers a compelling solution to data scarcity and privacy concerns, their effectiveness relies on the availability of substantial training data, often lacking in real-world scenarios. To overcome this limitation, we propose a novel methodology that explicitly integrates artificial inductive biases into the generative process to improve data quality in low-data regimes. Our framework leverages transfer learning and meta-learning techniques to construct and inject informative inductive biases into DGMs. We evaluate four approaches (pre-training, model averaging, Model-Agnostic Meta-Learning (MAML), and Domain Randomized Search (DRS)) and analyze their impact on the quality of the generated text. Experimental results show that incorporating inductive bias substantially improves performance, with transfer learning methods outperforming meta-learning, achieving up to 60\% gains in Jensen-Shannon divergence. The methodology is model-agnostic and especially relevant in domains such as healthcare and finance, where high-quality synthetic data are essential, and data availability is often limited.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Split Learning with Global Sampling</title>
<link>https://arxiv.org/abs/2407.15738</link>
<guid>https://arxiv.org/abs/2407.15738</guid>
<content:encoded><![CDATA[
arXiv:2407.15738v4 Announce Type: replace 
Abstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse Graining with Neural Operators for Simulating Chaotic Systems</title>
<link>https://arxiv.org/abs/2408.05177</link>
<guid>https://arxiv.org/abs/2408.05177</guid>
<content:encoded><![CDATA[
arXiv:2408.05177v5 Announce Type: replace 
Abstract: Accurately predicting the long-term behavior of chaotic systems is crucial for various applications such as climate modeling. However, achieving such predictions typically requires iterative computations over a dense spatiotemporal grid to account for the unstable nature of chaotic systems, which is expensive and impractical in many real-world situations. An alternative approach to such a full-resolved simulation is using a coarse grid and then correcting its errors through a \textit{closure model}, which approximates the overall information from fine scales not captured in the coarse-grid simulation. Recently, ML approaches have been used for closure modeling, but they typically require a large number of training samples from expensive fully-resolved simulations (FRS). In this work, we prove an even more fundamental limitation, i.e., the standard approach to learning closure models suffers from a large approximation error for generic problems, no matter how large the model is, and it stems from the non-uniqueness of the mapping. We propose an alternative end-to-end learning approach using a physics-informed neural operator (PINO) that overcomes this limitation by not using a closure model or a coarse-grid solver. We first train the PINO model on data from a coarse-grid solver and then fine-tune it with (a small amount of) FRS and physics-based losses on a fine grid. The discretization-free nature of neural operators means that they do not suffer from the restriction of a coarse grid that closure models face, and they can provably approximate the long-term statistics of chaotic systems. In our experiments, our PINO model achieves a 330x speedup compared to FRS with a relative error $\sim 10\%$. In contrast, the closure model coupled with a coarse-grid solver is $60$x slower than PINO while having a much higher error $\sim186\%$ when the closure model is trained on the same FRS dataset.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Approximation of Stationary Processes using the ARMA Model</title>
<link>https://arxiv.org/abs/2408.10610</link>
<guid>https://arxiv.org/abs/2408.10610</guid>
<content:encoded><![CDATA[
arXiv:2408.10610v4 Announce Type: replace 
Abstract: We look at a problem related to Autoregressive Moving Average (ARMA) models, on quantifying the approximation error between a true stationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function representation $x(L)$ of a stationary process $X_t$ and show that the $L^{\infty}$ norm of $x$ acts as a valid norm on $X_t$ that controls the $\ell^2$ norm of its Wold coefficients. We then show that a certain subspace of stationary processes, which includes ARMA models, forms a Banach algebra under the $L^{\infty}$ norm that respects the multiplicative structure of $H^{\infty}$ transfer functions and thus improves on the structural properties of the cepstral norm for ARMA models. The natural definition of invertibility in this algebra is consistent with the original definition of ARMA invertibility, and generalizes better to non-ARMA processes than Wiener's $\ell^1$ condition. Finally, we calculate some explicit approximation bounds in the simpler context of continuous transfer functions, and critique some heuristic ideas on Pad\'e approximations and parsimonious models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accumulator-Aware Post-Training Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2409.17092</link>
<guid>https://arxiv.org/abs/2409.17092</guid>
<content:encoded><![CDATA[
arXiv:2409.17092v2 Announce Type: replace 
Abstract: When quantizing weights and activations to increasingly narrower representations, the cost of additions begins to dominate that of multiplications in multiply-accumulate (MAC) units. Recent studies show that reducing addition costs via low-precision accumulation improves throughput, power, and area across inference platforms, albeit with an increased risk of overflow. Accumulator-aware quantization research has so far only considered the quantization-aware training (QAT) paradigm, in which models are fine-tuned or trained from scratch with quantization in the loop. As models and datasets continue to grow in size, QAT techniques become increasingly more expensive, which has motivated the recent surge in post-training quantization (PTQ) research. To bridge this gap, we introduce AXE, the first accumulator-aware quantization framework explicitly designed to endow overflow avoidance guarantees to PTQ algorithms. We present theoretical motivation for AXE and demonstrate its flexibility by implementing it on top of two existing algorithms: GPFQ and OPTQ. We design AXE to support multi-stage accumulation, opening the door to full datapath optimization for the first time. We evaluate AXE using recent language generation models; when quantizing Llama3 8B for a 16-bit multi-stage accumulation datapath, AXE maintains up to 98% of the FP16 perplexity, surpassing naive bit width manipulation by up to 15%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.09486</link>
<guid>https://arxiv.org/abs/2410.09486</guid>
<content:encoded><![CDATA[
arXiv:2410.09486v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially unsafe, interactions with their environments to learn effectively. These limitations confine RL agents to simulated environments, hindering their ability to learn directly in real-world settings. In this work, we present ActSafe, a novel model-based RL algorithm for safe and efficient exploration. ActSafe learns a well-calibrated probabilistic model of the system and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing pessimism w.r.t. the safety constraints. Under regularity assumptions on the constraints and dynamics, we show that ActSafe guarantees safety during learning while also obtaining a near-optimal policy in finite time. In addition, we propose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such as visual control. We empirically show that ActSafe obtains state-of-the-art performance in difficult exploration tasks on standard safe deep RL benchmarks while ensuring safety during learning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electricity Price Prediction Using Multi-Kernel Gaussian Process Regression Combined with Kernel-Based Support Vector Regression</title>
<link>https://arxiv.org/abs/2412.00123</link>
<guid>https://arxiv.org/abs/2412.00123</guid>
<content:encoded><![CDATA[
arXiv:2412.00123v4 Announce Type: replace 
Abstract: This paper presents a new hybrid model for predicting German electricity prices. The algorithm is based on a combination of Gaussian Process Regression (GPR) and Support Vector Regression (SVR). Although GPR is a competent model for learning stochastic patterns within data and for interpolation, its performance for out-of-sample data is not very promising. By choosing a suitable data-dependent covariance function, we can enhance the performance of GPR for the German hourly power prices being tested. However, since the out-of-sample prediction is dependent on the training data, the prediction is vulnerable to noise and outliers. To overcome this issue, a separate prediction is calculated using SVR, which applies margin-based optimization. This method is advantageous when dealing with non-linear processes and outliers, since only certain necessary points (support vectors) in the training data are responsible for regression. The individual predictions are then linearly combined using uniform weights. When tested on historic German power prices, this approach outperforms the publicly available benchmarks, namely the LASSO estimated autoregressive regression model, deep neural network provided in the recent research by [1].
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization</title>
<link>https://arxiv.org/abs/2412.12098</link>
<guid>https://arxiv.org/abs/2412.12098</guid>
<content:encoded><![CDATA[
arXiv:2412.12098v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfAlign: Inference-aware language model alignment</title>
<link>https://arxiv.org/abs/2412.19792</link>
<guid>https://arxiv.org/abs/2412.19792</guid>
<content:encoded><![CDATA[
arXiv:2412.19792v4 Announce Type: replace 
Abstract: Language model alignment is a critical step in training modern generative language models. Alignment targets to improve win rate of a sample from the aligned model against the base model. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-time methods. To this end, we propose a framework for inference-aware alignment (InfAlign), which aims to optimize inference-time win rate of the aligned policy against the base model. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a transformation of the reward. This motivates us to provide the calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. For best-of-N sampling and best-of-N jailbreaking, we propose specific transformations offering up to 3-8% improvement on inference-time win rates. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models</title>
<link>https://arxiv.org/abs/2501.08727</link>
<guid>https://arxiv.org/abs/2501.08727</guid>
<content:encoded><![CDATA[
arXiv:2501.08727v2 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an increasingly popular technique with many applications. Among the various PEFT methods, Low-Rank Adaptation (LoRA) and its variants have gained significant attention due to their effectiveness, enabling users to fine-tune models with limited computational resources. However, the approximation gap between the low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter-efficiency and better performance. To reduce this gap and further improve the power of LoRA, we propose a new PEFT method that combines two classes of adaptations, namely, transform and residual adaptations. In specific, we first apply a full-rank and dense transform to the pre-trained weight. This learnable transform is expected to align the pre-trained weight as closely as possible to the desired weight, thereby reducing the rank of the residual weight. Then, the residual part can be effectively approximated by more compact and parameter-efficient structures, with a smaller approximation error. To achieve ultra-parameter-efficiency in practice, we design highly flexible and effective tensor decompositions for both the transform and residual adaptations. Additionally, popular PEFT methods such as DoRA can be summarized under this transform plus residual adaptation scheme. Experiments are conducted on fine-tuning Stable Diffusion models in subject-driven and controllable generation. The results manifest that our method can achieve better performances and parameter efficiency compared to LoRA and several baselines.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles</title>
<link>https://arxiv.org/abs/2501.15544</link>
<guid>https://arxiv.org/abs/2501.15544</guid>
<content:encoded><![CDATA[
arXiv:2501.15544v4 Announce Type: replace 
Abstract: Generative artificial intelligence, particularly through large language models (LLMs), is poised to transform energy optimization and demand side management (DSM) within microgrids. This paper explores the integration of LLMs into energy management, emphasizing their roles in automating the optimization of DSM strategies with Internet of electric vehicles. We investigate challenges and solutions associated with DSM and explore the new opportunities presented by leveraging LLMs. Then, we propose an innovative solution that enhances LLMs with retrieval-augmented generation for automatic problem formulation, code generation, and customizing optimization. We present a case study to demonstrate the effectiveness of our proposed solution in charging scheduling and optimization for electric vehicles, highlighting our solution's significant advancements in energy efficiency and user adaptability. This work underscores the potential of LLMs for energy optimization and fosters a new era of intelligent DSM solutions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Forecasting from Short Time Series via Meta-learning</title>
<link>https://arxiv.org/abs/2501.16325</link>
<guid>https://arxiv.org/abs/2501.16325</guid>
<content:encoded><![CDATA[
arXiv:2501.16325v2 Announce Type: replace 
Abstract: Machine learning models can effectively forecast dynamical systems from time-series data, but they typically require large amounts of past data, making forecasting particularly challenging for systems with limited history. To overcome this, we introduce Meta-learning for Tailored Forecasting using Related Time Series (METAFORS), which generalizes knowledge across systems to enable forecasting in data-limited scenarios. By learning from a library of models trained on longer time series from potentially related systems, METAFORS builds and initializes a model tailored to short time-series data from the system of interest. Using a reservoir computing implementation and testing on simulated chaotic systems, we demonstrate that METAFORS can reliably predict both short-term dynamics and long-term statistics without requiring contextual labels. We see this even when test and related systems exhibit substantially different behaviors, highlighting METAFORS' strengths in data-limited scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Deep Continual Learning via Evolution</title>
<link>https://arxiv.org/abs/2502.06210</link>
<guid>https://arxiv.org/abs/2502.06210</guid>
<content:encoded><![CDATA[
arXiv:2502.06210v2 Announce Type: replace 
Abstract: Deep neural networks, despite their remarkable success, remain fundamentally limited in their ability to perform Continual Learning (CL). While most current methods aim to enhance the capabilities of a single model, Inspired by the collective learning mechanisms of human populations, we introduce Evolving Continual Learning (ECL), a framework that maintains and evolves a diverse population of neural network models. ECL continually searches for an optimal architecture for each introduced incremental task. This tailored model is trained on the corresponding task and archived as a specialized expert, contributing to a growing collection of skills. This approach inherently resolves the core CL challenges: stability is achieved through the isolation of expert models, while plasticity is greatly enhanced by evolving unique, task-specific architectures. Experimental results demonstrate that ECL significantly outperforms state-of-the-art individual-level CL methods. By shifting the focus from individual adaptation to collective evolution, ECL presents a novel path toward AI systems capable of CL.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage</title>
<link>https://arxiv.org/abs/2502.17264</link>
<guid>https://arxiv.org/abs/2502.17264</guid>
<content:encoded><![CDATA[
arXiv:2502.17264v2 Announce Type: replace 
Abstract: Conformal prediction is a powerful distribution-free framework for constructing prediction sets with coverage guarantees. Classical methods, such as split conformal prediction, provide marginal coverage, ensuring that the prediction set contains the label of a random test point with a target probability. However, these guarantees may not hold uniformly across different subpopulations, leading to disparities in coverage. Prior work has explored coverage guarantees conditioned on events related to the covariates and label of the test point. We present Kandinsky conformal prediction, a framework that significantly expands the scope of conditional coverage guarantees. In contrast to Mondrian conformal prediction, which restricts its coverage guarantees to disjoint groups -- reminiscent of the rigid, structured grids of Piet Mondrian's art -- our framework flexibly handles overlapping and fractional group memberships defined jointly on covariates and labels, reflecting the layered, intersecting forms in Wassily Kandinsky's compositions. Our algorithm unifies and extends existing methods, encompassing covariate-based group conditional, class conditional, and Mondrian conformal prediction as special cases, while achieving a minimax-optimal high-probability conditional coverage bound. Finally, we demonstrate the practicality of our approach through empirical evaluation on real-world datasets.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks</title>
<link>https://arxiv.org/abs/2504.10403</link>
<guid>https://arxiv.org/abs/2504.10403</guid>
<content:encoded><![CDATA[
arXiv:2504.10403v3 Announce Type: replace 
Abstract: Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affect Models Have Weak Generalizability to Atypical Speech</title>
<link>https://arxiv.org/abs/2504.16283</link>
<guid>https://arxiv.org/abs/2504.16283</guid>
<content:encoded><![CDATA[
arXiv:2504.16283v2 Announce Type: replace 
Abstract: Speech and voice conditions can alter the acoustic properties of speech, which could impact the performance of paralinguistic models for affect for people with atypical speech. We evaluate publicly available models for recognizing categorical and dimensional affect from speech on a dataset of atypical speech, comparing results to datasets of typical speech. We investigate three dimensions of speech atypicality: intelligibility, which is related to pronounciation; monopitch, which is related to prosody, and harshness, which is related to voice quality. We look at (1) distributional trends of categorical affect predictions within the dataset, (2) distributional comparisons of categorical affect predictions to similar datasets of typical speech, and (3) correlation strengths between text and speech predictions for spontaneous speech for valence and arousal. We find that the output of affect models is significantly impacted by the presence and degree of speech atypicalities. For instance, the percentage of speech predicted as sad is significantly higher for all types and grades of atypical speech when compared to similar typical speech datasets. In a preliminary investigation on improving robustness for atypical speech, we find that fine-tuning models on pseudo-labeled atypical speech data improves performance on atypical speech without impacting performance on typical speech. Our results emphasize the need for broader training and evaluation datasets for speech emotion models, and for modeling approaches that are robust to voice and speech differences.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Divergence: Measuring Fairness in Regression</title>
<link>https://arxiv.org/abs/2505.00830</link>
<guid>https://arxiv.org/abs/2505.00830</guid>
<content:encoded><![CDATA[
arXiv:2505.00830v2 Announce Type: replace 
Abstract: Fairness in machine learning research is commonly framed in the context of classification tasks, leaving critical gaps in regression. In this paper, we propose a novel approach to measure intersectional fairness in regression tasks, going beyond the focus on single protected attributes from existing work to consider combinations of all protected attributes. Furthermore, we contend that it is insufficient to measure the average error of groups without regard for imbalanced domain preferences. Accordingly, we propose Intersectional Divergence (ID) as the first fairness measure for regression tasks that 1) describes fair model behavior across multiple protected attributes and 2) differentiates the impact of predictions in target ranges most relevant to users. We extend our proposal demonstrating how ID can be adapted into a loss function, IDLoss, that satisfies convergence guarantees and has piecewise smooth properties that enable practical optimization. Through an extensive experimental evaluation, we demonstrate how ID allows unique insights into model behavior and fairness, and how incorporating IDLoss into optimization can considerably improve single-attribute and intersectional model fairness while maintaining a competitive balance in predictive performance.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning</title>
<link>https://arxiv.org/abs/2505.05702</link>
<guid>https://arxiv.org/abs/2505.05702</guid>
<content:encoded><![CDATA[
arXiv:2505.05702v3 Announce Type: replace 
Abstract: The absence of intrinsic adjacency relations and orientation systems in hypergraphs creates fundamental challenges for constructing sheaf Laplacians of arbitrary degrees. We resolve these limitations through symmetric simplicial sets derived directly from hypergraphs, called symmetric simplicial lifting, which encode all possible oriented subrelations within each hyperedge as ordered tuples. This construction canonically defines adjacency via facet maps while inherently preserving hyperedge provenance. We establish that the normalized degree zero sheaf Laplacian on our symmetric simplicial lifting reduces exactly to the traditional graph normalized sheaf Laplacian when restricted to graphs, validating its mathematical consistency with prior graph-based sheaf theory. Furthermore, the induced structure preserves all structural information from the original hypergraph, ensuring that every multi-way relational detail is faithfully retained. Leveraging this framework, we introduce Hypergraph Neural Sheaf Diffusion (HNSD), the first principled extension of neural sheaf diffusion to hypergraphs. HNSD operates via normalized degree zero sheaf Laplacian over symmetric simplicial lifting, resolving orientation ambiguity and adjacency sparsity inherent to hypergraph learning. Experimental evaluations demonstrate HNSDs competitive performance across established benchmarks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SinBasis Networks: Matrix-Equivalent Feature Extraction for Wave-Like Optical Spectrograms</title>
<link>https://arxiv.org/abs/2505.06275</link>
<guid>https://arxiv.org/abs/2505.06275</guid>
<content:encoded><![CDATA[
arXiv:2505.06275v2 Announce Type: replace 
Abstract: Wave-like images-from attosecond streaking spectrograms to optical spectra, audio mel-spectrograms and periodic video frames-encode critical harmonic structures that elude conventional feature extractors. We propose a unified, matrix-equivalent framework that reinterprets convolution and attention as linear transforms on flattened inputs, revealing filter weights as basis vectors spanning latent feature subspaces. To infuse spectral priors we apply elementwise $\sin(\cdot)$ mappings to each weight matrix. Embedding these transforms into CNN, ViT and Capsule architectures yields Sin-Basis Networks with heightened sensitivity to periodic motifs and built-in invariance to spatial shifts. Experiments on a diverse collection of wave-like image datasets-including 80,000 synthetic attosecond streaking spectrograms, thousands of Raman, photoluminescence and FTIR spectra, mel-spectrograms from AudioSet and cycle-pattern frames from Kinetics-demonstrate substantial gains in reconstruction accuracy, translational robustness and zero-shot cross-domain transfer. Theoretical analysis via matrix isomorphism and Mercer-kernel truncation quantifies how sinusoidal reparametrization enriches expressivity while preserving stability in data-scarce regimes. Sin-Basis Networks thus offer a lightweight, physics-informed approach to deep learning across all wave-form imaging modalities.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values</title>
<link>https://arxiv.org/abs/2505.07797</link>
<guid>https://arxiv.org/abs/2505.07797</guid>
<content:encoded><![CDATA[
arXiv:2505.07797v2 Announce Type: replace 
Abstract: Reinforcement learning agents can achieve super-human performance in complex decision-making tasks, but their behaviour is often difficult to understand and explain. This lack of explanation limits deployment, especially in safety-critical settings where understanding and trust are essential. We identify three core explanatory targets that together provide a comprehensive view of reinforcement learning agents: behaviour, outcomes, and predictions. We develop a unified theoretical framework for explaining these three elements of reinforcement learning agents through the influence of individual features that the agent observes in its environment. We derive feature influences by using Shapley values, which collectively and uniquely satisfy a set of well-motivated axioms for fair and consistent credit assignment. The proposed approach, Shapley Values for Explaining Reinforcement Learning (SVERL), provides a single theoretical framework to comprehensively and meaningfully explain reinforcement learning agents. It yields explanations with precise semantics that are not only interpretable but also mathematically justified, enabling us to identify and correct conceptual issues in prior explanations. Through illustrative examples, we show how SVERL produces useful, intuitive explanations of agent behaviour, outcomes, and predictions, which are not apparent from observing agent behaviour alone.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[
arXiv:2505.18102v2 Announce Type: replace 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A ZeNN architecture to avoid the Gaussian trap</title>
<link>https://arxiv.org/abs/2505.20553</link>
<guid>https://arxiv.org/abs/2505.20553</guid>
<content:encoded><![CDATA[
arXiv:2505.20553v2 Announce Type: replace 
Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order to overcome several shortcomings of standard multi-layer perceptrons (MLPs). Namely, in the large width limit, MLPs are non-parametric, they do not have a well-defined pointwise limit, they lose non-Gaussian attributes and become unable to perform feature learning; moreover, finite width MLPs perform poorly in learning high frequencies. The new ZeNN architecture is inspired by three simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they exhibit a rich asymptotic structure beyond Gaussianity, and perform feature learning. Moreover, when appropriate activation functions are chosen, (finite width) ZeNNs excel at learning high-frequency features of functions with low dimensional domains.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt before Continual Learning</title>
<link>https://arxiv.org/abs/2506.03956</link>
<guid>https://arxiv.org/abs/2506.03956</guid>
<content:encoded><![CDATA[
arXiv:2506.03956v3 Announce Type: replace 
Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). Although pre-trained models (PTMs) have provided a strong foundation for CL, existing approaches face a fundamental challenge in balancing these two competing objectives. Current methods typically address stability by freezing the PTM backbone, which severely limits the model's plasticity, particularly when incoming data distribution diverges largely from the pre-training data. Alternatively, sequentially fine-tuning the entire PTM can adapt to new knowledge but often leads to catastrophic forgetting, highlighting the critical stability-plasticity trade-off in PTM-based CL. To address this limitation, we propose Adapting PTMs before the core CL} process (ACL), a novel framework that introduces a plug-and-play adaptation phase prior to learning each new task. During this phase, ACL refines the PTM backbone by aligning embeddings with their original class prototypes while distancing them from irrelevant classes. This mechanism theoretically and empirically demonstrates desirable balance between stability and plasticity, significantly improving CL performance across benchmarks and integrated methods. Code is available at https://github.com/byyx666/ACL_code.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrokAlign: Geometric Characterisation and Acceleration of Grokking</title>
<link>https://arxiv.org/abs/2506.12284</link>
<guid>https://arxiv.org/abs/2506.12284</guid>
<content:encoded><![CDATA[
arXiv:2506.12284v2 Announce Type: replace 
Abstract: A key challenge for the machine learning community is to understand and accelerate the training dynamics of deep networks that lead to delayed generalisation and emergent robustness to input perturbations, also known as grokking. Prior work has associated phenomena like delayed generalisation with the transition of a deep network from a linear to a feature learning regime, and emergent robustness with changes to the network's functional geometry, in particular the arrangement of the so-called linear regions in deep networks employing continuous piecewise affine nonlinearities. Here, we explain how grokking is realised in the Jacobian of a deep network and demonstrate that aligning a network's Jacobians with the training data (in the sense of cosine similarity) ensures grokking under a low-rank Jacobian assumption. Our results provide a strong theoretical motivation for the use of Jacobian regularisation in optimizing deep networks -- a method we introduce as GrokAlign -- which we show empirically to induce grokking much sooner than more conventional regularizers like weight decay. Moreover, we introduce centroid alignment as a tractable and interpretable simplification of Jacobian alignment that effectively identifies and tracks the stages of deep network training dynamics. Accompanying webpage (https://thomaswalker1.github.io/blog/grokalign.html) and code (https://github.com/ThomasWalker1/grokalign).
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-dimensional Parallel Tempering for Constrained Optimization</title>
<link>https://arxiv.org/abs/2506.14781</link>
<guid>https://arxiv.org/abs/2506.14781</guid>
<content:encoded><![CDATA[
arXiv:2506.14781v2 Announce Type: replace 
Abstract: Sampling Boltzmann probability distributions plays a key role in machine learning and optimization, motivating the design of hardware accelerators such as Ising machines. While the Ising model can in principle encode arbitrary optimization problems, practical implementations are often hindered by soft constraints that either slow down mixing when too strong, or fail to enforce feasibility when too weak. We introduce a two-dimensional extension of the powerful parallel tempering algorithm (PT) that addresses this challenge by adding a second dimension of replicas interpolating the penalty strengths. This scheme ensures constraint satisfaction in the final replicas, analogous to low-energy states at low temperature. The resulting two-dimensional parallel tempering algorithm (2D-PT) improves mixing in heavily constrained replicas and eliminates the need to explicitly tune the penalty strength. In a representative example of graph sparsification with copy constraints, 2D-PT achieves near-ideal mixing, with Kullback-Leibler divergence decaying as O(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of magnitude speedup over conventional PT with the same number of replicas. The method applies broadly to constrained Ising problems and can be deployed on existing Ising machines.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement</title>
<link>https://arxiv.org/abs/2506.15692</link>
<guid>https://arxiv.org/abs/2506.15692</guid>
<content:encoded><![CDATA[
arXiv:2506.15692v2 Announce Type: replace 
Abstract: Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches to build such agents often rely heavily on inherent LLM knowledge and employ coarse exploration strategies that modify the entire code structure at once. This limits their ability to select effective task-specific models and perform deep exploration within specific components, such as experimenting extensively with feature engineering options. To overcome these, we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first leverages external knowledge by using a search engine to retrieve effective models from the web, forming an initial solution, then iteratively refines it by exploring various strategies targeting specific ML components. This exploration is guided by ablation studies analyzing the impact of individual code blocks. Furthermore, we introduce a novel ensembling method using an effective strategy suggested by MLE-STAR. Our experimental results show that MLE-STAR achieves medals in 64% of the Kaggle competitions on the MLE-bench Lite, significantly outperforming the best alternative.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Learning-Based Virtual Buffering for Analytical Global Placement</title>
<link>https://arxiv.org/abs/2506.17247</link>
<guid>https://arxiv.org/abs/2506.17247</guid>
<content:encoded><![CDATA[
arXiv:2506.17247v2 Announce Type: replace 
Abstract: Due to the skewed scaling of interconnect versus cell delay in modern technology nodes, placement with buffer porosity (i.e., cell density) awareness is essential for timing closure in physical synthesis flows. However, existing approaches face two key challenges: (i) traditional van Ginneken-Lillis-style buffering approaches are computationally expensive during global placement; and (ii) machine learning-based approaches, such as BufFormer, lack a thorough consideration of Electrical Rule Check (ERC) violations and fail to "close the loop" back into the physical design flow. In this work, we propose MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware analytical global placement framework, built on top of the OpenROAD infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based generative buffering approach to predict buffer types and locations, addressing ERC violations during global placement. We compare MLBuf-RePlAce against the default virtual buffering-based timing-driven global placer in OpenROAD, using open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts repositories. Without degradation of post-route power, MLBuf-RePlAce achieves (maximum, average) improvements of (56%, 31%) in total negative slack (TNS) within the open-source OpenROAD flow. When evaluated by completion in a commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of (53%, 28%) in TNS with an average of 0.2% improvement in post-route power.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks</title>
<link>https://arxiv.org/abs/2507.07675</link>
<guid>https://arxiv.org/abs/2507.07675</guid>
<content:encoded><![CDATA[
arXiv:2507.07675v2 Announce Type: replace 
Abstract: We analyze the layerwise effective dimension (rank of the feature matrix) in fully-connected ReLU networks of finite width. Specifically, for a fixed batch of $m$ inputs and random Gaussian weights, we derive closed-form expressions for the expected rank of the \$m\times n\$ hidden activation matrices. Our main result shows that $\mathbb{E}[EDim(\ell)]=m[1-(1-2/\pi)^\ell]+O(e^{-c m})$ so that the rank deficit decays geometrically with ratio $1-2 / \pi \approx 0.3634$. We also prove a sub-Gaussian concentration bound, and identify the "revival" depths at which the expected rank attains local maxima. In particular, these peaks occur at depths $\ell_k^*\approx(k+1/2)\pi/\log(1/\rho)$ with height $\approx (1-e^{-\pi/2}) m \approx 0.79m$. We further show that this oscillatory rank behavior is a finite-width phenomenon: under orthogonal weight initialization or strong negative-slope leaky-ReLU, the rank remains (nearly) full. These results provide a precise characterization of how random ReLU layers alternately collapse and partially revive the subspace of input variations, adding nuance to prior work on expressivity of deep networks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding</title>
<link>https://arxiv.org/abs/2507.09252</link>
<guid>https://arxiv.org/abs/2507.09252</guid>
<content:encoded><![CDATA[
arXiv:2507.09252v2 Announce Type: replace 
Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal point process (TPP) sampling by adapting speculative decoding (SD) techniques from language models. By identifying the structural similarities between thinning algorithms for TPPs and speculative decoding for language models, we develop an efficient sampling framework that leverages a smaller draft model to generate multiple candidate events, which are then verified by the larger target model in parallel. TPP-SD maintains the same output distribution as autoregressive sampling while achieving significant acceleration. Experiments on both synthetic and real datasets demonstrate that our approach produces samples from identical distributions as standard methods, but with 2-6$\times$ speedup. Our ablation studies analyze the impact of hyperparameters such as draft length and draft model size on sampling efficiency. TPP-SD bridges the gap between powerful Transformer TPP models and the practical need for rapid sequence sampling.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolPIF: A Parameter Interpolation Flow Model for Molecule Generation</title>
<link>https://arxiv.org/abs/2507.13762</link>
<guid>https://arxiv.org/abs/2507.13762</guid>
<content:encoded><![CDATA[
arXiv:2507.13762v3 Announce Type: replace 
Abstract: Advances in deep learning for molecular generation show promise in accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown impressive performance across diverse chemical tasks, with their success often ascribed to the paradigm of modeling in a low-variance parameter space. However, the Bayesian inference-based strategy imposes limitations on designing more flexible distribution transformation pathways, making it challenging to adapt to diverse data distributions and varied task requirements. Furthermore, the potential for simpler, more efficient parameter-space-based models is unexplored. To address this, we propose a novel Parameter Interpolation Flow model (named PIF) with detailed theoretical foundation, training, and inference procedures. We then develop MolPIF for structure-based drug design, demonstrating its superior performance across diverse metrics compared to baselines. This work validates the effectiveness of parameter-space-based generative modeling paradigm for molecules and offers new perspectives for model design.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots</title>
<link>https://arxiv.org/abs/2304.01430</link>
<guid>https://arxiv.org/abs/2304.01430</guid>
<content:encoded><![CDATA[
arXiv:2304.01430v3 Announce Type: replace-cross 
Abstract: We investigate the emergence of objects in visual perception in the absence of any semantic annotation. The resulting model has received no supervision, does not use any pre-trained features, and yet it can segment the domain of an image into multiple independently moving regions. The resulting motion segmentation method can handle an unknown and varying number of objects in real-time. The core multi-modal conditional encoder-decoder architecture has one modality (optical flow) feed the encoder to produce a collection of latent codes (slots), and the other modality (color image) conditions the decoder to generate the first modality (flow) from the slots. The training criterion is designed to foster 'information separation' among the slots, while the architecture explicitly allocates activations to individual slots, leading to a method we call Divided Attention (DivA). At test time, DivA handles a different number of objects and different image resolution than seen at training, and is invariant to permutations of the slots. DivA achieves state-of-the-art performance while tripling the runtime speed of comparable methods, up to 104 FPS, and reduces the performance gap from supervised methods to 12% or less. Objects bootstrapped by DivA can then be used to prime static classifiers via contrastive learning. On fewer than 5,000 video clips, training DINO on DivA's object proposals narrows the performance gap to ImageNet-based training by up to 30.2% compared to training directly on the video frames.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted least-squares approximation with determinantal point processes and generalized volume sampling</title>
<link>https://arxiv.org/abs/2312.14057</link>
<guid>https://arxiv.org/abs/2312.14057</guid>
<content:encoded><![CDATA[
arXiv:2312.14057v4 Announce Type: replace-cross 
Abstract: We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\boldsymbol{\varphi}$, using evaluations of the function at random points $x_1, \dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\boldsymbol{\varphi}(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation error in $L^2$ is almost surely bounded by the best approximation error measured in the $H$-norm. This includes the cases of functions from $L^\infty$ or reproducing kernel Hilbert spaces. Finally, we present an alternative strategy consisting in using independent repetitions of projection DPP (or volume sampling), yielding similar error bounds as with i.i.d. or volume sampling, but in practice with a much lower number of samples. Numerical experiments illustrate the performance of the different strategies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum-based gradient descent methods for Lie groups</title>
<link>https://arxiv.org/abs/2404.09363</link>
<guid>https://arxiv.org/abs/2404.09363</guid>
<content:encoded><![CDATA[
arXiv:2404.09363v2 Announce Type: replace-cross 
Abstract: Polyak's Heavy Ball (PHB; Polyak, 1964), a.k.a. Classical Momentum, and Nesterov's Accelerated Gradient (NAG; Nesterov, 1983) are well-established momentum-descent methods for optimization. Although the latter generally outperforms the former, primarily, generalizations of PHB-like methods to nonlinear spaces have not been sufficiently explored in the literature. In this paper, we propose a generalization of NAG-like methods for Lie group optimization. This generalization is based on the variational one-to-one correspondence between classical and accelerated momentum methods (Campos et al., 2023). We provide numerical experiments for chosen retractions on the group of rotations based on the Frobenius norm and the Rosenbrock function to demonstrate the effectiveness of our proposed methods, and that align with results of the Euclidean case, that is, a faster convergence rate for NAG.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling diverse robots by inferring Jacobian fields with deep networks</title>
<link>https://arxiv.org/abs/2407.08722</link>
<guid>https://arxiv.org/abs/2407.08722</guid>
<content:encoded><![CDATA[
arXiv:2407.08722v2 Announce Type: replace-cross 
Abstract: Mirroring the complex structures and diverse functions of natural organisms is a long-standing challenge in robotics. Modern fabrication techniques have greatly expanded the feasible hardware, but using these systems requires control software to translate the desired motions into actuator commands. Conventional robots can easily be modeled as rigid links connected by joints, but it remains an open challenge to model and control biologically inspired robots that are often soft or made of several materials, lack sensing capabilities, and may change their material properties with use. Here, we introduce a method that uses deep neural networks to map a video stream of a robot to its visuomotor Jacobian field (the sensitivity of all 3D points to the robot's actuators). Our method enables the control of robots from only a single camera, makes no assumptions about the robots' materials, actuation, or sensing, and is trained without expert intervention by observing the execution of random commands. We demonstrate our method on a diverse set of robot manipulators that vary in actuation, materials, fabrication, and cost. Our approach achieves accurate closed-loop control and recovers the causal dynamic structure of each robot. Because it enables robot control using a generic camera as the only sensor, we anticipate that our work will broaden the design space of robotic systems and serve as a starting point for lowering the barrier to robotic automation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2408.02123</link>
<guid>https://arxiv.org/abs/2408.02123</guid>
<content:encoded><![CDATA[
arXiv:2408.02123v3 Announce Type: replace-cross 
Abstract: Explainability in artificial intelligence (XAI) remains a crucial aspect for fostering trust and understanding in machine learning models. Current visual explanation techniques, such as gradient-based or class-activation-based methods, often exhibit a strong dependence on specific model architectures. Conversely, perturbation-based methods, despite being model-agnostic, are computationally expensive as they require evaluating models on a large number of forward passes. In this work, we introduce Foveation-based Explanations (FovEx), a novel XAI method inspired by human vision. FovEx seamlessly integrates biologically inspired perturbations by iteratively creating foveated renderings of the image and combines them with gradient-based visual explorations to determine locations of interest efficiently. These locations are selected to maximize the performance of the model to be explained with respect to the downstream task and then combined to generate an attribution map. We provide a thorough evaluation with qualitative and quantitative assessments on established benchmarks. Our method achieves state-of-the-art performance on both transformers (on 4 out of 5 metrics) and convolutional models (on 3 out of 5 metrics), demonstrating its versatility among various architectures. Furthermore, we show the alignment between the explanation map produced by FovEx and human gaze patterns (+14\% in NSS compared to RISE, +203\% in NSS compared to GradCAM). This comparison enhances our confidence in FovEx's ability to close the interpretation gap between humans and machines.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Transfer Learning for MNIST Classification Using a Hybrid Quantum-Classical Approach</title>
<link>https://arxiv.org/abs/2408.03351</link>
<guid>https://arxiv.org/abs/2408.03351</guid>
<content:encoded><![CDATA[
arXiv:2408.03351v2 Announce Type: replace-cross 
Abstract: We implement a hybrid quantum-classical model for image classification that compresses MNIST digit images into a low-dimensional feature space and then maps these features onto a 5-qubit quantum state. First, an autoencoder compresses each $28\times28$ image (784 pixels) into a 64-dimensional latent vector, preserving salient features of the digit with minimal reconstruction error. We further reduce the latent representation to 5 principal components using Principal Component Analysis (PCA), to match the 5 available qubits. These 5 features are encoded as rotation angles in a quantum circuit with 5 qubits. The quantum feature map applies single-qubit rotations ($R_y$ gates) proportional to the feature values, followed by a Hadamard gate and a cascade of entangling CNOT gates to produce a non-product entangled state. Measuring the 5-qubit state yields a 32-dimensional probability distribution over basis outcomes, which serves as a quantum-enhanced feature vector for classification. A classical neural network with a softmax output is then trained on these 32-dimensional quantum feature vectors to predict the digit class. We evaluate the hybrid model on the MNIST dataset and compare it to a purely classical baseline that uses the 64-dimensional autoencoder latent features for classification. The results show that the hybrid model can successfully classify digits, demonstrating the feasibility of integrating quantum computing in the classification pipeline, although its accuracy (about 75\% on test data) currently falls below the classical baseline (about 98\% on the same compressed data).
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-ANOVA: Analytical Model Decomposition using Automatic Integration</title>
<link>https://arxiv.org/abs/2408.12319</link>
<guid>https://arxiv.org/abs/2408.12319</guid>
<content:encoded><![CDATA[
arXiv:2408.12319v2 Announce Type: replace-cross 
Abstract: The analysis of variance (ANOVA) decomposition offers a systematic method to understand the interaction effects that contribute to a specific decision output. In this paper we introduce Neural-ANOVA, an approach to decompose neural networks into the sum of lower-order models using the functional ANOVA decomposition. Our approach formulates a learning problem, which enables fast analytical evaluation of integrals over subspaces that appear in the calculation of the ANOVA decomposition. Finally, we conduct numerical experiments to provide insights into the approximation properties compared to other regression approaches from the literature.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entanglement-induced provable and robust quantum learning advantages</title>
<link>https://arxiv.org/abs/2410.03094</link>
<guid>https://arxiv.org/abs/2410.03094</guid>
<content:encoded><![CDATA[
arXiv:2410.03094v2 Announce Type: replace-cross 
Abstract: Quantum computing holds unparalleled potentials to enhance machine learning. However, a demonstration of quantum learning advantage has not been achieved so far. We make a step forward by rigorously establishing a noise-robust, unconditional quantum learning advantage in expressivity, inference speed, and training efficiency, compared to commonly-used classical models. Our proof is information-theoretic and pinpoints the origin of this advantage: entanglement can be used to reduce the communication required by non-local tasks. In particular, we design a task that can be solved with certainty by quantum models with a constant number of parameters using entanglement, whereas commonly-used classical models must scale linearly to achieve a larger-than-exponentially-small accuracy. We show that the quantum model is trainable with constant resources and robust against constant noise. Through numerical and trapped-ion experiments on IonQ Aria, we demonstrate the desired advantage. Our results provide valuable guidance for demonstrating quantum learning advantages with current noisy intermediate-scale devices.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs</title>
<link>https://arxiv.org/abs/2410.16593</link>
<guid>https://arxiv.org/abs/2410.16593</guid>
<content:encoded><![CDATA[
arXiv:2410.16593v4 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian trace -- a proxy for the graph connectivity -- than random sampling, while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving Laplacian trace and GNN transferability compared to random sampling.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Gaussian Processes as Linear Model Predictive Controller</title>
<link>https://arxiv.org/abs/2412.04502</link>
<guid>https://arxiv.org/abs/2412.04502</guid>
<content:encoded><![CDATA[
arXiv:2412.04502v2 Announce Type: replace-cross 
Abstract: We introduce a novel algorithm for controlling linear time invariant systems in a tracking problem. The controller is based on a Gaussian Process (GP) whose realizations satisfy a system of linear ordinary differential equations with constant coefficients. Control inputs for tracking are determined by conditioning the prior GP on the setpoints, i.e. control as inference. The resulting Model Predictive Control scheme incorporates pointwise soft constraints by introducing virtual setpoints to the posterior Gaussian process. We show theoretically that our controller satisfies open-loop stability for the optimal control problem by leveraging general results from Bayesian inference and demonstrate this result in a numerical example.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Powered Numerical Relativity Surrogate for Binary Black Hole Waveforms</title>
<link>https://arxiv.org/abs/2412.06946</link>
<guid>https://arxiv.org/abs/2412.06946</guid>
<content:encoded><![CDATA[
arXiv:2412.06946v3 Announce Type: replace-cross 
Abstract: Gravitational-wave approximants are essential for gravitational-wave astronomy, allowing the coverage binary black hole parameter space for inference or match filtering without costly numerical relativity (NR) simulations, but generally trading some accuracy for computational efficiency. To reduce this trade-off, NR surrogate models can be constructed using interpolation within NR waveform space. We present a 2-stage training approach for neural network-based NR surrogate models. Initially trained on approximant-generated waveforms and then fine-tuned with NR data, these dual-stage artificial neural surrogate (\texttt{DANSur}) models offer rapid and competitively accurate waveform generation, generating millions in under 20ms on a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in the \textsc{bilby} framework, we show they can be used for parameter estimation tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights into resource utilization of code small language models serving with runtime engines and execution providers</title>
<link>https://arxiv.org/abs/2412.15441</link>
<guid>https://arxiv.org/abs/2412.15441</guid>
<content:encoded><![CDATA[
arXiv:2412.15441v2 Announce Type: replace-cross 
Abstract: The rapid growth of language models, particularly in code generation, requires substantial computational resources, raising concerns about energy consumption and environmental impact. Optimizing language models inference resource utilization is crucial, and Small Language Models (SLMs) offer a promising solution to reduce resource demands. Our goal is to analyze the impact of deep learning serving configurations, defined as combinations of runtime engines and execution providers, on resource utilization, in terms of energy consumption, execution time, and computing-resource utilization from the point of view of software engineers conducting inference in the context of code generation SLMs. We conducted a technology-oriented, multi-stage experimental pipeline using twelve code generation SLMs to investigate energy consumption, execution time, and computing-resource utilization across the configurations. Significant differences emerged across configurations. CUDA execution provider configurations outperformed CPU execution provider configurations in both energy consumption and execution time. Among the configurations, TORCH paired with CUDA demonstrated the greatest energy efficiency, achieving energy savings from 37.99% up to 89.16% compared to other serving configurations. Similarly, optimized runtime engines like ONNX with the CPU execution provider achieved from 8.98% up to 72.04% energy savings within CPU-based configurations. Also, TORCH paired with CUDA exhibited efficient computing-resource utilization. Serving configuration choice significantly impacts resource utilization. While further research is needed, we recommend the above configurations best suited to software engineers' requirements for enhancing serving resource utilization efficiency.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Product Neural Networks for Functional ANOVA Model</title>
<link>https://arxiv.org/abs/2502.15215</link>
<guid>https://arxiv.org/abs/2502.15215</guid>
<content:encoded><![CDATA[
arXiv:2502.15215v5 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as components), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel neural network which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. We call our proposed neural network ANOVA Tensor Product Neural Network (ANOVA-TPNN) since it is motivated by the tensor product basis expansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well. Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVCNet: Multi-View Contrastive Network for Motor Imagery Classification</title>
<link>https://arxiv.org/abs/2502.17482</link>
<guid>https://arxiv.org/abs/2502.17482</guid>
<content:encoded><![CDATA[
arXiv:2502.17482v4 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) enable neural interaction by decoding brain activity for external communication. Motor imagery (MI) decoding has received significant attention due to its intuitive mechanism. However, most existing models rely on single-stream architectures and overlook the multi-view nature of EEG signals, leading to limited performance and generalization. We propose a multi-view contrastive network (MVCNet), a dual-branch architecture that parallelly integrates CNN and Transformer blocks to capture both local spatial-temporal features and global temporal dependencies. To enhance the informativeness of training data, MVCNet incorporates a unified augmentation pipeline across time, frequency, and spatial domains. Two contrastive modules are further introduced: a cross-view contrastive module that enforces consistency of original and augmented views, and a cross-model contrastive module that aligns features extracted from both branches. Final representations are fused and jointly optimized by contrastive and classification losses. Experiments on five public MI datasets across three scenarios demonstrate that MVCNet consistently outperforms nine state-of-the-art MI decoding networks, highlighting its effectiveness and generalization ability. MVCNet provides a robust solution for MI decoding by integrating multi-view information and dual-branch modeling, contributing to the development of more reliable BCI systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lattice Protein Folding with Variational Annealing</title>
<link>https://arxiv.org/abs/2502.20632</link>
<guid>https://arxiv.org/abs/2502.20632</guid>
<content:encoded><![CDATA[
arXiv:2502.20632v2 Announce Type: replace-cross 
Abstract: Understanding the principles of protein folding is a cornerstone of computational biology, with implications for drug design, bioengineering, and the understanding of fundamental biological processes. Lattice protein folding models offer a simplified yet powerful framework for studying the complexities of protein folding, enabling the exploration of energetically optimal folds under constrained conditions. However, finding these optimal folds is a computationally challenging combinatorial optimization problem. In this work, we introduce a novel upper-bound training scheme that employs masking to identify the lowest-energy folds in two-dimensional Hydrophobic-Polar (HP) lattice protein folding. By leveraging Dilated Recurrent Neural Networks (RNNs) integrated with an annealing process driven by temperature-like fluctuations, our method accurately predicts optimal folds for benchmark systems of up to 60 beads. Our approach also effectively masks invalid folds from being sampled without compromising the autoregressive sampling properties of RNNs. This scheme is generalizable to three spatial dimensions and can be extended to lattice protein models with larger alphabets. Our findings emphasize the potential of advanced machine learning techniques in tackling complex protein folding problems and a broader class of constrained combinatorial optimization challenges.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D Scene Analogies with Neural Contextual Scene Maps</title>
<link>https://arxiv.org/abs/2503.15897</link>
<guid>https://arxiv.org/abs/2503.15897</guid>
<content:encoded><![CDATA[
arXiv:2503.15897v2 Announce Type: replace-cross 
Abstract: Understanding scene contexts is crucial for machines to perform tasks and adapt prior knowledge in unseen or noisy 3D environments. As data-driven learning is intractable to comprehensively encapsulate diverse ranges of layouts and open spaces, we propose teaching machines to identify relational commonalities in 3D spaces. Instead of focusing on point-wise or object-wise representations, we introduce 3D scene analogies, which are smooth maps between 3D scene regions that align spatial relationships. Unlike well-studied single instance-level maps, these scene-level maps smoothly link large scene regions, potentially enabling unique applications in trajectory transfer in AR/VR, long demonstration transfer for imitation learning, and context-aware object rearrangement. To find 3D scene analogies, we propose neural contextual scene maps, which extract descriptor fields summarizing semantic and geometric contexts, and holistically align them in a coarse-to-fine manner for map estimation. This approach reduces reliance on individual feature points, making it robust to input noise or shape variations. Experiments demonstrate the effectiveness of our approach in identifying scene analogies and transferring trajectories or object placements in diverse indoor scenes, indicating its potential for robotics and AR/VR applications. Project page including the code is available through this link: https://82magnolia.github.io/3d_scene_analogies/.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology</title>
<link>https://arxiv.org/abs/2503.17564</link>
<guid>https://arxiv.org/abs/2503.17564</guid>
<content:encoded><![CDATA[
arXiv:2503.17564v2 Announce Type: replace-cross 
Abstract: Prediction tasks in digital pathology are challenging due to the massive size of whole-slide images (WSIs) and the weak nature of training signals. Advances in computing, data availability, and self-supervised learning (SSL) have paved the way for slide-level foundation models (SLFMs) that can improve prediction tasks in low-data regimes. However, current methods under-utilize shared information between tasks and modalities. To overcome this challenge, we propose ModalTune, a novel fine-tuning framework which introduces the Modal Adapter to integrate new modalities without modifying SLFM weights. Additionally, we use large-language models (LLMs) to encode labels as text, capturing semantic relationships across multiple tasks and cancer types in a single training recipe. ModalTune achieves state-of-the-art (SOTA) results against both uni-modal and multi-modal models across four cancer types, jointly improving survival and cancer subtype prediction while remaining competitive in pan-cancer settings. Additionally, we show ModalTune is generalizable to two out-of-distribution (OOD) datasets. To our knowledge, this is the first unified fine-tuning framework for multi-modal, multi-task, and pan-cancer modeling in digital pathology.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accenture-NVS1: A Novel View Synthesis Dataset</title>
<link>https://arxiv.org/abs/2503.18711</link>
<guid>https://arxiv.org/abs/2503.18711</guid>
<content:encoded><![CDATA[
arXiv:2503.18711v2 Announce Type: replace-cross 
Abstract: This paper introduces ACC-NVS1, a specialized dataset designed for research on Novel View Synthesis specifically for airborne and ground imagery. Data for ACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The collection encompasses six diverse real-world scenes captured from both airborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1 addresses challenges such as varying altitudes and transient objects. This dataset is intended to supplement existing datasets, providing additional resources for comprehensive research, rather than serving as a benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations</title>
<link>https://arxiv.org/abs/2504.05422</link>
<guid>https://arxiv.org/abs/2504.05422</guid>
<content:encoded><![CDATA[
arXiv:2504.05422v3 Announce Type: replace-cross 
Abstract: As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints are available at: https://github.com/continental/EP-Diffuser.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v3 Announce Type: replace-cross 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation</title>
<link>https://arxiv.org/abs/2504.13201</link>
<guid>https://arxiv.org/abs/2504.13201</guid>
<content:encoded><![CDATA[
arXiv:2504.13201v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly becoming the cognitive core of Embodied Intelligence (EI) systems, such as robots and autonomous vehicles. However, this integration also exposes them to serious jailbreak risks, where malicious instructions can be transformed into dangerous physical actions. Existing defense mechanisms suffer from notable drawbacks--including high training costs, significant inference delays, and complex hyperparameter tuning--which limit their practical applicability. To address these challenges, we propose a novel and efficient inference-time defense framework: Concept Enhancement Engineering (CEE). CEE enhances the model's inherent safety mechanisms by directly manipulating its internal representations, requiring neither additional training nor external modules, thereby improving defense efficiency. Furthermore, CEE introduces a rotation-based control mechanism that enables stable and linearly tunable behavioral control of the model. This design eliminates the need for tedious manual tuning and avoids the output degradation issues commonly observed in other representation engineering methods. Extensive experiments across multiple EI safety benchmarks and diverse attack scenarios demonstrate that CEE significantly improves the defense success rates of various multimodal LLMs. It effectively mitigates safety risks while preserving high-quality generation and inference efficiency, offering a promising solution for deploying safer embodied intelligence systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation</title>
<link>https://arxiv.org/abs/2505.05085</link>
<guid>https://arxiv.org/abs/2505.05085</guid>
<content:encoded><![CDATA[
arXiv:2505.05085v2 Announce Type: replace-cross 
Abstract: Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling a deeper understanding of the underlying dynamics. The spectra of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We approach this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasise the dynamically adaptive quality of the machine-learned basis.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2505.19219</link>
<guid>https://arxiv.org/abs/2505.19219</guid>
<content:encoded><![CDATA[
arXiv:2505.19219v2 Announce Type: replace-cross 
Abstract: Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial intelligence and robotics, requiring the computation of collision-free paths for multiple agents navigating from their start locations to designated goals. As autonomous systems become increasingly prevalent in warehouses, urban transportation, and other complex environments, MAPF has evolved from a theoretical challenge to a critical enabler of real-world multi-robot coordination. This comprehensive survey bridges the long-standing divide between classical algorithmic approaches and emerging learning-based methods in MAPF research. We present a unified framework that encompasses search-based methods (including Conflict-Based Search, Priority-Based Search, and Large Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP formulations), and data-driven techniques (reinforcement learning, supervised learning, and hybrid strategies). Through systematic analysis of experimental practices across 200+ papers, we uncover significant disparities in evaluation methodologies, with classical methods typically tested on larger-scale instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy of evaluation metrics, environment types, and baseline selections, highlighting the need for standardized benchmarking protocols. Finally, we outline promising future directions including mixed-motive MAPF with game-theoretic considerations, language-grounded planning with large language models, and neural solver architectures that combine the rigor of classical methods with the flexibility of deep learning. This survey serves as both a comprehensive reference for researchers and a practical guide for deploying MAPF solutions in increasingly complex real-world applications.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Super Spreaders in Multilayer Networks</title>
<link>https://arxiv.org/abs/2505.20980</link>
<guid>https://arxiv.org/abs/2505.20980</guid>
<content:encoded><![CDATA[
arXiv:2505.20980v2 Announce Type: replace-cross 
Abstract: Identifying super-spreaders can be framed as a subtask of the influence maximisation problem. It seeks to pinpoint agents within a network that, if selected as single diffusion seeds, disseminate information most effectively. Multilayer networks, a specific class of heterogeneous graphs, can capture diverse types of interactions (e.g., physical-virtual or professional-social), and thus offer a more accurate representation of complex relational structures. In this work, we introduce a novel approach to identifying super-spreaders in such networks by leveraging graph neural networks. To this end, we construct a dataset by simulating information diffusion across hundreds of networks - to the best of our knowledge, the first of its kind tailored specifically to multilayer networks. We further formulate the task as a variation of the ranking prediction problem based on a four-dimensional vector that quantifies each agent's spreading potential: (i) the number of activations; (ii) the duration of the diffusion process; (iii) the peak number of activations; and (iv) the simulation step at which this peak occurs. Our model, TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom aggregation layer. This design enables generalisation to previously unseen data and adapts to varying graph sizes. In an extensive evaluation, we compare our model against classic centrality-based heuristics and competitive deep learning methods. The results, obtained across a broad spectrum of real-world and synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves superior performance in identifying high-impact nodes, while also offering improved interpretability through its structured output.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.21567</link>
<guid>https://arxiv.org/abs/2505.21567</guid>
<content:encoded><![CDATA[
arXiv:2505.21567v2 Announce Type: replace-cross 
Abstract: With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction</title>
<link>https://arxiv.org/abs/2506.10006</link>
<guid>https://arxiv.org/abs/2506.10006</guid>
<content:encoded><![CDATA[
arXiv:2506.10006v2 Announce Type: replace-cross 
Abstract: In breast cancer HER2 assessment, clinical evaluation relies on combined H&amp;E and IHC images, yet acquiring both modalities is often hindered by clinical constraints and cost. We propose an adaptive bimodal prediction framework that flexibly supports single- or dual-modality inputs through two core innovations: a dynamic branch selector activating modality completion or joint inference based on input availability, and a cross-modal GAN (CM-GAN) enabling feature-space reconstruction of missing modalities. This design dramatically improves H&amp;E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full dual-modality inputs, and maintains 90.28% reliability under single-modality conditions. The "dual-modality preferred, single-modality compatible" architecture delivers near-dual-modality accuracy without mandatory synchronized acquisition, offering a cost-effective solution for resource-limited regions and significantly improving HER2 assessment accessibility.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</title>
<link>https://arxiv.org/abs/2507.01631</link>
<guid>https://arxiv.org/abs/2507.01631</guid>
<content:encoded><![CDATA[
arXiv:2507.01631v2 Announce Type: replace-cross 
Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift</title>
<link>https://arxiv.org/abs/2507.07820</link>
<guid>https://arxiv.org/abs/2507.07820</guid>
<content:encoded><![CDATA[
arXiv:2507.07820v2 Announce Type: replace-cross 
Abstract: Current AI advances largely rely on scaling neural models and expanding training datasets to achieve generalization and robustness. Despite notable successes, this paradigm incurs significant environmental, economic, and ethical costs, limiting sustainability and equitable access. Inspired by biological sensory systems, where adaptation occurs dynamically at the input (e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive sensing as a necessary and foundational shift. Adaptive sensing proactively modulates sensor parameters (e.g., exposure, sensitivity, multimodal configurations) at the input level, significantly mitigating covariate shifts and improving efficiency. Empirical evidence from recent studies demonstrates that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass substantially larger models (e.g., OpenCLIP-H) trained with significantly more data and compute. We (i) outline a roadmap for broadly integrating adaptive sensing into real-world applications spanning humanoid, healthcare, autonomous systems, agriculture, and environmental monitoring, (ii) critically assess technical and ethical integration challenges, and (iii) propose targeted research directions, such as standardized benchmarks, real-time adaptive algorithms, multimodal integration, and privacy-preserving methods. Collectively, these efforts aim to transition the AI community toward sustainable, robust, and equitable artificial intelligence systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaViPlan : Language-Guided Visual Path Planning with RLVR</title>
<link>https://arxiv.org/abs/2507.12911</link>
<guid>https://arxiv.org/abs/2507.12911</guid>
<content:encoded><![CDATA[
arXiv:2507.12911v3 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) scenarios in autonomous driving refer to situations that deviate from the training domain, often leading to unexpected and potentially hazardous behavior from planners that lack prior exposure to such cases. Recently, Vision-Language Models (VLMs) have been introduced into autonomous driving research for their promising generalization capabilities in OOD settings. Early studies demonstrated that VLMs could recognize OOD scenarios and generate user-level decisions such as "go straight" or "turn right." However, a new challenge has emerged due to the misalignment between the VLM's high-level decisions or visual reasoning expressed in language, and the low-level predicted trajectories interpreted as actions. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics. This approach addresses the vision-language-action misalignment observed in existing VLMs fine-tuned via supervised learning, which can recognize driving scenarios but often produce context-unaware decisions. Experimental results demonstrate that our method improves situational awareness and decision-making under OOD conditions, highlighting its potential to mitigate the misalignment issue. This work introduces a promising post-training paradigm for VLM agents in the context of autonomous driving.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research</title>
<link>https://arxiv.org/abs/2507.19079</link>
<guid>https://arxiv.org/abs/2507.19079</guid>
<content:encoded><![CDATA[
arXiv:2507.19079v2 Announce Type: replace-cross 
Abstract: High-precision navigation and positioning systems are critical for applications in autonomous vehicles and mobile mapping, where robust and continuous localization is essential. To test and enhance the performance of algorithms, some research institutions and companies have successively constructed and publicly released datasets. However, existing datasets still suffer from limitations in sensor diversity and environmental coverage. To address these shortcomings and advance development in related fields, the SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset has been developed. This dataset integrates data from multiple sensors, including Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile resource for research in multi-sensor fusion and high-precision navigation. The dataset construction process is thoroughly documented, encompassing sensor configurations, coordinate system definitions, and calibration procedures for both cameras and LiDAR. A standardized framework for data collection and processing ensures consistency and scalability, enabling large-scale analysis. Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM) algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's applicability for advanced navigation research. Covering a wide range of real-world scenarios, including urban areas, campuses, tunnels, and suburban environments, the dataset offers a valuable tool for advancing navigation technologies and addressing challenges in complex environments. By providing a publicly accessible, high-quality dataset, this work aims to bridge gaps in sensor diversity, data accessibility, and environmental representation, fostering further innovation in the field.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs</title>
<link>https://arxiv.org/abs/2507.22074</link>
<guid>https://arxiv.org/abs/2507.22074</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Large Vision-Language Models, CIMR, multi-modal instructions, iterative reasoning

Summary:
CIMR is a novel framework designed to improve the capabilities of Large Language Models and Large Vision-Language Models when processing complex, multi-step multi-modal instructions. It introduces a context-aware iterative reasoning and self-correction module that operates in two stages: initial reasoning and response generation, followed by iterative refinement using multi-modal feedback. The framework integrates textual, visual, and contextual features dynamically at each step to enhance performance. Fine-tuned on the Visual Instruction Tuning dataset, CIMR achieves 91.5% accuracy on the Multi-modal Action Planning dataset, surpassing state-of-the-art models such as GPT-4V, LLaVA-1.5, MiniGPT-4, and InstructBLIP. This demonstrates the efficacy of CIMR's iterative reasoning and self-correction capabilities in handling complex tasks involving logical reasoning, dynamic feedback integration, and iterative self-correction. 

<br /><br />Summary: <div>
arXiv:2507.22074v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-modal instructions that require logical reasoning, dynamic feedback integration, and iterative self-correction. To address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a novel framework that introduces a context-aware iterative reasoning and self-correction module. CIMR operates in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. A dynamic fusion module deeply integrates textual, visual, and contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised Adaptation</title>
<link>https://arxiv.org/abs/2507.22075</link>
<guid>https://arxiv.org/abs/2507.22075</guid>
<content:encoded><![CDATA[
<div> adaptation, vision-language models, CLIP, pseudo-labeling, unsupervised

Summary: 
The paper introduces a novel adaptive pseudo-labeling framework for unsupervised adaptation in vision-language models like CLIP. Conventional pseudo-label filtering methods often fail in fully unsupervised settings due to noise in pseudo-labels. The proposed framework integrates prototype consistency and neighborhood-based consistency to enhance adaptation performance. PICS evaluates pseudo-label accuracy based on in-class feature compactness and cross-class feature separation, while NALR refines pseudo-labels dynamically using semantic similarities among neighboring samples. An adaptive weighting mechanism adjusts the influence of pseudo-labeled samples during training based on their estimated correctness. Extensive experiments on 11 benchmark datasets show that the method achieves state-of-the-art performance in unsupervised adaptation scenarios, providing more accurate pseudo-labels while maintaining computational efficiency. <div>
arXiv:2507.22075v1 Announce Type: new 
Abstract: In unsupervised adaptation for vision-language models such as CLIP, pseudo-labels derived from zero-shot predictions often exhibit significant noise, particularly under domain shifts or in visually complex scenarios. Conventional pseudo-label filtering approaches, which rely on fixed confidence thresholds, tend to be unreliable in fully unsupervised settings. In this work, we propose a novel adaptive pseudo-labeling framework that enhances CLIP's adaptation performance by integrating prototype consistency and neighborhood-based consistency. The proposed method comprises two key components: PICS, which assesses pseudo-label accuracy based on in-class feature compactness and cross-class feature separation; and NALR, which exploits semantic similarities among neighboring samples to refine pseudo-labels dynamically. Additionally, we introduce an adaptive weighting mechanism that adjusts the influence of pseudo-labeled samples during training according to their estimated correctness. Extensive experiments on 11 benchmark datasets demonstrate that our method achieves state-of-the-art performance in unsupervised adaptation scenarios, delivering more accurate pseudo-labels while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Prompt Refinement for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2507.22076</link>
<guid>https://arxiv.org/abs/2507.22076</guid>
<content:encoded><![CDATA[
<div> prompt sensitivity, text-to-image generation, closed-loop framework, test-time prompt refinement, multimodal large language model 

Summary: 
The article introduces a closed-loop framework called TIR to improve prompt sensitivity in text-to-image (T2I) generation models. The framework does not require additional training of the underlying T2I model. In TIR, a pretrained multimodal large language model (MLLM) analyzes the generated image and prompt at each step, refining the prompt based on detected misalignments. This iterative refinement process corrects errors and enhances visual coherence in image outputs. TIR showcases improved alignment and visual quality across benchmark datasets while maintaining compatibility with various T2I models. The approach mirrors the iterative refinement process employed by human artists, highlighting its effectiveness in enhancing T2I model performance. <div>
arXiv:2507.22076v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation models have made significant strides but still struggle with prompt sensitivity: even minor changes in prompt wording can yield inconsistent or inaccurate outputs. To address this challenge, we introduce a closed-loop, test-time prompt refinement framework that requires no additional training of the underlying T2I model, termed TIR. In our approach, each generation step is followed by a refinement step, where a pretrained multimodal large language model (MLLM) analyzes the output image and the user's prompt. The MLLM detects misalignments (e.g., missing objects, incorrect attributes) and produces a refined and physically grounded prompt for the next round of image generation. By iteratively refining the prompt and verifying alignment between the prompt and the image, TIR corrects errors, mirroring the iterative refinement process of human artists. We demonstrate that this closed-loop strategy improves alignment and visual coherence across multiple benchmark datasets, all while maintaining plug-and-play integration with black-box T2I models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-fidelity Bayesian Data-Driven Design of Energy Absorbing Spinodoid Cellular Structures</title>
<link>https://arxiv.org/abs/2507.22079</link>
<guid>https://arxiv.org/abs/2507.22079</guid>
<content:encoded><![CDATA[
<div> Finite Element Simulations, Data-Driven Design, Bayesian Optimization, Multi-Fidelity Modeling, Metamaterial Design

Summary:<br />
- Finite element simulations are becoming more accurate but also more computationally expensive, creating a need for data-driven design solutions.
- Bayesian optimization (BO) and multi-fidelity BO (MFBO) have been established as effective optimization methods for expensive objective functions.
- Using Sobol' samples and sensitivity analysis can help reduce the complexity of design problems in data-driven design.
- This study compares the performance of BO and MFBO in maximizing the energy absorption (EA) of spinodoid cellular structures for metamaterial design.
- Results show that MFBO can outperform BO by up to 11% in maximizing EA across various hyperparameter settings, supporting the utility of multi-fidelity techniques in expensive data-driven design problems.

<br /><br />Summary: <div>
arXiv:2507.22079v1 Announce Type: new 
Abstract: Finite element (FE) simulations of structures and materials are getting increasingly more accurate, but also more computationally expensive as a collateral result. This development happens in parallel with a growing demand of data-driven design. To reconcile the two, a robust and data-efficient optimization method called Bayesian optimization (BO) has been previously established as a technique to optimize expensive objective functions. In parallel, the mesh width of an FE model can be exploited to evaluate an objective at a lower or higher fidelity (cost & accuracy) level. The multi-fidelity setting applied to BO, called multi-fidelity BO (MFBO), has also seen previous success. However, BO and MFBO have not seen a direct comparison with when faced with with a real-life engineering problem, such as metamaterial design for deformation and absorption qualities. Moreover, sampling quality and assessing design parameter sensitivity is often an underrepresented part of data-driven design. This paper aims to address these shortcomings by employing Sobol' samples with variance-based sensitivity analysis in order to reduce design problem complexity. Furthermore, this work describes, implements, applies and compares the performance BO with that MFBO when maximizing the energy absorption (EA) problem of spinodoid cellular structures is concerned. The findings show that MFBO is an effective way to maximize the EA of a spinodoid structure and is able to outperform BO by up to 11% across various hyperparameter settings. The results, which are made open-source, serve to support the utility of multi-fidelity techniques across expensive data-driven design problems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow</title>
<link>https://arxiv.org/abs/2507.22082</link>
<guid>https://arxiv.org/abs/2507.22082</guid>
<content:encoded><![CDATA[
<div> deep learning, turbulence modeling, fluid dynamics, multiscale models, generative models

Summary: 
This article introduces the application of deep learning in turbulence modeling within fluid dynamics, utilizing complex datasets to enhance understanding of turbulent phenomena. It explores traditional and deep learning-based approaches in turbulence modeling, emphasizing the increasing availability of high-dimensional data from various sources. The report specifically delves into the integration of multiscale turbulence models with deep learning architectures and the usage of deep generative models for super-resolution reconstruction. By merging fluid dynamics with machine learning techniques, researchers can develop more comprehensive and accurate models for analyzing turbulent flows. <div>
arXiv:2507.22082v1 Announce Type: new 
Abstract: Deep learning provides a versatile suite of methods for extracting structured information from complex datasets, enabling deeper understanding of underlying fluid dynamic phenomena. The field of turbulence modeling, in particular, benefits from the growing availability of high-dimensional data obtained through experiments, field observations, and large-scale simulations spanning multiple spatio-temporal scales. This report presents a concise overview of both classical and deep learningbased approaches to turbulence modeling. It further investigates two specific challenges at the intersection of fluid dynamics and machine learning: the integration of multiscale turbulence models with deep learning architectures, and the application of deep generative models for super-resolution reconstruction
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Curriculum Learning using Parameter Continuation Methods</title>
<link>https://arxiv.org/abs/2507.22089</link>
<guid>https://arxiv.org/abs/2507.22089</guid>
<content:encoded><![CDATA[
<div> parameter continuation method, optimization, neural networks, homotopies, curriculum learning

Summary:
The article introduces a parameter continuation method for optimizing neural networks, which is closely linked to homotopies and curriculum learning. The proposed methods are both theoretically grounded and practical, showcasing improved generalization performance compared to ADAM, a state-of-the-art optimization technique, in supervised and unsupervised learning tasks. This method offers a novel approach to deep neural network optimization, opening up possibilities for more efficient and effective training processes. <div>
arXiv:2507.22089v1 Announce Type: new 
Abstract: In this work, we propose a parameter continuation method for the optimization of neural networks. There is a close connection between parameter continuation, homotopies, and curriculum learning. The methods we propose here are theoretically justified and practically effective for several problems in deep neural networks. In particular, we demonstrate better generalization performance than state-of-the-art optimization techniques such as ADAM for supervised and unsupervised learning tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization</title>
<link>https://arxiv.org/abs/2507.22090</link>
<guid>https://arxiv.org/abs/2507.22090</guid>
<content:encoded><![CDATA[
<div> S3, S4, hybrid activation functions, neural networks, deep learning <br />
<br />
Summary: Sigmoid-Softsign (S3) and its improved version (S4) are introduced as hybrid activation functions to address limitations of traditional functions in deep neural networks. S3 combines sigmoid and softsign, while S4 has a smooth transition mechanism controlled by a parameter k. S4 outperformed nine baselines in binary and multi-class classification, and regression tasks, achieving high accuracy and faster convergence. It maintains stable gradient flow and addresses dead neuron and vanishing gradient issues. S4's gradient range is [0.24, 0.59], reducing dead neurons compared to ReLU. The tunable parameter k in S4 allows adaptation to different tasks and network depths, making it a versatile choice for deep learning applications. These findings highlight the potential of hybrid activation functions in improving neural network training dynamics. <br /><br /> <div>
arXiv:2507.22090v1 Announce Type: new 
Abstract: Activation functions are critical components in deep neural networks, directly influencing gradient flow, training stability, and model performance. Traditional functions like ReLU suffer from dead neuron problems, while sigmoid and tanh exhibit vanishing gradient issues. We introduce two novel hybrid activation functions: S3 (Sigmoid-Softsign) and its improved version S4 (smoothed S3). S3 combines sigmoid for negative inputs with softsign for positive inputs, while S4 employs a smooth transition mechanism controlled by a steepness parameter k. We conducted comprehensive experiments across binary classification, multi-class classification, and regression tasks using three different neural network architectures. S4 demonstrated superior performance compared to nine baseline activation functions, achieving 97.4% accuracy on MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression. The function exhibited faster convergence (-19 for ReLU) and maintained stable gradient flow across network depths. Comparative analysis revealed S4's gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep networks. The S4 activation function addresses key limitations of existing functions through its hybrid design and smooth transition mechanism. The tunable parameter k allows adaptation to different tasks and network depths, making S4 a versatile choice for deep learning applications. These findings suggest that hybrid activation functions represent a promising direction for improving neural network training dynamics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic</title>
<link>https://arxiv.org/abs/2507.22174</link>
<guid>https://arxiv.org/abs/2507.22174</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Communication Networks, Graph Neural Networks, Recurrent Neural Networks, Spatial-Temporal <br />
Summary: <br />
This study introduces a new approach for optimizing packet routing in communication networks using a spatial-temporal reinforcement learning framework. Traditional RL algorithms based on Markov Decision Processes may not capture the complexity of dynamic traffic patterns and network topologies. The proposed method integrates Graph Neural Networks and Recurrent Neural Networks to capture spatial dynamics and temporal traffic patterns, resulting in improved routing decisions. Evaluation shows that the approach outperforms traditional RL techniques and is more robust to changes in network topology. <div>
arXiv:2507.22174v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become a well-established approach for optimizing packet routing in communication networks. Standard RL algorithms typically are based on the Markov Decision Process (MDP), which assumes that the current state of the environment provides all the necessary information for system evolution and decision-making. However, this Markovian assumption is invalid in many practical scenarios, making the MDP and RL frameworks inadequate to produce the optimal solutions. Additionally, traditional RL algorithms often employ function approximations (e.g., by neural networks) that do not explicitly capture the spatial relationships inherent in environments with complex network topologies. Communication networks are characterized by dynamic traffic patterns and arbitrary numbers of nodes and links, which further complicate the decision-making process. To address these challenges, we propose a spatial-temporal RL approach that integrates Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatial dynamics regarding network topology and temporal traffic patterns, respectively, to enhance routing decisions. Our evaluation demonstrates that the proposed method outperforms and is more robust to changes in the network topology when compared with traditional RL techniques.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SourceSplice: Source Selection for Machine Learning Tasks</title>
<link>https://arxiv.org/abs/2507.22186</link>
<guid>https://arxiv.org/abs/2507.22186</guid>
<content:encoded><![CDATA[
<div> Keywords: data quality, machine learning, data discovery, source selection, algorithm evaluation

Summary:
SourceGrasp and SourceSplice are introduced to address the challenge of selecting the best subset of data sources for constructing training datasets in machine learning tasks. SourceGrasp utilizes a metaheuristic approach based on greediness and randomization, while SourceSplice is inspired by gene splicing to select sources efficiently. Empirical evaluations on real-world and synthetic datasets show that SourceSplice outperforms in identifying subsets of data sources that maximize task utility with fewer explorations. The algorithms consider that sources contribute differently to task utility and must be carefully chosen. Sensitivity studies on SourceSplice's decision choices under various settings provide insights into its performance. This research highlights the importance of considering source quality for optimal performance in machine learning tasks. 

<br /><br />Summary: <div>
arXiv:2507.22186v1 Announce Type: new 
Abstract: Data quality plays a pivotal role in the predictive performance of machine learning (ML) tasks - a challenge amplified by the deluge of data sources available in modern organizations.Prior work in data discovery largely focus on metadata matching, semantic similarity or identifying tables that should be joined to answer a particular query, but do not consider source quality for high performance of the downstream ML task.This paper addresses the problem of determining the best subset of data sources that must be combined to construct the underlying training dataset for a given ML task.We propose SourceGrasp and SourceSplice, frameworks designed to efficiently select a suitable subset of sources that maximizes the utility of the downstream ML model.Both the algorithms rely on the core idea that sources (or their combinations) contribute differently to the task utility, and must be judiciously chosen.While SourceGrasp utilizes a metaheuristic based on a greediness criterion and randomization, the SourceSplice framework presents a source selection mechanism inspired from gene splicing - a core concept used in protein synthesis.We empirically evaluate our algorithms on three real-world datasets and synthetic datasets and show that, with significantly fewer subset explorations, SourceSplice effectively identifies subsets of data sources leading to high task utility.We also conduct studies reporting the sensitivity of SourceSplice to the decision choices under several settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Time-Series Dataset Similarity using Wasserstein Distance</title>
<link>https://arxiv.org/abs/2507.22189</link>
<guid>https://arxiv.org/abs/2507.22189</guid>
<content:encoded><![CDATA[
<div> Wasserstein distance, time-series dataset similarity, multivariate normal distribution, foundation models, inference performance<br />
Summary:<br />
This paper introduces a distribution-based method for measuring the similarity of time-series datasets using the Wasserstein distance. By treating time-series datasets as empirical instances of underlying multivariate normal distributions, the similarity between two datasets is calculated as the Wasserstein distance between their corresponding distributions. The approach is shown to be effective in identifying similar time-series datasets and estimating the performance of foundation models in various scenarios. Comprehensive experiments demonstrate a high correlation between the proposed similarity measure and inference loss, indicating its utility in model selection, finetuning, and visualization. The Wasserstein distance proves particularly valuable in out-of-distribution and transfer learning evaluation, providing a reliable measure of similarity that aids in model comparison and inference performance estimation.<br /> <div>
arXiv:2507.22189v1 Announce Type: new 
Abstract: The emergence of time-series foundation model research elevates the growing need to measure the (dis)similarity of time-series datasets. A time-series dataset similarity measure aids research in multiple ways, including model selection, finetuning, and visualization. In this paper, we propose a distribution-based method to measure time-series dataset similarity by leveraging the Wasserstein distance. We consider a time-series dataset an empirical instantiation of an underlying multivariate normal distribution (MVN). The similarity between two time-series datasets is thus computed as the Wasserstein distance between their corresponding MVNs. Comprehensive experiments and visualization show the effectiveness of our approach. Specifically, we show how the Wasserstein distance helps identify similar time-series datasets and facilitates inference performance estimation of foundation models in both out-of-distribution and transfer learning evaluation, with high correlations between our proposed measure and the inference loss (>0.60).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification</title>
<link>https://arxiv.org/abs/2507.22205</link>
<guid>https://arxiv.org/abs/2507.22205</guid>
<content:encoded><![CDATA[
<div> Keywords: remote fetal monitoring, CTG-Insight, multi-agent LLM system, interpretability, fetal health classification 

Summary: 
CTG-Insight is a novel multi-agent LLM system designed to provide structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals for remote fetal monitoring. The system decomposes CTG traces into five medically defined features: baseline, variability, accelerations, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. By following established medical guidelines, CTG-Insight offers a holistic classification of fetal health with high accuracy and F1-score. It outperforms deep learning models and single-agent LLM baselines, achieving state-of-the-art results (96.4% accuracy and 97.8% F1-score). Additionally, CTG-Insight provides transparent and interpretable outputs, enhancing the understanding of raw CTG data for expectant parents. This work contributes a valuable framework for interpretable and extensible CTG analysis in the field of remote fetal monitoring. 

<br /><br />Summary: <div>
arXiv:2507.22205v1 Announce Type: new 
Abstract: Remote fetal monitoring technologies are becoming increasingly common. Yet, most current systems offer limited interpretability, leaving expectant parents with raw cardiotocography (CTG) data that is difficult to understand. In this work, we present CTG-Insight, a multi-agent LLM system that provides structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals. Drawing from established medical guidelines, CTG-Insight decomposes each CTG trace into five medically defined features: baseline, variability, accelerations, decelerations, and sinusoidal pattern, each analyzed by a dedicated agent. A final aggregation agent synthesizes the outputs to deliver a holistic classification of fetal health, accompanied by a natural language explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare it against deep learning models and the single-agent LLM baseline. Results show that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) while producing transparent and interpretable outputs. This work contributes an interpretable and extensible CTG analysis framework.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability-Driven Feature Engineering for Mid-Term Electricity Load Forecasting in ERCOT's SCENT Region</title>
<link>https://arxiv.org/abs/2507.22220</link>
<guid>https://arxiv.org/abs/2507.22220</guid>
<content:encoded><![CDATA[
<div> Keywords: load forecasting, machine learning models, SHAP, feature engineering, forecasting accuracy

Summary: 
Accurate load forecasting is crucial for modern electric power systems, as demand is influenced by weather variability and temporal dynamics. This study compares the performance of different machine learning models - Linear Regression, XGBoost, LightGBM, and Long Short-Term Memory (LSTM) - for predicting system-wide electricity load up to one year ahead. Midterm forecasting is particularly important for maintenance scheduling, resource allocation, financial planning, and market participation. The paper highlights the use of SHAP (Shapley Additive Explanations) for enhancing model explainability by quantifying feature contributions. This approach assists in informed feature engineering, leading to improved transparency and forecasting accuracy. <div>
arXiv:2507.22220v1 Announce Type: new 
Abstract: Accurate load forecasting is essential to the operation of modern electric power systems. Given the sensitivity of electricity demand to weather variability and temporal dynamics, capturing non-linear patterns is essential for long-term planning. This paper presents a comparative analysis of machine learning models, Linear Regression, XGBoost, LightGBM, and Long Short-Term Memory (LSTM), for forecasting system-wide electricity load up to one year in advance. Midterm forecasting has shown to be crucial for maintenance scheduling, resource allocation, financial forecasting, and market participation. The paper places a focus on the use of a method called "Shapley Additive Explanations" (SHAP) to improve model explainability. SHAP enables the quantification of feature contributions, guiding informed feature engineering and improving both model transparency and forecasting accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction</title>
<link>https://arxiv.org/abs/2507.22229</link>
<guid>https://arxiv.org/abs/2507.22229</guid>
<content:encoded><![CDATA[
<div> Keywords: neuroscience, deep neural network, multimodal, fMRI responses, brain encoding competition

Summary: 
Neuroscience has traditionally been fragmented into specialized domains, hindering the development of a unified model of cognition. The TRIBE deep neural network predicts brain responses to stimuli across multiple modalities, cortical areas, and individuals, utilizing pretrained representations of text, audio, and video models. By integrating these modalities and utilizing a transformer for time-evolving data, TRIBE accurately models spatial and temporal fMRI responses to videos, outperforming competitors in the Algonauts 2025 brain encoding competition. While unimodal models excel in predicting corresponding cortical networks, the multimodal TRIBE model surpasses them in high-level associative cortices. Applied initially to perception and comprehension, TRIBE's approach sets the foundation for constructing an integrated model of brain representations to better understand human cognition. The code for TRIBE is available at the provided GitHub link. 

<br /><br />Summary: <div>
arXiv:2507.22229v1 Announce Type: new 
Abstract: Historically, neuroscience has progressed by fragmenting into specialized domains, each focusing on isolated modalities, tasks, or brain regions. While fruitful, this approach hinders the development of a unified model of cognition. Here, we introduce TRIBE, the first deep neural network trained to predict brain responses to stimuli across multiple modalities, cortical areas and individuals. By combining the pretrained representations of text, audio and video foundational models and handling their time-evolving nature with a transformer, our model can precisely model the spatial and temporal fMRI responses to videos, achieving the first place in the Algonauts 2025 brain encoding competition with a significant margin over competitors. Ablations show that while unimodal models can reliably predict their corresponding cortical networks (e.g. visual or auditory networks), they are systematically outperformed by our multimodal model in high-level associative cortices. Currently applied to perception and comprehension, our approach paves the way towards building an integrative model of representations in the human brain. Our code is available at https://github.com/facebookresearch/algonauts-2025.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training</title>
<link>https://arxiv.org/abs/2507.22250</link>
<guid>https://arxiv.org/abs/2507.22250</guid>
<content:encoded><![CDATA[
<div> framework, dataset construction, optimization, pre-training, scaling laws

Summary:
The article introduces a framework for optimizing domain-specific dataset construction in foundation model training. It focuses on estimating the quality of data sources to make optimal decisions about resource allocation for the pre-training phase. By analyzing performance gains relative to acquisition costs, the approach extends point estimate approaches to estimating scaling laws. This enables cost-effective resource allocation for different data acquisition methods and sources. Experiments on a 7 billion-parameter pre-trained model validate the approach. It shows that by running multiple annealing runs, one can efficiently estimate the scaling behaviors of data sources, leading to different conclusions than using point estimates. This data-driven decision-making approach enhances the selection and optimization of data sources for training specialized models. <div>
arXiv:2507.22250v1 Announce Type: new 
Abstract: We introduce a framework for optimizing domain-specific dataset construction in foundation model training. Specifically, we seek a cost-efficient way to estimate the quality of data sources (e.g. synthetically generated or filtered web data, etc.) in order to make optimal decisions about resource allocation for data sourcing from these sources for the stage two pre-training phase, aka annealing, with the goal of specializing a generalist pre-trained model to specific domains. Our approach extends the usual point estimate approaches, aka micro-annealing, to estimating scaling laws by performing multiple annealing runs of varying compute spent on data curation and training. This addresses a key limitation in prior work, where reliance on point estimates for data scaling decisions can be misleading due to the lack of rank invariance across compute scales -- a phenomenon we confirm in our experiments. By systematically analyzing performance gains relative to acquisition costs, we find that scaling curves can be estimated for different data sources. Such scaling laws can inform cost effective resource allocation across different data acquisition methods (e.g. synthetic data), data sources (e.g. user or web data) and available compute resources. We validate our approach through experiments on a pre-trained model with 7 billion parameters. We adapt it to: a domain well-represented in the pre-training data -- the medical domain, and a domain underrepresented in the pretraining corpora -- the math domain. We show that one can efficiently estimate the scaling behaviors of a data source by running multiple annealing runs, which can lead to different conclusions, had one used point estimates using the usual micro-annealing technique instead. This enables data-driven decision-making for selecting and optimizing data sources.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-centric learning: from external reward maximization to internal knowledge curation</title>
<link>https://arxiv.org/abs/2507.22255</link>
<guid>https://arxiv.org/abs/2507.22255</guid>
<content:encoded><![CDATA[
<div> Keywords: general intelligence, representational empowerment, adaptability, knowledge structures, internal representations  
Summary:  
- The traditional focus on external objectives in the pursuit of general intelligence is shifting towards a new paradigm of representational empowerment.  
- Representational empowerment involves an agent's ability to control and diversify its own knowledge structures, emphasizing internal control rather than external mastery.  
- This approach aims to create adaptable intelligent systems by focusing on shaping internal representations as a means of achieving better preparedness.  
- By measuring an agent's capacity to shape its own understanding, representational empowerment offers a new perspective on achieving true adaptability in artificial intelligence.  
- Viewing internal representations as the primary substrate for computing empowerment provides a unique lens for designing intelligent systems that are highly adaptable and capable of self-improvement.  
  
<br /><br />Summary: <div>
arXiv:2507.22255v1 Announce Type: new 
Abstract: The pursuit of general intelligence has traditionally centered on external objectives: an agent's control over its environments or mastery of specific tasks. This external focus, however, can produce specialized agents that lack adaptability. We propose representational empowerment, a new perspective towards a truly agent-centric learning paradigm by moving the locus of control inward. This objective measures an agent's ability to controllably maintain and diversify its own knowledge structures. We posit that the capacity -- to shape one's own understanding -- is an element for achieving better ``preparedness'' distinct from direct environmental influence. Focusing on internal representations as the main substrate for computing empowerment offers a new lens through which to design adaptable intelligent systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2507.22270</link>
<guid>https://arxiv.org/abs/2507.22270</guid>
<content:encoded><![CDATA[
<div> Conditional flow matching, CFM, continuous normalizing flows, Weighted Conditional Flow Matching, W-CFM<br />
<br />
Summary:<br />
Conditional flow matching (CFM) is a powerful framework for training continuous normalizing flows. However, standard CFM can produce non-straight paths between prior and target distributions, leading to slower and less accurate generations. Weighted Conditional Flow Matching (W-CFM) addresses this issue by modifying the CFM loss with a Gibbs kernel, improving trajectory straightness. W-CFM recovers entropic optimal transport coupling and shows equivalence to mini-batch OT in large-batch sizes. Empirical tests on various datasets demonstrate W-CFM's ability to achieve high sample quality, fidelity, and diversity comparable to other baselines while maintaining computational efficiency. <div>
arXiv:2507.22270v1 Announce Type: new 
Abstract: Conditional flow matching (CFM) has emerged as a powerful framework for training continuous normalizing flows due to its computational efficiency and effectiveness. However, standard CFM often produces paths that deviate significantly from straight-line interpolations between prior and target distributions, making generation slower and less accurate due to the need for fine discretization at inference. Recent methods enhance CFM performance by inducing shorter and straighter trajectories but typically rely on computationally expensive mini-batch optimal transport (OT). Drawing insights from entropic optimal transport (EOT), we propose Weighted Conditional Flow Matching (W-CFM), a novel approach that modifies the classical CFM loss by weighting each training pair $(x, y)$ with a Gibbs kernel. We show that this weighting recovers the entropic OT coupling up to some bias in the marginals, and we provide the conditions under which the marginals remain nearly unchanged. Moreover, we establish an equivalence between W-CFM and the minibatch OT method in the large-batch limit, showing how our method overcomes computational and performance bottlenecks linked to batch size. Empirically, we test our method on unconditional generation on various synthetic and real datasets, confirming that W-CFM achieves comparable or superior sample quality, fidelity, and diversity to other alternative baselines while maintaining the computational efficiency of vanilla CFM.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Cluster-Based Cross-Validation Strategies for Machine Learning Model Evaluation</title>
<link>https://arxiv.org/abs/2507.22299</link>
<guid>https://arxiv.org/abs/2507.22299</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-validation, Clustering algorithms, Mini Batch K-Means, Bias, Variance

Summary: 
Cluster-based cross-validation strategies were examined to address biases in model performance evaluation in machine learning. Different clustering algorithms were compared, and a new technique combining Mini Batch K-Means with class stratification was proposed. Experimental results on 20 datasets showed that this technique outperformed others in bias and variance on balanced datasets but did not reduce computational cost significantly. Traditional stratified cross-validation was found to be more effective for imbalanced datasets, showing lower bias, variance, and computational cost. No single clustering algorithm consistently outperformed others. This work contributes to enhancing model evaluation strategies, emphasizing the reliability of established techniques like stratified cross-validation and providing insights for improving performance evaluation in datasets with clustering characteristics. <br /><br />Summary: <div>
arXiv:2507.22299v1 Announce Type: new 
Abstract: Cross-validation plays a fundamental role in Machine Learning, enabling robust evaluation of model performance and preventing overestimation on training and validation data. However, one of its drawbacks is the potential to create data subsets (folds) that do not adequately represent the diversity of the original dataset, which can lead to biased performance estimates. The objective of this work is to deepen the investigation of cluster-based cross-validation strategies by analyzing the performance of different clustering algorithms through experimental comparison. Additionally, a new cross-validation technique that combines Mini Batch K-Means with class stratification is proposed. Experiments were conducted on 20 datasets (both balanced and imbalanced) using four supervised learning algorithms, comparing cross-validation strategies in terms of bias, variance, and computational cost. The technique that uses Mini Batch K-Means with class stratification outperformed others in terms of bias and variance on balanced datasets, though it did not significantly reduce computational cost. On imbalanced datasets, traditional stratified cross-validation consistently performed better, showing lower bias, variance, and computational cost, making it a safe choice for performance evaluation in scenarios with class imbalance. In the comparison of different clustering algorithms, no single algorithm consistently stood out as superior. Overall, this work contributes to improving predictive model evaluation strategies by providing a deeper understanding of the potential of cluster-based data splitting techniques and reaffirming the effectiveness of well-established strategies like stratified cross-validation. Moreover, it highlights perspectives for increasing the robustness and reliability of model evaluations, especially in datasets with clustering characteristics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS-SHRED: Enhancing SHRED for Robust Recovery of Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2507.22303</link>
<guid>https://arxiv.org/abs/2507.22303</guid>
<content:encoded><![CDATA[
<div> Compressed Sensing, Shallow Recurrent Decoder, Reconstruction, Spatiotemporal Dynamics, Adaptive Loss Function

Summary:
$\textbf{CS-SHRED}$ is a novel deep learning architecture that combines Compressed Sensing with a Shallow Recurrent Decoder to reconstruct spatiotemporal dynamics from incomplete or corrupted data. It utilizes a batch-based forward framework with $\ell_1$ regularization and an adaptive loss function combining MSE, MAE, and SNR regularization to robustly recover signals in challenging scenarios. The method shows significant improvements in reconstruction fidelity compared to traditional approaches, with better preservation of small-scale structures and increased robustness against noise and outliers. The jointly trained CS and SHRED design architecture, including an LSTM sequence model and shallow decoder network, proves effective in characterizing temporal evolution and modeling high-dimensional state spaces. The SNR-guided adaptive loss function enhances spatiotemporal data recovery, making $\textbf{CS-SHRED}$ a promising tool for various environmental, climatic, and scientific data analysis applications. 

<br /><br />Summary: 
- Integrates Compressed Sensing with Shallow Recurrent Decoder for spatiotemporal dynamics reconstruction 
- Utilizes batch-based forward framework and adaptive loss function 
- Achieves higher reconstruction fidelity and robustness 
- Combines LSTM sequence model with shallow decoder network 
- Promising tool for environmental, climatic, and scientific data analysis applications <div>
arXiv:2507.22303v1 Announce Type: new 
Abstract: We present $\textbf{CS-SHRED}$, a novel deep learning architecture that integrates Compressed Sensing (CS) into a Shallow Recurrent Decoder ($\textbf{SHRED}$) to reconstruct spatiotemporal dynamics from incomplete, compressed, or corrupted data. Our approach introduces two key innovations. First, by incorporating CS techniques into the $\textbf{SHRED}$ architecture, our method leverages a batch-based forward framework with $\ell_1$ regularization to robustly recover signals even in scenarios with sparse sensor placements, noisy measurements, and incomplete sensor acquisitions. Second, an adaptive loss function dynamically combines Mean Squared Error (MSE) and Mean Absolute Error (MAE) terms with a piecewise Signal-to-Noise Ratio (SNR) regularization, which suppresses noise and outliers in low-SNR regions while preserving fine-scale features in high-SNR regions.
  We validate $\textbf{CS-SHRED}$ on challenging problems including viscoelastic fluid flows, maximum specific humidity fields, sea surface temperature distributions, and rotating turbulent flows. Compared to the traditional $\textbf{SHRED}$ approach, $\textbf{CS-SHRED}$ achieves significantly higher reconstruction fidelity - as demonstrated by improved SSIM and PSNR values, lower normalized errors, and enhanced LPIPS scores-thereby providing superior preservation of small-scale structures and increased robustness against noise and outliers.
  Our results underscore the advantages of the jointly trained CS and SHRED design architecture which includes an LSTM sequence model for characterizing the temporal evolution with a shallow decoder network (SDN) for modeling the high-dimensional state space. The SNR-guided adaptive loss function for the spatiotemporal data recovery establishes $\textbf{CS-SHRED}$ as a promising tool for a wide range of applications in environmental, climatic, and scientific data analyses.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypernetworks for Model-Heterogeneous Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2507.22330</link>
<guid>https://arxiv.org/abs/2507.22330</guid>
<content:encoded><![CDATA[
<div> hypernetwork-based methods, federated learning, personalized learning, model heterogeneity, privacy

Summary: 
The paper focuses on addressing client model heterogeneity in personalized federated learning without relying on external data or requiring disclosure of client model architectures. The proposed framework, MH-pFedHN, utilizes a server-side hypernetwork to generate personalized parameters for each client's heterogeneous model. By incorporating a multi-head structure in the hypernetwork, clients with similar model sizes can share heads to promote knowledge sharing and reduce computation. Additionally, the framework introduces MH-pFedHNGD, which includes a lightweight global model to enhance generalization. Extensive experiments across multiple benchmarks demonstrate that the approach achieves competitive accuracy and strong generalization. The framework offers enhanced privacy and flexibility, making it a robust baseline for future research in model-heterogeneous personalized federated learning. 

<br /><br /> Summary: <div>
arXiv:2507.22330v1 Announce Type: new 
Abstract: Recent advances in personalized federated learning have focused on addressing client model heterogeneity. However, most existing methods still require external data, rely on model decoupling, or adopt partial learning strategies, which can limit their practicality and scalability. In this paper, we revisit hypernetwork-based methods and leverage their strong generalization capabilities to design a simple yet effective framework for heterogeneous personalized federated learning. Specifically, we propose MH-pFedHN, which leverages a server-side hypernetwork that takes client-specific embedding vectors as input and outputs personalized parameters tailored to each client's heterogeneous model. To promote knowledge sharing and reduce computation, we introduce a multi-head structure within the hypernetwork, allowing clients with similar model sizes to share heads. Furthermore, we further propose MH-pFedHNGD, which integrates an optional lightweight global model to improve generalization. Our framework does not rely on external datasets and does not require disclosure of client model architectures, thereby offering enhanced privacy and flexibility. Extensive experiments on multiple benchmarks and model settings demonstrate that our approach achieves competitive accuracy, strong generalization, and serves as a robust baseline for future research in model-heterogeneous personalized federated learning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parametrized Multi-Agent Routing via Deep Attention Models</title>
<link>https://arxiv.org/abs/2507.22338</link>
<guid>https://arxiv.org/abs/2507.22338</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, sequential decision-making, facility location, path optimization, neural networks

Summary:
<br />
The paper introduces a scalable deep learning framework, ParaSDM, for parametrized sequential decision-making problems where multiple agents optimize discrete action policies and shared continuous parameters. Specifically, the framework is applied to Facility-Location and Path Optimization (FLPO) tasks, where agents must determine optimal routes and facility locations to minimize transportation costs within a network. The proposed Shortest Path Network (SPN) neural policy model, integrated with the Maximum Entropy Principle, achieves significant speedups in policy inference and gradient computation over traditional MEP baselines. The FLPO approach outperforms metaheuristic methods in cost efficiency while matching the optimal cost achieved by Gurobi with a substantial speedup. These results demonstrate the effectiveness of structured deep models in solving large-scale mixed-integer optimization problems. 
<br />
Summary: <div>
arXiv:2507.22338v1 Announce Type: new 
Abstract: We propose a scalable deep learning framework for parametrized sequential decision-making (ParaSDM), where multiple agents jointly optimize discrete action policies and shared continuous parameters. A key subclass of this setting arises in Facility-Location and Path Optimization (FLPO), where multi-agent systems must simultaneously determine optimal routes and facility locations, aiming to minimize the cumulative transportation cost within the network. FLPO problems are NP-hard due to their mixed discrete-continuous structure and highly non-convex objective. To address this, we integrate the Maximum Entropy Principle (MEP) with a neural policy model called the Shortest Path Network (SPN)-a permutation-invariant encoder-decoder that approximates the MEP solution while enabling efficient gradient-based optimization over shared parameters. The SPN achieves up to 100$\times$ speedup in policy inference and gradient computation compared to MEP baselines, with an average optimality gap of approximately 6% across a wide range of problem sizes. Our FLPO approach yields over 10$\times$ lower cost than metaheuristic baselines while running significantly faster, and matches Gurobi's optimal cost with annealing at a 1500$\times$ speedup-establishing a new state of the art for ParaSDM problems. These results highlight the power of structured deep models for solving large-scale mixed-integer optimization tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSQ: Memory-Efficient Bit Sparsification Quantization</title>
<link>https://arxiv.org/abs/2507.22349</link>
<guid>https://arxiv.org/abs/2507.22349</guid>
<content:encoded><![CDATA[
<div> quantization, DNNs, mixed-precision, sparsity, efficiency

Summary:
Memory-Efficient Bit Sparsification Quantization (MSQ) is proposed to optimize model efficiency of deep neural networks (DNNs) on mobile and edge devices. MSQ utilizes round-clamp quantizer for differentiable computation of least significant bits (LSBs) and induces sparsity in these LSBs through regularization. It incorporates Hessian information for simultaneous pruning of multiple LSBs to enhance training efficiency. MSQ achieves up to 8.00x reduction in trainable parameters and up to 86% reduction in training time compared to previous quantization methods while maintaining competitive accuracy and compression rates. This approach offers a practical solution for training efficient DNNs on resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2507.22349v1 Announce Type: new 
Abstract: As deep neural networks (DNNs) see increased deployment on mobile and edge devices, optimizing model efficiency has become crucial. Mixed-precision quantization is widely favored, as it offers a superior balance between efficiency and accuracy compared to uniform quantization. However, finding the optimal precision for each layer is challenging. Recent studies utilizing bit-level sparsity have shown promise, yet they often introduce substantial training complexity and high GPU memory requirements. In this paper, we propose Memory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that addresses these limitations. MSQ applies a round-clamp quantizer to enable differentiable computation of the least significant bits (LSBs) from model weights. It further employs regularization to induce sparsity in these LSBs, enabling effective precision reduction without explicit bit-level parameter splitting. Additionally, MSQ incorporates Hessian information, allowing the simultaneous pruning of multiple LSBs to further enhance training efficiency. Experimental results show that MSQ achieves up to 8.00x reduction in trainable parameters and up to 86% reduction in training time compared to previous bit-level quantization, while maintaining competitive accuracy and compression rates. This makes it a practical solution for training efficient DNNs on resource-constrained devices.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of acoustic field in 1-D uniform duct with varying mean flow and temperature using neural networks</title>
<link>https://arxiv.org/abs/2507.22370</link>
<guid>https://arxiv.org/abs/2507.22370</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, physical laws, sound propagation, heterogeneous medium, acoustic field <br />
Summary:<br />
Neural networks influenced by physical laws are being used as a computational tool for studying sound propagation in one-dimensional ducts with heterogeneous media. The paper derives the governing equation for this scenario and transforms it into an optimization problem solved using neural networks. The model predicts acoustic pressure and particle velocity, which are validated against a traditional Runge-Kutta solver. The study also investigates the impact of temperature gradients on the acoustic field. Additionally, the paper showcases the application of machine learning techniques such as transfer learning and automatic differentiation in acoustic simulations. This research demonstrates the potential of neural networks in solving complex acoustic problems efficiently and accurately. <br /><br />Summary: <div>
arXiv:2507.22370v1 Announce Type: new 
Abstract: Neural networks constrained by the physical laws emerged as an alternate numerical tool. In this paper, the governing equation that represents the propagation of sound inside a one-dimensional duct carrying a heterogeneous medium is derived. The problem is converted into an unconstrained optimization problem and solved using neural networks. Both the acoustic state variables: acoustic pressure and particle velocity are predicted and validated with the traditional Runge-Kutta solver. The effect of the temperature gradient on the acoustic field is studied. Utilization of machine learning techniques such as transfer learning and automatic differentiation for acoustic applications is demonstrated.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</title>
<link>https://arxiv.org/abs/2507.22424</link>
<guid>https://arxiv.org/abs/2507.22424</guid>
<content:encoded><![CDATA[
<div> acceleration, Vision-Language-Action models, Speculative Decoding, action prediction, speed improvement
Summary:
Spec-VLA is a Speculative Decoding framework designed to accelerate Vision-Language-Action (VLA) models by incorporating efficient drafting and parallel verification. While traditional VLMs pose computational challenges, Spec-VLA introduces strategies to relax acceptance and boost generation speed by utilizing relative distances represented by action tokens. Empirical results demonstrate a 1.42 times speedup compared to the OpenVLA baseline, with a 44% increase in acceptance length. The success of Spec-VLA showcases the potential for speculative execution in VLA prediction scenarios, enhancing the efficiency of VLA models while maintaining a high success rate. <div>
arXiv:2507.22424v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Late Fusion Model for Problem-Solving Strategy Classification in a Machine Learning Game</title>
<link>https://arxiv.org/abs/2507.22426</link>
<guid>https://arxiv.org/abs/2507.22426</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine learning, stealth assessment, digital learning environments, multimodal fusion model, problem-solving strategies <br />
Summary: 
The research introduces a multimodal late fusion model that combines visual data from screencasts with structured action sequences to enhance the classification of students' problem-solving strategies in digital learning environments. Traditional approaches often overlook subtle behavioral cues associated with cognitive strategies, but this new model aims to address this limitation. In a pilot study involving 149 secondary school students playing an educational game, the multimodal fusion model demonstrated superior performance compared to unimodal baseline models, achieving a classification accuracy increase of over 15%. The findings suggest that multimodal machine learning has the potential to provide more nuanced and strategy-sensitive assessments, leading to improved adaptive support for learners in interactive learning settings. <br /><br />Summary: <div>
arXiv:2507.22426v1 Announce Type: new 
Abstract: Machine learning models are widely used to support stealth assessment in digital learning environments. Existing approaches typically rely on abstracted gameplay log data, which may overlook subtle behavioral cues linked to learners' cognitive strategies. This paper proposes a multimodal late fusion model that integrates screencast-based visual data and structured in-game action sequences to classify students' problem-solving strategies. In a pilot study with secondary school students (N=149) playing a multitouch educational game, the fusion model outperformed unimodal baseline models, increasing classification accuracy by over 15%. Results highlight the potential of multimodal ML for strategy-sensitive assessment and adaptive support in interactive learning contexts.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss</title>
<link>https://arxiv.org/abs/2507.22428</link>
<guid>https://arxiv.org/abs/2507.22428</guid>
<content:encoded><![CDATA[
<div> adversarial attacks, cross-entropy loss, gradient computation errors, floating-point arithmetic, robustness evaluation

Summary:
This paper analyzes the impact of floating-point computation errors on gradient-based adversarial attacks using the Cross-Entropy loss. The study covers four scenarios of attack outcomes and identifies floating-point underflow and rounding as significant factors contributing to gradient computation instability. A new loss function, T-MIFPE, is proposed to minimize the influence of floating-point errors by incorporating an optimal scaling factor. Experimental results on multiple datasets show that T-MIFPE outperforms existing loss functions in terms of attack potency and robustness evaluation accuracy. The study provides valuable insights into improving the accuracy and reliability of gradient computation in adversarial attacks through theoretical analysis and the development of the T-MIFPE loss function. 

<br /><br />Summary: <div>
arXiv:2507.22428v1 Announce Type: new 
Abstract: Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often suffer from overestimation due to relative errors in gradient computation induced by floating-point arithmetic. This paper provides a rigorous theoretical analysis of these errors, conducting the first comprehensive study of floating-point computation errors in gradient-based attacks across four distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful targeted attacks. We establish theoretical foundations characterizing the behavior of relative numerical errors under different attack conditions, revealing previously unknown patterns in gradient computation instability, and identify floating-point underflow and rounding as key contributors. Building on this insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which incorporates an optimal scaling factor $T = t^*$ to minimize the impact of floating-point errors, thereby enhancing the accuracy of gradient computation in adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss functions, including CE, C\&amp;W, DLR, and MIFPE, in terms of attack potency and robustness evaluation accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RANA: Robust Active Learning for Noisy Network Alignment</title>
<link>https://arxiv.org/abs/2507.22434</link>
<guid>https://arxiv.org/abs/2507.22434</guid>
<content:encoded><![CDATA[
<div> Keywords: Network alignment, Noise, Active learning, Robustness, Denoising <br />
Summary:<br />
The article introduces RANA, a Robust Active learning framework for noisy Network Alignment, addressing the issue of noise in network alignment, including structural and labeling noise. RANA incorporates the Noise-aware Selection Module to tackle structural noise by selecting node pairs using a noise-aware maximization objective. The Label Denoising Module is introduced to handle labeling noise by utilizing a multi-source fusion denoising strategy. RANA improves alignment accuracy by addressing noise while also considering label sparsity. The framework outperforms state-of-the-art active learning-based methods in alignment accuracy on real-world datasets. The code for RANA is available on GitHub at https://github.com/YXNan0110/RANA. <br /> <div>
arXiv:2507.22434v1 Announce Type: new 
Abstract: Network alignment has attracted widespread attention in various fields. However, most existing works mainly focus on the problem of label sparsity, while overlooking the issue of noise in network alignment, which can substantially undermine model performance. Such noise mainly includes structural noise from noisy edges and labeling noise caused by human-induced and process-driven errors. To address these problems, we propose RANA, a Robust Active learning framework for noisy Network Alignment. RANA effectively tackles both structure noise and label noise while addressing the sparsity of anchor link annotations, which can improve the robustness of network alignment models. Specifically, RANA introduces the proposed Noise-aware Selection Module and the Label Denoising Module to address structural noise and labeling noise, respectively. In the first module, we design a noise-aware maximization objective to select node pairs, incorporating a cleanliness score to address structural noise. In the second module, we propose a novel multi-source fusion denoising strategy that leverages model and twin node pairs labeling to provide more accurate labels for node pairs. Empirical results on three real-world datasets demonstrate that RANA outperforms state-of-the-art active learning-based methods in alignment accuracy. Our code is available at https://github.com/YXNan0110/RANA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function</title>
<link>https://arxiv.org/abs/2507.22446</link>
<guid>https://arxiv.org/abs/2507.22446</guid>
<content:encoded><![CDATA[
<div> activation functions, deep neural networks, adversarial attacks, model robustness, Rademacher complexity

Summary:
The paper explores the use of activation functions to enhance the robustness of deep neural networks against adversarial attacks. A new activation function called Rademacher Complexity Reduction Activation Function (RCR-AF) is proposed, combining the benefits of GELU and ReLU while controlling model sparsity and capacity through two hyperparameters. Theoretical analysis grounded in Rademacher complexity shows that these parameters directly influence the model's robustness. Empirical evaluations demonstrate that RCR-AF consistently outperforms ReLU, GELU, and Swish in clean accuracy and adversarial robustness. The RCR-AF activation function offers improved generalization and gradient stability, making it a promising approach for enhancing model safety in challenging applications.<br /><br />Summary: <div>
arXiv:2507.22446v1 Announce Type: new 
Abstract: Despite their widespread success, deep neural networks remain critically vulnerable to adversarial attacks, posing significant risks in safety-sensitive applications. This paper investigates activation functions as a crucial yet underexplored component for enhancing model robustness. We propose a Rademacher Complexity Reduction Activation Function (RCR-AF), a novel activation function designed to improve both generalization and adversarial resilience. RCR-AF uniquely combines the advantages of GELU (including smoothness, gradient stability, and negative information retention) with ReLU's desirable monotonicity, while simultaneously controlling both model sparsity and capacity through built-in clipping mechanisms governed by two hyperparameters, $\alpha$ and $\gamma$. Our theoretical analysis, grounded in Rademacher complexity, demonstrates that these parameters directly modulate the model's Rademacher complexity, offering a principled approach to enhance robustness. Comprehensive empirical evaluations show that RCR-AF consistently outperforms widely-used alternatives (ReLU, GELU, and Swish) in both clean accuracy under standard training and in adversarial robustness within adversarial training paradigms.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework</title>
<link>https://arxiv.org/abs/2507.22464</link>
<guid>https://arxiv.org/abs/2507.22464</guid>
<content:encoded><![CDATA[
<div> Keywords: eGFR, Large Multimodal Models, interpretability, abductive reasoning, healthcare 

Summary: 
This study introduces a collaborative framework to improve the accuracy and interpretability of estimated glomerular filtration rate (eGFR) prediction using Large Multimodal Models (LMMs). The framework incorporates visual knowledge transfer, abductive reasoning, and a short-term memory mechanism to enhance prediction accuracy and interpretability. Experimental results demonstrate that the proposed framework achieves predictive performance and interpretability similar to proprietary models, while also providing clinically meaningful explanations for each prediction. By combining predictive accuracy with clinically grounded interpretability, this method offers a promising approach to building AI systems for healthcare. <div>
arXiv:2507.22464v1 Announce Type: new 
Abstract: Accurate and interpretable prediction of estimated glomerular filtration rate (eGFR) is essential for managing chronic kidney disease (CKD) and supporting clinical decisions. Recent advances in Large Multimodal Models (LMMs) have shown strong potential in clinical prediction tasks due to their ability to process visual and textual information. However, challenges related to deployment cost, data privacy, and model reliability hinder their adoption. In this study, we propose a collaborative framework that enhances the performance of open-source LMMs for eGFR forecasting while generating clinically meaningful explanations. The framework incorporates visual knowledge transfer, abductive reasoning, and a short-term memory mechanism to enhance prediction accuracy and interpretability. Experimental results show that the proposed framework achieves predictive performance and interpretability comparable to proprietary models. It also provides plausible clinical reasoning processes behind each prediction. Our method sheds new light on building AI systems for healthcare that combine predictive accuracy with clinically grounded interpretability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proto-EVFL: Enhanced Vertical Federated Learning via Dual Prototype with Extremely Unaligned Data</title>
<link>https://arxiv.org/abs/2507.22488</link>
<guid>https://arxiv.org/abs/2507.22488</guid>
<content:encoded><![CDATA[
<div> Keywords: vertical federated learning, class imbalance, dual prototypes, feature aggregation, convergence rate <br />
Summary: 
Proto-EVFL addresses challenges in vertical federated learning by introducing dual prototypes to handle class imbalance issues. Class prototypes help learn relationships between classes, enabling prediction of unseen classes. A probabilistic dual prototype learning scheme dynamically selects unaligned samples based on optimal transport cost and class prior probability. A mixed prior guided module combines local and global class probabilities to guide sample selection. An adaptive gated feature aggregation strategy mitigates feature contribution inconsistency by dynamically weighting and aggregating local features across parties. Proto-EVFL is the first bi-level optimization framework in VFL with a convergence rate of 1/\sqrt T. Experimental results show Proto-EVFL outperforms baselines, even in a zero-shot scenario with one unseen class, demonstrating its superiority in handling class imbalance and improving model performance. <br /> 
Summary: <div>
arXiv:2507.22488v1 Announce Type: new 
Abstract: In vertical federated learning (VFL), multiple enterprises address aligned sample scarcity by leveraging massive locally unaligned samples to facilitate collaborative learning. However, unaligned samples across different parties in VFL can be extremely class-imbalanced, leading to insufficient feature representation and limited model prediction space. Specifically, class-imbalanced problems consist of intra-party class imbalance and inter-party class imbalance, which can further cause local model bias and feature contribution inconsistency issues, respectively. To address the above challenges, we propose Proto-EVFL, an enhanced VFL framework via dual prototypes. We first introduce class prototypes for each party to learn relationships between classes in the latent space, allowing the active party to predict unseen classes. We further design a probabilistic dual prototype learning scheme to dynamically select unaligned samples by conditional optimal transport cost with class prior probability. Moreover, a mixed prior guided module guides this selection process by combining local and global class prior probabilities. Finally, we adopt an \textit{adaptive gated feature aggregation strategy} to mitigate feature contribution inconsistency by dynamically weighting and aggregating local features across different parties. We proved that Proto-EVFL, as the first bi-level optimization framework in VFL, has a convergence rate of 1/\sqrt T. Extensive experiments on various datasets validate the superiority of our Proto-EVFL. Even in a zero-shot scenario with one unseen class, it outperforms baselines by at least 6.97%
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning</title>
<link>https://arxiv.org/abs/2507.22499</link>
<guid>https://arxiv.org/abs/2507.22499</guid>
<content:encoded><![CDATA[
<div> machine unlearning, harmful content, generative models, Loss-based Reweighting Unlearning, data difficulty

Summary:
Loss-based Reweighting Unlearning (LoReUn) is introduced as a technique to address the risks of harmful content generation in machine learning models. Existing machine unlearning methods often treat all data equally, making it challenging to effectively forget difficult data. The authors demonstrate that the loss of data can reflect its difficulty, leading to the development of LoReUn. This approach dynamically reweights data during the unlearning process, improving the prevention of harmful content in image classification and generation tasks. LoReUn bridges the gap between existing machine unlearning methods and exact unlearning, with minimal computational overhead. The results show enhanced performance in preventing harmful content generation in text-to-image diffusion models. <div>
arXiv:2507.22499v1 Announce Type: new 
Abstract: Recent generative models face significant risks of producing harmful content, which has underscored the importance of machine unlearning (MU) as a critical technique for eliminating the influence of undesired data. However, existing MU methods typically assign the same weight to all data to be forgotten, which makes it difficult to effectively forget certain data that is harder to unlearn than others. In this paper, we empirically demonstrate that the loss of data itself can implicitly reflect its varying difficulty. Building on this insight, we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective plug-and-play strategy that dynamically reweights data during the unlearning process with minimal additional computational overhead. Our approach significantly reduces the gap between existing MU methods and exact unlearning in both image classification and generation tasks, effectively enhancing the prevention of harmful content generation in text-to-image diffusion models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry of nonlinear forecast reconciliation</title>
<link>https://arxiv.org/abs/2507.22500</link>
<guid>https://arxiv.org/abs/2507.22500</guid>
<content:encoded><![CDATA[
<div> Keywords: forecast reconciliation, constraints, probabilistic settings, nonlinear contexts, JAX Python package <br />
Summary: <br />
- Forecast reconciliation techniques have been extensively studied in the forecasting literature, with recent efforts focusing on extending these methods to probabilistic settings.<br />
- This paper fills a gap in the literature by providing formal theorems that demonstrate error reduction in nonlinear contexts, particularly for hypersurfaces with constant-sign curvature.<br />
- The authors also present probabilistic guarantees for hypersurfaces with non-constant-sign curvature and general vector-valued functions.<br />
- An exact analog of a theorem from previous work is derived, showcasing the effectiveness of reconciliation methods in reducing errors.<br />
- To enhance reproducibility and practical adoption of the findings, the authors have developed a JAX-based Python package that implements the presented theorems and reconciliation procedures upon publication. <br /> <div>
arXiv:2507.22500v1 Announce Type: new 
Abstract: Forecast reconciliation, an ex-post technique applied to forecasts that must satisfy constraints, has been a prominent topic in the forecasting literature over the past two decades. Recently, several efforts have sought to extend reconciliation methods to the probabilistic settings. Nevertheless, formal theorems demonstrating error reduction in nonlinear contexts, analogous to those presented in Panagiotelis et al.(2021), are still lacking. This paper addresses that gap by establishing such theorems for various classes of nonlinear hypersurfaces and vector-valued functions. Specifically, we derive an exact analog of Theorem 3.1 from Panagiotelis et al.(2021) for hypersurfaces with constant-sign curvature. Additionally, we provide probabilistic guarantees for the broader case of hypersurfaces with non-constant-sign curvature and for general vector-valued functions. To support reproducibility and practical adoption, we release a JAX-based Python package, \emph{to be released upon publication}, implementing the presented theorems and reconciliation procedures.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmilesT5: Domain-specific pretraining for molecular language models</title>
<link>https://arxiv.org/abs/2507.22514</link>
<guid>https://arxiv.org/abs/2507.22514</guid>
<content:encoded><![CDATA[
<div> SMILES strings, molecular property prediction, neural networks, domain-specific pretraining tasks, text-to-text pretraining <br />
Summary: 
The article discusses the use of neural networks in molecular property prediction, focusing on learning the language of molecules represented by SMILES strings. It introduces novel domain-specific text-to-text pretraining tasks that have shown improved performance in six classification-based molecular property prediction benchmarks compared to traditional methods. Through ablation studies, the article demonstrates that these tasks enhance data and computational efficiency. Additionally, pretrained embeddings from the model can be used as fixed inputs in a downstream machine learning classifier, offering comparable performance to fine-tuning but with lower computational overhead. This research highlights the potential of leveraging natural language processing techniques, such as masked language modelling, in the field of drug discovery and development. <div>
arXiv:2507.22514v1 Announce Type: new 
Abstract: Molecular property prediction is an increasingly critical task within drug discovery and development. Typically, neural networks can learn molecular properties using graph-based, language-based or feature-based methods. Recent advances in natural language processing have highlighted the capabilities of neural networks to learn complex human language using masked language modelling. These approaches to training large transformer-based deep learning models have also been used to learn the language of molecules, as represented by simplified molecular-input line-entry system (SMILES) strings. Here, we present novel domain-specific text-to-text pretraining tasks that yield improved performance in six classification-based molecular property prediction benchmarks, relative to both traditional likelihood-based training and previously proposed fine-tuning tasks. Through ablation studies, we show that data and computational efficiency can be improved by using these domain-specific pretraining tasks. Finally, the pretrained embeddings from the model can be used as fixed inputs into a downstream machine learning classifier and yield comparable performance to finetuning but with much lower computational overhead.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data</title>
<link>https://arxiv.org/abs/2507.22524</link>
<guid>https://arxiv.org/abs/2507.22524</guid>
<content:encoded><![CDATA[
<div> GCN, event sequence prediction, self-tuning toolkit, graph representations, prediction accuracy<br />
Summary:<br />
The article introduces HGCN(O), a self-tuning toolkit that utilizes Graph Convolutional Network (GCN) models for event sequence prediction. The toolkit includes four GCN architectures (O-GCN, T-GCN, TP-GCN, TE-GCN) with different configurations to optimize prediction accuracy and stability for balanced and unbalanced datasets. By integrating multiple graph representations and considering node- and graph-level attributes and temporal dependencies, HGCN(O) outperforms traditional approaches in predictive business process monitoring (PBPM) applications. Experimental results demonstrate that GCNConv models are particularly effective for unbalanced data, while all models perform consistently well on balanced datasets. HGCN(O) offers a versatile and powerful solution for accurately forecasting future events or states in a business process based on event logs.<br /> <div>
arXiv:2507.22524v1 Announce Type: new 
Abstract: We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network (GCN) models for event sequence prediction. Featuring four GCN architectures (O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our toolkit integrates multiple graph representations of event sequences with different choices of node- and graph-level attributes and in temporal dependencies via edge weights, optimising prediction accuracy and stability for balanced and unbalanced datasets. Extensive experiments show that GCNConv models excel on unbalanced data, while all models perform consistently on balanced data. Experiments also confirm the superior performance of HGCN(O) over traditional approaches. Applications include Predictive Business Process Monitoring (PBPM), which predicts future events or states of a business process based on event logs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FGFP: A Fractional Gaussian Filter and Pruning for Deep Neural Networks Compression</title>
<link>https://arxiv.org/abs/2507.22527</link>
<guid>https://arxiv.org/abs/2507.22527</guid>
<content:encoded><![CDATA[
<div> Fractional Gaussian filter, pruning, computational complexity, Adaptive Unstructured Pruning, compression ratios <br />
<br />Summary: <br />
Network compression techniques are crucial for deploying Deep Neural Networks (DNNs) on edge devices. The Fractional Gaussian Filters and Pruning (FGFP) framework introduced in this study leverages fractional-order differential calculus and Gaussian functions to construct efficient filters with minimal parameters. By utilizing Grunwald-Letnikov fractional derivatives to approximate fractional differential equations, computational complexity is reduced. The FGFP framework also incorporates Adaptive Unstructured Pruning (AUP) to achieve higher compression ratios. Experimental results demonstrate superior accuracy and compression performance compared to recent methods. For instance, on CIFAR-10, ResNet-20 achieves a minor 1.52% accuracy drop while reducing model size by 85.2%, and on ImageNet2012, ResNet-50 achieves just a 1.63% accuracy decrease while reducing model size by 69.1%. <div>
arXiv:2507.22527v1 Announce Type: new 
Abstract: Network compression techniques have become increasingly important in recent years because the loads of Deep Neural Networks (DNNs) are heavy for edge devices in real-world applications. While many methods compress neural network parameters, deploying these models on edge devices remains challenging. To address this, we propose the fractional Gaussian filter and pruning (FGFP) framework, which integrates fractional-order differential calculus and Gaussian function to construct fractional Gaussian filters (FGFs). To reduce the computational complexity of fractional-order differential operations, we introduce Gr\"unwald-Letnikov fractional derivatives to approximate the fractional-order differential equation. The number of parameters for each kernel in FGF is minimized to only seven. Beyond the architecture of Fractional Gaussian Filters, our FGFP framework also incorporates Adaptive Unstructured Pruning (AUP) to achieve higher compression ratios. Experiments on various architectures and benchmarks show that our FGFP framework outperforms recent methods in accuracy and compression. On CIFAR-10, ResNet-20 achieves only a 1.52% drop in accuracy while reducing the model size by 85.2%. On ImageNet2012, ResNet-50 achieves only a 1.63% drop in accuracy while reducing the model size by 69.1%.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accident-Driven Congestion Prediction and Simulation: An Explainable Framework Using Advanced Clustering and Bayesian Networks</title>
<link>https://arxiv.org/abs/2507.22529</link>
<guid>https://arxiv.org/abs/2507.22529</guid>
<content:encoded><![CDATA[
<div> AutoML, Deep Embedding Clustering, Bayesian Network, Traffic Congestion, Urban Mobility<br />
<br />
Summary:<br />
Traffic congestion caused by uncertainties like accidents is a major problem in urban areas, leading to delays, emissions, and safety risks. To tackle this issue, a robust framework is proposed for predicting accident impact on congestion. Automated Machine Learning-enhanced Deep Embedding Clustering is used to assign congestion labels to accident data, while a Bayesian Network predicts congestion probability. The Simulation of Urban Mobility is employed to validate the Bayesian Network's predictions with evidence-based scenarios. The results show that the AutoML-enhanced DEC outperforms traditional clustering methods. The BN model achieves a high accuracy of 95.6%, indicating its understanding of the complex relationship between accidents and congestion. Validation in SUMO confirms the BN model's reliability in predicting congestion states accurately, fostering smooth urban mobility. <br /> <div>
arXiv:2507.22529v1 Announce Type: new 
Abstract: Traffic congestion due to uncertainties, such as accidents, is a significant issue in urban areas, as the ripple effect of accidents causes longer delays, increased emissions, and safety concerns. To address this issue, we propose a robust framework for predicting the impact of accidents on congestion. We implement Automated Machine Learning (AutoML)-enhanced Deep Embedding Clustering (DEC) to assign congestion labels to accident data and predict congestion probability using a Bayesian Network (BN). The Simulation of Urban Mobility (SUMO) simulation is utilized to evaluate the correctness of BN predictions using evidence-based scenarios. Results demonstrate that the AutoML-enhanced DEC has outperformed traditional clustering approaches. The performance of the proposed BN model achieved an overall accuracy of 95.6%, indicating its ability to understand the complex relationship of accidents causing congestion. Validation in SUMO with evidence-based scenarios demonstrated that the BN model's prediction of congestion states closely matches those of SUMO, indicating the high reliability of the proposed BN model in ensuring smooth urban mobility.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law</title>
<link>https://arxiv.org/abs/2507.22543</link>
<guid>https://arxiv.org/abs/2507.22543</guid>
<content:encoded><![CDATA[
<div> Keywords: tokenization, natural language processing, Zipf's law, vocabulary size, model performance

Summary:
Tokenization is a crucial aspect of natural language processing and other sequence modeling fields, with vocabulary size playing a significant role in model performance. Traditional methods of selecting vocabulary size often rely on heuristics or dataset-specific choices. This work introduces a systematic approach for determining vocabulary size by analyzing token frequency distributions using Zipf's law. The study shows a strong correlation between downstream task performance and adherence to power-law behavior, indicating that aligning with Zipfian scaling can enhance both model efficiency and effectiveness. Experimental results across various domains such as NLP, genomics, and chemistry consistently show that models achieve optimal performance when token distributions closely follow Zipf's law. This research establishes Zipfian alignment as a robust and generalizable criterion for selecting vocabulary size. 

<br /><br />Summary: <div>
arXiv:2507.22543v1 Announce Type: new 
Abstract: Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamics-Inspired Computing with Oscillatory Neural Networks for Inverse Matrix Computation</title>
<link>https://arxiv.org/abs/2507.22544</link>
<guid>https://arxiv.org/abs/2507.22544</guid>
<content:encoded><![CDATA[
<div> Thermodynamic-inspired computing paradigm, oscillatory neural networks, Ising machines, linear algebra problems, inverse matrix <br />
Summary: <br />
The article introduces a new computing paradigm based on oscillatory neural networks (ONNs), traditionally used for Ising machines in optimization. This study explores ONNs' potential in solving linear algebra problems, specifically the inverse matrix, by leveraging thermodynamic principles. The linear approximation of the coupled Kuramoto oscillator model is found to provide the inverse matrix solution theoretically. Numerical simulations confirm the effectiveness of this approach, highlighting optimal parameter regimes for accurate computation. This research demonstrates the versatility of ONNs beyond optimization tasks and their potential for tackling linear algebra problems efficiently by exploiting thermodynamic concepts. <div>
arXiv:2507.22544v1 Announce Type: new 
Abstract: We describe a thermodynamic-inspired computing paradigm based on oscillatory neural networks (ONNs). While ONNs have been widely studied as Ising machines for tackling complex combinatorial optimization problems, this work investigates their feasibility in solving linear algebra problems, specifically the inverse matrix. Grounded in thermodynamic principles, we analytically demonstrate that the linear approximation of the coupled Kuramoto oscillator model leads to the inverse matrix solution. Numerical simulations validate the theoretical framework, and we examine the parameter regimes that computation has the highest accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepC4: Deep Conditional Census-Constrained Clustering for Large-scale Multitask Spatial Disaggregation of Urban Morphology</title>
<link>https://arxiv.org/abs/2507.22554</link>
<guid>https://arxiv.org/abs/2507.22554</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainable development, disaster risk reduction, deep learning, spatial disaggregation, urban morphology <br />
Summary: <br />
The article presents a novel deep learning-based approach called Deep Conditional Census-Constrained Clustering (DeepC4) to improve mapping of urban morphology in developing economies for sustainable development and disaster risk reduction. This approach integrates local census statistics as cluster-level constraints and considers multiple conditional label relationships in a joint multitask learning process. By enhancing the quality of mapping in Rwanda at a third-level administrative unit using the 2022 census data, DeepC4 addresses challenges related to model uncertainties and discrepancies with validated census statistics. This advance in spatial disaggregation techniques offers a way to audit existing coarse-grained information derived from satellite imagery and geospatial datasets at large scales, contributing to a better understanding of urban environments and vulnerabilities for informed decision-making towards achieving global frameworks by 2030. <br /> <div>
arXiv:2507.22554v1 Announce Type: new 
Abstract: To understand our global progress for sustainable development and disaster risk reduction in many developing economies, two recent major initiatives - the Uniform African Exposure Dataset of the Global Earthquake Model (GEM) Foundation and the Modelling Exposure through Earth Observation Routines (METEOR) Project - implemented classical spatial disaggregation techniques to generate large-scale mapping of urban morphology using the information from various satellite imagery and its derivatives, geospatial datasets of the built environment, and subnational census statistics. However, the local discrepancy with well-validated census statistics and the propagated model uncertainties remain a challenge in such coarse-to-fine-grained mapping problems, specifically constrained by weak and conditional label supervision. Therefore, we present Deep Conditional Census-Constrained Clustering (DeepC4), a novel deep learning-based spatial disaggregation approach that incorporates local census statistics as cluster-level constraints while considering multiple conditional label relationships in a joint multitask learning of the patterns of satellite imagery. To demonstrate, compared to GEM and METEOR, we enhanced the quality of Rwandan maps of urban morphology, specifically building exposure and physical vulnerability, at the third-level administrative unit from the 2022 census. As the world approaches the conclusion of our global frameworks in 2030, our work has offered a new deep learning-based mapping technique towards a spatial auditing of our existing coarse-grained derived information at large scales.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAR: Visual Analysis for Rashomon Set of Machine Learning Models' Performance</title>
<link>https://arxiv.org/abs/2507.22556</link>
<guid>https://arxiv.org/abs/2507.22556</guid>
<content:encoded><![CDATA[
<div> Visualization, Machine Learning models, Rashomon set, comparison, VAR

Summary:<br />
The article introduces the VAR visualization solution for comparing closely matched Machine Learning (ML) models in the Rashomon set. Existing analysis on the Rashomon set has focused on vertical structural comparisons, but the lack of effective visualization methods for horizontally comparing specific features prompted the development of VAR. This solution combines heatmaps and scatter plots to enable ML model developers to identify the optimal model under specific conditions. By utilizing VAR, developers can better understand the characteristics of the Rashomon set and make informed decisions on selecting the most suitable model for their needs. <div>
arXiv:2507.22556v1 Announce Type: new 
Abstract: Evaluating the performance of closely matched machine learning(ML) models under specific conditions has long been a focus of researchers in the field of machine learning. The Rashomon set is a collection of closely matched ML models, encompassing a wide range of models with similar accuracies but different structures. Traditionally, the analysis of these sets has focused on vertical structural analysis, which involves comparing the corresponding features at various levels within the ML models. However, there has been a lack of effective visualization methods for horizontally comparing multiple models with specific features. We propose the VAR visualization solution. VAR uses visualization to perform comparisons of ML models within the Rashomon set. This solution combines heatmaps and scatter plots to facilitate the comparison. With the help of VAR, ML model developers can identify the optimal model under specific conditions and better understand the Rashomon set's overall characteristics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div> framework, differential privacy, deep reinforcement learning, language models, privacy budget
Summary:
The article introduces RLDP, a novel framework that integrates differential privacy optimization with deep reinforcement learning for improving the trade-off between data privacy and model utility in large language models trained on sensitive datasets. RLDP dynamically adjusts gradient-clipping thresholds and noise magnitude per parameter during model training to optimize utility while maintaining privacy guarantees. Through extensive experiments on various language models, RLDP achieves significant perplexity reductions and downstream utility gains, outperforming existing methods. It accelerates model training by efficiently allocating the privacy budget, reducing the gradient-update budget by up to 71%. RLDP also demonstrates robustness against privacy attacks while honoring differential privacy constraints. <div>
arXiv:2507.22565v1 Announce Type: new 
Abstract: The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity</title>
<link>https://arxiv.org/abs/2507.22570</link>
<guid>https://arxiv.org/abs/2507.22570</guid>
<content:encoded><![CDATA[
<div> monotone matrices, deep learning, explainable AI, matrix classification, characteristic polynomial
Summary: 
This work introduces a novel methodology that combines deep learning with explainable AI to classify matrices based on abstract algebraic properties. By leveraging neural networks and XAI techniques, interpretable rules are derived from learned strategies. Focusing on monotone matrices with nonnegative inverses, the study establishes a dataset from randomly generated matrices in the range of (-1,1). Through deep neural network algorithms, it is determined that the absolute values of the two lowest-order coefficients of the characteristic polynomial, denoted as $c_0$ and $c_1, are sufficient for accurate classification. A data-driven analysis of 18,000 random $7\times7$ matrices reveals a probabilistic bound of $\lvert c_{0}/c_{1}\rvert\le0.18$ for identifying monotone matrices. This constraint translates to a minimal trace of $5.7$ for the inverse of a monotone matrix. <div>
arXiv:2507.22570v1 Announce Type: new 
Abstract: This work demonstrates a methodology for using deep learning to discover simple, practical criteria for classifying matrices based on abstract algebraic properties. By combining a high-performance neural network with explainable AI (XAI) techniques, we can distill a model's learned strategy into human-interpretable rules. We apply this approach to the challenging case of monotone matrices, defined by the condition that their inverses are entrywise nonnegative. Despite their simple definition, an easy characterization in terms of the matrix elements or the derived parameters is not known. Here, we present, to the best of our knowledge, the first systematic machine-learning approach for deriving a practical criterion that distinguishes monotone from non-monotone matrices. After establishing a labelled dataset by randomly generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ deep neural network algorithms for classifying the matrices as monotone or non-monotone, using both their entries and a comprehensive set of matrix features. By saliency methods, such as integrated gradients, we identify among all features, two matrix parameters which alone provide sufficient information for the matrix classification, with $95\%$ accuracy, namely the absolute values of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's characteristic polynomial. A data-driven study of 18,000 random $7\times7$ matrices shows that the monotone class obeys $\lvert c_{0}/c_{1}\rvert\le0.18$ with probability $>99.98\%$; because $\lvert c_{0}/c_{1}\rvert = 1/\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound $\mathrm{tr}(A^{-1})\ge5.7$.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning of geometrical cell division rules</title>
<link>https://arxiv.org/abs/2507.22587</link>
<guid>https://arxiv.org/abs/2507.22587</guid>
<content:encoded><![CDATA[
<div> Keywords: cell division, plant tissue organization, cell geometry, deep neural network, Arabidopsis thaliana embryo cells

Summary:<br />
The study focuses on the positioning of new cellular walls during cell division in plants, a key factor in shaping tissue organization. Traditional approaches have relied on predefined geometrical rules to link cell shape to division orientation, leading to limitations in understanding complex division patterns. In this work, a data-based approach utilizing deep neural networks is introduced to investigate the relationship between cell geometry and division plane positioning. By training a UNet architecture on image-based cell representations, the model successfully predicts division patterns based on mother cell geometry, even for previously unexplainable embryo division patterns in Arabidopsis thaliana. These findings demonstrate the potential of deep networks in unraveling complex cell division mechanisms and generating new hypotheses for the control of division positioning.<br /> 

Summary: <div>
arXiv:2507.22587v1 Announce Type: new 
Abstract: The positioning of new cellular walls during cell division plays a key role in shaping plant tissue organization. The influence of cell geometry on the positioning of division planes has been previously captured into various geometrical rules. Accordingly, linking cell shape to division orientation has relied on the comparison between observed division patterns and predictions under specific rules. The need to define a priori the tested rules is a fundamental limitation of this hypothesis-driven approach. As an alternative, we introduce a data-based approach to investigate the relation between cell geometry and division plane positioning, exploiting the ability of deep neural network to learn complex relationships across multidimensional spaces. Adopting an image-based cell representation, we show how division patterns can be learned and predicted from mother cell geometry using a UNet architecture modified to operate on cell masks. Using synthetic data and A. thaliana embryo cells, we evaluate the model performances on a wide range of diverse cell shapes and division patterns. We find that the trained model accounted for embryo division patterns that were previously irreconcilable under existing geometrical rules. Our work shows the potential of deep networks to understand cell division patterns and to generate new hypotheses on the control of cell division positioning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity</title>
<link>https://arxiv.org/abs/2507.22633</link>
<guid>https://arxiv.org/abs/2507.22633</guid>
<content:encoded><![CDATA[
<div> Keywords: federated fine-tuning, hybrid heterogeneity, matrix aggregation, multi-task knowledge interference, H2Tune <br />
Summary:  
H2Tune addresses the challenges of heterogeneous task requirements and resource limitations in hybrid heterogeneous federated fine-tuning. It introduces sparsified triple matrix decomposition to align hidden dimensions, relation-guided matrix layer alignment to handle heterogeneous layer structures, and alternating task-knowledge disentanglement mechanism to separate shared and specific knowledge. The proposed framework guarantees convergence with a rate of O(1/\sqrt{T}). Experimental results demonstrate H2Tune outperforms existing methods by up to 15.4% in accuracy. The code for H2Tune is publicly available for further exploration. <br /> 
Summary: <div>
arXiv:2507.22633v1 Announce Type: new 
Abstract: Different from existing federated fine-tuning (FFT) methods for foundation models, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored scenario where clients exhibit double heterogeneity in model architectures and downstream tasks. This hybrid heterogeneity introduces two significant challenges: 1) heterogeneous matrix aggregation, where clients adopt different large-scale foundation models based on their task requirements and resource limitations, leading to dimensional mismatches during LoRA parameter aggregation; and 2) multi-task knowledge interference, where local shared parameters, trained with both task-shared and task-specific knowledge, cannot ensure only task-shared knowledge is transferred between clients. To address these challenges, we propose H2Tune, a federated foundation model fine-tuning with hybrid heterogeneity. Our framework H2Tune consists of three key components: (i) sparsified triple matrix decomposition to align hidden dimensions across clients through constructing rank-consistent middle matrices, with adaptive sparsification based on client resources; (ii) relation-guided matrix layer alignment to handle heterogeneous layer structures and representation capabilities; and (iii) alternating task-knowledge disentanglement mechanism to decouple shared and specific knowledge of local model parameters through alternating optimization. Theoretical analysis proves a convergence rate of O(1/\sqrt{T}). Extensive experiments show our method achieves up to 15.4% accuracy improvement compared to state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/H2Tune-1407.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transductive Model Selection under Prior Probability Shift</title>
<link>https://arxiv.org/abs/2507.22647</link>
<guid>https://arxiv.org/abs/2507.22647</guid>
<content:encoded><![CDATA[
<div> transductive learning, supervised machine learning, inductive learning, dataset shift, hyperparameter optimization
Summary:
Transductive learning, a type of supervised machine learning, involves labeling a finite set of unlabelled data available at training time. This context, affected by dataset shift, presents challenges when the Independent and Identically Distributed (IID) assumption does not hold. The proposed method for transductive classification tackles model selection, specifically hyperparameter optimization, in the presence of prior probability shift common in anti-causal learning problems. Unlike traditional approaches that rely on cross-validation of labeled training data, this method optimizes hyperparameters directly on the unlabelled data. Experimental results demonstrate the effectiveness of this approach in enhancing model performance in transductive classification tasks. <br /><br />Summary: <div>
arXiv:2507.22647v1 Announce Type: new 
Abstract: Transductive learning is a supervised machine learning task in which, unlike in traditional inductive learning, the unlabelled data that require labelling are a finite set and are available at training time. Similarly to inductive learning contexts, transductive learning contexts may be affected by dataset shift, i.e., may be such that the IID assumption does not hold. We here propose a method, tailored to transductive classification contexts, for performing model selection (i.e., hyperparameter optimisation) when the data exhibit prior probability shift, an important type of dataset shift typical of anti-causal learning problems. In our proposed method the hyperparameters can be optimised directly on the unlabelled data to which the trained classifier must be applied; this is unlike traditional model selection methods, that are based on performing cross-validation on the labelled training data. We provide experimental results that show the benefits brought about by our method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster-Based Random Forest Visualization and Interpretation</title>
<link>https://arxiv.org/abs/2507.22665</link>
<guid>https://arxiv.org/abs/2507.22665</guid>
<content:encoded><![CDATA[
<div> Random forests, machine learning, decision trees, interpretability, visualization<br />
Summary:<br />
This paper introduces a visualization method and system to enhance the interpretability of random forests, a machine learning technique composed of multiple decision trees. The approach clusters similar decision trees to facilitate understanding the model's overall performance without the need to analyze each tree individually. A new distance metric considering decision rules and predictions of tree pairs is proposed for meaningful clustering. The Feature Plot visualizes the topological positions of features, while the Rule Plot displays decision rules, aiding in interpretation. The effectiveness of this method is demonstrated through a case study on the "Glass" dataset and a small user study, showcasing its potential for improving the interpretability of random forests. <br />Summary: <div>
arXiv:2507.22665v1 Announce Type: new 
Abstract: Random forests are a machine learning method used to automatically classify datasets and consist of a multitude of decision trees. While these random forests often have higher performance and generalize better than a single decision tree, they are also harder to interpret. This paper presents a visualization method and system to increase interpretability of random forests. We cluster similar trees which enables users to interpret how the model performs in general without needing to analyze each individual decision tree in detail, or interpret an oversimplified summary of the full forest. To meaningfully cluster the decision trees, we introduce a new distance metric that takes into account both the decision rules as well as the predictions of a pair of decision trees. We also propose two new visualization methods that visualize both clustered and individual decision trees: (1) The Feature Plot, which visualizes the topological position of features in the decision trees, and (2) the Rule Plot, which visualizes the decision rules of the decision trees. We demonstrate the efficacy of our approach through a case study on the "Glass" dataset, which is a relatively complex standard machine learning dataset, as well as a small user study.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Prediction of CAR T-Cell Cytotoxicity with Quantum-Kernel Methods</title>
<link>https://arxiv.org/abs/2507.22710</link>
<guid>https://arxiv.org/abs/2507.22710</guid>
<content:encoded><![CDATA[
<div> CAR T-cells, Chimeric antigen receptor, Co-stimulatory domains, Quantum computing, Data-constrained<br />
Summary:<br />
Chimeric antigen receptor (CAR) T-cells are engineered to target tumor cells by binding to specific antigens, with activation and proliferation regulated by co-stimulatory domains. The vast combinatorial space of possible CAR T-cell constructs poses challenges in experimental testing. A quantum approach, using Projected Quantum Kernel (PQK) on a gate-based quantum computer, offers a solution by embedding classical data into a high-dimensional Hilbert space. This approach demonstrates enhanced classification performance for predicting CAR T cytotoxicity, particularly in areas with lesser available information. The study showcases the potential of quantum computing in addressing data-constrained problems and improving learning outcomes for specific signaling domains and domain positions within CAR T-cell constructs. <div>
arXiv:2507.22710v1 Announce Type: new 
Abstract: Chimeric antigen receptor (CAR) T-cells are T-cells engineered to recognize and kill specific tumor cells. Through their extracellular domains, CAR T-cells bind tumor cell antigens which triggers CAR T activation and proliferation. These processes are regulated by co-stimulatory domains present in the intracellular region of the CAR T-cell. Through integrating novel signaling components into the co-stimulatory domains, it is possible to modify CAR T-cell phenotype. Identifying and experimentally testing new CAR constructs based on libraries of co-stimulatory domains is nontrivial given the vast combinatorial space defined by such libraries. This leads to a highly data constrained, poorly explored combinatorial problem, where the experiments undersample all possible combinations. We propose a quantum approach using a Projected Quantum Kernel (PQK) to address this challenge. PQK operates by embedding classical data into a high dimensional Hilbert space and employs a kernel method to measure sample similarity. Using 61 qubits on a gate-based quantum computer, we demonstrate the largest PQK application to date and an enhancement in the classification performance over purely classical machine learning methods for CAR T cytotoxicity prediction. Importantly, we show improved learning for specific signaling domains and domain positions, particularly where there was lower information highlighting the potential for quantum computing in data-constrained problems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models</title>
<link>https://arxiv.org/abs/2507.22766</link>
<guid>https://arxiv.org/abs/2507.22766</guid>
<content:encoded><![CDATA[
<div> Keywords: Sensor-based sorting, Bayesian Optimization, Gaussian process regression, Process parameters, Material stream composition <br />
Summary: <br />
Sensor-based sorting systems are essential for separating material streams efficiently. This paper presents an approach to continuously optimize, monitor, and adjust the process parameters of such systems. By utilizing Bayesian Optimization with Gaussian process regression models, specific system behavior requirements can be met while considering uncertainties. This approach minimizes the number of experiments needed and takes into account uncertainties in sorting accuracies. Two optimization targets based on material output streams are considered. The method was evaluated with three example process parameters, showcasing its effectiveness in achieving accurate and efficient sorting in sensor-based sorting systems. <div>
arXiv:2507.22766v1 Announce Type: new 
Abstract: Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization</title>
<link>https://arxiv.org/abs/2507.22767</link>
<guid>https://arxiv.org/abs/2507.22767</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network distillation, interpretable AI, Jacobian-based regularizer, regression benchmarks, predictive accuracy

Summary:
In this work, the focus is on distilling large neural networks into simpler and more interpretable symbolic formulas. The existing process of distillation often results in low-fidelity student models due to the complexity of functions learned by standard networks. A novel training paradigm is proposed, involving a Jacobian-based regularizer to actively encourage the teacher network to learn smoother and more amenable functions for distillation. Extensive experiments on real-world regression benchmarks show the effectiveness of this method. By optimizing the regularization strength for each problem, a significant improvement in the fidelity of the distilled symbolic model is achieved, with a 120% increase in R^2 score relative to standard distillation, while maintaining the teacher's predictive accuracy. This work offers a practical and principled approach to enhancing the fidelity of interpretable models derived from complex neural networks. 

<br /><br />Summary: <div>
arXiv:2507.22767v1 Announce Type: new 
Abstract: Distilling large neural networks into simple, human-readable symbolic formulas is a promising path toward trustworthy and interpretable AI. However, this process is often brittle, as the complex functions learned by standard networks are poor targets for symbolic discovery, resulting in low-fidelity student models. In this work, we propose a novel training paradigm to address this challenge. Instead of passively distilling a pre-trained network, we introduce a \textbf{Jacobian-based regularizer} that actively encourages the ``teacher'' network to learn functions that are not only accurate but also inherently smoother and more amenable to distillation. We demonstrate through extensive experiments on a suite of real-world regression benchmarks that our method is highly effective. By optimizing the regularization strength for each problem, we improve the $R^2$ score of the final distilled symbolic model by an average of \textbf{120\% (relative)} compared to the standard distillation pipeline, all while maintaining the teacher's predictive accuracy. Our work presents a practical and principled method for significantly improving the fidelity of interpretable models extracted from complex neural networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-free estimation of clinically relevant performance metrics under distribution shifts</title>
<link>https://arxiv.org/abs/2507.22776</link>
<guid>https://arxiv.org/abs/2507.22776</guid>
<content:encoded><![CDATA[
<div> performance monitoring, image classification models, confidence scores, clinical domain, dataset shifts

Summary:
The study focuses on performance monitoring of image classification models in a clinical setting where ground-truth labels are often unavailable. The research introduces new methods to estimate the full confusion matrix, allowing for a more comprehensive assessment of model performance. These methods were tested on chest x-ray data under real-world distribution shifts and simulated covariate and prevalence shifts. The results showed promising outcomes in predicting clinically relevant counting metrics. However, the study also highlighted limitations and failure modes of current performance estimation techniques, especially in simulated shift scenarios. The findings underscore the need for a better understanding of real-world deployment contexts to effectively implement performance monitoring techniques for postmarket surveillance of medical AI models. 

<br /><br />Summary: <div>
arXiv:2507.22776v1 Announce Type: new 
Abstract: Performance monitoring is essential for safe clinical deployment of image classification models. However, because ground-truth labels are typically unavailable in the target dataset, direct assessment of real-world model performance is infeasible. State-of-the-art performance estimation methods address this by leveraging confidence scores to estimate the target accuracy. Despite being a promising direction, the established methods mainly estimate the model's accuracy and are rarely evaluated in a clinical domain, where strong class imbalances and dataset shifts are common. Our contributions are twofold: First, we introduce generalisations of existing performance prediction methods that directly estimate the full confusion matrix. Then, we benchmark their performance on chest x-ray data in real-world distribution shifts as well as simulated covariate and prevalence shifts. The proposed confusion matrix estimation methods reliably predicted clinically relevant counting metrics on medical images under distribution shifts. However, our simulated shift scenarios exposed important failure modes of current performance estimation techniques, calling for a better understanding of real-world deployment contexts when implementing these performance monitoring techniques for postmarket surveillance of medical AI models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DO-EM: Density Operator Expectation Maximization</title>
<link>https://arxiv.org/abs/2507.22786</link>
<guid>https://arxiv.org/abs/2507.22786</guid>
<content:encoded><![CDATA[
<div> DOMs, Generative Modeling, Expectation-Maximization, Quantum Computing, Quantum Interleaved Deep Boltzmann Machines <br />
<br />
Summary: <br />
Density operators, essential in quantum computing, are now being used in machine learning for generative modeling. Traditional training algorithms for density operator models (DOMs) struggle with real-world datasets like MNIST. To address this, an Expectation-Maximization framework was developed to train latent variable models based on DOMs using classical hardware efficiently. The formulation of the Expectation step as a quantum information projection problem led to the creation of the Density Operator Expectation Maximization (DO-EM) algorithm, ensuring non-decreasing log-likelihood in model optimization. The algorithm was successfully applied to Quantum Interleaved Deep Boltzmann Machines (QiDBMs), yielding impressive results in image generation on the MNIST dataset with a significant reduction in the Frchet Inception Distance compared to classical DBMs. <div>
arXiv:2507.22786v1 Announce Type: new 
Abstract: Density operators, quantum generalizations of probability distributions, are gaining prominence in machine learning due to their foundational role in quantum computing. Generative modeling based on density operator models (\textbf{DOMs}) is an emerging field, but existing training algorithms -- such as those for the Quantum Boltzmann Machine -- do not scale to real-world data, such as the MNIST dataset. The Expectation-Maximization algorithm has played a fundamental role in enabling scalable training of probabilistic latent variable models on real-world datasets. \textit{In this paper, we develop an Expectation-Maximization framework to learn latent variable models defined through \textbf{DOMs} on classical hardware, with resources comparable to those used for probabilistic models, while scaling to real-world data.} However, designing such an algorithm is nontrivial due to the absence of a well-defined quantum analogue to conditional probability, which complicates the Expectation step. To overcome this, we reformulate the Expectation step as a quantum information projection (QIP) problem and show that the Petz Recovery Map provides a solution under sufficient conditions. Using this formulation, we introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an iterative Minorant-Maximization procedure that optimizes a quantum evidence lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing log-likelihood across iterations for a broad class of models. Finally, we present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a \textbf{DOM} that can be trained with the same resources as a DBM. When trained with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms larger classical DBMs in image generation on the MNIST dataset, achieving a 40--60\% reduction in the Fr\'echet Inception Distance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-Core: A Simple, Scalable and Balanced RLHF Trainer</title>
<link>https://arxiv.org/abs/2507.22789</link>
<guid>https://arxiv.org/abs/2507.22789</guid>
<content:encoded><![CDATA[
arXiv:2507.22789v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling. In this paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions. G-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models</title>
<link>https://arxiv.org/abs/2507.22798</link>
<guid>https://arxiv.org/abs/2507.22798</guid>
<content:encoded><![CDATA[
arXiv:2507.22798v1 Announce Type: new 
Abstract: We present a foundation model-derived method to identify highly informative tokens and events in electronic health records. Our approach considers incoming data in the entire context of a patient's hospitalization and so can flag anomalous events that rule-based approaches would consider within a normal range. We demonstrate that the events our model flags are significant for predicting downstream patient outcomes and that a fraction of events identified as carrying little information can safely be dropped. Additionally, we show how informativeness can help interpret the predictions of prognostic models trained on foundation model-derived representations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks</title>
<link>https://arxiv.org/abs/2507.22832</link>
<guid>https://arxiv.org/abs/2507.22832</guid>
<content:encoded><![CDATA[
arXiv:2507.22832v1 Announce Type: new 
Abstract: In this paper we argue that ReLU networks learn an implicit linear model we can actually tap into. We describe that alleged model formally and show that we can approximately pull its decision boundary back to the input space with certain simple modification to the backward pass. The resulting gradients (called excitation pullbacks) reveal high-resolution input- and target-specific features of remarkable perceptual alignment on a number of popular ImageNet-pretrained deep architectures. This strongly suggests that neural networks do, in fact, rely on learned interpretable patterns that can be recovered after training. Thus, our findings may have profound implications for knowledge discovery and the development of dependable artificial systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAF-Net: Phase-Aligned Frequency Decoupling Network for Multi-Process Manufacturing Quality Prediction</title>
<link>https://arxiv.org/abs/2507.22840</link>
<guid>https://arxiv.org/abs/2507.22840</guid>
<content:encoded><![CDATA[
arXiv:2507.22840v1 Announce Type: new 
Abstract: Accurate quality prediction in multi-process manufacturing is critical for industrial efficiency but hindered by three core challenges: time-lagged process interactions, overlapping operations with mixed periodicity, and inter-process dependencies in shared frequency bands. To address these, we propose PAF-Net, a frequency decoupled time series prediction framework with three key innovations: (1) A phase-correlation alignment method guided by frequency domain energy to synchronize time-lagged quality series, resolving temporal misalignment. (2) A frequency independent patch attention mechanism paired with Discrete Cosine Transform (DCT) decomposition to capture heterogeneous operational features within individual series. (3) A frequency decoupled cross attention module that suppresses noise from irrelevant frequencies, focusing exclusively on meaningful dependencies within shared bands. Experiments on 4 real-world datasets demonstrate PAF-Net's superiority. It outperforms 10 well-acknowledged baselines by 7.06% lower MSE and 3.88% lower MAE. Our code is available at https://github.com/StevenLuan904/PAF-Net-Official.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents</title>
<link>https://arxiv.org/abs/2507.22844</link>
<guid>https://arxiv.org/abs/2507.22844</guid>
<content:encoded><![CDATA[
arXiv:2507.22844v1 Announce Type: new 
Abstract: The development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term inefficient exploration. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning how to reason coherently. To address this, we introduce RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection, and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method. On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new state-of-the-art results, with our 7B model reaching an 83.6% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Differentially Private Power Method</title>
<link>https://arxiv.org/abs/2507.22849</link>
<guid>https://arxiv.org/abs/2507.22849</guid>
<content:encoded><![CDATA[
arXiv:2507.22849v1 Announce Type: new 
Abstract: We propose a novel Decentralized Differentially Private Power Method (D-DP-PM) for performing Principal Component Analysis (PCA) in networked multi-agent settings. Unlike conventional decentralized PCA approaches where each agent accesses the full n-dimensional sample space, we address the challenging scenario where each agent observes only a subset of dimensions through row-wise data partitioning. Our method ensures $(\epsilon,\delta)$-Differential Privacy (DP) while enabling collaborative estimation of global eigenvectors across the network without requiring a central aggregator. We achieve this by having agents share only local embeddings of the current eigenvector iterate, leveraging both the inherent privacy from random initialization and carefully calibrated Gaussian noise additions. We prove that our algorithm satisfies the prescribed $(\epsilon,\delta)$-DP guarantee and establish convergence rates that explicitly characterize the impact of the network topology. Our theoretical analysis, based on linear dynamics and high-dimensional probability theory, provides tight bounds on both privacy and utility. Experiments on real-world datasets demonstrate that D-DP-PM achieves superior privacy-utility tradeoffs compared to naive local DP approaches, with particularly strong performance in moderate privacy regimes ($\epsilon\in[2, 5]$). The method converges rapidly, allowing practitioners to trade iterations for enhanced privacy while maintaining competitive utility.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model</title>
<link>https://arxiv.org/abs/2507.22854</link>
<guid>https://arxiv.org/abs/2507.22854</guid>
<content:encoded><![CDATA[
arXiv:2507.22854v1 Announce Type: new 
Abstract: We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning</title>
<link>https://arxiv.org/abs/2507.19647</link>
<guid>https://arxiv.org/abs/2507.19647</guid>
<content:encoded><![CDATA[
arXiv:2507.19647v1 Announce Type: cross 
Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to learn from human expert demonstrations by framing the task as a supervised learning problem. However, IL often suffers from causal confusion, where agents misinterpret spurious correlations as causal relationships, leading to poor performance in testing environments with distribution shift. To address this issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a novel method that leverages the human gaze data gathered during the data collection phase to guide the representation learning in IL. GABRIL utilizes a regularization loss which encourages the model to focus on causally relevant features identified through expert gaze and consequently mitigates the effects of confounding variables. We validate our approach in Atari environments and the Bench2Drive benchmark in CARLA by collecting human gaze datasets and applying our method in both domains. Experimental results show that the improvement of GABRIL over behavior cloning is around 179% more than the same number for other baselines in the Atari and 76% in the CARLA setup. Finally, we show that our method provides extra explainability when compared to regular IL agents.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling and Distilling Transformer Models for sEMG</title>
<link>https://arxiv.org/abs/2507.22094</link>
<guid>https://arxiv.org/abs/2507.22094</guid>
<content:encoded><![CDATA[
arXiv:2507.22094v1 Announce Type: cross 
Abstract: Surface electromyography (sEMG) signals offer a promising avenue for developing innovative human-computer interfaces by providing insights into muscular activity. However, the limited volume of training data and computational constraints during deployment have restricted the investigation of scaling up the model size for solving sEMG tasks. In this paper, we demonstrate that vanilla transformer models can be effectively scaled up on sEMG data and yield improved cross-user performance up to 110M parameters, surpassing the model size regime investigated in other sEMG research (usually <10M parameters). We show that >100M-parameter models can be effectively distilled into models 50x smaller with minimal loss of performance (<1.5% absolute). This results in efficient and expressive models suitable for complex real-time sEMG tasks in real-world environments.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Posterior Bayesian Neural Networks with Dependent Weights</title>
<link>https://arxiv.org/abs/2507.22095</link>
<guid>https://arxiv.org/abs/2507.22095</guid>
<content:encoded><![CDATA[
arXiv:2507.22095v1 Announce Type: cross 
Abstract: In this paper we consider posterior Bayesian fully connected and feedforward deep neural networks with dependent weights. Particularly, if the likelihood is Gaussian, we identify the distribution of the wide width limit and provide an algorithm to sample from the network. In the shallow case we explicitly compute the distribution of the output, proving that it is a Gaussian mixture. All the theoretical results are numerically validated.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Truthful Representations Flip Under Deceptive Instructions?</title>
<link>https://arxiv.org/abs/2507.22149</link>
<guid>https://arxiv.org/abs/2507.22149</guid>
<content:encoded><![CDATA[
arXiv:2507.22149v1 Announce Type: cross 
Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stacked SVD or SVD stacked? A Random Matrix Theory perspective on data integration</title>
<link>https://arxiv.org/abs/2507.22170</link>
<guid>https://arxiv.org/abs/2507.22170</guid>
<content:encoded><![CDATA[
arXiv:2507.22170v1 Announce Type: cross 
Abstract: Modern data analysis increasingly requires identifying shared latent structure across multiple high-dimensional datasets. A commonly used model assumes that the data matrices are noisy observations of low-rank matrices with a shared singular subspace. In this case, two primary methods have emerged for estimating this shared structure, which vary in how they integrate information across datasets. The first approach, termed Stack-SVD, concatenates all the datasets, and then performs a singular value decomposition (SVD). The second approach, termed SVD-Stack, first performs an SVD separately for each dataset, then aggregates the top singular vectors across these datasets, and finally computes a consensus amongst them. While these methods are widely used, they have not been rigorously studied in the proportional asymptotic regime, which is of great practical relevance in today's world of increasing data size and dimensionality. This lack of theoretical understanding has led to uncertainty about which method to choose and limited the ability to fully exploit their potential. To address these challenges, we derive exact expressions for the asymptotic performance and phase transitions of these two methods and develop optimal weighting schemes to further improve both methods. Our analysis reveals that while neither method uniformly dominates the other in the unweighted case, optimally weighted Stack-SVD dominates optimally weighted SVD-Stack. We extend our analysis to accommodate multiple shared components, and provide practical algorithms for estimating optimal weights from data, offering theoretical guidance for method selection in practical data integration problems. Extensive numerical simulations and semi-synthetic experiments on genomic data corroborate our theoretical findings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models</title>
<link>https://arxiv.org/abs/2507.22187</link>
<guid>https://arxiv.org/abs/2507.22187</guid>
<content:encoded><![CDATA[
arXiv:2507.22187v1 Announce Type: cross 
Abstract: We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Together: Cross and Joint Covariances Enhance Signal Detectability in Undersampled Data</title>
<link>https://arxiv.org/abs/2507.22207</link>
<guid>https://arxiv.org/abs/2507.22207</guid>
<content:encoded><![CDATA[
arXiv:2507.22207v1 Announce Type: cross 
Abstract: Many data-science applications involve detecting a shared signal between two high-dimensional variables. Using random matrix theory methods, we determine when such signal can be detected and reconstructed from sample correlations, despite the background of sampling noise induced correlations. We consider three different covariance matrices constructed from two high-dimensional variables: their individual self covariance, their cross covariance, and the self covariance of the concatenated (joint) variable, which incorporates the self and the cross correlation blocks. We observe the expected Baik, Ben Arous, and P\'ech\'e detectability phase transition in all these covariance matrices, and we show that joint and cross covariance matrices always reconstruct the shared signal earlier than the self covariances. Whether the joint or the cross approach is better depends on the mismatch of dimensionalities between the variables. We discuss what these observations mean for choosing the right method for detecting linear correlations in data and how these findings may generalize to nonlinear statistical dependencies.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent-Aware Neural Query Reformulation for Behavior-Aligned Product Search</title>
<link>https://arxiv.org/abs/2507.22213</link>
<guid>https://arxiv.org/abs/2507.22213</guid>
<content:encoded><![CDATA[
arXiv:2507.22213v1 Announce Type: cross 
Abstract: Understanding and modeling buyer intent is a foundational challenge in optimizing search query reformulation within the dynamic landscape of e-commerce search systems. This work introduces a robust data pipeline designed to mine and analyze large-scale buyer query logs, with a focus on extracting fine-grained intent signals from both explicit interactions and implicit behavioral cues. Leveraging advanced sequence mining techniques and supervised learning models, the pipeline systematically captures patterns indicative of latent purchase intent, enabling the construction of a high-fidelity, intent-rich dataset. The proposed framework facilitates the development of adaptive query rewrite strategies by grounding reformulations in inferred user intent rather than surface-level lexical signals. This alignment between query rewriting and underlying user objectives enhances both retrieval relevance and downstream engagement metrics. Empirical evaluations across multiple product verticals demonstrate measurable gains in precision-oriented relevance metrics, underscoring the efficacy of intent-aware reformulation. Our findings highlight the value of intent-centric modeling in bridging the gap between sparse user inputs and complex product discovery goals, and establish a scalable foundation for future research in user-aligned neural retrieval and ranking systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation biases: will we achieve complete understanding by analyzing representations?</title>
<link>https://arxiv.org/abs/2507.22216</link>
<guid>https://arxiv.org/abs/2507.22216</guid>
<content:encoded><![CDATA[
arXiv:2507.22216v1 Announce Type: cross 
Abstract: A common approach in neuroscience is to study neural representations as a means to understand a system -- increasingly, by relating the neural representations to the internal representations learned by computational models. However, a recent work in machine learning (Lampinen, 2024) shows that learned feature representations may be biased to over-represent certain features, and represent others more weakly and less-consistently. For example, simple (linear) features may be more strongly and more consistently represented than complex (highly nonlinear) features. These biases could pose challenges for achieving full understanding of a system through representational analysis. In this perspective, we illustrate these challenges -- showing how feature representation biases can lead to strongly biased inferences from common analyses like PCA, regression, and RSA. We also present homomorphic encryption as a simple case study of the potential for strong dissociation between patterns of representation and computation. We discuss the implications of these results for representational comparisons between systems, and for neuroscience more generally.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Concept Drift with Deprecated Permissions in Android Malware Detection</title>
<link>https://arxiv.org/abs/2507.22231</link>
<guid>https://arxiv.org/abs/2507.22231</guid>
<content:encoded><![CDATA[
arXiv:2507.22231v1 Announce Type: cross 
Abstract: Permission analysis is a widely used method for Android malware detection. It involves examining the permissions requested by an application to access sensitive data or perform potentially malicious actions. In recent years, various machine learning (ML) algorithms have been applied to Android malware detection using permission-based features and feature selection techniques, often achieving high accuracy. However, these studies have largely overlooked important factors such as protection levels and the deprecation or restriction of permissions due to updates in the Android OS -- factors that can contribute to concept drift.
  In this study, we investigate the impact of deprecated and restricted permissions on the performance of machine learning models. A large dataset containing 166 permissions was used, encompassing more than 70,000 malware and benign applications. Various machine learning and deep learning algorithms were employed as classifiers, along with different concept drift detection strategies. The results suggest that Android permissions are highly effective features for malware detection, with the exclusion of deprecated and restricted permissions having only a marginal impact on model performance. In some cases, such as with CNN, accuracy improved. Excluding these permissions also enhanced the detection of concept drift using a year-to-year analysis strategy. Dataset balancing further improved model performance, reduced low-accuracy instances, and enhanced concept drift detection via the Kolmogorov-Smirnov test.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification</title>
<link>https://arxiv.org/abs/2507.22274</link>
<guid>https://arxiv.org/abs/2507.22274</guid>
<content:encoded><![CDATA[
arXiv:2507.22274v1 Announce Type: cross 
Abstract: The analysis of fundus images is critical for the early detection and diagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and Age-related Macular Degeneration (AMD). Traditional diagnostic workflows, however, often depend on manual interpretation and are both time- and resource-intensive. To address these limitations, we propose an automated and interpretable clinical decision support framework based on a hybrid feature extraction model called HOG-CNN. Our key contribution lies in the integration of handcrafted Histogram of Oriented Gradients (HOG) features with deep convolutional neural network (CNN) representations. This fusion enables our model to capture both local texture patterns and high-level semantic features from retinal fundus images. We evaluated our model on three public benchmark datasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for Glaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates consistently high performance. It achieves 98.5\% accuracy and 99.2 AUC for binary DR classification, and 94.2 AUC for five-class DR classification. On the IC-AMD dataset, it attains 92.8\% accuracy, 94.8\% precision, and 94.5 AUC, outperforming several state-of-the-art models. For Glaucoma detection on ORIGA, our model achieves 83.9\% accuracy and 87.2 AUC, showing competitive performance despite dataset limitations. We show, through comprehensive appendix studies, the complementary strength of combining HOG and CNN features. The model's lightweight and interpretable design makes it particularly suitable for deployment in resource-constrained clinical environments. These results position HOG-CNN as a robust and scalable tool for automated retinal disease screening.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations</title>
<link>https://arxiv.org/abs/2507.22289</link>
<guid>https://arxiv.org/abs/2507.22289</guid>
<content:encoded><![CDATA[
arXiv:2507.22289v1 Announce Type: cross 
Abstract: Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</title>
<link>https://arxiv.org/abs/2507.22291</link>
<guid>https://arxiv.org/abs/2507.22291</guid>
<content:encoded><![CDATA[
arXiv:2507.22291v1 Announce Type: cross 
Abstract: Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform all previous featurization approaches tested on a diverse set of mapping evaluations without re-training. We will release a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Asynchronous Decentralised Optimisation Algorithm for Nonconvex Problems</title>
<link>https://arxiv.org/abs/2507.22311</link>
<guid>https://arxiv.org/abs/2507.22311</guid>
<content:encoded><![CDATA[
arXiv:2507.22311v1 Announce Type: cross 
Abstract: In this paper, we consider nonconvex decentralised optimisation and learning over a network of distributed agents. We develop an ADMM algorithm based on the Randomised Block Coordinate Douglas-Rachford splitting method which enables agents in the network to distributedly and asynchronously compute a set of first-order stationary solutions of the problem. To the best of our knowledge, this is the first decentralised and asynchronous algorithm for solving nonconvex optimisation problems with convergence proof. The numerical examples demonstrate the efficiency of the proposed algorithm for distributed Phase Retrieval and sparse Principal Component Analysis problems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Neural Signatures of Semantic Evaluations in Depression and Suicidality</title>
<link>https://arxiv.org/abs/2507.22313</link>
<guid>https://arxiv.org/abs/2507.22313</guid>
<content:encoded><![CDATA[
arXiv:2507.22313v1 Announce Type: cross 
Abstract: Depression and suicidality profoundly impact cognition and emotion, yet objective neurophysiological biomarkers remain elusive. We investigated the spatiotemporal neural dynamics underlying affective semantic processing in individuals with varying levels of clinical severity of depression and suicidality using multivariate decoding of electroencephalography (EEG) data. Participants (N=137) completed a sentence evaluation task involving emotionally charged self-referential statements while EEG was recorded. We identified robust, neural signatures of semantic processing, with peak decoding accuracy between 300-600 ms -- a window associated with automatic semantic evaluation and conflict monitoring. Compared to healthy controls, individuals with depression and suicidality showed earlier onset, longer duration, and greater amplitude decoding responses, along with broader cross-temporal generalization and increased activation of frontocentral and parietotemporal components. These findings suggest altered sensitivity and impaired disengagement from emotionally salient content in the clinical groups, advancing our understanding of the neurocognitive basis of mental health and providing a principled basis for developing reliable EEG-based biomarkers of depression and suicidality.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks</title>
<link>https://arxiv.org/abs/2507.22339</link>
<guid>https://arxiv.org/abs/2507.22339</guid>
<content:encoded><![CDATA[
arXiv:2507.22339v1 Announce Type: cross 
Abstract: Low Earth Orbit (LEO) satellites are emerging as key components of 6G networks, with many already deployed to support large-scale Earth observation and sensing related tasks. Federated Learning (FL) presents a promising paradigm for enabling distributed intelligence in these resource-constrained and dynamic environments. However, achieving reliable convergence, while minimizing both processing time and energy consumption, remains a substantial challenge, particularly in heterogeneous and partially unlabeled satellite networks. To address this challenge, we propose a novel semi-supervised federated learning framework tailored for LEO satellite networks with hierarchical clustering aggregation. To further reduce communication overhead, we integrate sparsification and adaptive weight quantization techniques. In addition, we divide the FL clustering into two stages: satellite cluster aggregation stage and Ground Stations (GSs) aggregation stage. The supervised learning at GSs guides selected Parameter Server (PS) satellites, which in turn support fully unlabeled satellites during the federated training process. Extensive experiments conducted on a satellite network testbed demonstrate that our proposal can significantly reduce processing time (up to 3x) and energy consumption (up to 4x) compared to other comparative methods while maintaining model accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution</title>
<link>https://arxiv.org/abs/2507.22343</link>
<guid>https://arxiv.org/abs/2507.22343</guid>
<content:encoded><![CDATA[
arXiv:2507.22343v1 Announce Type: cross 
Abstract: State-space models are pivotal for dynamic system analysis but often struggle with outlier data that deviates from Gaussian distributions, frequently exhibiting skewness and heavy tails. This paper introduces a robust extension utilizing the asymmetric Laplace distribution, specifically tailored to capture these complex characteristics. We propose an efficient variational Bayes algorithm and a novel single-loop parameter estimation strategy, significantly enhancing the efficiency of the filtering, smoothing, and parameter estimation processes. Our comprehensive experiments demonstrate that our methods provide consistently robust performance across various noise settings without the need for manual hyperparameter adjustments. In stark contrast, existing models generally rely on specific noise conditions and necessitate extensive manual tuning. Moreover, our approach uses far fewer computational resources, thereby validating the model's effectiveness and underscoring its potential for practical applications in fields such as robust control and financial modeling.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations</title>
<link>https://arxiv.org/abs/2507.22380</link>
<guid>https://arxiv.org/abs/2507.22380</guid>
<content:encoded><![CDATA[
arXiv:2507.22380v1 Announce Type: cross 
Abstract: Recent developments in imitation learning have considerably advanced robotic manipulation. However, current techniques in imitation learning can suffer from poor generalization, limiting performance even under relatively minor domain shifts. In this work, we aim to enhance the generalization capabilities of complex imitation learning algorithms to handle unpredictable changes from the training environments to deployment environments. To avoid confusion caused by observations that are not relevant to the target task, we propose to explicitly learn the causal relationship between observation components and expert actions, employing a framework similar to [6], where a causal structural function is learned by intervention on the imitation learning policy. Disentangling the feature representation from image input as in [6] is hard to satisfy in complex imitation learning process in robotic manipulation, we theoretically clarify that this requirement is not necessary in causal relationship learning. Therefore, we propose a simple causal structure learning framework that can be easily embedded in recent imitation learning architectures, such as the Action Chunking Transformer [31]. We demonstrate our approach using a simulation of the ALOHA [31] bimanual robot arms in Mujoco, and show that the method can considerably mitigate the generalization problem of existing complex imitation learning algorithms.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Set Invariance with Probability One for Controlled Diffusion: Score-based Approach</title>
<link>https://arxiv.org/abs/2507.22385</link>
<guid>https://arxiv.org/abs/2507.22385</guid>
<content:encoded><![CDATA[
arXiv:2507.22385v1 Announce Type: cross 
Abstract: Given a controlled diffusion and a connected, bounded, Lipschitz set, when is it possible to guarantee controlled set invariance with probability one? In this work, we answer this question by deriving the necessary and sufficient conditions for the same in terms of gradients of certain log-likelihoods -- a.k.a. score vector fields -- for two cases: given finite time horizon and infinite time horizon. The deduced conditions comprise a score-based test that provably certifies or falsifies the existence of Markovian controllers for given controlled set invariance problem data. Our results are constructive in the sense when the problem data passes the proposed test, we characterize all controllers guaranteeing the desired set invariance. When the problem data fails the proposed test, there does not exist a controller that can accomplish the desired set invariance with probability one. The computation in the proposed tests involve solving certain Dirichlet boundary value problems, and in the finite horizon case, can also account for additional constraint of hitting a target subset at the terminal time. We illustrate the results using several semi-analytical and numerical examples.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs</title>
<link>https://arxiv.org/abs/2507.22387</link>
<guid>https://arxiv.org/abs/2507.22387</guid>
<content:encoded><![CDATA[
arXiv:2507.22387v1 Announce Type: cross 
Abstract: Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gems: Group Emotion Profiling Through Multimodal Situational Understanding</title>
<link>https://arxiv.org/abs/2507.22393</link>
<guid>https://arxiv.org/abs/2507.22393</guid>
<content:encoded><![CDATA[
arXiv:2507.22393v1 Announce Type: cross 
Abstract: Understanding individual, group and event level emotions along with contextual information is crucial for analyzing a multi-person social situation. To achieve this, we frame emotion comprehension as the task of predicting fine-grained individual emotion to coarse grained group and event level emotion. We introduce GEMS that leverages a multimodal swin-transformer and S3Attention based architecture, which processes an input scene, group members, and context information to generate joint predictions. Existing multi-person emotion related benchmarks mainly focus on atomic interactions primarily based on emotion perception over time and group level. To this end, we extend and propose VGAF-GEMS to provide more fine grained and holistic analysis on top of existing group level annotation of VGAF dataset. GEMS aims to predict basic discrete and continuous emotions (including valence and arousal) as well as individual, group and event level perceived emotions. Our benchmarking effort links individual, group and situational emotional responses holistically. The quantitative and qualitative comparisons with adapted state-of-the-art models demonstrate the effectiveness of GEMS framework on VGAF-GEMS benchmarking. We believe that it will pave the way of further research. The code and data is available at: https://github.com/katariaak579/GEMS
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINR: Implicit Neural Representations with Masked Image Modelling</title>
<link>https://arxiv.org/abs/2507.22404</link>
<guid>https://arxiv.org/abs/2507.22404</guid>
<content:encoded><![CDATA[
arXiv:2507.22404v1 Announce Type: cross 
Abstract: Self-supervised learning methods like masked autoencoders (MAE) have shown significant promise in learning robust feature representations, particularly in image reconstruction-based pretraining task. However, their performance is often strongly dependent on the masking strategies used during training and can degrade when applied to out-of-distribution data. To address these limitations, we introduce the masked implicit neural representations (MINR) framework that synergizes implicit neural representations with masked image modeling. MINR learns a continuous function to represent images, enabling more robust and generalizable reconstructions irrespective of masking strategies. Our experiments demonstrate that MINR not only outperforms MAE in in-domain scenarios but also in out-of-distribution settings, while reducing model complexity. The versatility of MINR extends to various self-supervised learning applications, confirming its utility as a robust and efficient alternative to existing frameworks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Normalizing Flows with Kernel Density Estimation in Estimating Risk of Automated Driving Systems</title>
<link>https://arxiv.org/abs/2507.22429</link>
<guid>https://arxiv.org/abs/2507.22429</guid>
<content:encoded><![CDATA[
arXiv:2507.22429v1 Announce Type: cross 
Abstract: The development of safety validation methods is essential for the safe deployment and operation of Automated Driving Systems (ADSs). One of the goals of safety validation is to prospectively evaluate the risk of an ADS dealing with real-world traffic. Scenario-based assessment is a widely-used approach, where test cases are derived from real-world driving data. To allow for a quantitative analysis of the system performance, the exposure of the scenarios must be accurately estimated. The exposure of scenarios at parameter level is expressed using a Probability Density Function (PDF). However, assumptions about the PDF, such as parameter independence, can introduce errors, while avoiding assumptions often leads to oversimplified models with limited parameters to mitigate the curse of dimensionality.
  This paper considers the use of Normalizing Flows (NF) for estimating the PDF of the parameters. NF are a class of generative models that transform a simple base distribution into a complex one using a sequence of invertible and differentiable mappings, enabling flexible, high-dimensional density estimation without restrictive assumptions on the PDF's shape. We demonstrate the effectiveness of NF in quantifying risk and risk uncertainty of an ADS, comparing its performance with Kernel Density Estimation (KDE), a traditional method for non-parametric PDF estimation. While NF require more computational resources compared to KDE, NF is less sensitive to the curse of dimensionality. As a result, NF can improve risk uncertainty estimation, offering a more precise assessment of an ADS's safety.
  This work illustrates the potential of NF in scenario-based safety. Future work involves experimenting more with using NF for scenario generation and optimizing the NF architecture, transformation types, and training hyperparameters to further enhance their applicability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection</title>
<link>https://arxiv.org/abs/2507.22447</link>
<guid>https://arxiv.org/abs/2507.22447</guid>
<content:encoded><![CDATA[
arXiv:2507.22447v1 Announce Type: cross 
Abstract: With the rapid expansion of web-based applications and cloud services, malicious JavaScript code continues to pose significant threats to user privacy, system integrity, and enterprise security. But, detecting such threats remains challenging due to sophisticated code obfuscation techniques and JavaScript's inherent language characteristics, particularly its nested closure structures and syntactic flexibility. In this work, we propose DeCoda, a hybrid defense framework that combines large language model (LLM)-based deobfuscation with code graph learning: (1) We first construct a sophisticated prompt-learning pipeline with multi-stage refinement, where the LLM progressively reconstructs the original code structure from obfuscated inputs and then generates normalized Abstract Syntax Tree (AST) representations; (2) In JavaScript ASTs, dynamic typing scatters semantically similar nodes while deeply nested functions fracture scope capturing, introducing structural noise and semantic ambiguity. To address these challenges, we then propose to learn hierarchical code graph representations via a Cluster-wise Graph that synergistically integrates graph transformer network, node clustering, and node-to-cluster attention to simultaneously capture both local node-level semantics and global cluster-induced structural relationships from AST graph. Experimental results demonstrate that our method achieves F1-scores of 94.64% and 97.71% on two benchmark datasets, demonstrating absolute improvements of 10.74% and 13.85% over state-of-the-art baselines. In false-positive control evaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers 4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing baseline. These results highlight the effectiveness of LLM-based deobfuscation and underscore the importance of modeling cluster-level relationships in detecting malicious code.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Language Models as Zero-Shot Deepfake Detectors</title>
<link>https://arxiv.org/abs/2507.22469</link>
<guid>https://arxiv.org/abs/2507.22469</guid>
<content:encoded><![CDATA[
arXiv:2507.22469v1 Announce Type: cross 
Abstract: The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVM-GP: Uncertainty-Aware PDE Solver via coupling latent variable model and Gaussian process</title>
<link>https://arxiv.org/abs/2507.22493</link>
<guid>https://arxiv.org/abs/2507.22493</guid>
<content:encoded><![CDATA[
arXiv:2507.22493v1 Announce Type: cross 
Abstract: We propose a novel probabilistic framework, termed LVM-GP, for uncertainty quantification in solving forward and inverse partial differential equations (PDEs) with noisy data. The core idea is to construct a stochastic mapping from the input to a high-dimensional latent representation, enabling uncertainty-aware prediction of the solution. Specifically, the architecture consists of a confidence-aware encoder and a probabilistic decoder. The encoder implements a high-dimensional latent variable model based on a Gaussian process (LVM-GP), where the latent representation is constructed by interpolating between a learnable deterministic feature and a Gaussian process prior, with the interpolation strength adaptively controlled by a confidence function learned from data. The decoder defines a conditional Gaussian distribution over the solution field, where the mean is predicted by a neural operator applied to the latent representation, allowing the model to learn flexible function-to-function mapping. Moreover, physical laws are enforced as soft constraints in the loss function to ensure consistency with the underlying PDE structure. Compared to existing approaches such as Bayesian physics-informed neural networks (B-PINNs) and deep ensembles, the proposed framework can efficiently capture functional dependencies via merging a latent Gaussian process and neural operator, resulting in competitive predictive accuracy and robust uncertainty quantification. Numerical experiments demonstrate the effectiveness and reliability of the method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaDent: A dataset for automated tooth pathology detection</title>
<link>https://arxiv.org/abs/2507.22512</link>
<guid>https://arxiv.org/abs/2507.22512</guid>
<content:encoded><![CDATA[
arXiv:2507.22512v1 Announce Type: cross 
Abstract: In this article, we present a new unique dataset for dental research - AlphaDent. This dataset is based on the DSLR camera photographs of the teeth of 295 patients and contains over 1200 images. The dataset is labeled for solving the instance segmentation problem and is divided into 9 classes. The article provides a detailed description of the dataset and the labeling format. The article also provides the details of the experiment on neural network training for the Instance Segmentation problem using this dataset. The results obtained show high quality of predictions. The dataset is published under an open license; and the training/inference code and model weights are also available under open licenses.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A surrogate model for topology optimisation of elastic structures via parametric autoencoders</title>
<link>https://arxiv.org/abs/2507.22539</link>
<guid>https://arxiv.org/abs/2507.22539</guid>
<content:encoded><![CDATA[
arXiv:2507.22539v1 Announce Type: cross 
Abstract: A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\%$ while achieving discrepancies below $4\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning</title>
<link>https://arxiv.org/abs/2507.22553</link>
<guid>https://arxiv.org/abs/2507.22553</guid>
<content:encoded><![CDATA[
arXiv:2507.22553v1 Announce Type: cross 
Abstract: Prompt-based continual learning provides a rehearsal-free solution by tuning small sets of parameters while keeping pre-trained models frozen. To meet the complex demands of sequential tasks, it is crucial to integrate task-specific knowledge within prompts effectively. However, existing works rely on either fixed learned prompts (i.e., prompts whose representations remain unchanged during new task learning) or on prompts generated from an entangled task-shared space, limiting the representational diversity of the integrated prompt. To address this issue, we propose a novel prompt-evolving mechanism to adaptively aggregate base prompts (i.e., task-specific prompts) into a unified prompt while ensuring diversity. By transforming and aligning base prompts, both previously learned and newly introduced, our approach continuously evolves accumulated knowledge to facilitate learning new tasks. We further introduce a learnable probabilistic gate that adaptively determines which layers to activate during the evolution process. We validate our method on image classification and video action recognition tasks in class-incremental learning, achieving average gains of 9.07% and 7.40% over existing methods across all scenarios.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
arXiv:2507.22564v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP</title>
<link>https://arxiv.org/abs/2507.22576</link>
<guid>https://arxiv.org/abs/2507.22576</guid>
<content:encoded><![CDATA[
arXiv:2507.22576v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is an important building block in trustworthy image recognition systems as unknown classes may arise at test-time. OOD detection methods typically revolve around a single classifier, leading to a split in the research field between the classical supervised setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot setting (class names fed as prompts to CLIP). In both cases, an overarching challenge is that the OOD detection performance is implicitly constrained by the classifier's capabilities on in-distribution (ID) data. In this work, we show that given a little open-mindedness from both ends, remarkable OOD detection can be achieved by instead creating a heterogeneous ensemble - COOkeD combines the predictions of a closed-world classifier trained end-to-end on a specific dataset, a zero-shot CLIP classifier, and a linear probe classifier trained on CLIP image features. While bulky at first sight, this approach is modular, post-hoc and leverages the availability of pre-trained VLMs, thus introduces little overhead compared to training a single standard classifier. We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also consider more challenging, realistic settings ranging from training-time label noise, to test-time covariate shift, to zero-shot shift which has been previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art performance and greater robustness compared to both classical and CLIP-based OOD detection methods. Code is available at https://github.com/glhr/COOkeD
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mean-Field Theory of $\Theta$-Expectations</title>
<link>https://arxiv.org/abs/2507.22577</link>
<guid>https://arxiv.org/abs/2507.22577</guid>
<content:encoded><![CDATA[
arXiv:2507.22577v1 Announce Type: cross 
Abstract: The canonical theory of sublinear expectations, a foundation of stochastic calculus under ambiguity, is insensitive to the non-convex geometry of primitive uncertainty models. This paper develops a new stochastic calculus for a structured class of such non-convex models. We introduce a class of fully coupled Mean-Field Forward-Backward Stochastic Differential Equations where the BSDE driver is defined by a pointwise maximization over a law-dependent, non-convex set. Mathematical tractability is achieved via a uniform strong concavity assumption on the driver with respect to the control variable, which ensures the optimization admits a unique and stable solution. A central contribution is to establish the Lipschitz stability of this optimizer from primitive geometric and regularity conditions, which underpins the entire well-posedness theory. We prove local and global well-posedness theorems for the FBSDE system. The resulting valuation functional, the $\Theta$-Expectation, is shown to be dynamically consistent and, most critically, to violate the axiom of sub-additivity. This, along with its failure to be translation invariant, demonstrates its fundamental departure from the convex paradigm. This work provides a rigorous foundation for stochastic calculus under a class of non-convex, endogenous ambiguity.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
arXiv:2507.22581v1 Announce Type: cross 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2507.22632</link>
<guid>https://arxiv.org/abs/2507.22632</guid>
<content:encoded><![CDATA[
arXiv:2507.22632v1 Announce Type: cross 
Abstract: Domain adaptation seeks to leverage the abundant label information in a source domain to improve classification performance in a target domain with limited labels. While the field has seen extensive methodological development, its theoretical foundations remain relatively underexplored. Most existing theoretical analyses focus on simplified settings where the source and target domains share the same input space and relate target-domain performance to measures of domain discrepancy. Although insightful, these analyses may not fully capture the behavior of modern approaches that align domains into a shared space via feature transformations. In this paper, we present a comprehensive theoretical study of domain adaptation algorithms based on domain alignment. We consider the joint learning of domain-aligning feature transformations and a shared classifier in a semi-supervised setting. We first derive generalization bounds in a broad setting, in terms of covering numbers of the relevant function classes. We then extend our analysis to characterize the sample complexity of domain-adaptive neural networks employing maximum mean discrepancy (MMD) or adversarial objectives. Our results rely on a rigorous analysis of the covering numbers of these architectures. We show that, for both MMD-based and adversarial models, the sample complexity admits an upper bound that scales quadratically with network depth and width. Furthermore, our analysis suggests that in semi-supervised settings, robustness to limited labeled target data can be achieved by scaling the target loss proportionally to the square root of the number of labeled target samples. Experimental evaluation in both shallow and deep settings lends support to our theoretical findings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>trAIce3D: A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images</title>
<link>https://arxiv.org/abs/2507.22635</link>
<guid>https://arxiv.org/abs/2507.22635</guid>
<content:encoded><![CDATA[
arXiv:2507.22635v1 Announce Type: cross 
Abstract: The shape of a cell contains essential information about its function within the biological system. Segmenting these structures from large-scale 3D microscopy images is challenging, limiting clinical insights especially for microglia, immune-associated cells involved in neurodegenerative diseases. Existing segmentation methods mainly focus on cell bodies, struggle with overlapping structures, perform poorly on noisy images, require hyperparameter tuning for each new dataset, or rely on tedious semi-automated approaches. We introduce trAIce3D, a deep-learning architecture designed for precise microglia segmentation, capturing both somas and branches. It employs a two-stage approach: first, a 3D U-Net with vision transformers in the encoder detects somas using a sliding-window technique to cover the entire image. Then, the same architecture, enhanced with cross-attention blocks in skip connections, refines each soma and its branches by using soma coordinates as a prompt and a 3D window around the target cell as input. Training occurs in two phases: self-supervised Soma Segmentation, followed by prompt-based Branch Segmentation, leveraging pre-trained weights from the first phase. Trained and evaluated on a dataset of 41,230 microglial cells, trAIce3D significantly improves segmentation accuracy and generalization, enabling scalable analysis of complex cellular morphologies. While optimized for microglia, its architecture can extend to other intricate cell types, such as neurons and astrocytes, broadening its impact on neurobiological research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction</title>
<link>https://arxiv.org/abs/2507.22640</link>
<guid>https://arxiv.org/abs/2507.22640</guid>
<content:encoded><![CDATA[
arXiv:2507.22640v1 Announce Type: cross 
Abstract: Offline reinforcement learning (offline RL) offers a promising framework for developing control strategies in chemical process systems using historical data, without the risks or costs of online experimentation. This work investigates the application of offline RL to the safe and efficient control of an exothermic polymerisation continuous stirred-tank reactor. We introduce a Gymnasium-compatible simulation environment that captures the reactor's nonlinear dynamics, including reaction kinetics, energy balances, and operational constraints. The environment supports three industrially relevant scenarios: startup, grade change down, and grade change up. It also includes reproducible offline datasets generated from proportional-integral controllers with randomised tunings, providing a benchmark for evaluating offline RL algorithms in realistic process control tasks.
  We assess behaviour cloning and implicit Q-learning as baseline algorithms, highlighting the challenges offline agents face, including steady-state offsets and degraded performance near setpoints. To address these issues, we propose a novel deployment-time safety layer that performs gradient-based action correction using input convex neural networks (PICNNs) as learned cost models. The PICNN enables real-time, differentiable correction of policy actions by descending a convex, state-conditioned cost surface, without requiring retraining or environment interaction.
  Experimental results show that offline RL, particularly when combined with convex action correction, can outperform traditional control approaches and maintain stability across all scenarios. These findings demonstrate the feasibility of integrating offline RL with interpretable and safety-aware corrections for high-stakes chemical process control, and lay the groundwork for more reliable data-driven automation in industrial systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
arXiv:2507.22758v1 Announce Type: cross 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision</title>
<link>https://arxiv.org/abs/2507.22760</link>
<guid>https://arxiv.org/abs/2507.22760</guid>
<content:encoded><![CDATA[
arXiv:2507.22760v1 Announce Type: cross 
Abstract: As neural networks (NNs) become increasingly prevalent in safety-critical neural network-controlled cyber-physical systems (NNCSs), formally guaranteeing their safety becomes crucial. For these systems, safety must be ensured throughout their entire operation, necessitating infinite-time horizon verification. To verify the infinite-time horizon safety of NNCSs, recent approaches leverage Differential Dynamic Logic (dL). However, these dL-based guarantees rely on idealized, real-valued NN semantics and fail to account for roundoff errors introduced by finite-precision implementations. This paper bridges the gap between theoretical guarantees and real-world implementations by incorporating robustness under finite-precision perturbations -- in sensing, actuation, and computation -- into the safety verification. We model the problem as a hybrid game between a good Demon, responsible for control actions, and a bad Angel, introducing perturbations. This formulation enables formal proofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this bound, we employ state-of-the-art mixed-precision fixed-point tuners to synthesize sound and efficient implementations, thus providing a complete end-to-end solution. We evaluate our approach on case studies from the automotive and aeronautics domains, producing efficient NN implementations with rigorous infinite-time horizon safety guarantees.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection</title>
<link>https://arxiv.org/abs/2507.22772</link>
<guid>https://arxiv.org/abs/2507.22772</guid>
<content:encoded><![CDATA[
arXiv:2507.22772v1 Announce Type: cross 
Abstract: Despite outstanding results, machine learning-based Android malware detection models struggle with concept drift, where rapidly evolving malware characteristics degrade model effectiveness. This study examines the impact of concept drift on Android malware detection, evaluating two datasets and nine machine learning and deep learning algorithms, as well as Large Language Models (LLMs). Various feature types--static, dynamic, hybrid, semantic, and image-based--were considered. The results showed that concept drift is widespread and significantly affects model performance. Factors influencing the drift include feature types, data environments, and detection methods. Balancing algorithms helped with class imbalance but did not fully address concept drift, which primarily stems from the dynamic nature of the malware landscape. No strong link was found between the type of algorithm used and concept drift, the impact was relatively minor compared to other variables since hyperparameters were not fine-tuned, and the default algorithm configurations were used. While LLMs using few-shot learning demonstrated promising detection performance, they did not fully mitigate concept drift, highlighting the need for further investigation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies</title>
<link>https://arxiv.org/abs/2507.22782</link>
<guid>https://arxiv.org/abs/2507.22782</guid>
<content:encoded><![CDATA[
arXiv:2507.22782v1 Announce Type: cross 
Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amorphous Solid Model of Vectorial Hopfield Neural Networks</title>
<link>https://arxiv.org/abs/2507.22787</link>
<guid>https://arxiv.org/abs/2507.22787</guid>
<content:encoded><![CDATA[
arXiv:2507.22787v1 Announce Type: cross 
Abstract: We present a vectorial extension of the Hopfield associative memory model inspired by the theory of amorphous solids, where binary neural states are replaced by unit vectors $\mathbf{s}_i \in \mathbb{R}^3$ on the sphere $S^2$. The generalized Hebbian learning rule creates a block-structured weight matrix through outer products of stored pattern vectors, analogous to the Hessian matrix structure in amorphous solids. We demonstrate that this model exhibits quantifiable structural properties characteristic of disordered materials: energy landscapes with deep minima for stored patterns versus random configurations (energy gaps $\sim 7$ units), strongly anisotropic correlations encoded in the weight matrix (anisotropy ratios $\sim 10^2$), and order-disorder transitions controlled by the pattern density $\gamma = P/(N \cdot d)$. The enhanced memory capacity ($\gamma_c \approx 0.55$ for a fully-connected network) compared to binary networks ($\gamma_c \approx 0.138$) and the emergence of orientational correlations establish connections between associative memory mechanisms and amorphous solid physics, particularly in systems with continuous orientational degrees of freedom. We also unveil the scaling with the coordination number $Z$ of the memory capacity: $\gamma_c \sim (Z-6)$ from the isostatic point $Z_c =6$ of the 3D elastic network, which closely mirrors the scaling of the shear modulus $G \sim (Z-6)$ in 3D central-force spring networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgrid BoostCNN: Efficient Boosting of Convolutional Networks via Gradient-Guided Feature Selection</title>
<link>https://arxiv.org/abs/2507.22842</link>
<guid>https://arxiv.org/abs/2507.22842</guid>
<content:encoded><![CDATA[
arXiv:2507.22842v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) have achieved remarkable success across a wide range of machine learning tasks by leveraging hierarchical feature learning through deep architectures. However, the large number of layers and millions of parameters often make CNNs computationally expensive to train, requiring extensive time and manual tuning to discover optimal architectures. In this paper, we introduce a novel framework for boosting CNN performance that integrates dynamic feature selection with the principles of BoostCNN. Our approach incorporates two key strategies: subgrid selection and importance sampling, to guide training toward informative regions of the feature space. We further develop a family of algorithms that embed boosting weights directly into the network training process using a least squares loss formulation. This integration not only alleviates the burden of manual architecture design but also enhances accuracy and efficiency. Experimental results across several fine-grained classification benchmarks demonstrate that our boosted CNN variants consistently outperform conventional CNNs in both predictive performance and training speed.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning on Riemannian Manifolds: A Gradient-Free Projection-Based Approach</title>
<link>https://arxiv.org/abs/2507.22855</link>
<guid>https://arxiv.org/abs/2507.22855</guid>
<content:encoded><![CDATA[
arXiv:2507.22855v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a powerful paradigm for collaborative model training across distributed clients while preserving data privacy. However, existing FL algorithms predominantly focus on unconstrained optimization problems with exact gradient information, limiting its applicability in scenarios where only noisy function evaluations are accessible or where model parameters are constrained. To address these challenges, we propose a novel zeroth-order projection-based algorithm on Riemannian manifolds for FL. By leveraging the projection operator, we introduce a computationally efficient zeroth-order Riemannian gradient estimator. Unlike existing estimators, ours requires only a simple Euclidean random perturbation, eliminating the need to sample random vectors in the tangent space, thus reducing computational cost. Theoretically, we first prove the approximation properties of the estimator and then establish the sublinear convergence of the proposed algorithm, matching the rate of its first-order counterpart. Numerically, we first assess the efficiency of our estimator using kernel principal component analysis. Furthermore, we apply the proposed algorithm to two real-world scenarios: zeroth-order attacks on deep neural networks and low-rank neural network training to validate the theoretical findings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synchronization of mean-field models on the circle</title>
<link>https://arxiv.org/abs/2507.22857</link>
<guid>https://arxiv.org/abs/2507.22857</guid>
<content:encoded><![CDATA[
arXiv:2507.22857v1 Announce Type: cross 
Abstract: This paper considers a mean-field model of $n$ interacting particles whose state space is the unit circle, a generalization of the classical Kuramoto model. Global synchronization is said to occur if after starting from almost any initial state, all particles coalesce to a common point on the circle. We propose a general synchronization criterion in terms of $L_1$-norm of the third derivative of the particle interaction function. As an application we resolve a conjecture for the so-called self-attention dynamics (stylized model of transformers), by showing synchronization for all $\beta \ge -0.16$, which significantly extends the previous bound of $0\le \beta \le 1$ from Criscitiello, Rebjock, McRae, and Boumal (2024). We also show that global synchronization does not occur when $\beta < -2/3$.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh based segmentation for automated margin line generation on incisors receiving crown treatment</title>
<link>https://arxiv.org/abs/2507.22859</link>
<guid>https://arxiv.org/abs/2507.22859</guid>
<content:encoded><![CDATA[
arXiv:2507.22859v1 Announce Type: cross 
Abstract: Dental crowns are essential dental treatments for restoring damaged or missing teeth of patients. Recent design approaches of dental crowns are carried out using commercial dental design software. Once a scan of a preparation is uploaded to the software, a dental technician needs to manually define a precise margin line on the preparation surface, which constitutes a non-repeatable and inconsistent procedure. This work proposes a new framework to determine margin lines automatically and accurately using deep learning. A dataset of incisor teeth was provided by a collaborating dental laboratory to train a deep learning segmentation model. A mesh-based neural network was modified by changing its input channels and used to segment the prepared tooth into two regions such that the margin line is contained within the boundary faces separating the two regions. Next, k-fold cross-validation was used to train 5 models, and a voting classifier technique was used to combine their results to enhance the segmentation. After that, boundary smoothing and optimization using the graph cut method were applied to refine the segmentation results. Then, boundary faces separating the two regions were selected to represent the margin line faces. A spline was approximated to best fit the centers of the boundary faces to predict the margin line. Our results show that an ensemble model combined with maximum probability predicted the highest number of successful test cases (7 out of 13) based on a maximum distance threshold of 200 m (representing human error) between the predicted and ground truth point clouds. It was also demonstrated that the better the quality of the preparation, the smaller the divergence between the predicted and ground truth margin lines (Spearman's rank correlation coefficient of -0.683). We provide the train and test datasets for the community.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content</title>
<link>https://arxiv.org/abs/2507.22873</link>
<guid>https://arxiv.org/abs/2507.22873</guid>
<content:encoded><![CDATA[
arXiv:2507.22873v1 Announce Type: cross 
Abstract: The increasing complexity of content rendering in modern games has led to a problematic growth in the workload of the GPU. In this paper, we propose an AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient super-resolution (ESR) models which could offload the workload on the GPU to a low-power device such as a neural processing unit (NPU). The LCS is trained on GameIR image pairs natively rendered at low and high resolution. We utilize adversarial training to encourage reconstruction of perceptually important details, and apply reparameterization and quantization techniques to reduce model complexity and size. In our comparative analysis we evaluate the LCS alongside the publicly available AMD hardware-based Edge Adaptive Scaling Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different metrics, and find that the LCS achieves better perceptual quality, demonstrating the potential of ESR models for upscaling on resource-constrained devices.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency of Feature Attribution in Deep Learning Architectures for Multi-Omics</title>
<link>https://arxiv.org/abs/2507.22877</link>
<guid>https://arxiv.org/abs/2507.22877</guid>
<content:encoded><![CDATA[
arXiv:2507.22877v1 Announce Type: cross 
Abstract: Machine and deep learning have grown in popularity and use in biological research over the last decade but still present challenges in interpretability of the fitted model. The development and use of metrics to determine features driving predictions and increase model interpretability continues to be an open area of research. We investigate the use of Shapley Additive Explanations (SHAP) on a multi-view deep learning model applied to multi-omics data for the purposes of identifying biomolecules of interest. Rankings of features via these attribution methods are compared across various architectures to evaluate consistency of the method. We perform multiple computational experiments to assess the robustness of SHAP and investigate modeling approaches and diagnostics to increase and measure the reliability of the identification of important features. Accuracy of a random-forest model fit on subsets of features selected as being most influential as well as clustering quality using only these features are used as a measure of effectiveness of the attribution method. Our findings indicate that the rankings of features resulting from SHAP are sensitive to the choice of architecture as well as different random initializations of weights, suggesting caution when using attribution methods on multi-view deep learning models applied to multi-omics data. We present an alternative, simple method to assess the robustness of identification of important biomolecules.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Graph Learning: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2202.13852</link>
<guid>https://arxiv.org/abs/2202.13852</guid>
<content:encoded><![CDATA[
arXiv:2202.13852v3 Announce Type: replace 
Abstract: Graph representation learning in Euclidean space, despite its widespread adoption and proven utility in many domains, often struggles to effectively capture the inherent hierarchical and complex relational structures prevalent in real-world data, particularly for datasets exhibiting a highly non-Euclidean latent anatomy or power-law distributions. Hyperbolic geometry, with its constant negative curvature and exponential growth property, naturally accommodates such structures, offering a promising alternative for learning rich graph representations. This survey paper provides a comprehensive review of the rapidly evolving field of Hyperbolic Graph Learning (HGL). We systematically categorize and analyze existing methods broadly dividing them into (1) hyperbolic graph embedding-based techniques, (2) graph neural network-based hyperbolic models, and (3) emerging paradigms. Beyond methodologies, we extensively discuss diverse applications of HGL across multiple domains, including recommender systems, knowledge graphs, bioinformatics, and other relevant scenarios, demonstrating the broad applicability and effectiveness of hyperbolic geometry in real-world graph learning tasks. Most importantly, we identify several key challenges that serve as directions for advancing HGL, including handling complex data structures, developing geometry-aware learning objectives, ensuring trustworthy and scalable implementations, and integrating with foundation models, e.g., large language models. We highlight promising research opportunities in this exciting interdisciplinary area. A comprehensive repository can be found at https://github.com/digailab/awesome-hyperbolic-graph-learning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Introduction to Modern Statistical Learning</title>
<link>https://arxiv.org/abs/2207.10185</link>
<guid>https://arxiv.org/abs/2207.10185</guid>
<content:encoded><![CDATA[
arXiv:2207.10185v2 Announce Type: replace 
Abstract: This work in progress aims to provide a unified introduction to statistical learning, building up slowly from classical models like the GMM and HMM to modern neural networks like the VAE and diffusion models. There are today many internet resources that explain this or that new machine-learning algorithm in isolation, but they do not (and cannot, in so brief a space) connect these algorithms with each other or with the classical literature on statistical models, out of which the modern algorithms emerged. Also conspicuously lacking is a single notational system which, although unfazing to those already familiar with the material (like the authors of these posts), raises a significant barrier to the novice's entry. Likewise, I have aimed to assimilate the various models, wherever possible, to a single framework for inference and learning, showing how (and why) to change one model into another with minimal alteration (some of them novel, others from the literature).
  Some background is of course necessary. I have assumed the reader is familiar with basic multivariable calculus, probability and statistics, and linear algebra. The goal of this book is certainly not completeness, but rather to draw a more or less straight-line path from the basics to the extremely powerful new models of the last decade. The goal then is to complement, not replace, such comprehensive texts as Bishop's \emph{Pattern Recognition and Machine Learning}, which is now 15 years old.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Privacy and Robustness for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2403.16591</link>
<guid>https://arxiv.org/abs/2403.16591</guid>
<content:encoded><![CDATA[
arXiv:2403.16591v5 Announce Type: replace 
Abstract: The widespread adoption of machine learning necessitates robust privacy protection alongside algorithmic resilience. While Local Differential Privacy (LDP) provides foundational guarantees, sophisticated adversaries with prior knowledge demand more nuanced Bayesian privacy notions, such as Maximum Bayesian Privacy (MBP) and Average Bayesian Privacy (ABP), first introduced by \cite{zhang2022no}. Concurrently, machine learning systems require inherent robustness against data perturbations and adversarial manipulations. This paper systematically investigates the intricate theoretical relationships among LDP, MBP, and ABP. Crucially, we bridge these privacy concepts with algorithmic robustness, particularly within the Probably Approximately Correct (PAC) learning framework. Our work demonstrates that privacy-preserving mechanisms inherently confer PAC robustness. We present key theoretical results, including the formalization of the established LDP-MBP relationship, novel bounds between MBP and ABP, and a proof demonstrating PAC robustness from MBP. Furthermore, we establish a novel theoretical relationship quantifying how privacy leakage directly influences an algorithm's input robustness. These results provide a unified theoretical framework for understanding and optimizing the privacy-robustness trade-off, paving the way for the development of more secure, trustworthy, and resilient machine learning systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Federated Learning with On-device Training and Communication in 8-bit Floating Point</title>
<link>https://arxiv.org/abs/2407.02610</link>
<guid>https://arxiv.org/abs/2407.02610</guid>
<content:encoded><![CDATA[
arXiv:2407.02610v2 Announce Type: replace 
Abstract: Recent work has shown that 8-bit floating point (FP8) can be used for efficiently training neural networks with reduced computational cost compared to training in FP32/FP16. In this work, we investigate the use of FP8 training in a federated learning context. This approach brings not only the usual benefits of FP8 which are desirable for on-device training at the edge, but also reduces client-server communication costs due to significant weight compression. We present a novel method for combining FP8 client training while maintaining a global FP32 server model and provide convergence analysis. Experiments with various machine learning models and datasets show that our method consistently yields communication reductions of at least 2.9x across a variety of tasks and models compared to an FP32 baseline to achieve the same trained model accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation for Healthcare QA</title>
<link>https://arxiv.org/abs/2407.18044</link>
<guid>https://arxiv.org/abs/2407.18044</guid>
<content:encoded><![CDATA[
arXiv:2407.18044v2 Announce Type: replace 
Abstract: Deploying Large Language Models (LLMs) for healthcare question answering requires robust methods to ensure accuracy and reliability. This work introduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for enhancing Retrieval-Augmented Generation (RAG) systems in healthcare question-answering by pre-aligning user queries with a database of curated, answerable questions derived from healthcare content. A key component of QB-RAG is an LLM-based filtering mechanism that ensures that only relevant and answerable questions are included in the database, enabling reliable reference query generation at scale. We provide theoretical motivation for QB-RAG, conduct a comparative analysis of existing retrieval enhancement techniques, and introduce a generalizable, comprehensive evaluation framework that assesses both the retrieval effectiveness and the quality of the generated response based on faithfulness, relevance, and adherence to the guideline. Our empirical evaluation on a healthcare data set demonstrates the superior performance of QB-RAG compared to existing retrieval methods, highlighting its practical value in building trustworthy digital health applications for health question-answering.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDBA: A Stealthy and Long-Lasting Durable Backdoor Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2409.14805</link>
<guid>https://arxiv.org/abs/2409.14805</guid>
<content:encoded><![CDATA[
arXiv:2409.14805v2 Announce Type: replace 
Abstract: Federated learning is a promising approach for training machine learning models while preserving data privacy. However, its distributed nature makes it vulnerable to backdoor attacks, particularly in NLP tasks, where related research remains limited. This paper introduces SDBA, a novel backdoor attack mechanism designed for NLP tasks in federated learning environments. Through a systematic analysis across LSTM and GPT-2 models, we identify the most vulnerable layers for backdoor injection and achieve both stealth and long-lasting durability by applying layer-wise gradient masking and top-k% gradient masking. Also, to evaluate the task generalizability of SDBA, we additionally conduct experiments on the T5 model. Experiments on next-token prediction, sentiment analysis, and question answering tasks show that SDBA outperforms existing backdoors in terms of durability and effectively bypasses representative defense mechanisms, demonstrating notable performance in transformer-based models such as GPT-2. These results highlight the urgent need for robust defense strategies in NLP-based federated learning systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring biological processes with intrinsic noise from cross-sectional data</title>
<link>https://arxiv.org/abs/2410.07501</link>
<guid>https://arxiv.org/abs/2410.07501</guid>
<content:encoded><![CDATA[
arXiv:2410.07501v2 Announce Type: replace 
Abstract: Inferring dynamical models from data continues to be a significant challenge in computational biology, especially given the stochastic nature of many biological processes. We explore a common scenario in omics, where statistically independent cross-sectional samples are available at a few time points, and the goal is to infer the underlying diffusion process that generated the data. Existing inference approaches often simplify or ignore noise intrinsic to the system, compromising accuracy for the sake of optimization ease. We circumvent this compromise by inferring the phase-space probability flow that shares the same time-dependent marginal distributions as the underlying stochastic process. Our approach, probability flow inference (PFI), disentangles force from intrinsic stochasticity while retaining the algorithmic ease of ODE inference. Analytically, we prove that for Ornstein-Uhlenbeck processes the regularized PFI formalism yields a unique solution in the limit of well-sampled distributions. In practical applications, we show that PFI enables accurate parameter and force estimation in high-dimensional stochastic reaction networks, and that it allows inference of cell differentiation dynamics with molecular noise, outperforming state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Representation Collapse in Vector Quantized Models with One Linear Layer</title>
<link>https://arxiv.org/abs/2411.02038</link>
<guid>https://arxiv.org/abs/2411.02038</guid>
<content:encoded><![CDATA[
arXiv:2411.02038v2 Announce Type: replace 
Abstract: Vector Quantization (VQ) is essential for discretizing continuous representations in unsupervised learning but suffers from representation collapse, causing low codebook utilization and limiting scalability. Existing solutions often rely on complex optimizations or reduce latent dimensionality, which compromises model capacity and fails to fully solve the problem. We identify the root cause as disjoint codebook optimization, where only a few code vectors are updated via gradient descent. To fix this, we propose \textbf{Sim}ple\textbf{VQ}, which reparameterizes code vectors through a learnable linear transformation layer over a latent basis, optimizing the \textit{entire linear space} rather than nearest \textit{individual code vectors}. Although the multiplication of two linear matrices is equivalent to applying a single linear layer, this simple approach effectively prevents collapse. Extensive experiments on image and audio tasks demonstrate that SimVQ improves codebook usage, is easy to implement, and generalizes well across modalities and architectures.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning in Echo State Networks for Input Reconstruction</title>
<link>https://arxiv.org/abs/2501.11409</link>
<guid>https://arxiv.org/abs/2501.11409</guid>
<content:encoded><![CDATA[
arXiv:2501.11409v4 Announce Type: replace 
Abstract: Echo state networks (ESNs) are a class of recurrent neural networks in which only the readout layer is trainable, while the recurrent and input layers are fixed. This architectural constraint enables computationally efficient processing of time-series data. Traditionally, the readout layer in ESNs is trained using supervised learning with target outputs. In this study, we focus on input reconstruction (IR), where the readout layer is trained to reconstruct the input time series fed into the ESN. We show that IR can be achieved through unsupervised learning (UL), without access to supervised targets, provided that the ESN parameters are known a priori and satisfy invertibility conditions. This formulation allows applications relying on IR, such as dynamical system replication and noise filtering, to be reformulated within the UL framework via straightforward integration with existing algorithms. Our results suggest that prior knowledge of ESN parameters can reduce reliance on supervision, thereby establishing a new principle: not only by fixing part of the network parameters but also by exploiting their specific values. Furthermore, our UL-based algorithms for input reconstruction and related tasks are suitable for autonomous processing, offering insights into how analogous computational mechanisms might operate in the brain in principle. These findings contribute to a deeper understanding of the mathematical foundations of ESNs and their relevance to models in computational neuroscience.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Evolution Strategies to Train Transformers in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.13883</link>
<guid>https://arxiv.org/abs/2501.13883</guid>
<content:encoded><![CDATA[
arXiv:2501.13883v2 Announce Type: replace 
Abstract: We explore the capability of evolution strategies to train an agent with a policy based on a transformer architecture in a reinforcement learning setting. We performed experiments using OpenAI's highly parallelizable evolution strategy to train Decision Transformer in the MuJoCo Humanoid locomotion environment and in the environment of Atari games, testing the ability of this black-box optimization technique to train even such relatively large and complicated models (compared to those previously tested in the literature). The examined evolution strategy proved to be, in general, capable of achieving strong results and managed to produce high-performing agents, showcasing evolution's ability to tackle the training of even such complex models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2502.00201</link>
<guid>https://arxiv.org/abs/2502.00201</guid>
<content:encoded><![CDATA[
arXiv:2502.00201v2 Announce Type: replace 
Abstract: This paper systematically reviews advancements in deep learning (DL) techniques for financial fraud detection, a critical issue in the financial sector. Using the Kitchenham systematic literature review approach, 57 studies published between 2019 and 2024 were analyzed. The review highlights the effectiveness of various deep learning models such as Convolutional Neural Networks, Long Short-Term Memory, and transformers across domains such as credit card transactions, insurance claims, and financial statement audits. Performance metrics such as precision, recall, F1-score, and AUC-ROC were evaluated. Key themes explored include the impact of data privacy frameworks and advancements in feature engineering and data preprocessing. The study emphasizes challenges such as imbalanced datasets, model interpretability, and ethical considerations, alongside opportunities for automation and privacy-preserving techniques such as blockchain integration and Principal Component Analysis. By examining trends over the past five years, this review identifies critical gaps and promising directions for advancing DL applications in financial fraud detection, offering actionable insights for researchers and practitioners.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Online Adaption for Time Series Foundation Model Forecasts</title>
<link>https://arxiv.org/abs/2502.12920</link>
<guid>https://arxiv.org/abs/2502.12920</guid>
<content:encoded><![CDATA[
arXiv:2502.12920v3 Announce Type: replace 
Abstract: Foundation models (FMs) have emerged as a promising approach for time series forecasting. While effective, FMs typically remain fixed during deployment due to the high computational costs of learning them online. Consequently, deployed FMs fail to adapt their forecasts to current data characteristics, despite the availability of online feedback from newly arriving data. This raises the question of whether FM performance can be enhanced by the efficient usage of this feedback. We propose ELF to answer this question. ELF is a lightweight mechanism for the online adaption of FM forecasts in response to online feedback. ELF consists of two parts: a) the ELF-Forecaster which is used to learn the current data distribution; and b) the ELF-Weighter which is used to combine the forecasts of the FM and the ELF-Forecaster. We evaluate the performance of ELF in conjunction with several recent FMs across a suite of standard time series datasets. In all of our experiments we find that using ELF improves performance. This work demonstrates how efficient usage of online feedback can be used to improve FM forecasts.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Koopman-Based Generalization of Deep Reinforcement Learning With Application to Wireless Communications</title>
<link>https://arxiv.org/abs/2503.02961</link>
<guid>https://arxiv.org/abs/2503.02961</guid>
<content:encoded><![CDATA[
arXiv:2503.02961v2 Announce Type: replace 
Abstract: Deep Reinforcement Learning (DRL) is a key machine learning technology driving progress across various scientific and engineering fields, including wireless communication. However, its limited interpretability and generalizability remain major challenges. In supervised learning, generalizability is commonly evaluated through the generalization error using information-theoretic methods. In DRL, the training data is sequential and not independent and identically distributed (i.i.d.), rendering traditional information-theoretic methods unsuitable for generalizability analysis. To address this challenge, this paper proposes a novel analytical method for evaluating the generalizability of DRL. Specifically, we first model the evolution of states and actions in trained DRL algorithms as unknown discrete, stochastic, and nonlinear dynamical functions. Then, we employ a data-driven identification method, the Koopman operator, to approximate these functions, and propose two interpretable representations. Based on these interpretable representations, we develop a rigorous mathematical approach to evaluate the generalizability of DRL algorithms. This approach is formulated using the spectral feature analysis of the Koopman operator, leveraging the H_\infty norm. Finally, we apply this generalization analysis to compare the soft actor-critic method, widely recognized as a robust DRL approach, against the proximal policy optimization algorithm for an unmanned aerial vehicle-assisted mmWave wireless communication scenario.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWLViz: An Open-World Benchmark for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.07631</link>
<guid>https://arxiv.org/abs/2503.07631</guid>
<content:encoded><![CDATA[
arXiv:2503.07631v3 Announce Type: replace 
Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive State-Space Mamba for Real-Time Sensor Data Anomaly Detection</title>
<link>https://arxiv.org/abs/2503.22743</link>
<guid>https://arxiv.org/abs/2503.22743</guid>
<content:encoded><![CDATA[
arXiv:2503.22743v2 Announce Type: replace 
Abstract: State-space modeling has emerged as a powerful paradigm for sequence analysis in various tasks such as natural language processing, time-series forecasting, and signal processing. In this work, we propose an \emph{Adaptive State-Space Mamba} (\textbf{ASSM}) framework for real-time sensor data anomaly detection. While state-space models have been previously employed for image processing applications (e.g., style transfer \cite{wang2024stylemamba}), our approach leverages the core idea of sequential hidden states to tackle a significantly different domain: detecting anomalies on streaming sensor data.
  In particular, we introduce an adaptive gating mechanism that dynamically modulates the hidden state update based on contextual and learned statistical cues. This design ensures that our model remains computationally efficient and scalable, even under rapid data arrival rates. Extensive experiments on real-world and synthetic sensor datasets demonstrate that our method achieves superior detection performance compared to existing baselines. Our approach is easily extensible to other time-series tasks that demand rapid and reliable detection capabilities.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Graph Self-Training with Expectation-Maximization Regularization</title>
<link>https://arxiv.org/abs/2503.22744</link>
<guid>https://arxiv.org/abs/2503.22744</guid>
<content:encoded><![CDATA[
arXiv:2503.22744v2 Announce Type: replace 
Abstract: In this paper, we propose a novel \emph{uncertainty-aware graph self-training} approach for semi-supervised node classification. Our method introduces an Expectation-Maximization (EM) regularization scheme to incorporate an uncertainty mechanism during pseudo-label generation and model retraining. Unlike conventional graph self-training pipelines that rely on fixed pseudo-labels, our approach iteratively refines label confidences with an EM-inspired uncertainty measure. This ensures that the predictive model focuses on reliable graph regions while gradually incorporating ambiguous nodes. Inspired by prior work on uncertainty-aware self-training techniques~\cite{wang2024uncertainty}, our framework is designed to handle noisy graph structures and feature spaces more effectively. Through extensive experiments on several benchmark graph datasets, we demonstrate that our method outperforms strong baselines by a margin of up to 2.5\% in accuracy while maintaining lower variance in performance across multiple runs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Uncertainty-Aware Self-Training with Stochastic Node Labeling</title>
<link>https://arxiv.org/abs/2503.22745</link>
<guid>https://arxiv.org/abs/2503.22745</guid>
<content:encoded><![CDATA[
arXiv:2503.22745v2 Announce Type: replace 
Abstract: Self-training has become a popular semi-supervised learning technique for leveraging unlabeled data. However, the over-confidence of pseudo-labels remains a key challenge. In this paper, we propose a novel \emph{graph-based uncertainty-aware self-training} (GUST) framework to combat over-confidence in node classification. Drawing inspiration from the uncertainty integration idea introduced by Wang \emph{et al.}~\cite{wang2024uncertainty}, our method largely diverges from previous self-training approaches by focusing on \emph{stochastic node labeling} grounded in the graph topology. Specifically, we deploy a Bayesian-inspired module to estimate node-level uncertainty, incorporate these estimates into the pseudo-label generation process via an expectation-maximization (EM)-like step, and iteratively update both node embeddings and adjacency-based transformations. Experimental results on several benchmark graph datasets demonstrate that our GUST framework achieves state-of-the-art performance, especially in settings where labeled data is extremely sparse.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning: Comparative Analysis of Clustering Techniques on High-Dimensional Data</title>
<link>https://arxiv.org/abs/2503.23215</link>
<guid>https://arxiv.org/abs/2503.23215</guid>
<content:encoded><![CDATA[
arXiv:2503.23215v2 Announce Type: replace 
Abstract: This paper presents a comprehensive comparative analysis of prominent clustering algorithms K-means, DBSCAN, and Spectral Clustering on high-dimensional datasets. We introduce a novel evaluation framework that assesses clustering performance across multiple dimensionality reduction techniques (PCA, t-SNE, and UMAP) using diverse quantitative metrics. Experiments conducted on MNIST, Fashion-MNIST, and UCI HAR datasets reveal that preprocessing with UMAP consistently improves clustering quality across all algorithms, with Spectral Clustering demonstrating superior performance on complex manifold structures. Our findings show that algorithm selection should be guided by data characteristics, with Kmeans excelling in computational efficiency, DBSCAN in handling irregular clusters, and Spectral Clustering in capturing complex relationships. This research contributes a systematic approach for evaluating and selecting clustering techniques for high dimensional data applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Level Competitive Pok\'emon via Scalable Offline Reinforcement Learning with Transformers</title>
<link>https://arxiv.org/abs/2504.04395</link>
<guid>https://arxiv.org/abs/2504.04395</guid>
<content:encoded><![CDATA[
arXiv:2504.04395v2 Announce Type: replace 
Abstract: Competitive Pok\'emon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pok\'emon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players. All agent checkpoints, training details, datasets, and baselines are available at https://metamon.tech.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining</title>
<link>https://arxiv.org/abs/2504.13932</link>
<guid>https://arxiv.org/abs/2504.13932</guid>
<content:encoded><![CDATA[
arXiv:2504.13932v3 Announce Type: replace 
Abstract: The growing use of large language models has raised environmental and economic concerns about their intensity of resource usage during inference. Serving these models to each user requires substantial energy and water for cooling. Model compression techniques like quantization can shrink large language models and make them more resource efficient at the cost of potential performance degradation. Quantization methods compress model size through replacing their high-precision parameters by quantized values of lower precision. Among existing methods, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining is unlikely to be feasible through partial training. (2) This gain may depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. This publicly available method relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's accuracy degradation by 10.85% and 7.54% respectively. A Python implementation of the proposed quantization method is publicly available on GitHub https://github.com/TokuyuSou/ULB-SAPR.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence</title>
<link>https://arxiv.org/abs/2504.19259</link>
<guid>https://arxiv.org/abs/2504.19259</guid>
<content:encoded><![CDATA[
arXiv:2504.19259v2 Announce Type: replace 
Abstract: The Kullback-Leibler (KL) divergence plays a central role in probabilistic machine learning, where it commonly serves as the canonical loss function. Optimization in such settings is often performed over the probability simplex, where the choice of parameterization significantly impacts convergence. In this work, we study the problem of minimizing the KL divergence and analyze the behavior of gradient-based optimization algorithms under two dual coordinate systems within the framework of information geometry$-$ the exponential family ($\theta$ coordinates) and the mixture family ($\eta$ coordinates). We compare Euclidean gradient descent (GD) in these coordinates with the coordinate-invariant natural gradient descent (NGD), where the natural gradient is a Riemannian gradient that incorporates the intrinsic geometry of the underlying statistical model. In continuous time, we prove that the convergence rates of GD in the $\theta$ and $\eta$ coordinates provide lower and upper bounds, respectively, on the convergence rate of NGD. Moreover, under affine reparameterizations of the dual coordinates, the convergence rates of GD in $\eta$ and $\theta$ coordinates can be scaled to $2c$ and $\frac{2}{c}$, respectively, for any $c>0$, while NGD maintains a fixed convergence rate of $2$, remaining invariant to such transformations and sandwiched between them. Although this suggests that NGD may not exhibit uniformly superior convergence in continuous time, we demonstrate that its advantages become pronounced in discrete time, where it achieves faster convergence and greater robustness to noise, outperforming GD. Our analysis hinges on bounding the spectrum and condition number of the Hessian of the KL divergence at the optimum, which coincides with the Fisher information matrix.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repetition Makes Perfect: Recurrent Graph Neural Networks Match Message Passing Limit</title>
<link>https://arxiv.org/abs/2505.00291</link>
<guid>https://arxiv.org/abs/2505.00291</guid>
<content:encoded><![CDATA[
arXiv:2505.00291v2 Announce Type: replace 
Abstract: We precisely characterize the expressivity of computable Recurrent Graph Neural Networks (recurrent GNNs). We prove that recurrent GNNs with finite-precision parameters, sum aggregation, and ReLU activation, can compute any graph algorithm that respects the natural message-passing invariance induced by the Color Refinement (or Weisfeiler-Leman) algorithm. While it is well known that the expressive power of GNNs is limited by this invariance [Morris et al., AAAI 2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually match this limit. This is in contrast to non-recurrent GNNs, which have the power of Weisfeiler-Leman only in a very weak, "non-uniform", sense where each graph size requires a different GNN to compute with. Our construction introduces only a polynomial overhead in both time and space.
  Furthermore, we show that by incorporating random initialization, for connected graphs recurrent GNNs can express all graph algorithms. In particular, any polynomial-time graph algorithm can be emulated on connected graphs in polynomial time by a recurrent GNN with random initialization.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging</title>
<link>https://arxiv.org/abs/2505.14136</link>
<guid>https://arxiv.org/abs/2505.14136</guid>
<content:encoded><![CDATA[
arXiv:2505.14136v2 Announce Type: replace 
Abstract: Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models. However, current MoE models typically use only few experts due to prohibitive training and inference cost. We propose Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of magnitude more experts and uses model merging to avoid almost any test-time overhead. We show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt. TTT has recently been shown to significantly improve language models, but is computationally expensive. We find that performance of TTMM improves with more experts and approaches the performance of TTT. Moreover, we find that with a 1B parameter base model, TTMM is more than 100x faster than TTT at test-time by amortizing the cost of TTT at train-time. Thus, TTMM offers a promising cost-effective approach to scale test-time training.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation</title>
<link>https://arxiv.org/abs/2505.17579</link>
<guid>https://arxiv.org/abs/2505.17579</guid>
<content:encoded><![CDATA[
arXiv:2505.17579v3 Announce Type: replace 
Abstract: In this paper, we propose a novel framework for ownership verification of deep neural network (DNN) models for image classification tasks. It allows verification of model identity by both the rightful owner and third party without presenting the original model. We assume a gray-box scenario where an unauthorized user owns a model that is illegally copied from the original model, provides services in a cloud environment, and the user throws images and receives the classification results as a probability distribution of output classes. The framework applies a white-box adversarial attack to align the output probability of a specific class to a designated value. Due to the knowledge of original model, it enables the owner to generate such adversarial examples. We propose a simple but effective adversarial attack method based on the iterative Fast Gradient Sign Method (FGSM) by introducing control parameters. Experimental results confirm the effectiveness of the identification of DNN models using adversarial attack.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-based Reinforcement Learning to Predict the Future</title>
<link>https://arxiv.org/abs/2505.17989</link>
<guid>https://arxiv.org/abs/2505.17989</guid>
<content:encoded><![CDATA[
arXiv:2505.17989v3 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been an effective approach for improving Large Language Models' reasoning in domains such as coding and mathematics. Here, we apply RLVR methods towards forecasting future real-world events - a challenging task for RL due to the very noisy (and delayed) outcomes involved. Using a novel dataset of recent questions from a prediction market, and accompanying relevant news headlines, we show that a compact (14B) reasoning model can be trained to match or surpass the predictive accuracy of frontier models like o1, while greatly improving probabilistic calibration. The model's performance is also practically meaningful: in a Polymarket trading simulation, we estimate that its bets would have yielded a return on investment of over 10% across all questions in the test set. We detail and compare approaches used in training our model, including augmenting our training-data with synthetic prediction questions, guardrails for learning stability, and median prediction sampling at inference-time.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title>
<link>https://arxiv.org/abs/2505.19194</link>
<guid>https://arxiv.org/abs/2505.19194</guid>
<content:encoded><![CDATA[
arXiv:2505.19194v2 Announce Type: replace 
Abstract: Adversarial attack reveals the vulnerability of deep learning models. For about a decade, countless attack and defense methods have been proposed, leading to robustified classifiers and better understanding of models. Among these methods, curvature-based approaches have attracted attention because it is assumed that high curvature may give rise to rough decision boundary. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation(DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack(CDBA) with improved performance using the dynamically estimated curvature.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v3 Announce Type: replace 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models</title>
<link>https://arxiv.org/abs/2505.20930</link>
<guid>https://arxiv.org/abs/2505.20930</guid>
<content:encoded><![CDATA[
arXiv:2505.20930v2 Announce Type: replace 
Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction technique for accelerating reliability assessments of complex power system. Recently, data-driven surrogate models have been proposed as lower-level models in the MLMC framework due to their high correlation and negligible execution time once trained. However, in resource adequacy assessments, pre-labeled datasets are typically unavailable. For large-scale systems, the efficiency gains from surrogate models are often offset by the substantial time required for labeling training data. Therefore, this paper introduces a speed metric that accounts for training time in evaluating MLMC efficiency. Considering the total time budget is limited, a vote-by-committee active learning approach is proposed to reduce the required labeling calls. A case study demonstrates that, within a given computational budget, active learning in combination with MLMC can result in a substantial reduction variance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs</title>
<link>https://arxiv.org/abs/2505.24110</link>
<guid>https://arxiv.org/abs/2505.24110</guid>
<content:encoded><![CDATA[
arXiv:2505.24110v2 Announce Type: replace 
Abstract: We present a formal and constructive simulation framework for nondeterministic finite automata (NFAs) using standard feedforward ReLU neural networks. Unlike prior approaches that rely on recurrent architectures or post hoc extraction methods, our formulation symbolically encodes automaton states as binary vectors, transitions as sparse linear transformations, and nondeterministic branching - including {\epsilon}-closures - as compositions of shared ReLU layers. We prove that every regular language can be recognized exactly by a depth-unrolled ReLU network with shared parameters, independent of input length. Our construction yields not only formal equivalence between NFAs and ReLU networks, but also practical trainability: we demonstrate that the networks can learn NFA acceptance behavior through gradient descent using standard supervised data. Extensive experiments validate all theoretical results, achieving perfect or near-perfect agreement on acceptance, state propagation, and closure dynamics. This work establishes a new bridge between symbolic automata theory and modern neural architectures, showing that feedforward networks can perform precise, interpretable, and trainable symbolic computation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory First: A Curriculum for Discovering Diverse Policies</title>
<link>https://arxiv.org/abs/2506.01568</link>
<guid>https://arxiv.org/abs/2506.01568</guid>
<content:encoded><![CDATA[
arXiv:2506.01568v2 Announce Type: replace 
Abstract: Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima. In this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel. However, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity. To improve diversity optimization in RL, we therefore propose a curriculum that first explores at the trajectory level before learning step-based policies. In our empirical evaluation, we provide novel insights into the shortcoming of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields</title>
<link>https://arxiv.org/abs/2506.08146</link>
<guid>https://arxiv.org/abs/2506.08146</guid>
<content:encoded><![CDATA[
arXiv:2506.08146v2 Announce Type: replace 
Abstract: We propose a new framework for identifying mechanical properties of heterogeneous materials without a closed-form constitutive equation. Given a full-field measurement of the displacement field, for instance as obtained from digital image correlation (DIC), a continuous approximation of the strain field is obtained by training a neural network that incorporates Fourier features to effectively capture sharp gradients in the data. A physics-based data-driven method built upon ordinary neural differential equations (NODEs) is employed to discover constitutive equations. The NODE framework can represent arbitrary materials while satisfying constraints in the theory of constitutive equations by default. To account for heterogeneity, a hyper-network is defined, where the input is the material coordinate system, and the output is the NODE-based constitutive equation. The parameters of the hyper-network are optimized by minimizing a multi-objective loss function that includes penalty terms for violations of the strong form of the equilibrium equations of elasticity and the associated Neumann boundary conditions. We showcase the framework with several numerical examples, including heterogeneity arising from variations in material parameters, spatial transitions from isotropy to anisotropy, material identification in the presence of noise, and, ultimately, application to experimental data. As the numerical results suggest, the proposed approach is robust and general in identifying the mechanical properties of heterogeneous materials with very few assumptions, making it a suitable alternative to classical inverse methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis</title>
<link>https://arxiv.org/abs/2506.11378</link>
<guid>https://arxiv.org/abs/2506.11378</guid>
<content:encoded><![CDATA[
arXiv:2506.11378v2 Announce Type: replace 
Abstract: Sampling in score-based diffusion models can be performed by solving either a reverse-time stochastic differential equation (SDE) parameterized by an arbitrary time-dependent stochasticity parameter or a probability flow ODE, corresponding to the stochasticity parameter set to zero. In this work, we study the effect of this stochasticity on the generation process through bounds on the Kullback-Leibler (KL) divergence, complementing the analysis with numerical and analytical examples. Our main results apply to linear forward SDEs with additive noise and Lipschitz-continuous score functions, and quantify how errors from the prior distribution and score approximation propagate under different choices of the stochasticity parameter. The theoretical bounds are derived using log-Sobolev inequalities for the marginals of the forward process, which enable a more effective control of the KL divergence decay along sampling. For exact score functions, we find that stochasticity acts as an error-correcting mechanism, decreasing KL divergence along the sampling trajectory. For an approximate score function, there is a trade-off between error correction and score error amplification, so that stochasticity can either improve or worsen the performance, depending on the structure of the score error. Numerical experiments on simple datasets and a fully analytical example are included to illustrate and enlighten the theoretical results.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localization</title>
<link>https://arxiv.org/abs/2506.13362</link>
<guid>https://arxiv.org/abs/2506.13362</guid>
<content:encoded><![CDATA[
arXiv:2506.13362v2 Announce Type: replace 
Abstract: We propose two new methods based/inspired by machine learning for tabular data and distance-free localization to enhance the covariance estimations in an ensemble data assimilation. The main goal is to enhance the data assimilation results by mitigating loss of variance due to sampling errors. We also analyze the suitability of several machine learning models and the balance between accuracy and computational cost of the covariance estimations. We introduce two distance-free localization techniques leveraging machine learning methods specifically tailored for tabular data. The methods are integrated into the Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The results show that the proposed localizations improve covariance accuracy and enhance data assimilation and uncertainty quantification results. We observe reduced variance loss for the input variables using the proposed methods. Furthermore, we compare several machine learning models, assessing their suitability for the problem in terms of computational cost, and quality of the covariance estimation and data match. The influence of ensemble size is also investigated, providing insights into balancing accuracy and computational efficiency. Our findings demonstrate that certain machine learning models are more suitable for this problem. This study introduces two novel methods that mitigate variance loss for model parameters in ensemble-based data assimilation, offering practical solutions that are easy to implement and do not require any additional numerical simulation or hyperparameter tuning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Floating-Point Neural Networks Are Provably Robust Universal Approximators</title>
<link>https://arxiv.org/abs/2506.16065</link>
<guid>https://arxiv.org/abs/2506.16065</guid>
<content:encoded><![CDATA[
arXiv:2506.16065v2 Announce Type: replace 
Abstract: The classical universal approximation (UA) theorem for neural networks establishes mild conditions under which a feedforward neural network can approximate a continuous function $f$ with arbitrary accuracy. A recent result shows that neural networks also enjoy a more general interval universal approximation (IUA) theorem, in the sense that the abstract interpretation semantics of the network using the interval domain can approximate the direct image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with arbitrary accuracy. These theorems, however, rest on the unrealistic assumption that the neural network computes over infinitely precise real numbers, whereas their software implementations in practice compute over finite-precision floating-point numbers. An open question is whether the IUA theorem still holds in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural networks that proves their remarkable ability to perfectly capture the direct image map of any rounded target function $f$, showing no limits exist on their expressiveness. Our IUA theorem in the floating-point setting exhibits material differences from the real-valued setting, which reflects the fundamental distinctions between these two computational models. This theorem also implies surprising corollaries, which include (i) the existence of provably robust floating-point neural networks; and (ii) the computational completeness of the class of straight-line programs that use only floating-point additions and multiplications for the class of all floating-point programs that halt.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocketStack: Level-aware deep recursive ensemble learning framework with adaptive feature fusion and model pruning dynamics</title>
<link>https://arxiv.org/abs/2506.16965</link>
<guid>https://arxiv.org/abs/2506.16965</guid>
<content:encoded><![CDATA[
arXiv:2506.16965v2 Announce Type: replace 
Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking used to integrate predictions from multiple base learners through a meta-model. However, deep stacking remains rare, as most designs prioritize horizontal diversity over recursive depth due to model complexity, feature redundancy, and computational burden. To address these challenges, RocketStack, a level-aware recursive ensemble framework, is introduced and explored up to ten stacking levels, extending beyond prior architectures. The framework incrementally prunes weaker learners at each level, enabling deeper stacking without excessive complexity. To mitigate early performance saturation, mild Gaussian noise is added to out-of-fold (OOF) scores before pruning, and compared against strict OOF pruning. Further both per-level and periodic feature compressions are explored using attention-based selection, Simple, Fast, Efficient (SFE) filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class), linear-trend tests confirmed rising accuracy with depth in most variants, and the top performing meta-model at each level increasingly outperformed the strongest standalone ensemble. In the binary subset, periodic SFE with mild OOF-score randomization reached 97.08% at level 10, 5.14% above the strict-pruning configuration and cut runtime by 10.5% relative to no compression. In the multi-class subset, periodic attention selection reached 98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing runtime by 56.1% and feature dimensionality by 74% compared to no compression. These findings highlight mild randomization as an effective regularizer and periodic compression as a stabilizer. Echoing the design of multistage rockets in aerospace (prune, compress, propel) RocketStack achieves deep recursive ensembling with tractable complexity.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data</title>
<link>https://arxiv.org/abs/2506.20132</link>
<guid>https://arxiv.org/abs/2506.20132</guid>
<content:encoded><![CDATA[
arXiv:2506.20132v2 Announce Type: replace 
Abstract: Wildfires are increasing in intensity and severity at an alarming rate. Recent advances in AI and publicly available satellite data enable monitoring critical wildfire risk factors globally, at high resolution and low latency. Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is valuable for both wildfire research and operational response. However, ground-based LFMC samples are both labor intensive and costly to acquire, resulting in sparse and infrequent updates. In this work, we explore the use of a pretrained, highly-multimodal earth-observation model for generating large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves significant improvements over previous methods using randomly initialized models (20 reduction in RMSE). We provide an automated pipeline that enables rapid generation of these LFMC maps across the United States, and demonstrate its effectiveness in two regions recently impacted by wildfire (Eaton and Palisades).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A case for data valuation transparency via DValCards</title>
<link>https://arxiv.org/abs/2506.23349</link>
<guid>https://arxiv.org/abs/2506.23349</guid>
<content:encoded><![CDATA[
arXiv:2506.23349v2 Announce Type: replace 
Abstract: Following the rise in popularity of data-centric machine learning (ML), various data valuation methods have been proposed to quantify the contribution of each datapoint to desired ML model performance metrics (e.g., accuracy). Beyond the technical applications of data valuation methods (e.g., data cleaning, data acquisition, etc.), it has been suggested that within the context of data markets, data buyers might utilize such methods to fairly compensate data owners. Here we demonstrate that data valuation metrics are inherently biased and unstable under simple algorithmic design choices, resulting in both technical and ethical implications. By analyzing 9 tabular classification datasets and 6 data valuation methods, we illustrate how (1) common and inexpensive data pre-processing techniques can drastically alter estimated data values; (2) subsampling via data valuation metrics may increase class imbalance; and (3) data valuation metrics may undervalue underrepresented group data. Consequently, we argue in favor of increased transparency associated with data valuation in-the-wild and introduce the novel Data Valuation Cards (DValCards) framework towards this aim. The proliferation of DValCards will reduce misuse of data valuation metrics, including in data pricing, and build trust in responsible ML systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Collaborative Attention Network for Link Prediction in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.03947</link>
<guid>https://arxiv.org/abs/2507.03947</guid>
<content:encoded><![CDATA[
arXiv:2507.03947v2 Announce Type: replace 
Abstract: Knowledge graphs offer a structured representation of real-world entities and their relationships, enabling a wide range of applications from information retrieval to automated reasoning. In this paper, we conduct a systematic comparison between traditional rule-based approaches and modern deep learning methods for link prediction. We focus on KBGAT, a graph neural network model that leverages multi-head attention to jointly encode both entity and relation features within local neighborhood structures. To advance this line of research, we introduce \textbf{GCAT} (Graph Collaborative Attention Network), a refined model that enhances context aggregation and interaction between heterogeneous nodes. Experimental results on four widely-used benchmark datasets demonstrate that GCAT not only consistently outperforms rule-based methods but also achieves competitive or superior performance compared to existing neural embedding models. Our findings highlight the advantages of attention-based architectures in capturing complex relational patterns for knowledge graph completion tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
arXiv:2507.08771v2 Announce Type: replace 
Abstract: To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction</title>
<link>https://arxiv.org/abs/2507.10078</link>
<guid>https://arxiv.org/abs/2507.10078</guid>
<content:encoded><![CDATA[
arXiv:2507.10078v2 Announce Type: replace 
Abstract: Deep learning models incorporating linear SSMs have gained attention for capturing long-range dependencies in sequential data. However, their large parameter sizes pose challenges for deployment on resource-constrained devices. In this study, we propose an efficient parameter reduction method for these models by applying $H^{2}$ model order reduction techniques from control theory to their linear SSM components. In experiments, the LRA benchmark results show that the model compression based on our proposed method outperforms an existing method using the Balanced Truncation, while successfully reducing the number of parameters in the SSMs to $1/32$ without sacrificing the performance of the original models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Quantised Representations Isolated to Anisotropic Functions</title>
<link>https://arxiv.org/abs/2507.12070</link>
<guid>https://arxiv.org/abs/2507.12070</guid>
<content:encoded><![CDATA[
arXiv:2507.12070v2 Announce Type: replace 
Abstract: This paper presents a novel methodology for determining representational alignment, which builds upon the existing Spotlight Resonance method. Particularly, this new tool is used to gain insight into how discrete representations can emerge and organise in autoencoder models, through a controlled ablation study in which only the activation function is altered. Using this technique, the validity of whether function-driven symmetries can act as implicit inductive biases on representations is determined. Representations are found to tend to discretise when the activation functions are defined through a discrete algebraic permutation-equivariant symmetry. In contrast, they remain continuous under a continuous algebraic orthogonal-equivariant definition. This confirms the hypothesis: algebraic symmetries of network primitives can carry unintended inductive biases which produce task-independent artefactual structures in representations. The discrete symmetry of contemporary forms is shown to be a strong predictor for the induction of discrete representations transformed from otherwise continuous structures -- a quantisation effect. This motivates further reassessment of functional forms in common usage. Moreover, this supports a general causal model for one mode in which discrete representations may form, and could constitute a prerequisite for downstream interpretability phenomena, including grandmother neurons, discrete coding schemes, general linear features and possibly Superposition. Hence, this tool and proposed mechanism for the influence of functional form on representations may provide insights into emergent interpretability research. Finally, preliminary results indicate that quantisation of representations appears to correlate with a measurable increase in reconstruction error, reinforcing previous conjectures that this collapse can be detrimental.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Low-Frequency Bias of In-Context Learning of Representations</title>
<link>https://arxiv.org/abs/2507.13540</link>
<guid>https://arxiv.org/abs/2507.13540</guid>
<content:encoded><![CDATA[
arXiv:2507.13540v2 Announce Type: replace 
Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new behaviors from the input sequence alone without any parameter updates. Recent studies have shown that ICL can surpass the original meaning learned in pretraining stage through internalizing the structure the data-generating process (DGP) of the prompt into the hidden representations. However, the mechanisms by which LLMs achieve this ability is left open. In this paper, we present the first rigorous explanation of such phenomena by introducing a unified framework of double convergence, where hidden representations converge both over context and across layers. This double convergence process leads to an implicit bias towards smooth (low-frequency) representations, which we prove analytically and verify empirically. Our theory explains several open empirical observations, including why learned representations exhibit globally structured but locally distorted geometry, and why their total energy decays without vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness towards high-frequency noise, which we empirically confirm. These results provide new insights into the underlying mechanisms of ICL, and a theoretical foundation to study it that hopefully extends to more general data distributions and settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Individual Fairness in Deepfake Detection</title>
<link>https://arxiv.org/abs/2507.14326</link>
<guid>https://arxiv.org/abs/2507.14326</guid>
<content:encoded><![CDATA[
arXiv:2507.14326v2 Announce Type: replace 
Abstract: Generative AI models have substantially improved the realism of synthetic media, yet their misuse through sophisticated DeepFakes poses significant risks. Despite recent advances in deepfake detection, fairness remains inadequately addressed, enabling deepfake markers to exploit biases against specific populations. While previous studies have emphasized group-level fairness, individual fairness (i.e., ensuring similar predictions for similar individuals) remains largely unexplored. In this work, we identify for the first time that the original principle of individual fairness fundamentally fails in the context of deepfake detection, revealing a critical gap previously unexplored in the literature. To mitigate it, we propose the first generalizable framework that can be integrated into existing deepfake detectors to enhance individual fairness and generalization. Extensive experiments conducted on leading deepfake datasets demonstrate that our approach significantly improves individual fairness while maintaining robust detection performance, outperforming state-of-the-art methods. The code is available at https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Unlearning: Forgetting Distributions, Not Just Samples</title>
<link>https://arxiv.org/abs/2507.15112</link>
<guid>https://arxiv.org/abs/2507.15112</guid>
<content:encoded><![CDATA[
arXiv:2507.15112v2 Announce Type: replace 
Abstract: Machine unlearning seeks to remove unwanted information from trained models, initially at the individual-sample level, but increasingly at the level of entire sub-populations. In many deployments, models must delete whole topical domains to satisfy privacy, legal, or quality requirements, e.g., removing several users' posts under GDPR or copyrighted web content. Existing unlearning tools remain largely sample-oriented, and straightforward point deletion often leaves enough residual signal for downstream learners to recover the unwanted domain. We introduce distributional unlearning, a data-centric, model-agnostic framework that asks: Given examples from an unwanted distribution and a retained distribution, what is the smallest set of points whose removal makes the edited dataset far from the unwanted domain yet close to the retained one? Using Kullback-Leibler divergence to quantify removal and preservation, we derive the exact Pareto frontier in the Gaussian case and prove that any model retrained on the edited data incurs log-loss shifts bounded by the divergence thresholds. We propose a simple distance-based selection rule satisfying these constraints with a quadratic reduction in deletion budget compared to random removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam, and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on retained performance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</title>
<link>https://arxiv.org/abs/2307.14364</link>
<guid>https://arxiv.org/abs/2307.14364</guid>
<content:encoded><![CDATA[
arXiv:2307.14364v2 Announce Type: replace-cross 
Abstract: Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Law of Capacity Gap in Distilling Language Models</title>
<link>https://arxiv.org/abs/2311.07052</link>
<guid>https://arxiv.org/abs/2311.07052</guid>
<content:encoded><![CDATA[
arXiv:2311.07052v4 Announce Type: replace-cross 
Abstract: Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the \textit{curse of capacity gap}, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the \textit{law of capacity gap} inducted from a preliminary study on distilling a broad range of small-scale (<3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Historical Climate Fields With Deep Learning</title>
<link>https://arxiv.org/abs/2311.18348</link>
<guid>https://arxiv.org/abs/2311.18348</guid>
<content:encoded><![CDATA[
arXiv:2311.18348v2 Announce Type: replace-cross 
Abstract: Historical records of climate fields are often sparse due to missing measurements, especially before the introduction of large-scale satellite missions. Several statistical and model-based methods have been introduced to fill gaps and reconstruct historical records. Here, we employ a recently introduced deep-learning approach based on Fourier convolutions, trained on numerical climate model output, to reconstruct historical climate fields. Using this approach we are able to realistically reconstruct large and irregular areas of missing data, as well as reconstruct known historical events such as strong El Ni\~no and La Ni\~na with very little given information. Our method outperforms the widely used statistical kriging method as well as other recent machine learning approaches. The model generalizes to higher resolutions than the ones it was trained on and can be used on a variety of climate fields. Moreover, it allows inpainting of masks never seen before during the model training.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIX: A Knowledge and Interaction-Centric Metacognitive Framework for Task Generalization</title>
<link>https://arxiv.org/abs/2402.05346</link>
<guid>https://arxiv.org/abs/2402.05346</guid>
<content:encoded><![CDATA[
arXiv:2402.05346v3 Announce Type: replace-cross 
Abstract: People aptly exhibit general intelligence behaviors through flexible problem-solving and the ability to adapt to novel situations by reusing and applying high-level knowledge acquired over time. In contrast, artificial agents tend to be specialists, lacking such generalist behaviors. To bridge this gap, artificial agents will require understanding and exploiting critical structured knowledge representations. We introduce a metacognitive reasoning framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects, by leveraging a type space, facilitate the learning of transferable interaction concepts and promote generalization. This framework offers a principled approach for integrating knowledge into reinforcement learning and holds promise as an enabler for generalist behaviors in artificial intelligence, robotics, and autonomous systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML</title>
<link>https://arxiv.org/abs/2402.12630</link>
<guid>https://arxiv.org/abs/2402.12630</guid>
<content:encoded><![CDATA[
arXiv:2402.12630v2 Announce Type: replace-cross 
Abstract: We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models $\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \citep{nori2019interpretml}. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation</title>
<link>https://arxiv.org/abs/2403.19776</link>
<guid>https://arxiv.org/abs/2403.19776</guid>
<content:encoded><![CDATA[
arXiv:2403.19776v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a powerful and popular technique for personalization, enabling efficient adaptation of pre-trained image generation models for specific tasks without comprehensive retraining. While employing individual pre-trained LoRA models excels at representing single concepts, such as those representing a specific dog or a cat, utilizing multiple LoRA models to capture a variety of concepts in a single image still poses a significant challenge. Existing methods often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). We introduce CLoRA, a training-free approach that addresses these limitations by updating the attention maps of multiple LoRA models at test-time, and leveraging the attention maps to create semantic masks for fusing latent representations. This enables the generation of composite images that accurately reflect the characteristics of each LoRA. Our comprehensive qualitative and quantitative evaluations demonstrate that CLoRA significantly outperforms existing methods in multi-concept image generation using LoRAs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cooperative Network Architecture: Learning Structured Networks as Representation of Sensory Patterns</title>
<link>https://arxiv.org/abs/2407.05650</link>
<guid>https://arxiv.org/abs/2407.05650</guid>
<content:encoded><![CDATA[
arXiv:2407.05650v4 Announce Type: replace-cross 
Abstract: We introduce the Cooperative Network Architecture (CNA), a model that represents sensory signals using structured, recurrently connected networks of neurons, termed "nets." Nets are dynamically assembled from overlapping net fragments, which are learned based on statistical regularities in sensory input. This architecture offers robustness to noise, deformation, and out-of-distribution data, addressing challenges in current vision systems from a novel perspective. We demonstrate that net fragments can be learned without supervision and flexibly recombined to encode novel patterns, enabling figure completion and resilience to noise. Our findings establish CNA as a promising paradigm for developing neural representations that integrate local feature processing with global structure formation, providing a foundation for future research on invariant object recognition.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer</title>
<link>https://arxiv.org/abs/2407.10247</link>
<guid>https://arxiv.org/abs/2407.10247</guid>
<content:encoded><![CDATA[
arXiv:2407.10247v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) into corporate strategy has become critical for organizations seeking to maintain a competitive advantage in the digital age. As AI transforms business models, operations, and decision-making, the need for dedicated executive leadership to guide, govern, and orchestrate this transformation becomes increasingly evident. This paper examines emerging future scenarios across three domains: the AI Economy, the AI Organization, and Competition in the Age of AI. These domains reveal environmental, structural, and strategic tensions that existing C-suite roles struggle to resolve. In response, the paper develops a theory-informed framework for the Chief AI Officer (CAIO), outlining the distinct functions and capabilities required to guide and govern AI at scale. Drawing on illustrative cases and emerging practice, this conceptualization clarifies the CAIOs unique role within the executive landscape and presents a forward-looking research agenda. This paper advances the discourse on AI leadership by offering a theory-driven rationale for the strategic integration of AI at the executive level and by positioning the Chief AI Officer as a distinct and necessary role within modern organizations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neutral Residues: Revisiting Adapters for Model Extension</title>
<link>https://arxiv.org/abs/2410.02744</link>
<guid>https://arxiv.org/abs/2410.02744</guid>
<content:encoded><![CDATA[
arXiv:2410.02744v2 Announce Type: replace-cross 
Abstract: We address the problem of extending a pretrained large language model to a new domain that was not seen during training. Standard techniques, such as finetuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we revisit and improve adapters to extend LLMs from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperform competing approaches such as finetuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Non-Random Extreme Learning Machine</title>
<link>https://arxiv.org/abs/2411.16229</link>
<guid>https://arxiv.org/abs/2411.16229</guid>
<content:encoded><![CDATA[
arXiv:2411.16229v2 Announce Type: replace-cross 
Abstract: The Extreme Learning Machine (ELM) is a growing statistical technique widely applied to regression problems. In essence, ELMs are single-layer neural networks where the hidden layer weights are randomly sampled from a specific distribution, while the output layer weights are learned from the data. Two of the key challenges with this approach are the architecture design, specifically determining the optimal number of neurons in the hidden layer, and the method's sensitivity to the random initialization of hidden layer weights.
  This paper introduces a new and enhanced learning algorithm for regression tasks, the Effective Non-Random ELM (ENR-ELM), which simplifies the architecture design and eliminates the need for random hidden layer weight selection. The proposed method incorporates concepts from signal processing, such as basis functions and projections, into the ELM framework. We introduce two versions of the ENR-ELM: the approximated ENR-ELM and the incremental ENR-ELM. Experimental results on both synthetic and real datasets demonstrate that our method overcomes the problems of traditional ELM while maintaining comparable predictive performance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis</title>
<link>https://arxiv.org/abs/2412.20104</link>
<guid>https://arxiv.org/abs/2412.20104</guid>
<content:encoded><![CDATA[
arXiv:2412.20104v5 Announce Type: replace-cross 
Abstract: Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.06680</link>
<guid>https://arxiv.org/abs/2501.06680</guid>
<content:encoded><![CDATA[
arXiv:2501.06680v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have become a promising approach to enhancing perception and decision-making in autonomous driving. The gap remains in applying VLMs to understand complex scenarios interacting with pedestrians and efficient vehicle deployment. In this paper, we propose a knowledge distillation method that transfers knowledge from large-scale vision-language foundation models to efficient vision networks, and we apply it to pedestrian behavior prediction and scene understanding tasks, achieving promising results in generating more diverse and comprehensive semantic attributes. We also utilize multiple pre-trained models and ensemble techniques to boost the model's performance. We further examined the effectiveness of the model after knowledge distillation; the results show significant metric improvements in open-vocabulary perception and trajectory prediction tasks, which can potentially enhance the end-to-end performance of autonomous driving.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skull-stripping induces shortcut learning in MRI-based Alzheimer's disease classification</title>
<link>https://arxiv.org/abs/2501.15831</link>
<guid>https://arxiv.org/abs/2501.15831</guid>
<content:encoded><![CDATA[
arXiv:2501.15831v3 Announce Type: replace-cross 
Abstract: Objectives:  High classification accuracy of Alzheimer's disease (AD) from structural MRI has been achieved using deep neural networks, yet the specific image features contributing to these decisions remain unclear. In this study, the contributions of T1-weighted (T1w) gray-white matter texture, volumetric information, and preprocessing -- particularly skull-stripping -- were systematically assessed.
  Methods: A dataset of 990 matched T1w MRIs from AD patients and cognitively normal controls from the ADNI database were used. Preprocessing was varied through skull-stripping and intensity binarization to isolate texture and shape contributions. A 3D convolutional neural network was trained on each configuration, and classification performance was compared using exact McNemar tests with discrete Bonferroni-Holm correction. Feature relevance was analyzed using Layer-wise Relevance Propagation, image similarity metrics, and spectral clustering of relevance maps.
  Results: Despite substantial differences in image content, classification accuracy, sensitivity, and specificity remained stable across preprocessing conditions. Models trained on binarized images preserved performance, indicating minimal reliance on gray-white matter texture. Instead, volumetric features -- particularly brain contours introduced through skull-stripping -- were consistently used by the models.
  Conclusions: This behavior reflects a shortcut learning phenomenon, where preprocessing artifacts act as potentially unintended cues. The resulting Clever Hans effect emphasizes the critical importance of interpretability tools to reveal hidden biases and to ensure robust and trustworthy deep learning in medical imaging.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ball-Proximal (="Broximal") Point Method: a New Algorithm, Convergence Theory, and Applications</title>
<link>https://arxiv.org/abs/2502.02002</link>
<guid>https://arxiv.org/abs/2502.02002</guid>
<content:encoded><![CDATA[
arXiv:2502.02002v2 Announce Type: replace-cross 
Abstract: Non-smooth and non-convex global optimization poses significant challenges across various applications, where standard gradient-based methods often struggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or Ball Point Method (BPM) for short - a novel algorithmic framework inspired by the classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we show, sheds new light on several foundational optimization paradigms and phenomena, including non-convex and non-smooth optimization, acceleration, smoothing, adaptive stepsize selection, and trust-region methods. At the core of BPM lies the ball-proximal ("broximal") operator, which arises from the classical proximal operator by replacing the quadratic distance penalty by a ball constraint. Surprisingly, and in sharp contrast with the sublinear rate of PPM in the nonsmooth convex regime, we prove that BPM converges linearly and in a finite number of steps in the same regime. Furthermore, by introducing the concept of ball-convexity, we prove that BPM retains the same global convergence guarantees under weaker assumptions, making it a powerful tool for a broader class of potentially non-convex optimization problems. Just like PPM plays the role of a conceptual method inspiring the development of practically efficient algorithms and algorithmic elements, e.g., gradient descent, adaptive step sizes, acceleration (Ahn & Sra, 2020), and "W" in AdamW (Zhuang et al., 2022), we believe that BPM should be understood in the same manner: as a blueprint and inspiration for further development.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning</title>
<link>https://arxiv.org/abs/2502.13820</link>
<guid>https://arxiv.org/abs/2502.13820</guid>
<content:encoded><![CDATA[
arXiv:2502.13820v3 Announce Type: replace-cross 
Abstract: Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEarthSensing: Large-Scale Fine-Grained Benchmark for Open-World Remote Sensing</title>
<link>https://arxiv.org/abs/2502.20668</link>
<guid>https://arxiv.org/abs/2502.20668</guid>
<content:encoded><![CDATA[
arXiv:2502.20668v2 Announce Type: replace-cross 
Abstract: The advancement of remote sensing, including satellite systems, facilitates the continuous acquisition of remote sensing imagery globally, introducing novel challenges for achieving open-world tasks. Deployed models need to continuously adjust to a constant influx of new data, which frequently exhibits diverse shifts from the data encountered during the training phase. To effectively handle the new data, models are required to detect semantic shifts, adapt to covariate shifts, and continuously update their parameters without forgetting learned knowledge, as has been considered in works on a variety of open-world tasks. However, existing studies are typically conducted within a single dataset to simulate realistic conditions, with a lack of large-scale benchmarks capable of evaluating multiple open-world tasks. In this paper, we introduce \textbf{OpenEarthSensing (OES)}, a large-scale fine-grained benchmark for open-world remote sensing. OES includes 189 scene and object categories, covering the vast majority of potential semantic shifts that may occur in the real world. Additionally, to provide a more comprehensive testbed for evaluating the generalization performance, OES encompasses five data domains with significant covariate shifts, including two RGB satellite domains, one RGB aerial domain, one multispectral RGB domain, and one infrared domain. We evaluate the baselines and existing methods for diverse tasks on OES, demonstrating that it serves as a meaningful and challenging benchmark for open-world remote sensing. The proposed dataset OES is available at https://haiv-lab.github.io/OES.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVFlow: Preserving Paralinguistic Elements with Conditional Flow Matching for Zero-Shot AV2AV Multilingual Translation</title>
<link>https://arxiv.org/abs/2503.11026</link>
<guid>https://arxiv.org/abs/2503.11026</guid>
<content:encoded><![CDATA[
arXiv:2503.11026v2 Announce Type: replace-cross 
Abstract: Despite recent advances in text-to-speech (TTS) models, audio-visual-to-audio-visual (AV2AV) translation still faces a critical challenge: maintaining speaker consistency between the original and translated vocal and facial features. To address this issue, we propose a conditional flow matching (CFM) zero-shot audio-visual renderer that utilizes strong dual guidance from both audio and visual modalities. By leveraging multimodal guidance with CFM, our model robustly preserves speaker-specific characteristics and enhances zero-shot AV2AV translation abilities. For the audio modality, we enhance the CFM process by integrating robust speaker embeddings with x-vectors, which serve to bolster speaker consistency. Additionally, we convey emotional nuances to the face rendering module. The guidance provided by both audio and visual cues remains independent of semantic or linguistic content, allowing our renderer to effectively handle zero-shot translation tasks for monolingual speakers in different languages. We empirically demonstrate that the inclusion of high-quality mel-spectrograms conditioned on facial information not only enhances the quality of the synthesized speech but also positively influences facial generation, leading to overall performance improvements in LSE and FID score. Our code is available at https://github.com/Peter-SungwooCho/MAVFlow.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Lag, RAG: Training-Free Adversarial Detection Using RAG</title>
<link>https://arxiv.org/abs/2504.04858</link>
<guid>https://arxiv.org/abs/2504.04858</guid>
<content:encoded><![CDATA[
arXiv:2504.04858v3 Announce Type: replace-cross 
Abstract: Adversarial patch attacks pose a major threat to vision systems by embedding localized perturbations that mislead deep models. Traditional defense methods often require retraining or fine-tuning, making them impractical for real-world deployment. We propose a training-free Visual Retrieval-Augmented Generation (VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial patch detection. By retrieving visually similar patches and images that resemble stored attacks in a continuously expanding database, VRAG performs generative reasoning to identify diverse attack types, all without additional training or fine-tuning. We extensively evaluate open-source large-scale VLMs, including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO model achieves up to 95 percent classification accuracy, setting a new state-of-the-art for open-source adversarial patch detection. Gemini-2.0 attains the highest overall accuracy, 98 percent, but remains closed-source. Experimental results demonstrate VRAG's effectiveness in identifying a variety of adversarial patches with minimal human annotation, paving the way for robust, practical defenses against evolving adversarial patch attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data</title>
<link>https://arxiv.org/abs/2504.06923</link>
<guid>https://arxiv.org/abs/2504.06923</guid>
<content:encoded><![CDATA[
arXiv:2504.06923v3 Announce Type: replace-cross 
Abstract: Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility.
  In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we improve on an existing approach for automatically selecting the optimal number of bins, and achieve high utility while reducing both privacy budget consumption and computational overhead.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation</title>
<link>https://arxiv.org/abs/2504.10487</link>
<guid>https://arxiv.org/abs/2504.10487</guid>
<content:encoded><![CDATA[
arXiv:2504.10487v2 Announce Type: replace-cross 
Abstract: In this paper, we challenge the conventional practice in Open-Vocabulary Semantic Segmentation (OVSS) of using averaged class-wise text embeddings, which are typically obtained by encoding each class name with multiple templates (e.g., a photo of , a sketch of a ). We investigate the impact of templates for OVSS, and find that for each class, there exist single-template classifiers--which we refer to as class-experts--that significantly outperform the conventional averaged classifier. First, to identify these class-experts, we introduce a novel approach that estimates them without any labeled data or training. By leveraging the class-wise prediction entropy of single-template classifiers, we select those yielding the lowest entropy as the most reliable class-experts. Second, we combine the outputs of class-experts in a new fusion process. Our plug-and-play method, coined FLOSS, is orthogonal and complementary to existing OVSS methods, offering an improvement without the need for additional labels or training. Extensive experiments show that FLOSS consistently enhances state-of-the-art OVSS models, generalizes well across datasets with different distribution shifts, and delivers substantial improvements in low-data scenarios where only a few unlabeled images are available. Our code is available at https://github.com/yasserben/FLOSS .
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.21354</link>
<guid>https://arxiv.org/abs/2505.21354</guid>
<content:encoded><![CDATA[
arXiv:2505.21354v2 Announce Type: replace-cross 
Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSS: Multi-Objective Optimization for Stable Rule Sets</title>
<link>https://arxiv.org/abs/2506.08030</link>
<guid>https://arxiv.org/abs/2506.08030</guid>
<content:encoded><![CDATA[
arXiv:2506.08030v2 Announce Type: replace-cross 
Abstract: We present MOSS, a multi-objective optimization framework for constructing stable sets of decision rules. MOSS incorporates three important criteria for interpretability: sparsity, accuracy, and stability, into a single multi-objective optimization framework. Importantly, MOSS allows a practitioner to rapidly evaluate the trade-off between accuracy and stability in sparse rule sets in order to select an appropriate model. We develop a specialized cutting plane algorithm in our framework to rapidly compute the Pareto frontier between these two objectives, and our algorithm scales to problem instances beyond the capabilities of commercial optimization solvers. Our experiments show that MOSS outperforms state-of-the-art rule ensembles in terms of both predictive performance and stability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Information Distribution in Transformer Architectures through Entropy Analysis</title>
<link>https://arxiv.org/abs/2507.15347</link>
<guid>https://arxiv.org/abs/2507.15347</guid>
<content:encoded><![CDATA[
arXiv:2507.15347v2 Announce Type: replace-cross 
Abstract: This work explores entropy analysis as a tool for probing information distribution within Transformer-based architectures. By quantifying token-level uncertainty and examining entropy patterns across different stages of processing, we aim to investigate how information is managed and transformed within these models. As a case study, we apply the methodology to a GPT-based large language model, illustrating its potential to reveal insights into model behavior and internal representations. This approach may offer insights into model behavior and contribute to the development of interpretability and evaluation frameworks for transformer-based models
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review</title>
<link>https://arxiv.org/abs/2507.16876</link>
<guid>https://arxiv.org/abs/2507.16876</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal machine learning, histopathology, molecular data, cancer prognostication, overall survival <br />
Summary: 
Multimodal machine learning integrating histopathology and molecular data has shown promise for cancer prognostication. A systematic review of 48 studies across 19 cancer types found that approaches such as regularised Cox regression, classical machine learning, and deep learning have been utilized, with reported c-indices ranging from 0.550 to 0.857. Multimodal models generally outperformed unimodal ones, but all studies displayed unclear or high bias, limited external validation, and a lack of focus on clinical utility. The field of multimodal WSI-omics survival prediction is rapidly advancing but requires improved methodological rigor, broader datasets, and thorough clinical evaluation. This research was funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK, under Project 104687, with support from the UKRI Industrial Strategy Challenge Fund. <br /><br /> <div>
arXiv:2507.16876v2 Announce Type: replace-cross 
Abstract: Multimodal machine learning integrating histopathology and molecular data shows promise for cancer prognostication. We systematically reviewed studies combining whole slide images (WSIs) and high-throughput omics to predict overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL (12/08/2024), plus citation screening, identified eligible studies. Data extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).
  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all used The Cancer Genome Atlas. Approaches included regularised Cox regression (n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged 0.550-0.857; multimodal models typically outperformed unimodal ones. However, all studies showed unclear/high bias, limited external validation, and little focus on clinical utility.
  Multimodal WSI-omics survival prediction is a fast-growing field with promising results but needs improved methodological rigor, broader datasets, and clinical evaluation.
  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687), supported by UKRI Industrial Strategy Challenge Fund.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing High Class Imbalance in Multi-Class Diabetic Retinopathy Severity Grading with Augmentation and Transfer Learning</title>
<link>https://arxiv.org/abs/2507.17121</link>
<guid>https://arxiv.org/abs/2507.17121</guid>
<content:encoded><![CDATA[
<div> Framework, Deep Learning, Diabetic Retinopathy, Transfer Learning, Data Augmentation
Summary:
The paper presents a robust deep learning framework for binary and five-class diabetic retinopathy (DR) classification. By leveraging transfer learning and extensive data augmentation, the model addresses class imbalance and limited training data challenges. For binary classification, the model achieves state-of-the-art accuracy of 98.9% with high precision, recall, F1-score, and AUC. In the more challenging five-class severity classification task, the model obtains competitive accuracy and AUC, outperforming existing approaches. EfficientNet-B0 and ResNet34 are found to offer optimal trade-offs between accuracy and computational efficiency. The results highlight the effectiveness of combining class-balanced augmentation with transfer learning for high-performance DR diagnosis. The proposed framework provides a scalable and accurate solution for DR screening, with the potential for deployment in real-world clinical settings.<br /><br />Summary: <div>
arXiv:2507.17121v2 Announce Type: replace-cross 
Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and early diagnosis through automated retinal image analysis can significantly reduce the risk of blindness. This paper presents a robust deep learning framework for both binary and five-class DR classification, leveraging transfer learning and extensive data augmentation to address the challenges of class imbalance and limited training data. We evaluate a range of pretrained convolutional neural network architectures, including variants of ResNet and EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of 98.9%, and an AUC of 99.4%. In the more challenging five-class severity classification task, our model obtains a competitive accuracy of 84.6% and an AUC of 94.1%, outperforming several existing approaches. Our findings also demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced augmentation with transfer learning for high-performance DR diagnosis. The proposed framework provides a scalable and accurate solution for DR screening, with potential for deployment in real-world clinical environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback</title>
<link>https://arxiv.org/abs/2507.17294</link>
<guid>https://arxiv.org/abs/2507.17294</guid>
<content:encoded><![CDATA[
<div> Keywords: Tactile feedback, Vision-Language-Action models, Multi-modal datasets, Robot policies, Contact-rich manipulation<br />
Summary:<br />
The research introduces VLA-Touch, a method that integrates tactile sensing into generalist robot policies without the need for fine-tuning the base VLA model. The approach utilizes a pretrained tactile-language model to provide semantic tactile feedback for task planning and a diffusion-based controller to enhance action execution with tactile signals during contact-rich manipulation. This dual-level integration of tactile feedback aims to improve task planning efficiency and execution precision in real-world scenarios. By leveraging these innovations, VLA-Touch demonstrates successful enhancements in both high-level task planning and low-level manipulation tasks. The open-sourced code for VLA-Touch is available at the provided URL. <div>
arXiv:2507.17294v2 Announce Type: replace-cross 
Abstract: Tactile feedback is generally recognized to be crucial for effective interaction with the physical world. However, state-of-the-art Vision-Language-Action (VLA) models lack the ability to interpret and use tactile signals, limiting their effectiveness in contact-rich tasks. Incorporating tactile feedback into these systems is challenging due to the absence of large multi-modal datasets. We present VLA-Touch, an approach that enhances generalist robot policies with tactile sensing \emph{without fine-tuning} the base VLA. Our method introduces two key innovations: (1) a pipeline that leverages a pretrained tactile-language model that provides semantic tactile feedback for high-level task planning, and (2) a diffusion-based controller that refines VLA-generated actions with tactile signals for contact-rich manipulation. Through real-world experiments, we demonstrate that our dual-level integration of tactile feedback improves task planning efficiency while enhancing execution precision. Code is open-sourced at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students</title>
<link>https://arxiv.org/abs/2507.21109</link>
<guid>https://arxiv.org/abs/2507.21109</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Neural Networks, Catastrophic Forgetting, Continual Learning, Active Recall Probe, Spaced Repetition<br />
Summary:<br />
This paper introduces a new continual learning approach called Task Focused Consolidation with Spaced Recall (TFC-SR) inspired by human learning strategies. TFC-SR includes an Active Recall Probe that periodically evaluates the model's memory to stabilize past knowledge. Testing on Split MNIST and Split CIFAR-100 benchmarks shows that TFC-SR outperforms leading regularization and replay-based methods. Results demonstrate significantly improved performance, with TFC-SR achieving a final accuracy of 13.17% on Split CIFAR-100 compared to standard replay's 7.40%. The advantage of TFC-SR lies in the stabilization effect of the probe, rather than the difference in replay volume. Analysis reveals a trade-off between memory size and performance, showing TFC-SR's effectiveness in memory-constrained environments. However, higher replay volume remains more effective in memory-abundant settings. The study emphasizes the importance of integrating active memory retrieval mechanisms into continual learning systems.<br /><br />Summary: <div>
arXiv:2507.21109v1 Announce Type: new 
Abstract: Deep Neural Networks often suffer from a critical limitation known as Catastrophic Forgetting, where performance on past tasks degrades after learning new ones. This paper introduces a novel continual learning approach inspired by human learning strategies like Active Recall, Deliberate Practice and Spaced Repetition, named Task Focused Consolidation with Spaced Recall (TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we termed the Active Recall Probe. It is a periodic, task-aware evaluation of the model's memory that stabilizes the representations of past knowledge. We test TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading regularization-based and replay-based baselines. Our results show that TFC-SR performs significantly better than these methods. For instance, on the Split CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's 7.40%. We demonstrate that this advantage comes from the stabilizing effect of the probe itself, and not from the difference in replay volume. Additionally, we analyze the trade-off between memory size and performance and show that while TFC-SR performs better in memory-constrained environments, higher replay volume is still more effective when available memory is abundant. We conclude that TFC-SR is a robust and efficient approach, highlighting the importance of integrating active memory retrieval mechanisms into continual learning systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks</title>
<link>https://arxiv.org/abs/2507.21119</link>
<guid>https://arxiv.org/abs/2507.21119</guid>
<content:encoded><![CDATA[
<div> Keywords: optical network failure detection, class imbalance mitigation, threshold adjustment, Random Under-sampling (RUS), F1 gain 

Summary: 
Threshold Adjustment was found to be the most effective technique for mitigating class imbalance in optical network failure detection, resulting in a significant F1 gain of 15.3%. On the other hand, Random Under-sampling (RUS) was identified as providing the fastest inference speed, highlighting a trade-off between performance and complexity. The comparison of pre-, in-, and post-processing techniques sheds light on the various approaches available for addressing class imbalance in this context. Additionally, the study emphasizes the importance of selecting the most suitable technique based on the specific requirements and constraints of the application. Overall, this research contributes valuable insights into optimizing the detection of failures in optical networks by addressing the challenges posed by class imbalance. 

<br /><br />Summary: <div>
arXiv:2507.21119v1 Announce Type: new 
Abstract: We compare pre-, in-, and post-processing techniques for class imbalance mitigation in optical network failure detection. Threshold Adjustment achieves the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the fastest inference, highlighting a key performance-complexity trade-off.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Geometry of Data</title>
<link>https://arxiv.org/abs/2507.21135</link>
<guid>https://arxiv.org/abs/2507.21135</guid>
<content:encoded><![CDATA[
<div> Hermitian matrices, Hilbert space, quantum geometry, intrinsic dimension, quantum metric<br />
<br />
Summary: Quantum Cognition Machine Learning (QCML) utilizes Hermitian matrices to represent data features and maps data points to states in Hilbert space, providing a quantum geometric description of the dataset. This quantum geometry endows the data with rich geometric and topological structures such as intrinsic dimension, quantum metric, and Berry curvature, derived directly from the data. QCML captures global properties of the data while avoiding the curse of dimensionality inherent in local methods. The approach is demonstrated on synthetic and real-world examples, illustrating its potential to advance understanding of cognitive phenomena within the framework of quantum cognition. <div>
arXiv:2507.21135v1 Announce Type: new 
Abstract: We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as quantum geometry. In QCML, features of the data are represented by learned Hermitian matrices, and data points are mapped to states in Hilbert space. The quantum geometry description endows the dataset with rich geometric and topological structure - including intrinsic dimension, quantum metric, and Berry curvature - derived directly from the data. QCML captures global properties of data, while avoiding the curse of dimensionality inherent in local methods. We illustrate this on a number of synthetic and real-world examples. Quantum geometric representation of QCML could advance our understanding of cognitive phenomena within the framework of quantum cognition.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning</title>
<link>https://arxiv.org/abs/2507.21136</link>
<guid>https://arxiv.org/abs/2507.21136</guid>
<content:encoded><![CDATA[
<div> kernels, unsupervised learners, dimensionality reduction, interpretability, machine learning  
Summary:  
- The study focuses on designing unsupervised and supervised learning methods using independence criteria to capture nonlinearities in data structure.  
- Three independence criteria were proposed and applied to develop dimensionality reduction methods, which were evaluated for contrast, accuracy, and interpretability in linear and neural nonlinear settings.  
- The results showed superior performance compared to baseline methods like tSNE, PCA, and VAE, both with supervised and unsupervised learners and layer sharing.  
- The new methods offer a more interpretable approach to machine learning, providing researchers with enhanced capabilities for variability capture and data diversity.  
- This research opens up possibilities for improved machine learning models with increased interpretability and performance.  
<br /><br />Summary: <div>
arXiv:2507.21136v1 Announce Type: new 
Abstract: Unsupervised and supervised learning methods conventionally use kernels to capture nonlinearities inherent in data structure. However experts have to ensure their proposed nonlinearity maximizes variability and capture inherent diversity of data. We reviewed all independence criteria to design unsupervised learners. Then we proposed 3 independence criteria and used them to design unsupervised and supervised dimensionality reduction methods. We evaluated contrast, accuracy and interpretability of these methods in both linear and neural nonlinear settings. The results show that the methods have outperformed the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and layer sharing) and opened a new line of interpretable machine learning (ML) for the researchers.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning</title>
<link>https://arxiv.org/abs/2507.21147</link>
<guid>https://arxiv.org/abs/2507.21147</guid>
<content:encoded><![CDATA[
<div> Keywords: wildfires, risk management, deep learning, contrastive learning, weather data<br />
Summary:<br />
Wildfires have significant impacts on ecosystems and human health, especially in regions like the Mediterranean experiencing climate change. Developing advanced risk management strategies using technology is crucial. However, the imbalanced data distribution poses challenges for training deep learning models for wildfire prediction. Utilizing a contrastive framework can enhance latent representations of dynamic features of patches, improving performance. The proposed morphology-based curriculum contrastive learning approach addresses regional diversity issues and allows for smaller patch sizes without sacrificing accuracy. To reduce computational costs and enable more frequent updates with current weather forecasts, adopting such a framework is beneficial. Experimental analysis confirms the effectiveness of these modeling strategies. <div>
arXiv:2507.21147v1 Announce Type: new 
Abstract: Wildfires significantly impact natural ecosystems and human health, leading to biodiversity loss, increased hydrogeological risks, and elevated emissions of toxic substances. Climate change exacerbates these effects, particularly in regions with rising temperatures and prolonged dry periods, such as the Mediterranean. This requires the development of advanced risk management strategies that utilize state-of-the-art technologies. However, in this context, the data show a bias toward an imbalanced setting, where the incidence of wildfire events is significantly lower than typical situations. This imbalance, coupled with the inherent complexity of high-dimensional spatio-temporal data, poses significant challenges for training deep learning architectures. Moreover, since precise wildfire predictions depend mainly on weather data, finding a way to reduce computational costs to enable more frequent updates using the latest weather forecasts would be beneficial. This paper investigates how adopting a contrastive framework can address these challenges through enhanced latent representations for the patch's dynamic features. We thus introduce a new morphology-based curriculum contrastive learning that mitigates issues associated with diverse regional characteristics and enables the use of smaller patch sizes without compromising performance. An experimental analysis is performed to validate the effectiveness of the proposed modeling strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unfolding for MIMO Signal Detection</title>
<link>https://arxiv.org/abs/2507.21152</link>
<guid>https://arxiv.org/abs/2507.21152</guid>
<content:encoded><![CDATA[
<div> Keywords: deep unfolding neural network, MIMO detector, complex-valued computations, Wirtinger calculus, DPST

Summary: 
The paper introduces a novel deep unfolding neural network-based MIMO detector called Dynamic Partially Shrinkage Thresholding (DPST), utilizing complex-valued computations with Wirtinger calculus. Unlike previous methods that use real-valued approximations, DPST operates directly in the complex domain, enhancing signal processing accuracy. With a minimal number of trainable parameters, DPST simplifies training while offering superior detection performance in terms of speed and computational efficiency. The results from numerical experiments demonstrate the effectiveness of DPST, making it a viable solution for advanced massive MIMO systems.<br /><br />Summary: <div>
arXiv:2507.21152v1 Announce Type: new 
Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables efficient, interpretable, and low-complexity MIMO signal detection. Unlike prior approaches that rely on real-valued approximations, our method operates natively in the complex domain, aligning with the fundamental nature of signal processing tasks. The proposed algorithm requires only a small number of trainable parameters, allowing for simplified training. Numerical results demonstrate that the proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers</title>
<link>https://arxiv.org/abs/2507.21153</link>
<guid>https://arxiv.org/abs/2507.21153</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Reinforcement Learning, energy management, data centers, renewable energy, sustainability

Summary: 
Deep Reinforcement Learning (DRL) is implemented in an energy management system for e-commerce data centers to improve efficiency, cost-effectiveness, and environmental sustainability. The system dynamically integrates renewable energy sources, energy storage, and grid power to adapt to fluctuating energy availability. Results show a 38% reduction in energy costs compared to traditional methods. The DRL-optimized system also achieves a low SLA violation rate of 1.5%, an 82% improvement in energy efficiency, and a 45% reduction in carbon emissions. A cumulative reward of 950 demonstrates superior performance in balancing objectives. Rigorous testing validates the effectiveness of the DRL model's architecture and parameters, offering a robust solution for data center energy management. The study highlights the potential of DRL in advancing energy optimization strategies and addressing sustainability challenges.<br /><br />Summary: <div>
arXiv:2507.21153v1 Announce Type: new 
Abstract: This paper explores the implementation of a Deep Reinforcement Learning (DRL)-optimized energy management system for e-commerce data centers, aimed at enhancing energy efficiency, cost-effectiveness, and environmental sustainability. The proposed system leverages DRL algorithms to dynamically manage the integration of renewable energy sources, energy storage, and grid power, adapting to fluctuating energy availability in real time. The study demonstrates that the DRL-optimized system achieves a 38\% reduction in energy costs, significantly outperforming traditional Reinforcement Learning (RL) methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for heuristic methods. The DRL-optimized approach also results in an 82\% improvement in energy efficiency, surpassing other methods, and a 45\% reduction in carbon emissions, making it the most environmentally friendly solution. The system's cumulative reward of 950 reflects its superior performance in balancing multiple objectives. Through rigorous testing and ablation studies, the paper validates the effectiveness of the DRL model's architecture and parameters, offering a robust solution for energy management in data centers. The findings highlight the potential of DRL in advancing energy optimization strategies and addressing sustainability challenges.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPADE-S: A Sparsity-Robust Foundational Forecaster</title>
<link>https://arxiv.org/abs/2507.21155</link>
<guid>https://arxiv.org/abs/2507.21155</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, deep learning, magnitude, sparsity, demand forecasting

Summary:
SPADE-S is introduced as a robust forecasting architecture designed to address the challenges posed by low-magnitude and sparse time series data. Existing models often struggle with such data due to biases in loss functions, sampling methods, and encoding techniques. SPADE-S mitigates these biases, leading to enhanced prediction accuracy. Empirical results demonstrate SPADE-S outperforms current state-of-the-art approaches in demand forecasting across various use cases. Depending on the quantile forecast and magnitude of the series, SPADE-S can achieve up to 15% improvement in forecast accuracy. This improvement translates to P90 overall forecast accuracy gains of 2.21%, 6.58%, and 4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95% for three different datasets from a large online retailer, containing between 3 million and 700 million series.

<br /><br />Summary: <div>
arXiv:2507.21155v1 Announce Type: new 
Abstract: Despite significant advancements in time series forecasting, accurate modeling of time series with strong heterogeneity in magnitude and/or sparsity patterns remains challenging for state-of-the-art deep learning architectures. We identify several factors that lead existing models to systematically underperform on low-magnitude and sparse time series, including loss functions with implicit biases toward high-magnitude series, training-time sampling methods, and limitations of time series encoding methods.
  SPADE-S is a robust forecasting architecture that significantly reduces magnitude- and sparsity-based systematic biases and improves overall prediction accuracy. Empirical results demonstrate that SPADE-S outperforms existing state-of-the-art approaches across a diverse set of use cases in demand forecasting. In particular, we show that, depending on the quantile forecast and magnitude of the series, SPADE-S can improve forecast accuracy by up to 15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and 4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%, respectively, for each of three distinct datasets, ranging from 3 million to 700 million series, from a large online retailer.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Out-of-Distribution Data: A Survey</title>
<link>https://arxiv.org/abs/2507.21160</link>
<guid>https://arxiv.org/abs/2507.21160</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Distribution shift, Covariate shift, Concept shift, OOD data<br />
Summary: 
This paper addresses the challenge of distribution shifts in Machine Learning, focusing on covariate and concept/semantic shifts. The authors highlight the importance of handling distribution shifts and discuss existing methods to detect and mitigate them. They propose the need for a model that can effectively handle all types of distribution shifts simultaneously. The paper provides a comprehensive review of literature in this area and emphasizes the relevance of out-of-distribution (OOD) data, which has been overlooked in previous surveys. The authors call for further research in distribution shift handling mechanisms to bridge the gap between training and deployment stages. Overall, the paper offers insights into the challenges posed by distribution shifts and encourages advancements in addressing this critical issue in data-driven applications. <br /><br />Summary: <div>
arXiv:2507.21160v1 Announce Type: new 
Abstract: In the field of Machine Learning (ML) and data-driven applications, one of the significant challenge is the change in data distribution between the training and deployment stages, commonly known as distribution shift. This paper outlines different mechanisms for handling two main types of distribution shifts: (i) Covariate shift: where the value of features or covariates change between train and test data, and (ii) Concept/Semantic-shift: where model experiences shift in the concept learned during training due to emergence of novel classes in the test phase. We sum up our contributions in three folds. First, we formalize distribution shifts, recite on how the conventional method fails to handle them adequately and urge for a model that can simultaneously perform better in all types of distribution shifts. Second, we discuss why handling distribution shifts is important and provide an extensive review of the methods and techniques that have been developed to detect, measure, and mitigate the effects of these shifts. Third, we discuss the current state of distribution shift handling mechanisms and propose future research directions in this area. Overall, we provide a retrospective synopsis of the literature in the distribution shift, focusing on OOD data that had been overlooked in the existing surveys.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.21164</link>
<guid>https://arxiv.org/abs/2507.21164</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised anomaly detection, representation learning, one-class SVM, medical imaging, robustness<br />
<br />
Summary: <br />
This article presents a novel method for unsupervised anomaly detection (UAD) that tightly couples representation learning with an analytically solvable one-class SVM (OCSVM). Unlike existing approaches, this method directly aligns latent features with the OCSVM decision boundary, enhancing performance in detecting anomalies without labeled data. The model is evaluated on a new benchmark using MNIST-C and a challenging brain MRI subtle lesion detection task. It successfully targets small, non-hyperintense lesions in MRI, demonstrating robustness to domain shifts including corruption types in MNIST-C and scanner/age variations in MRI. The results highlight the potential of the proposed method for general UAD and real-world medical imaging applications. The source code for this method is publicly available, promoting reproducibility and further research in this area. <br /> <div>
arXiv:2507.21164v1 Announce Type: new 
Abstract: Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled data, a necessity in many machine learning applications where anomalous samples are rare or not available. Most state-of-the-art methods fall into two categories: reconstruction-based approaches, which often reconstruct anomalies too well, and decoupled representation learning with density estimators, which can suffer from suboptimal feature spaces. While some recent methods attempt to couple feature learning and anomaly detection, they often rely on surrogate objectives, restrict kernel choices, or introduce approximations that limit their expressiveness and robustness. To address this challenge, we propose a novel method that tightly couples representation learning with an analytically solvable one-class SVM (OCSVM), through a custom loss formulation that directly aligns latent features with the OCSVM decision boundary. The model is evaluated on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI subtle lesion detection task. Unlike most methods that focus on large, hyperintense lesions at the image level, our approach succeeds to target small, non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a more clinically relevant scenario. Both experiments evaluate a form of robustness to domain shifts, including corruption types in MNIST-C and scanner/age variations in MRI. Results demonstrate performance and robustness of our proposed mode,highlighting its potential for general UAD and real-world medical imaging applications. The source code is available at https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation</title>
<link>https://arxiv.org/abs/2507.21166</link>
<guid>https://arxiv.org/abs/2507.21166</guid>
<content:encoded><![CDATA[
<div> Keywords: structured interaction, self-evolving framework, collaborative ensemble, reasoning performance, intelligence scalability

Summary: 
AGORA, a self-evolving framework, introduces structured interaction as a scalable axis for enhancing reasoning capabilities. By leveraging collaborative ensembles, AGORA surpasses monolithic systems in performance on mathematical benchmarks by up to 4.45 percentage points. This improvement is attributed to the emergent ability of the group, showcasing the synthesis of collective capabilities that individual models cannot achieve on their own. The results highlight the importance of fostering collaborative ecosystems to drive intelligence enhancements and push the boundaries of complex reasoning. The study emphasizes that interaction among models can lead to significant advancements in intelligence scalability, suggesting that the future of artificial intelligence lies in the engineering of collaborative systems. 

<br /><br />Summary: <div>
arXiv:2507.21166v1 Announce Type: new 
Abstract: Progress in complex reasoning is constrained by the static nature of the current training datasets. We propose structured interaction as a new scaling axis, moving beyond the prevailing paradigm of increasing model parameters. Our self-evolving framework, AGORA, enables a collaborative ensemble to achieve reasoning performance exceeding state-of-the-art monolithic systems by up to 4.45 percentage points on challenging mathematical benchmarks. This gain stems from group emergent ability-the synthesis of collective capabilities unattainable by isolated models, validating interaction as a scalable driver of intelligence. Our results position the engineering of collaborative ecosystems as a vital frontier for capability emergence.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Adapted Interpretation Framework for Machine Learning Models</title>
<link>https://arxiv.org/abs/2507.21179</link>
<guid>https://arxiv.org/abs/2507.21179</guid>
<content:encoded><![CDATA[
<div> XGBoost, Machine Learning, Sarcopenia, Interpretability, LAI-ML  
Summary: The study introduces the LAI-ML framework to enhance interpretability in sarcopenia risk assessment. By distilling feature attributions from XGBoost models into a probabilistic format and utilizing a Large Language Model guided by reinforcement learning, LAI-ML achieved 83% prediction accuracy. The framework outperformed the baseline XGBoost model by 13% and demonstrated enhanced reasoning by correcting predictions in 21.7% of discordant cases. LAI-ML effectively bridges the gap between predictive accuracy and narrative transparency, offering a deployable solution to the "black-box" issue in medical AI. <br /><br /> <div>
arXiv:2507.21179v1 Announce Type: new 
Abstract: Background & Aims: High-performance machine learning models like XGBoost are often "black boxes," limiting their clinical adoption due to a lack of interpretability. This study aims to bridge the gap between predictive accuracy and narrative transparency for sarcopenia risk assessment. Methods: We propose the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge distillation architecture. LAI-ML transforms feature attributions from a trained XGBoost model into a probabilistic format using specialized techniques (HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement learning loop and case-based retrieval, then generates data-faithful diagnostic narratives. Results: The LAI-ML framework achieved 83% prediction accuracy, significantly outperforming the baseline XGBoost model, 13% higher. Notably, the LLM not only replicated the teacher model's logic but also corrected its predictions in 21.7% of discordant cases, demonstrating enhanced reasoning. Conclusion: LAI-ML effectively translates opaque model predictions into trustworthy and interpretable clinical insights, offering a deployable solution to the "black-box" problem in medical AI.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
<div> Preference Optimization, Large Language Models, Maximum a Posteriori, Prior Reward Knowledge, Alignment Performance <br />
Summary:<br />
The article introduces Maximum a Posteriori Preference Optimization (MaPPO), a framework that incorporates prior reward knowledge into preference learning for large language models (LLMs). Unlike existing methods like Direct Preference Optimization (DPO) that treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO utilizes a Maximum a Posteriori (MaP) objective to integrate prior reward estimates. This approach enhances alignment with human preferences by avoiding oversimplified binary classification of responses. MaPPO requires no additional hyperparameters and supports preference optimization in both offline and online settings. It can also be used as a plugin with consistent improvements over DPO variants such as SimPO, IPO, and CPO. Empirical evaluations on standard benchmarks show that MaPPO consistently improves alignment performance across different model sizes without sacrificing computational efficiency. <br /> <div>
arXiv:2507.21183v1 Announce Type: new 
Abstract: As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
<div> Keywords: scaling laws, neural networks, evolutionary algorithms, large language models, automated framework

Summary:
The article introduces EvoSLD, an automated framework for Scaling Law Discovery (SLD) that utilizes evolutionary algorithms guided by Large Language Models (LLMs) to discover scaling laws in neural networks. EvoSLD is designed to handle scaling variables, control variables, and response metrics across diverse experimental settings. Evaluated on five real-world scenarios from recent literature, EvoSLD was able to rediscover exact human-derived laws in two cases and outperform them in others, achieving significant reductions in normalized mean squared error on held-out test sets. Compared to baseline methods like symbolic regression, EvoSLD demonstrated superior accuracy, interpretability, and efficiency, showcasing its potential to accelerate AI research and discovery of fundamental mathematical relationships in neural networks.<br /><br />Summary: <div>
arXiv:2507.21184v1 Announce Type: new 
Abstract: Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs</title>
<link>https://arxiv.org/abs/2507.21188</link>
<guid>https://arxiv.org/abs/2507.21188</guid>
<content:encoded><![CDATA[
<div> clinical decision support, LLMs, latent robustness, LAPD, Latent Diagnosis Flip Rate

Summary: 
The study discusses the limitations of current language models (LLMs) in clinical decision support systems, which often fail to maintain performance when faced with small but clinically significant alterations in input. These failures are not easily detected by standard NLP metrics. To address this issue, the authors propose a new evaluation framework called LAPD (Latent Agentic Perturbation Diagnostics). They introduce a diagnostic signal called Latent Diagnosis Flip Rate (LDFR) to measure the model's instability when embeddings cross decision boundaries in latent space. The study uses structured adversarial edits, such as masking symptoms or negating findings, to simulate common ambiguities and omissions in clinical notes. The findings show that even minimal changes can lead to latent fragility in clinical LLMs. The study validates these results on real clinical notes from the DiReCT benchmark. This highlights the importance of geometry-aware auditing in ensuring the safety and reliability of clinical AI systems.
<br /><br />Summary: <div>
arXiv:2507.21188v1 Announce Type: new 
Abstract: LLMs for clinical decision support often fail under small but clinically meaningful input shifts such as masking a symptom or negating a finding, despite high performance on static benchmarks. These reasoning failures frequently go undetected by standard NLP metrics, which are insensitive to latent representation shifts that drive diagnosis instability. We propose a geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation Diagnostics), which systematically probes the latent robustness of clinical LLMs under structured adversarial edits. Within this framework, we introduce Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that captures representational instability when embeddings cross decision boundaries in PCA-reduced latent space. Clinical notes are generated using a structured prompting pipeline grounded in diagnostic reasoning, then perturbed along four axes: masking, negation, synonym replacement, and numeric variation to simulate common ambiguities and omissions. We compute LDFR across both foundation and clinical LLMs, finding that latent fragility emerges even under minimal surface-level changes. Finally, we validate our findings on 90 real clinical notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of LDFR beyond synthetic settings. Our results reveal a persistent gap between surface robustness and semantic stability, underscoring the importance of geometry-aware auditing in safety-critical clinical AI.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning</title>
<link>https://arxiv.org/abs/2507.21189</link>
<guid>https://arxiv.org/abs/2507.21189</guid>
<content:encoded><![CDATA[
<div> Hilbert spaces, neural networks, functional analysis, spectral theory, machine learning <br />
Summary: This report explores a novel approach to machine learning by framing learning tasks in infinite-dimensional Hilbert spaces. By incorporating tools from functional analysis, signal processing, and spectral theory, the study delves into concepts such as RKHS, spectral operator learning, and wavelet-domain representations. It presents a rigorous mathematical framework for learning in Hilbert spaces, showcasing recent models based on scattering transforms and Koopman operators. The report also discusses the advantages and limitations of this approach compared to traditional neural architectures. Lastly, it outlines future directions for scalable and interpretable machine learning rooted in Hilbertian signal processing. <br /> <div>
arXiv:2507.21189v1 Announce Type: new 
Abstract: Traditional machine learning models, particularly neural networks, are rooted in finite-dimensional parameter spaces and nonlinear function approximations. This report explores an alternative formulation where learning tasks are expressed as sampling and computation in infinite dimensional Hilbert spaces, leveraging tools from functional analysis, signal processing, and spectral theory. We review foundational concepts such as Reproducing Kernel Hilbert Spaces (RKHS), spectral operator learning, and wavelet-domain representations. We present a rigorous mathematical formulation of learning in Hilbert spaces, highlight recent models based on scattering transforms and Koopman operators, and discuss advantages and limitations relative to conventional neural architectures. The report concludes by outlining directions for scalable and interpretable machine learning grounded in Hilbertian signal processing.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals</title>
<link>https://arxiv.org/abs/2507.21190</link>
<guid>https://arxiv.org/abs/2507.21190</guid>
<content:encoded><![CDATA[
<div> Graph Laplacian Wavelet Transforms, Non-neural learning framework, Multiscale filtering, Symbolic logic, Graph spectral domain <br />
Summary: <br />
A new non-neural learning framework based on Graph Laplacian Wavelet Transforms (GLWT) is introduced. Unlike traditional neural networks, this model operates in the graph spectral domain, utilizing structured multiscale filtering, nonlinear shrinkage, and symbolic logic over wavelet coefficients. Signals on graph nodes undergo decomposition via GLWT, modulation with interpretable nonlinearities, and recombination for tasks such as denoising and token classification. The system enables compositional reasoning through a symbolic domain-specific language (DSL). Experimental results on synthetic graph denoising and linguistic token graphs show competitive performance compared to lightweight Graph Neural Networks (GNNs) with greater transparency and efficiency. This work presents a principled, interpretable, and resource-efficient alternative to deep neural architectures for learning on graphs. <br /> <div>
arXiv:2507.21190v1 Announce Type: new 
Abstract: We present a fully non neural learning framework based on Graph Laplacian Wavelet Transforms (GLWT). Unlike traditional architectures that rely on convolutional, recurrent, or attention based neural networks, our model operates purely in the graph spectral domain using structured multiscale filtering, nonlinear shrinkage, and symbolic logic over wavelet coefficients. Signals defined on graph nodes are decomposed via GLWT, modulated with interpretable nonlinearities, and recombined for downstream tasks such as denoising and token classification. The system supports compositional reasoning through a symbolic domain-specific language (DSL) over graph wavelet activations. Experiments on synthetic graph denoising and linguistic token graphs demonstrate competitive performance against lightweight GNNs with far greater transparency and efficiency. This work proposes a principled, interpretable, and resource-efficient alternative to deep neural architectures for learning on graphs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Adaptive Structure Learning for Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2507.21191</link>
<guid>https://arxiv.org/abs/2507.21191</guid>
<content:encoded><![CDATA[
<div> Graph Convolutional Networks, GCNs, heterophilic graphs, structure learning, long-range dependencies
Summary:
Structure learning in Graph Convolutional Networks (GCNs) is proposed to address the challenge of capturing long-range dependencies between non-local nodes in heterophilic graphs. The traditional message-passing paradigm hinders information sharing between distant nodes of the same class in such graphs. By parameterizing the adjacency matrix and rewiring edges, the proposed method extends the hop span of shallow GCNs for improved performance in downstream tasks. However, the effectiveness of the method is dependent on the specific graph structure and may not be consistent in node classification tasks across heterophilic graphs. The approach aims to mitigate oversmoothing issues and enhance performance by enabling the GCNs to capture non-local relationships efficiently. <div>
arXiv:2507.21191v1 Announce Type: new 
Abstract: Graph Convolutional Networks (GCNs) gained traction for graph representation learning, with recent attention on improving performance on heterophilic graphs for various real-world applications. The localized feature aggregation in a typical message-passing paradigm hinders the capturing of long-range dependencies between non-local nodes of the same class. The inherent connectivity structure in heterophilic graphs often conflicts with information sharing between distant nodes of same class. We propose structure learning to rewire edges in shallow GCNs itself to avoid performance degradation in downstream discriminative tasks due to oversmoothing. Parameterizing the adjacency matrix to learn connections between non-local nodes and extend the hop span of shallow GCNs facilitates the capturing of long-range dependencies. However, our method is not generalizable across heterophilic graphs and performs inconsistently on node classification task contingent to the graph structure.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks</title>
<link>https://arxiv.org/abs/2507.21196</link>
<guid>https://arxiv.org/abs/2507.21196</guid>
<content:encoded><![CDATA[
<div> EdgeAgentX-DT, military networks, digital twin simulations, generative AI-driven scenario training, edge intelligence<br />
Summary:<br />
EdgeAgentX-DT is introduced as an extension of the EdgeAgentX framework, enhancing edge intelligence in military networks through digital twin simulations and generative AI-driven scenario training. The system utilizes network digital twins synchronized with real-world edge devices to create a secure training environment. Generative AI methods are employed to generate diverse scenarios for agent training, resulting in faster learning convergence, higher network throughput, reduced latency, and improved resilience against attacks and failures. Experimental simulations demonstrate the superiority of EdgeAgentX-DT over baseline methods in sustaining operational performance in complex tactical scenarios. The integration of digital-twin-enabled generative training shows promise in strengthening edge AI deployments in contested environments.<br /> <div>
arXiv:2507.21196v1 Announce Type: new 
Abstract: We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework that integrates digital twin simulations and generative AI-driven scenario training to significantly enhance edge intelligence in military networks. EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized with real-world edge devices, to provide a secure, realistic environment for training and validation. Leveraging generative AI methods, such as diffusion models and transformers, the system creates diverse and adversarial scenarios for robust simulation-based agent training. Our multi-layer architecture includes: (1) on-device edge intelligence; (2) digital twin synchronization; and (3) generative scenario training. Experimental simulations demonstrate notable improvements over EdgeAgentX, including faster learning convergence, higher network throughput, reduced latency, and improved resilience against jamming and node failures. A case study involving a complex tactical scenario with simultaneous jamming attacks, agent failures, and increased network loads illustrates how EdgeAgentX-DT sustains operational performance, whereas baseline methods fail. These results highlight the potential of digital-twin-enabled generative training to strengthen edge AI deployments in contested environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction</title>
<link>https://arxiv.org/abs/2507.21197</link>
<guid>https://arxiv.org/abs/2507.21197</guid>
<content:encoded><![CDATA[
<div> Machine learning interpretation, EHR data, AdaptHetero, subgroup-specific modeling, SHAP-based interpretation<br />
<br />
Summary: AdaptHetero is a novel framework for machine learning interpretation in electronic health records (EHRs), focusing on guiding subgroup-specific modeling within hospital systems. By leveraging SHAP-based interpretation and unsupervised clustering, the framework identifies heterogeneous model behaviors in predicting ICU mortality, in-hospital death, and hidden hypoxemia across three large-scale EHR datasets. This approach transforms interpretability insights into actionable guidance for tailoring model training and evaluation for different subpopulations, leading to improved predictive performance. AdaptHetero enhances the identification of clinically meaningful subgroup-specific characteristics, helping to uncover actionable insights and build clinician trust in the use of machine learning in healthcare settings. <div>
arXiv:2507.21197v1 Announce Type: new 
Abstract: Machine learning interpretation has primarily been leveraged to build clinician trust and uncover actionable insights in EHRs. However, the intrinsic complexity and heterogeneity of EHR data limit its effectiveness in guiding subgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven framework that transforms interpretability insights into actionable guidance for tailoring model training and evaluation across subpopulations within individual hospital systems. Evaluated on three large-scale EHR datasets - GOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies heterogeneous model behaviors in predicting ICU mortality, in-hospital death, and hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised clustering, the framework enhances the identification of clinically meaningful subgroup-specific characteristics, leading to improved predictive performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Gradient Inversion Risks in Practical Language Model Training</title>
<link>https://arxiv.org/abs/2507.21198</link>
<guid>https://arxiv.org/abs/2507.21198</guid>
<content:encoded><![CDATA[
<div> privacy threat, federated learning, gradient inversion attack, language models, Grab <br />
Summary:<br />
- The gradient inversion attack poses a significant privacy threat to federated learning, especially in continuous domains like vision models.
- Existing concerns about the attack's effectiveness on language models have been underestimated due to the challenges posed by discrete tokens in text data.
- A new attack named Grab is proposed, featuring hybrid optimization processes to overcome practical training settings' challenges.
- Grab can recover up to 92.9% of private training data, outperforming existing attack strategies significantly.
- This work sheds light on the potential privacy threats in federated learning training mode for language models, offering valuable insights for future research. <br /> <div>
arXiv:2507.21198v1 Announce Type: new 
Abstract: The gradient inversion attack has been demonstrated as a significant privacy threat to federated learning (FL), particularly in continuous domains such as vision models. In contrast, it is often considered less effective or highly dependent on impractical training settings when applied to language models, due to the challenges posed by the discrete nature of tokens in text data. As a result, its potential privacy threats remain largely underestimated, despite FL being an emerging training method for language models. In this work, we propose a domain-specific gradient inversion attack named Grab (gradient inversion with hybrid optimization). Grab features two alternating optimization processes to address the challenges caused by practical training settings, including a simultaneous optimization on dropout masks between layers for improved token recovery and a discrete optimization for effective token sequencing. Grab can recover a significant portion (up to 92.9% recovery rate) of the private training data, outperforming the attack strategy of utilizing discrete optimization with an auxiliary model by notable improvements of up to 28.9% recovery rate in benchmark settings and 48.5% recovery rate in practical settings. Grab provides a valuable step forward in understanding this privacy threat in the emerging FL training mode of language models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications</title>
<link>https://arxiv.org/abs/2507.21199</link>
<guid>https://arxiv.org/abs/2507.21199</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive multimodal applications, large language models, mixture-of-experts, ContextLoRA, ContextGear

Summary:
The paper introduces a novel paradigm for Interactive Multimodal Applications (IMAs) using a single compositional Large Language Model (LLM) over wireless networks. The proposed method, ContextLoRA, guides the LLM to learn structured context among IMAs by constructing a task dependency graph. This allows the LLM to adapt to diverse IMA objectives and capture latent dependencies between tasks. To optimize the training procedure, ContextGear, a scheduling strategy, minimizes computational and communication costs through a strategic grouping mechanism. Experiments on benchmarks demonstrate the superiority of ContextLoRA and ContextGear. The prototype on a real-world wireless testbed shows practical applicability for various IMAs. The code will be released to the community. 

<br /><br />Summary: <div>
arXiv:2507.21199v1 Announce Type: new 
Abstract: Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each LLM trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional LLM over wireless networks. The two primary challenges include 1) guiding a single LLM to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the LLM in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an LLM to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate LLM composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the LLM to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and communication costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Limited and Imperfect Data</title>
<link>https://arxiv.org/abs/2507.21205</link>
<guid>https://arxiv.org/abs/2507.21205</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, long-tail data, generative models, inductive regularization, domain adaptation

Summary:
This thesis addresses the challenge of training deep neural networks on imperfect and limited real-world data distributions. The research is divided into four segments. The first segment focuses on learning generative models from long-tail data, enabling diverse image generation for minority classes. The second segment introduces inductive regularization schemes to improve generalization on tail classes without requiring explicit image generation. The third segment proposes algorithms for optimizing relevant metrics in learning from long-tailed data with limited annotation, particularly in a semi-supervised setting. Finally, the fourth segment deals with efficient domain adaptation of models to diverse domains with minimal labeled samples, offering a solution for adapting to varied data distributions. The overall aim is to develop robust algorithms that can effectively learn from diverse, real-world data distributions without the need for labor-intensive curation processes. 

<br /><br />Summary: <div>
arXiv:2507.21205v1 Announce Type: new 
Abstract: The distribution of data in the world (eg, internet, etc.) significantly differs from the well-curated datasets and is often over-populated with samples from common categories. The algorithms designed for well-curated datasets perform suboptimally when used for learning from imperfect datasets with long-tailed imbalances and distribution shifts. To expand the use of deep models, it is essential to overcome the labor-intensive curation process by developing robust algorithms that can learn from diverse, real-world data distributions. Toward this goal, we develop practical algorithms for Deep Neural Networks which can learn from limited and imperfect data present in the real world. This thesis is divided into four segments, each covering a scenario of learning from limited or imperfect data. The first part of the thesis focuses on Learning Generative Models from Long-Tail Data, where we mitigate the mode-collapse and enable diverse aesthetic image generations for tail (minority) classes. In the second part, we enable effective generalization on tail classes through Inductive Regularization schemes, which allow tail classes to generalize as effectively as the head classes without requiring explicit generation of images. In the third part, we develop algorithms for Optimizing Relevant Metrics for learning from long-tailed data with limited annotation (semi-supervised), followed by the fourth part, which focuses on the Efficient Domain Adaptation of the model to various domains with very few to zero labeled samples.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubbleformer: Forecasting Boiling with Transformers</title>
<link>https://arxiv.org/abs/2507.21244</link>
<guid>https://arxiv.org/abs/2507.21244</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, forecasting, boiling dynamics, nucleation, spatiotemporal<br />
<br />
Summary: 
Bubbleformer is a novel transformer-based model that can predict boiling dynamics, including nucleation and interface evolution, without the need for future input during inference. By integrating factorized axial attention and frequency-aware scaling, Bubbleformer is able to accurately forecast long-range boiling dynamics across various fluids, geometries, and operating conditions. The model, evaluated using physics-based metrics in chaotic systems, demonstrates high physical fidelity in predicting heat-flux consistency, interface geometry, and mass conservation. Additionally, BubbleML 2.0 dataset, spanning diverse working fluids and boiling configurations, is released to support the evaluation of the model. Bubbleformer sets new benchmark results in the prediction and forecasting of two-phase boiling flows. <br /><br /> <div>
arXiv:2507.21244v1 Announce Type: new 
Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors</title>
<link>https://arxiv.org/abs/2507.21260</link>
<guid>https://arxiv.org/abs/2507.21260</guid>
<content:encoded><![CDATA[
<div> Keywords: inverse problem, deep generative models, protein structure generation, Adam-PnP, experimental data

Summary:
Adam-PnP is introduced as a Plug-and-Play framework for guiding a pre-trained protein diffusion model using gradients from multiple experimental sources. This framework includes an adaptive noise estimation scheme and dynamic modality weighting, reducing the need for manual hyperparameter tuning. It aims to integrate noisy data from various sources to improve accuracy in protein structure generation tasks. By incorporating these techniques, Adam-PnP demonstrates significantly enhanced accuracy in complex reconstruction tasks. This approach addresses the challenge of integrating noisy experimental data into deep generative models for protein structure generation. Adam-PnP provides a more automated and effective method for handling multiple heterogeneous sources of experimental data, showcasing improved results compared to existing methods. Overall, Adam-PnP offers a promising solution for enhancing protein structure generation using deep generative models.<br /><br />Summary: <div>
arXiv:2507.21260v1 Announce Type: new 
Abstract: In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Polynomial Chaos Expansion</title>
<link>https://arxiv.org/abs/2507.21273</link>
<guid>https://arxiv.org/abs/2507.21273</guid>
<content:encoded><![CDATA[
<div> basis polynomials, uncertainty quantification, surrogate modeling, deep learning, high-dimensional problems 

Summary:
DeepPCE combines polynomial chaos expansion with probabilistic circuits to address the scalability issue of PCE in high-dimensional problems. It is a deep generalization of PCE that effectively scales to high-dimensional input spaces. DeepPCE achieves predictive performance similar to multi-layer perceptrons (MLPs) while maintaining the ability to compute exact statistical inferences through simple forward passes. It enables tractable inference of statistical quantities such as means, variances, covariances, and Sobol sensitivity indices. By leveraging ideas from deep learning, DeepPCE provides a practical solution for understanding complex systems with a large number of parameters. <div>
arXiv:2507.21273v1 Announce Type: new 
Abstract: Polynomial chaos expansion (PCE) is a classical and widely used surrogate modeling technique in physical simulation and uncertainty quantification. By taking a linear combination of a set of basis polynomials - orthonormal with respect to the distribution of uncertain input parameters - PCE enables tractable inference of key statistical quantities, such as (conditional) means, variances, covariances, and Sobol sensitivity indices, which are essential for understanding the modeled system and identifying influential parameters and their interactions. As the number of basis functions grows exponentially with the number of parameters, PCE does not scale well to high-dimensional problems. We address this challenge by combining PCE with ideas from probabilistic circuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep generalization of PCE that scales effectively to high-dimensional input spaces. DeepPCE achieves predictive performance comparable to that of multi-layer perceptrons (MLPs), while retaining PCE's ability to compute exact statistical inferences via simple forward passes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations</title>
<link>https://arxiv.org/abs/2507.21274</link>
<guid>https://arxiv.org/abs/2507.21274</guid>
<content:encoded><![CDATA[
<div> Keywords: recommendation systems, diversity, novelty, reinforcement learning, large language models

Summary:
LAAC (LLM-guided Adversarial Actor Critic) is a novel method proposed for recommendation systems that prioritizes diversity and novelty while maintaining accuracy. The method leverages large language models (LLMs) to suggest novel items and trains a lightweight policy to refine these suggestions using system-specific data. LAAC formulates training as a bilevel optimization, allowing the critic to favor promising novel actions and the actor to improve beyond LLM recommendations. Regularization is applied to mitigate overestimation of unreliable LLM suggestions. Experimental results on real-world datasets show that LAAC outperforms existing baselines in diversity, novelty, and accuracy, while remaining robust on imbalanced data. The method effectively integrates LLM knowledge without requiring expensive fine-tuning. 

<br /><br />Summary: <div>
arXiv:2507.21274v1 Announce Type: new 
Abstract: In recommendation systems, diversity and novelty are essential for capturing varied user preferences and encouraging exploration, yet many systems prioritize click relevance. While reinforcement learning (RL) has been explored to improve diversity, it often depends on random exploration that may not align with user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a novel method that leverages large language models (LLMs) as reference policies to suggest novel items, while training a lightweight policy to refine these suggestions using system-specific data. The method formulates training as a bilevel optimization between actor and critic networks, enabling the critic to selectively favor promising novel actions and the actor to improve its policy beyond LLM recommendations. To mitigate overestimation of unreliable LLM suggestions, we apply regularization that anchors critic values for unexplored items close to well-estimated dataset actions. Experiments on real-world datasets show that LAAC outperforms existing baselines in diversity, novelty, and accuracy, while remaining robust on imbalanced data, effectively integrating LLM knowledge without expensive fine-tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics</title>
<link>https://arxiv.org/abs/2507.21299</link>
<guid>https://arxiv.org/abs/2507.21299</guid>
<content:encoded><![CDATA[
<div> autoencoder, reduced-order modeling, chaotic dynamics, neural ordinary differential equation, hybrid model 

Summary:
A new hybrid reduced order model (ROM) is proposed for predicting chaotic dynamics using a combination of data-driven techniques and known physics from a full-order model (FOM). The model incorporates an autoencoder to find coordinates on an invariant manifold and projects the FOM vector field onto this manifold. The physics-derived vector field is then adjusted using dynamic data or used as a Bayesian prior updated with data. The neural ordinary differential equation approach is employed for these processes. Simulations based on the Kuramoto-Sivashinsky and complex Ginzburg-Landau equations show that the hybrid approach significantly improves time-series predictions compared to a data-only approach. The hybrid model performs well in scenarios with abundant data, sparse data, and even when the FOM contains incorrect parameter values. <div>
arXiv:2507.21299v1 Announce Type: new 
Abstract: While data-driven techniques are powerful tools for reduced-order modeling of systems with chaotic dynamics, great potential remains for leveraging known physics (i.e. a full-order model (FOM)) to improve predictive capability. We develop a hybrid reduced order model (ROM), informed by both data and FOM, for evolving spatiotemporal chaotic dynamics on an invariant manifold whose coordinates are found using an autoencoder. This approach projects the vector field of the FOM onto the invariant manifold; then, this physics-derived vector field is either corrected using dynamic data, or used as a Bayesian prior that is updated with data. In both cases, the neural ordinary differential equation approach is used. We consider simulated data from the Kuramoto-Sivashinsky and complex Ginzburg-Landau equations. Relative to the data-only approach, for scenarios of abundant data, scarce data, and even an incorrect FOM (i.e. erroneous parameter values), the hybrid approach yields substantially improved time-series predictions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation</title>
<link>https://arxiv.org/abs/2507.21350</link>
<guid>https://arxiv.org/abs/2507.21350</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, neuro-symbolic, reconstruction, elasticity, physics-informed neural networks

Summary: 
This paper introduces a new neuro-symbolic framework for reconstructing and simulating elastic objects from sparse multi-view image sequences without explicit geometric information using a combination of neural networks and symbolic equations. By integrating a neural radiance field (NeRF) for object reconstruction with physics-informed neural networks (PINN) that incorporate the governing partial differential equations of elasticity, the method learns a spatiotemporal representation of deforming objects that combines image supervision and symbolic physical constraints. Complex boundary and initial conditions are handled using an energy-constrained Physics-Informed Neural Network architecture, improving simulation accuracy and result explainability. This approach aims to address the limitations of purely empirical methods and traditional numerical solvers by leveraging both data-driven learning and foundational scientific knowledge. <br /><br />Summary: <div>
arXiv:2507.21350v1 Announce Type: new 
Abstract: Neural networks have emerged as a powerful tool for modeling physical systems, offering the ability to learn complex representations from limited data while integrating foundational scientific knowledge. In particular, neuro-symbolic approaches that combine data-driven learning, the neuro, with symbolic equations and rules, the symbolic, address the tension between methods that are purely empirical, which risk straying from established physical principles, and traditional numerical solvers that demand complete geometric knowledge and can be prohibitively expensive for high-fidelity simulations. In this work, we present a novel neuro-symbolic framework for reconstructing and simulating elastic objects directly from sparse multi-view image sequences, without requiring explicit geometric information. Specifically, we integrate a neural radiance field (NeRF) for object reconstruction with physics-informed neural networks (PINN) that incorporate the governing partial differential equations of elasticity. In doing so, our method learns a spatiotemporal representation of deforming objects that leverages both image supervision and symbolic physical constraints. To handle complex boundary and initial conditions, which are traditionally confronted using finite element methods, boundary element methods, or sensor-based measurements, we employ an energy-constrained Physics-Informed Neural Network architecture. This design enhances both simulation accuracy and the explainability of results.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Contrastive Diffusion-based Network (CDNet) for Time Series Classification</title>
<link>https://arxiv.org/abs/2507.21357</link>
<guid>https://arxiv.org/abs/2507.21357</guid>
<content:encoded><![CDATA[
<div> Contrastive Diffusion-based Network, time series classification, deep learning models, noise, multimodal distributions<br />
Summary:<br />
The article introduces CDNet, a Contrastive Diffusion-based Network for enhancing time series classification with deep learning models. CDNet generates informative positive and negative samples using a learned diffusion process, addressing challenges like class similarity, noise, and multimodal distributions. Unlike traditional denoising methods, CDNet learns transitions between samples, both within and across classes, through convolutional approximations of reverse diffusion steps. A theoretically grounded CNN-based mechanism enables denoising and mode coverage, and an uncertainty-weighted composite loss ensures robust training. Experiments on UCR Archive and simulated datasets show that CDNet significantly improves the performance of deep learning classifiers, particularly in noisy, similar, and multimodal data conditions.<br /> <div>
arXiv:2507.21357v1 Announce Type: new 
Abstract: Deep learning models are widely used for time series classification (TSC) due to their scalability and efficiency. However, their performance degrades under challenging data conditions such as class similarity, multimodal distributions, and noise. To address these limitations, we propose CDNet, a Contrastive Diffusion-based Network that enhances existing classifiers by generating informative positive and negative samples via a learned diffusion process. Unlike traditional diffusion models that denoise individual samples, CDNet learns transitions between samples--both within and across classes--through convolutional approximations of reverse diffusion steps. We introduce a theoretically grounded CNN-based mechanism to enable both denoising and mode coverage, and incorporate an uncertainty-weighted composite loss for robust training. Extensive experiments on the UCR Archive and simulated datasets demonstrate that CDNet significantly improves state-of-the-art (SOTA) deep learning classifiers, particularly under noisy, similar, and multimodal conditions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2507.21386</link>
<guid>https://arxiv.org/abs/2507.21386</guid>
<content:encoded><![CDATA[
<div> Neural Combinatorial Optimization, Vehicle Routing Problems, Heterogeneous Capacitated Vehicle Routing Problem, ECHO solver, dual-modality node encoder, Parameter-Free Cross-Attention mechanism, data augment strategy, Reinforcement Learning training, state-of-the-art performance, generalization ability

Summary: 
The article discusses the limitations of current solvers for Vehicle Routing Problems and introduces ECHO, an efficient Neural Combinatorial Optimization solver specifically designed for the min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP). ECHO utilizes a dual-modality node encoder to capture local topological relationships and a Parameter-Free Cross-Attention mechanism to improve decision-making processes. Additionally, a tailored data augment strategy is introduced to enhance stability during Reinforcement Learning training. Experimental results show that ECHO outperforms existing solvers across different scenarios, demonstrating superior performance and generalization. Ablation studies confirm the effectiveness of the proposed methods in optimizing the MMHCVRP solving process.<br /><br />Summary: <div>
arXiv:2507.21386v1 Announce Type: new 
Abstract: Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed to address Vehicle Routing Problems (VRPs). However, most of these solvers focus exclusively on single-vehicle VRP variants, overlooking the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), which involves multiple vehicles. Existing MMHCVRP solvers typically select a vehicle and its next node to visit at each decoding step, but often make myopic decoding decisions and overlook key properties of MMHCVRP, including local topological relationships, vehicle permutation invariance, and node symmetry, resulting in suboptimal performance. To better address these limitations, we propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed dual-modality node encoder to capture local topological relationships among nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in the preceding decoding step. Finally, leveraging vehicle permutation invariance and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to stabilize the Reinforcement Learning training process. To assess the performance of ECHO, we conduct extensive experiments. The experimental results demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying numbers of vehicles and nodes, and exhibits well-performing generalization across both scales and distribution patterns. Finally, ablation studies validate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systolic Array-based Accelerator for State-Space Models</title>
<link>https://arxiv.org/abs/2507.21394</link>
<guid>https://arxiv.org/abs/2507.21394</guid>
<content:encoded><![CDATA[
<div> Keywords: Sequence modeling, State-Space Models, Hardware accelerator, Systolic arrays, Long-range sequence tasks

Summary: 
State-Space Models (SSMs) are efficient in processing long data sequences compared to traditional neural models, but require intensive compute and memory resources. To address this, a specialized hardware accelerator named EpochCore has been introduced, utilizing systolic arrays and a versatile processing element (PE) called LIMA-PE. This accelerator, along with a novel dataflow called ProDF, significantly boosts performance and energy efficiency for SSM-based models. EpochCore achieves 250x performance gains and 45x energy efficiency improvement over traditional accelerators, with a slight increase in area cost. Moreover, it shows around 2,000x improvement in latency for long-range sequence tasks compared to GPU operations. This hardware solution offers a promising approach to enhance the effectiveness of SSMs in sequence modeling tasks. 

<br /><br />Summary: <div>
arXiv:2507.21394v1 Announce Type: new 
Abstract: Sequence modeling is crucial for AI to understand temporal data and detect complex time-dependent patterns. While recurrent neural networks (RNNs), convolutional neural networks (CNNs), and Transformers have advanced in capturing long-range dependencies, they struggle with achieving high accuracy with very long sequences due to limited memory retention (fixed context window). State-Space Models (SSMs) leverage exponentially decaying memory enabling lengthy context window and so they process very long data sequences more efficiently than recurrent and Transformer-based models. Unlike traditional neural models like CNNs and RNNs, SSM-based models require solving differential equations through continuous integration, making training and inference both compute- and memory-intensive on conventional CPUs and GPUs. In this paper we introduce a specialized hardware accelerator, EpochCore, for accelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed to enhance the energy efficiency and throughput of inference of SSM-based models for long-range sequence tasks. Within the SA, we propose a versatile processing element (PE) called LIMA-PE to perform traditional and specialized MAC operations to support traditional DNNs and SSMs. To complement the EpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables highly efficient execution of SSM-based models. By leveraging the LIMA-PE microarchitecture and ProDF, EpochCore achieves on average 250x gains in performance and 45x improvement in energy efficiency, at the expense of 2x increase in area cost over traditional SA-based accelerators, and around ~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel operations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach</title>
<link>https://arxiv.org/abs/2507.21397</link>
<guid>https://arxiv.org/abs/2507.21397</guid>
<content:encoded><![CDATA[
<div> weighted-Chebychev, actor-critic, multi-objective, reinforcement learning, Pareto-stationary<br />
<br />
Summary:<br />
The article introduces a Multi-Objective weighted-Chebyshev Actor-critic (MOCHA) algorithm for multi-objective reinforcement learning (MORL). This algorithm combines weighted-Chebychev and actor-critic frameworks to explore Pareto-stationary solutions systematically with a finite-time sample complexity guarantee. The sample complexity of MOCHA algorithm depends on the minimum entry of a weight vector in WC-scarlarization for finding an epsilon-Pareto-stationary solution. By selecting appropriate learning rates, the sample complexity for exploration can be approximately O(epsilon^{-2}). Simulation studies on a KuaiRand offline dataset demonstrate that MOCHA outperforms other baseline MORL approaches in terms of performance. <div>
arXiv:2507.21397v1 Announce Type: new 
Abstract: In many multi-objective reinforcement learning (MORL) applications, being able to systematically explore the Pareto-stationary solutions under multiple non-convex reward objectives with theoretical finite-time sample complexity guarantee is an important and yet under-explored problem. This motivates us to take the first step and fill the important gap in MORL. Specifically, in this paper, we propose a \uline{M}ulti-\uline{O}bjective weighted-\uline{CH}ebyshev \uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates the weighted-Chebychev (WC) and actor-critic framework to enable Pareto-stationarity exploration systematically with finite-time sample complexity guarantee. Sample complexity result of MOCHA algorithm reveals an interesting dependency on $p_{\min}$ in finding an $\epsilon$-Pareto-stationary solution, where $p_{\min}$ denotes the minimum entry of a given weight vector $\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the sample complexity for each exploration can be $\tilde{\mathcal{O}}(\epsilon^{-2})$. Furthermore, simulation studies on a large KuaiRand offline dataset, show that the performance of MOCHA algorithm significantly outperforms other baseline MORL approaches.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Leakage and Redundancy in the LIT-PCBA Benchmark</title>
<link>https://arxiv.org/abs/2507.21404</link>
<guid>https://arxiv.org/abs/2507.21404</guid>
<content:encoded><![CDATA[
<div> Keywords: LIT-PCBA, data leakage, duplication, redundancy, benchmark<br />
Summary:<br />
The LIT-PCBA benchmark, commonly used for virtual screening, has been found to be compromised due to serious data integrity issues. Analysis revealed extensive data leakage, with duplicated inactives and repeated ligands within the dataset. The presence of near-duplicate ligands and highly similar active pairs between training and validation sets raises concerns about the chemical diversity claimed. As a result, models trained on LIT-PCBA tend to memorize rather than generalize. A baseline implementation relying on exploiting these flaws outperformed state-of-the-art models, including deep neural networks. These findings highlight the benchmark's unsuitability for fair model evaluation and cast doubt on previous research using it. The authors offer tools to assist in creating more rigorous and reliable datasets in the future.<br /> <div>
arXiv:2507.21404v1 Announce Type: new 
Abstract: LIT-PCBA is a widely used benchmark for virtual screening, but our audit reveals it is fundamentally compromised. The dataset suffers from egregious data leakage, rampant duplication, and pervasive analog redundancy -- flaws that invalidate its use for fair model evaluation. Notably, we identify 2,491 inactives duplicated across training and validation sets, and thousands more repeated within individual data splits (2,945 in training, 789 in validation). Critically, three ligands in the query set -- meant to represent unseen test cases -- are leaked: two appear in the training set, one in validation. Structural redundancy compounds these issues: for some targets, over 80% of query ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1 alone, we find 323 highly similar active pairs between training and validation sets, invalidating claims of chemical diversity. These and other flaws collectively cause models trained on LIT-PCBA to memorize rather than generalize. To demonstrate the consequences of these data integrity failures, we implement a trivial memorization-based baseline -- using no learning, no physics, and no modeling -- that outperforms state-of-the-art models, including deep neural networks like CHEESE, on LIT-PCBA simply by exploiting these artifacts. Our findings render the benchmark unfit for its intended purpose and call into question previous results based on its use. We share this audit to raise awareness and provide tooling to help the community develop more rigorous and reliable datasets going forward. All scripts necessary to reproduce our audit and the baseline implementation are available at: https://github.com/sievestack/LIT-PCBA-audit
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring</title>
<link>https://arxiv.org/abs/2507.21422</link>
<guid>https://arxiv.org/abs/2507.21422</guid>
<content:encoded><![CDATA[
<div> hierarchical rewiring, graph neural networks, torque metric, representation learning, noisy graphs
Summary: 
Graph Neural Networks (GNNs) have shown effectiveness in learning from graph-structured data, but native graph interactions can hinder this process. To address this, a torque-driven hierarchical rewiring strategy inspired by classical mechanics is proposed. A torque metric is used to dynamically adjust message passing, considering structural distance and energy scores to guide information aggregation. High-torque edges are pruned, and low-torque links are added to enhance signal relevance and suppress noise in node representations. Evaluations on benchmark datasets demonstrate superior performance on both heterophilous and homophilous graphs, while maintaining accuracy on noisy graphs. This approach improves representation learning and enhances robustness in graph-based tasks. 
<br /><br />Summary: <div>
arXiv:2507.21422v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning from graph-structured data, leveraging message passing to diffuse information and update node representations. However, most efforts have suggested that native interactions encoded in the graph may not be friendly for this process, motivating the development of graph rewiring methods. In this work, we propose a torque-driven hierarchical rewiring strategy, inspired by the notion of torque in classical mechanics, dynamically modulating message passing to improve representation learning in heterophilous graphs and enhance robustness against noisy graphs. Specifically, we define an interference-aware torque metric that integrates structural distance and energy scores to quantify the perturbation induced by edges, thereby encouraging each node to aggregate information from its nearest low-energy neighbors. We use the metric to hierarchically reconfigure the receptive field of each layer by judiciously pruning high-torque edges and adding low-torque links, suppressing propagation noise and boosting pertinent signals. Extensive evaluations on benchmark datasets show that our approach surpasses state-of-the-art methods on both heterophilous and homophilous graphs, and maintains high accuracy on noisy graph.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse</title>
<link>https://arxiv.org/abs/2507.21433</link>
<guid>https://arxiv.org/abs/2507.21433</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, formal logic tasks, MemShare, KV cache, memory overhead<br />
Summary:<br />
Large Reasoning Models (LRMs) have advanced in mathematical reasoning and formal logic tasks but lead to high memory overhead during inference due to lengthy chain-of-thought sequences. LRMs produce similar intermediate reasoning steps, resulting in similar KV cache states across layers. MemShare is introduced as a novel KV cache management approach to reduce memory overhead by identifying and reusing KV cache blocks efficiently. It employs a collaborative filtering algorithm for zero copy cache reuse, resulting in improved throughput and accuracy. Experimental results show up to 84.79% throughput enhancement with better accuracy compared to existing KV cache management methods. <br /> <div>
arXiv:2507.21433v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have achieved significant advances in mathematical reasoning and formal logic tasks. However, their tendency to generate lengthy chain-of-thought sequences leads to substantial memory overhead during inference. We observe that LRMs frequently produce highly similar intermediate reasoning steps, which correspond to similar KV cache states across layers. Motivated by this observation, we propose MemShare, a novel KV cache management approach that effectively reduces memory overhead. MemShare employs a collaborative filtering algorithm to efficiently identify reusable KV cache blocks and enables zero copy cache reuse to significantly reduce memory overhead, improve throughput while maintaining accuracy. Experimental results demonstrate that MemShare delivers up to 84.79\% improvement in throughput while maintaining better accuracy compared to existing KV cache management methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems</title>
<link>https://arxiv.org/abs/2507.21437</link>
<guid>https://arxiv.org/abs/2507.21437</guid>
<content:encoded><![CDATA[
<div> Physics-informed neural networks, Physics-informed DeepONet, singularly perturbed problems, Prandtl-Van Dyke neural network, Prandtl-Van Dyke Deep Operator Network <br />
<br />
Summary: 
The article introduces two novel frameworks, PVD-Net and PVD-ONet, for solving partial differential equations without relying on data. PVD-Net is tailored for stability-focused and high-accuracy modeling, with two versions targeting different requirements. The PVD-Net architecture is based on Prandtl's and Van Dyke's matching conditions for stability and high-accuracy scenarios, respectively. PVD-ONet extends PVD-Net to operator learning by using multiple DeepONet modules to map initial conditions to solution operators. Numerical experiments demonstrate the superior performance of PVD-Net and PVD-ONet over existing methods for multi-scale problems, showcasing their potential in solving challenging boundary layer problems efficiently. <div>
arXiv:2507.21437v1 Announce Type: new 
Abstract: Physics-informed neural networks and Physics-informed DeepONet excel in solving partial differential equations; however, they often fail to converge for singularly perturbed problems. To address this, we propose two novel frameworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning extension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely on governing equations without data. To address varying task-specific requirements, both PVD-Net and PVD-ONet are developed in two distinct versions, tailored respectively for stability-focused and high-accuracy modeling. The leading-order PVD-Net adopts a two-network architecture combined with Prandtl's matching condition, targeting stability-prioritized scenarios. The high-order PVD-Net employs a five-network design with Van Dyke's matching principle to capture fine-scale boundary layer structures, making it ideal for high-accuracy scenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by assembling multiple DeepONet modules, directly mapping initial conditions to solution operators and enabling instant predictions for an entire family of boundary layer problems without retraining. Numerical experiments on various models show that our proposed methods consistently outperform existing baselines under various error metrics, thereby offering a powerful new approach for multi-scale problems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training</title>
<link>https://arxiv.org/abs/2507.21452</link>
<guid>https://arxiv.org/abs/2507.21452</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Policies, knowledge distillation, imitation learning, acceleration methods, accuracy improvement

Summary: 
RAGDP (Retrieve-Augmented Generation for Diffusion Policies) is proposed as a framework to expedite the inference of pre-trained Diffusion Policies (DPs) without requiring additional training. By encoding observation-action pairs and utilizing a knowledge base of expert demonstrations, RAGDP efficiently generates actions by extracting the most similar expert action and combining it with noise removal steps. This approach improves accuracy and speed trade-off without the need for extra training. In comparison to distillation models like Consistency Policy (CP), RAGDP demonstrates a 7% increase in accuracy even with 20 times acceleration. Overall, RAGDP enhances the performance of DPs in imitation learning tasks by reducing the time needed for multiple noise removal steps while maintaining high accuracy levels. 

<br /><br />Summary: <div>
arXiv:2507.21452v1 Announce Type: new 
Abstract: Diffusion Policies (DPs) have attracted attention for their ability to achieve significant accuracy improvements in various imitation learning tasks. However, DPs depend on Diffusion Models, which require multiple noise removal steps to generate a single action, resulting in long generation times. To solve this problem, knowledge distillation-based methods such as Consistency Policy (CP) have been proposed. However, these methods require a significant amount of training time, especially for difficult tasks. In this study, we propose RAGDP (Retrieve-Augmented Generation for Diffusion Policies) as a novel framework that eliminates the need for additional training using a knowledge base to expedite the inference of pre-trained DPs. In concrete, RAGDP encodes observation-action pairs through the DP encoder to construct a vector database of expert demonstrations. During inference, the current observation is embedded, and the most similar expert action is extracted. This extracted action is combined with an intermediate noise removal step to reduce the number of steps required compared to the original diffusion step. We show that by using RAGDP with the base model and existing acceleration methods, we improve the accuracy and speed trade-off with no additional training. Even when accelerating the models 20 times, RAGDP maintains an advantage in accuracy, with a 7% increase over distillation models such as CP.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity-Constrained Continual Learning</title>
<link>https://arxiv.org/abs/2507.21479</link>
<guid>https://arxiv.org/abs/2507.21479</guid>
<content:encoded><![CDATA[
<div> capacity constraints, agents, resource allocation, continual learning, linear-quadratic-Gaussian

Summary:
This paper addresses the issue of resource allocation for agents with limited capacity in the context of continual learning. The focus is on the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem, providing a solution under specific technical conditions. Additionally, the optimal allocation of capacity across sub-problems is explored for problems that can be broken down into smaller tasks. The findings offer insights into the theoretical aspects of learning under capacity constraints, marking a significant step in understanding how agents should allocate their resources for optimal performance. <div>
arXiv:2507.21479v1 Announce Type: new 
Abstract: Any agents we can possibly build are subject to capacity constraints, as memory and compute resources are inherently finite. However, comparatively little attention has been dedicated to understanding how agents with limited capacity should allocate their resources for optimal performance. The goal of this paper is to shed some light on this question by studying a simple yet relevant continual learning problem: the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a solution to this problem under appropriate technical conditions. Moreover, for problems that can be decomposed into a set of sub-problems, we also demonstrate how to optimally allocate capacity across these sub-problems in the steady state. We view the results of this paper as a first step in the systematic theoretical study of learning under capacity constraints.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning</title>
<link>https://arxiv.org/abs/2507.21494</link>
<guid>https://arxiv.org/abs/2507.21494</guid>
<content:encoded><![CDATA[
<div> memory-based algorithms, test-time adaptation, federated learning, Latte framework, decentralized settings

Summary:
The article introduces Latte, a novel framework for test-time adaptation in decentralized settings like federated learning. Latte allows each client to maintain its local memory for storing embeddings and an external memory for storing class prototypes from other clients. This approach enables clients to retrieve prototypes from similar clients under the server's coordination during communication, enhancing model performance. Latte utilizes both embedding similarity and uncertainty for local adaptation. The theoretical analysis demonstrates that Latte effectively leverages in-distribution clients while remaining robust to out-of-distribution clients. Extensive experiments on domain adaptation and corruption benchmarks show that Latte achieves superior performance in decentralized settings with minimal communication and computation costs. The code for Latte is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2507.21494v1 Announce Type: new 
Abstract: Test-time adaptation with pre-trained vision-language models has gained increasing attention for addressing distribution shifts during testing. Among these approaches, memory-based algorithms stand out due to their training-free nature and ability to leverage historical test data. However, existing test-time adaptation methods are typically designed for a single domain with abundant data. In decentralized settings such as federated learning, applying these methods individually to each client suffers from limited test data, while directly sharing a single global memory via the server prevents proper personalization to each client's unique distribution. To address this, we propose Latte, a novel framework where each client maintains a local memory to store embeddings from its own historical test data and an external memory to store class prototypes from other relevant clients. During communication, each client retrieves prototypes from similar clients under the server's coordination to expand its memory. For local adaptation, Latte utilizes both embedding similarity and uncertainty to enhance model performance. Our theoretical analysis shows that Latte effectively leverages in-distribution clients while remaining robust to out-of-distribution clients. Extensive experiments on domain adaptation and corruption benchmarks validate that Latte achieves superior performance in decentralized settings, while introducing only negligible communication and computation costs. Our code is available at https://github.com/baowenxuan/Latte .
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Benchmarking of LLM Agents: A Survey</title>
<link>https://arxiv.org/abs/2507.21504</link>
<guid>https://arxiv.org/abs/2507.21504</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based agents, evaluation, taxonomy, challenges, research directions

Summary: 
This survey paper explores the complex landscape of evaluating Large Language Model (LLM) agents in AI applications. The authors introduce a comprehensive taxonomy that categorizes existing evaluation work based on evaluation objectives (agent behavior, capabilities, reliability, safety) and evaluation process (interaction modes, datasets, benchmarks, metric computation methods, tooling). They identify specific challenges for enterprise applications, such as role-based data access, reliability guarantees, dynamic interactions, and compliance concerns. The paper also suggests future research directions, calling for holistic, realistic, and scalable evaluation methods. The ultimate goal of this work is to provide a structured framework for systematic assessment of LLM agents, facilitating their deployment in real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2507.21504v1 Announce Type: new 
Abstract: The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series</title>
<link>https://arxiv.org/abs/2507.21531</link>
<guid>https://arxiv.org/abs/2507.21531</guid>
<content:encoded><![CDATA[
<div> Keywords: manifold hypothesis, stochastic differential equations, latent variable models, neural time series, computational efficiency<br />

Summary:
The article introduces a novel hierarchical stochastic differential equation (SDE) model that aims to uncover the underlying manifold structure of high-dimensional neural time series data. The model balances computational efficiency and interpretability by assuming that the manifold trajectory can be reconstructed from a sparse set of samples. It uses Brownian bridge SDEs to model the latent space, with points sampled from a multivariate marked point process. These Brownian bridges define the drift of a second set of SDEs, which are then mapped to the observed data. The model's continuous, differentiable latent process can effectively capture the complexity of time series data as the number of manifold points increases. Training and inference procedures are derived, demonstrating that the computational cost of inference scales linearly with the length of the observation data. Validation on synthetic data and neural recordings shows that the model accurately recovers the underlying manifold structure and scales effectively with data dimensionality.<br /><br />Summary: <div>
arXiv:2507.21531v1 Announce Type: new 
Abstract: The manifold hypothesis suggests that high-dimensional neural time series lie on a low-dimensional manifold shaped by simpler underlying dynamics. To uncover this structure, latent dynamical variable models such as state-space models, recurrent neural networks, neural ordinary differential equations, and Gaussian Process Latent Variable Models are widely used. We propose a novel hierarchical stochastic differential equation (SDE) model that balances computational efficiency and interpretability, addressing key limitations of existing methods. Our model assumes the trajectory of a manifold can be reconstructed from a sparse set of samples from the manifold trajectory. The latent space is modeled using Brownian bridge SDEs, with points - specified in both time and value - sampled from a multivariate marked point process. These Brownian bridges define the drift of a second set of SDEs, which are then mapped to the observed data. This yields a continuous, differentiable latent process capable of modeling arbitrarily complex time series as the number of manifold points increases. We derive training and inference procedures and show that the computational cost of inference scales linearly with the length of the observation data. We then validate our model on both synthetic data and neural recordings to demonstrate that it accurately recovers the underlying manifold structure and scales effectively with data dimensionality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Distributions are Effective Neural Network Outputs for Event Prediction</title>
<link>https://arxiv.org/abs/2507.21616</link>
<guid>https://arxiv.org/abs/2507.21616</guid>
<content:encoded><![CDATA[
<div> Neural network, categorical probability distribution, next spike prediction, temporal point process models, dataset <br />
<br />
Summary: The study explores the effectiveness of using a simple neural network output, a categorical probability distribution, for predicting the next spike in temporal point processes. It highlights that this output structure is often overlooked in existing models due to limitations in available datasets and regularization effects. By extending and creating new datasets, the study shows that outputting a simple categorical distribution can be competitive across a wide range of scenarios. The research suggests that many current datasets do not fully capture underlying event generating processes, leading to the reliance on model size and constraints for performance. This study sheds light on the importance of dataset quality and the potential benefits of utilizing straightforward output structures in temporal point process models. <br /><br /> <div>
arXiv:2507.21616v1 Announce Type: new 
Abstract: We demonstrate the effectiveness of using a simple neural network output, a categorical probability distribution, for the task of next spike prediction. This case study motivates an investigation into why this simple output structure is not commonly used with neural temporal point process models. We find evidence that many existing datasets for evaluating temporal point process models do not reveal much information about the underlying event generating processes, and many existing models perform well due to regularization effects of model size and constraints on output structure. We extend existing datasets and create new ones in order to explore outside of this information limited regime and find that outputting a simple categorical distribution is competitive across a wide range of datasets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Genome Embeddings</title>
<link>https://arxiv.org/abs/2507.21648</link>
<guid>https://arxiv.org/abs/2507.21648</guid>
<content:encoded><![CDATA[
<div> Hyperbolic CNNs, DNA sequence modeling, genome interpretation, Transposable Elements Benchmark, evolutionary significance <br />
Summary:
This study introduces a novel application of hyperbolic CNNs for more expressive DNA sequence representations, aligning machine learning models with biological system structure. Outperforming Euclidean models on genome interpretation benchmarks, the hyperbolic approach even exceeds state-of-the-art performance on GUE datasets. The Transposable Elements Benchmark is introduced, shedding light on an understudied genome component. The study explores how hyperbolic models recognize genomic signal under different data conditions, offering insights into dataset embeddings' hyperbolicity. The results suggest the hyperbolic framework as a robust paradigm for genome representation learning, showcasing the potential for improved understanding of core functional and regulatory behaviors. The code and benchmark datasets for replication are provided on GitHub at https://github.com/rrkhan/HGE. <br /> <div>
arXiv:2507.21648v1 Announce Type: new 
Abstract: Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets--the Transposable Elements Benchmark--which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs</title>
<link>https://arxiv.org/abs/2507.21653</link>
<guid>https://arxiv.org/abs/2507.21653</guid>
<content:encoded><![CDATA[
<div> Graph-Enhanced LLMs, Text-only prompting, Dual Granularity Prompting, Fraud detection, Graph learning<br />
Summary:<br />
The article discusses the challenges faced in fraud detection applications with the use of graph learning techniques, specifically in handling dense textual information in heterogeneous graphs. The proposed solution, Dual Granularity Prompting (DGP), aims to mitigate information overload by summarizing neighbor information into concise prompts while preserving fine-grained details for the target node. DGP employs tailored summarization strategies for textual and numerical features, effectively compressing verbose content into informative prompts. Experimental results demonstrate that DGP improves fraud detection performance by up to 6.8% compared to existing methods, showcasing the potential of Graph-Enhanced LLMs in fraud detection applications. <br /> <div>
arXiv:2507.21653v1 Announce Type: new 
Abstract: Real-world fraud detection applications benefit from graph learning techniques that jointly exploit node features, often rich in textual data, and graph structural information. Recently, Graph-Enhanced LLMs emerge as a promising graph learning approach that converts graph information into prompts, exploiting LLMs' ability to reason over both textual and structural information. Among them, text-only prompting, which converts graph information to prompts consisting solely of text tokens, offers a solution that relies only on LLM tuning without requiring additional graph-specific encoders. However, text-only prompting struggles on heterogeneous fraud-detection graphs: multi-hop relations expand exponentially with each additional hop, leading to rapidly growing neighborhoods associated with dense textual information. These neighborhoods may overwhelm the model with long, irrelevant content in the prompt and suppress key signals from the target node, thereby degrading performance. To address this challenge, we propose Dual Granularity Prompting (DGP), which mitigates information overload by preserving fine-grained textual details for the target node while summarizing neighbor information into coarse-grained text prompts. DGP introduces tailored summarization strategies for different data modalities, bi-level semantic abstraction for textual fields and statistical aggregation for numerical features, enabling effective compression of verbose neighbor content into concise, informative prompts. Experiments across public and industrial datasets demonstrate that DGP operates within a manageable token budget while improving fraud detection performance by up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of Graph-Enhanced LLMs for fraud detection.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2507.21670</link>
<guid>https://arxiv.org/abs/2507.21670</guid>
<content:encoded><![CDATA[
<div> level-set theory, classification, uncertainty quantification, prevalence, Bayes optimal

Summary:
In this paper, the authors explore the relationship between machine learning models and uncertainty quantification using a diagnostics-based approach. They demonstrate that certain self-consistent ML models can be seen as class-conditional probability distributions through a level-set theory of classification. By analyzing binary Bayes optimal classifiers, they show that the boundary sets can be interpreted as level-sets of pairwise density ratios. Parameterizing Bayes classifiers based on prevalence reveals important properties such as monotonicity and class-switching, aiding in deducing density ratios and uncertainty in class assignments. The authors extend their analysis to the multiclass case, deriving normalization and self-consistency conditions essential for probabilistic interpretations of ML models. These findings inform a broader uncertainty propagation framework for UQ in ML. <div>
arXiv:2507.21670v1 Announce Type: new 
Abstract: Machine learning (ML) is often viewed as a powerful data analysis tool that is easy to learn because of its black-box nature. Yet this very nature also makes it difficult to quantify confidence in predictions extracted from ML models, and more fundamentally, to understand how such models are mathematical abstractions of training data. The goal of this paper is to unravel these issues and their connections to uncertainty quantification (UQ) by pursuing a line of reasoning motivated by diagnostics. In such settings, prevalence - i.e. the fraction of elements in class - is often of inherent interest. Here we analyze the many interpretations of prevalence to derive a level-set theory of classification, which shows that certain types of self-consistent ML models are equivalent to class-conditional probability distributions. We begin by studying the properties of binary Bayes optimal classifiers, recognizing that their boundary sets can be reinterpreted as level-sets of pairwise density ratios. By parameterizing Bayes classifiers in terms of the prevalence, we then show that they satisfy important monotonicity and class-switching properties that can be used to deduce the density ratios without direct access to the boundary sets. Moreover, this information is sufficient for tasks such as constructing the multiclass Bayes-optimal classifier and estimating inherent uncertainty in the class assignments. In the multiclass case, we use these results to deduce normalization and self-consistency conditions, the latter being equivalent to the law of total probability for classifiers. We also show that these are necessary conditions for arbitrary ML models to have valid probabilistic interpretations. Throughout we demonstrate how this analysis informs the broader task of UQ for ML via an uncertainty propagation framework.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting</title>
<link>https://arxiv.org/abs/2507.21710</link>
<guid>https://arxiv.org/abs/2507.21710</guid>
<content:encoded><![CDATA[
<div> Keywords: commodity demand forecasting, deep learning, Gated Recurrent Unit (GRU), physics-informed neural network (PINN), economic constraint

Summary:
PREIG is a new deep learning framework designed for accurate commodity demand forecasting by integrating GRU architecture and PINN principles. It enforces a negative elasticity constraint between price and demand through a customized loss function, ensuring model predictions align with economic theory. The hybrid optimization strategy of NAdam, L-BFGS, and Population-Based Training enhances predictive performance and stability. Experiments show PREIG outperforms traditional econometric models and deep learning baselines in RMSE and MAPE. Compared to GRU, PREIG maintains interpretability while achieving strong predictive accuracy. By combining domain knowledge, optimization theory, and deep learning, PREIG offers a robust, scalable solution for nonlinear time series forecasting in economics.<br /><br />Summary: PREIG is a novel deep learning framework for commodity demand forecasting that integrates GRU architecture with PINN principles and enforces an economic constraint. It outperforms traditional models and deep learning baselines in accuracy, maintaining interpretability and scalability while offering a robust solution for nonlinear time series forecasting. <div>
arXiv:2507.21710v1 Announce Type: new 
Abstract: Accurately forecasting commodity demand remains a critical challenge due to volatile market dynamics, nonlinear dependencies, and the need for economically consistent predictions. This paper introduces PREIG, a novel deep learning framework tailored for commodity demand forecasting. The model uniquely integrates a Gated Recurrent Unit (GRU) architecture with physics-informed neural network (PINN) principles by embedding a domain-specific economic constraint: the negative elasticity between price and demand. This constraint is enforced through a customized loss function that penalizes violations of the physical rule, ensuring that model predictions remain interpretable and aligned with economic theory. To further enhance predictive performance and stability, PREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS with Population-Based Training (POP). Experiments across multiple commodities datasets demonstrate that PREIG significantly outperforms traditional econometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both RMSE and MAPE. When compared with GRU,PREIG maintains good explainability while still performing well in prediction. By bridging domain knowledge, optimization theory and deep learning, PREIG provides a robust, interpretable, and scalable solution for high-dimensional nonlinear time series forecasting in economy.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins</title>
<link>https://arxiv.org/abs/2507.21720</link>
<guid>https://arxiv.org/abs/2507.21720</guid>
<content:encoded><![CDATA[
<div> Neural network, Hydrofluoroolefin refrigerants, Thermodynamic properties, Extended corresponding state model, Machine learning<br />
Summary:<br />
The article introduces a neural network extended corresponding state model to predict the thermodynamic properties of hydrofluoroolefin refrigerants, addressing the lack of reliable data hindering their discovery. The model incorporates a graph neural network module to characterize fluids based on molecular structures, enhancing generalization. Training with accurate data and evaluating through cross-validation, the model outperforms conventional methods with improved accuracy in density and energy properties. Results show average absolute deviations of 1.49% and 2.42% for density in liquid and supercritical regions, and improved accuracy in residual entropy and enthalpy. By integrating physics knowledge into machine learning, the proposed model accelerates the discovery of novel hydrofluoroolefin refrigerants.<br /><br />Summary: <div>
arXiv:2507.21720v1 Announce Type: new 
Abstract: Hydrofluoroolefins are considered the most promising next-generation refrigerants due to their extremely low global warming potential values, which can effectively mitigate the global warming effect. However, the lack of reliable thermodynamic data hinders the discovery and application of newer and superior hydrofluoroolefin refrigerants. In this work, integrating the strengths of theoretical method and data-driven method, we proposed a neural network extended corresponding state model to predict the residual thermodynamic properties of hydrofluoroolefin refrigerants. The innovation is that the fluids are characterized through their microscopic molecular structures by the inclusion of graph neural network module and the specialized design of model architecture to enhance its generalization ability. The proposed model is trained using the highly accurate data of available known fluids, and evaluated via the leave-one-out cross-validation method. Compared to conventional extended corresponding state models or cubic equation of state, the proposed model shows significantly improved accuracy for density and energy properties in liquid and supercritical regions, with average absolute deviation of 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for residual entropy, 1.85% and 1.34% for residual enthalpy. These results demonstrate the effectiveness of embedding physics knowledge into the machine learning model. The proposed neural network extended corresponding state model is expected to significantly accelerate the discovery of novel hydrofluoroolefin refrigerants.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation</title>
<link>https://arxiv.org/abs/2507.21738</link>
<guid>https://arxiv.org/abs/2507.21738</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, over-unlearning, zero-shot unlearning, ZS-PAG, adversarial samples

Summary: 
Machine unlearning aims to remove specific samples from a trained model, facing the challenge of over-unlearning. Existing methods rely on access to remaining data, making them impractical for zero-shot unlearning scenarios. The ZS-PAG framework introduces innovations to address this gap: approximating remaining data with adversarial samples, pinpointing a specific subspace for unlearning, and designing an influence-based pseudo-labeling strategy to improve model performance post-unlearning. The method is theoretically guaranteed and outperforms baselines in experiments across various benchmarks.<br /><br />Summary: <div>
arXiv:2507.21738v1 Announce Type: new 
Abstract: Machine unlearning aims to remove the influence of specific samples from a trained model. A key challenge in this process is over-unlearning, where the model's performance on the remaining data significantly drops due to the change in the model's parameters. Existing unlearning algorithms depend on the remaining data to prevent this issue. As such, these methods are inapplicable in a more practical scenario, where only the unlearning samples are available (i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to fill this gap. Our approach offers three key innovations: (1) we approximate the inaccessible remaining data by generating adversarial samples; (2) leveraging the generated samples, we pinpoint a specific subspace to perform the unlearning process, therefore preventing over-unlearning in the challenging zero-shot scenario; and (3) we consider the influence of the unlearning process on the remaining samples and design an influence-based pseudo-labeling strategy. As a result, our method further improves the model's performance after unlearning. The proposed method holds a theoretical guarantee, and experiments on various benchmarks validate the effectiveness and superiority of our proposed method over several baselines.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>evoxels: A differentiable physics framework for voxel-based microstructure simulations</title>
<link>https://arxiv.org/abs/2507.21748</link>
<guid>https://arxiv.org/abs/2507.21748</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, microscopy, predictive simulations, data-driven optimization, evoxels 

Summary: 
materials science intersection experimentalists advanced microscopy uncover micro- nano scale structure theorists computational scientists models processing structure properties bridging domains essential inverse material design start desired performance work backwards optimal microstructures manufacturing routes integrating high-resolution imaging predictive simulations data-driven optimization accelerates discovery deepens understanding process-structure-property relationships differentiable physics framework evoxels based Pythonic unified voxel-based approach integrates segmented 3D microscopy data physical simulations inverse modeling machine learning. <div>
arXiv:2507.21748v1 Announce Type: new 
Abstract: Materials science inherently spans disciplines: experimentalists use advanced microscopy to uncover micro- and nanoscale structure, while theorists and computational scientists develop models that link processing, structure, and properties. Bridging these domains is essential for inverse material design where you start from desired performance and work backwards to optimal microstructures and manufacturing routes. Integrating high-resolution imaging with predictive simulations and data-driven optimization accelerates discovery and deepens understanding of process-structure-property relationships. The differentiable physics framework evoxels is based on a fully Pythonic, unified voxel-based approach that integrates segmented 3D microscopy data, physical simulations, inverse modeling, and machine learning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TempRe: Template generation for single and direct multi-step retrosynthesis</title>
<link>https://arxiv.org/abs/2507.21762</link>
<guid>https://arxiv.org/abs/2507.21762</guid>
<content:encoded><![CDATA[
arXiv:2507.21762v1 Announce Type: new 
Abstract: Retrosynthesis planning remains a central challenge in molecular discovery due to the vast and complex chemical reaction space. While traditional template-based methods offer tractability, they suffer from poor scalability and limited generalization, and template-free generative approaches risk generating invalid reactions. In this work, we propose TempRe, a generative framework that reformulates template-based approaches as sequence generation, enabling scalable, flexible, and chemically plausible retrosynthesis. We evaluated TempRe across single-step and multi-step retrosynthesis tasks, demonstrating its superiority over both template classification and SMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe achieves strong top-k route accuracy. Furthermore, we extend TempRe to direct multi-step synthesis route generation, providing a lightweight and efficient alternative to conventional single-step and search-based approaches. These results highlight the potential of template generative modeling as a powerful paradigm in computer-aided synthesis planning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer</title>
<link>https://arxiv.org/abs/2507.21799</link>
<guid>https://arxiv.org/abs/2507.21799</guid>
<content:encoded><![CDATA[
arXiv:2507.21799v1 Announce Type: new 
Abstract: The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box transformers, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex sparse rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box transformer to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box transformer with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations</title>
<link>https://arxiv.org/abs/2507.21803</link>
<guid>https://arxiv.org/abs/2507.21803</guid>
<content:encoded><![CDATA[
arXiv:2507.21803v1 Announce Type: new 
Abstract: Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering a sustainable future. The process, which involves injecting supercritical CO$_2$ into underground formations, a method already widely used for Enhanced Oil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and addresses climate change but also extends the operational lifespan and sustainability of oil fields and platforms, easing the shift toward greener practices. This paper delivers a thorough comparative evaluation of strategies for optimizing decision variables in CCS project development, employing a derivative-free technique known as Bayesian Optimization. In addition to Gaussian Processes, which usually serve as the gold standard in BO, various novel stochastic models were examined and compared within a BO framework. This research investigates the effectiveness of utilizing more exotic stochastic models than GPs for BO in environments where GPs have been shown to underperform, such as in cases with a large number of decision variables or multiple objective functions that are not similarly scaled. By incorporating Net Present Value (NPV) as a key objective function, the proposed framework demonstrates its potential to improve economic viability while ensuring the sustainable deployment of CCS technologies. Ultimately, this study represents the first application in the reservoir engineering industry of the growing body of BO research, specifically in the search for more appropriate stochastic models, highlighting its potential as a preferred method for enhancing sustainability in the energy sector.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Fourier Neural Operators via Effective Field Theory</title>
<link>https://arxiv.org/abs/2507.21833</link>
<guid>https://arxiv.org/abs/2507.21833</guid>
<content:encoded><![CDATA[
arXiv:2507.21833v1 Announce Type: new 
Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for high-dimensional partial-differential equations, yet their stability, generalization and frequency behavior lack a principled explanation. We present the first systematic effective-field-theory analysis of FNOs in an infinite-dimensional function space, deriving closed recursion relations for the layer kernel and four-point vertex and then examining three practically important settings-analytic activations, scale-invariant cases and architectures with residual connections. The theory shows that nonlinear activations inevitably couple frequency inputs to high-frequency modes that are otherwise discarded by spectral truncation, and experiments confirm this frequency transfer. For wide networks we obtain explicit criticality conditions on the weight-initialization ensemble that keep small input perturbations to have uniform scale across depth, and empirical tests validate these predictions. Taken together, our results quantify how nonlinearity enables neural operators to capture non-trivial features, supply criteria for hyper-parameter selection via criticality analysis, and explain why scale-invariant activations and residual connections enhance feature learning in FNOs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Interpretable Ordinary Differential Equations from Noisy Data</title>
<link>https://arxiv.org/abs/2507.21841</link>
<guid>https://arxiv.org/abs/2507.21841</guid>
<content:encoded><![CDATA[
arXiv:2507.21841v1 Announce Type: new 
Abstract: The data-driven discovery of interpretable models approximating the underlying dynamics of a physical system has gained attraction in the past decade. Current approaches employ pre-specified functional forms or basis functions and often result in models that lack physical meaning and interpretability, let alone represent the true physics of the system. We propose an unsupervised parameter estimation methodology that first finds an approximate general solution, followed by a spline transformation to linearly estimate the coefficients of the governing ordinary differential equation (ODE). The approximate general solution is postulated using the same functional form as the analytical solution of a general homogeneous, linear, constant-coefficient ODE. An added advantage is its ability to produce a high-fidelity, smooth functional form even in the presence of noisy data. The spline approximation obtains gradient information from the functional form which are linearly independent and creates the basis of the gradient matrix. This gradient matrix is used in a linear system to find the coefficients of the ODEs. From the case studies, we observed that our modeling approach discovers ODEs with high accuracy and also promotes sparsity in the solution without using any regularization techniques. The methodology is also robust to noisy data and thus allows the integration of data-driven techniques into real experimental setting for data-driven learning of physical phenomena.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2507.21898</link>
<guid>https://arxiv.org/abs/2507.21898</guid>
<content:encoded><![CDATA[
arXiv:2507.21898v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVDs) are a main cause of mortality globally, accounting for 31% of all deaths. This study involves a cardiovascular disease (CVD) dataset comprising 68,119 records to explore the influence of numerical (age, height, weight, blood pressure, BMI) and categorical gender, cholesterol, glucose, smoking, alcohol, activity) factors on CVD occurrence. We have performed statistical analyses, including t-tests, Chi-square tests, and ANOVA, to identify strong associations between CVD and elderly people, hypertension, higher weight, and abnormal cholesterol levels, while physical activity (a protective factor). A logistic regression model highlights age, blood pressure, and cholesterol as primary risk factors, with unexpected negative associations for smoking and alcohol, suggesting potential data issues. Model performance comparisons reveal CatBoost as the top performer with an accuracy of 0.734 and an ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824). Data challenges, including outliers and skewed distributions, indicate a need for improved preprocessing to enhance predictive reliability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-state Protein Design with DynamicMPNN</title>
<link>https://arxiv.org/abs/2507.21938</link>
<guid>https://arxiv.org/abs/2507.21938</guid>
<content:encoded><![CDATA[
arXiv:2507.21938v1 Announce Type: new 
Abstract: Structural biology has long been dominated by the one sequence, one structure, one function paradigm, yet many critical biological processes - from enzyme catalysis to membrane transport - depend on proteins that adopt multiple conformational states. Existing multi-state design approaches rely on post-hoc aggregation of single-state predictions, achieving poor experimental success rates compared to single-state design. We introduce DynamicMPNN, an inverse folding model explicitly trained to generate sequences compatible with multiple conformations through joint learning across conformational ensembles. Trained on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13% on structure-normalized RMSD across our challenging multi-state protein benchmark.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLA-Centric Automated Algorithm Selection Framework for Cloud Environments</title>
<link>https://arxiv.org/abs/2507.21963</link>
<guid>https://arxiv.org/abs/2507.21963</guid>
<content:encoded><![CDATA[
arXiv:2507.21963v1 Announce Type: new 
Abstract: Cloud computing offers on-demand resource access, regulated by Service-Level Agreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA violations can impact efficiency and CSP profitability. In this work, we propose an SLA-aware automated algorithm-selection framework for combinatorial optimization problems in resource-constrained cloud environments. The framework uses an ensemble of machine learning models to predict performance and rank algorithm-hardware pairs based on SLA constraints. We also apply our framework to the 0-1 knapsack problem. We curate a dataset comprising instance specific features along with memory usage, runtime, and optimality gap for 6 algorithms. As an empirical benchmark, we evaluate the framework on both classification and regression tasks. Our ablation study explores the impact of hyperparameters, learning approaches, and large language models effectiveness in regression, and SHAP-based interpretability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generative Ad Text on Facebook using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21983</link>
<guid>https://arxiv.org/abs/2507.21983</guid>
<content:encoded><![CDATA[
arXiv:2507.21983v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI), in particular large language models (LLMs), is poised to drive transformative economic change. LLMs are pre-trained on vast text data to learn general language patterns, but a subsequent post-training phase is critical to align them for specific real-world tasks. Reinforcement learning (RL) is the leading post-training technique, yet its economic impact remains largely underexplored and unquantified. We examine this question through the lens of the first deployment of an RL-trained LLM for generative advertising on Facebook. Integrated into Meta's Text Generation feature, our model, "AdLlama," powers an AI tool that helps advertisers create new variations of human-written ad text. To train this model, we introduce reinforcement learning with performance feedback (RLPF), a post-training method that uses historical ad performance data as a reward signal. In a large-scale 10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad variations, we find that AdLlama improves click-through rates by 6.7% (p=0.0296) compared to a supervised imitation model trained on curated ads. This represents a substantial improvement in advertiser return on investment on Facebook. We also find that advertisers who used AdLlama generated more ad variations, indicating higher satisfaction with the model's outputs. To our knowledge, this is the largest study to date on the use of generative AI in an ecologically valid setting, offering an important data point quantifying the tangible impact of RL post-training. Furthermore, the results show that RLPF is a promising and generalizable approach for metric-driven post-training that bridges the gap between highly capable language models and tangible outcomes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2507.21992</link>
<guid>https://arxiv.org/abs/2507.21992</guid>
<content:encoded><![CDATA[
arXiv:2507.21992v1 Announce Type: new 
Abstract: We investigate whether knowledge distillation (KD) from multiple heterogeneous teacher models can enhance the generation of transferable adversarial examples. A lightweight student model is trained using two KD strategies: curriculum-based switching and joint optimization, with ResNet50 and DenseNet-161 as teachers. The trained student is then used to generate adversarial examples using FG, FGS, and PGD attacks, which are evaluated against a black-box target model (GoogLeNet). Our results show that student models distilled from multiple teachers achieve attack success rates comparable to ensemble-based baselines, while reducing adversarial example generation time by up to a factor of six. An ablation study further reveals that lower temperature settings and the inclusion of hard-label supervision significantly enhance transferability. These findings suggest that KD can serve not only as a model compression technique but also as a powerful tool for improving the efficiency and effectiveness of black-box adversarial attacks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning</title>
<link>https://arxiv.org/abs/2507.22032</link>
<guid>https://arxiv.org/abs/2507.22032</guid>
<content:encoded><![CDATA[
arXiv:2507.22032v1 Announce Type: new 
Abstract: This paper proposes a machine learning-based approach for identifying honey floral and geographical sources using mineral element profiles. The proposed method comprises two steps: preprocessing and classification. The preprocessing phase involves missing-value treatment and data normalization. In the classification phase, we employ various supervised classification models for discriminating between six botanical sources and 13 geographical origins of honey. We test the classifiers' performance on a publicly available honey mineral element dataset. The dataset contains mineral element profiles of honeys from various floral and geographical origins. Results show that mineral element content in honey provides discriminative information useful for classifying honey botanical and geographical sources. Results also show that the Random Forests (RF) classifier obtains the best performance on this dataset, achieving a cross-validation accuracy of 99.30% for classifying honey botanical origins and 98.01% for classifying honey geographical origins.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Informed Deep Reinforcement Learning for Inventory Management</title>
<link>https://arxiv.org/abs/2507.22040</link>
<guid>https://arxiv.org/abs/2507.22040</guid>
<content:encoded><![CDATA[
arXiv:2507.22040v1 Announce Type: new 
Abstract: This paper investigates the application of Deep Reinforcement Learning (DRL) to classical inventory management problems, with a focus on practical implementation considerations. We apply a DRL algorithm based on DirectBackprop to several fundamental inventory management scenarios including multi-period systems with lost sales (with and without lead times), perishable inventory management, dual sourcing, and joint inventory procurement and removal. The DRL approach learns policies across products using only historical information that would be available in practice, avoiding unrealistic assumptions about demand distributions or access to distribution parameters. We demonstrate that our generic DRL implementation performs competitively against or outperforms established benchmarks and heuristics across these diverse settings, while requiring minimal parameter tuning. Through examination of the learned policies, we show that the DRL approach naturally captures many known structural properties of optimal policies derived from traditional operations research methods. To further improve policy performance and interpretability, we propose a Structure-Informed Policy Network technique that explicitly incorporates analytically-derived characteristics of optimal policies into the learning process. This approach can help interpretability and add robustness to the policy in out-of-sample performance, as we demonstrate in an example with realistic demand data. Finally, we provide an illustrative application of DRL in a non-stationary setting. Our work bridges the gap between data-driven learning and analytical insights in inventory management while maintaining practical applicability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling</title>
<link>https://arxiv.org/abs/2507.22045</link>
<guid>https://arxiv.org/abs/2507.22045</guid>
<content:encoded><![CDATA[
arXiv:2507.22045v1 Announce Type: new 
Abstract: Continuous-time deep learning models, such as neural ordinary differential equations (ODEs), offer a promising framework for surrogate modeling of complex physical systems. A central challenge in training these models lies in learning expressive yet stable time-varying weights, particularly under computational constraints. This work investigates weight parameterization strategies that constrain the temporal evolution of weights to a low-dimensional subspace spanned by polynomial basis functions. We evaluate both monomial and Legendre polynomial bases within neural ODE and residual network (ResNet) architectures under discretize-then-optimize and optimize-then-discretize training paradigms. Experimental results across three high-dimensional benchmark problems show that Legendre parameterizations yield more stable training dynamics, reduce computational cost, and achieve accuracy comparable to or better than both monomial parameterizations and unconstrained weight models. These findings elucidate the role of basis choice in time-dependent weight parameterization and demonstrate that using orthogonal polynomial bases offers a favorable tradeoff between model expressivity and training efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Demand Forecasting via Dual-Strategy Ensembling</title>
<link>https://arxiv.org/abs/2507.22053</link>
<guid>https://arxiv.org/abs/2507.22053</guid>
<content:encoded><![CDATA[
arXiv:2507.22053v1 Announce Type: new 
Abstract: Accurate demand forecasting is critical for supply chain optimization, yet remains difficult in practice due to hierarchical complexity, domain shifts, and evolving external factors. While recent foundation models offer strong potential for time series forecasting, they often suffer from architectural rigidity and limited robustness under distributional change. In this paper, we propose a unified ensemble framework that enhances the performance of foundation models for sales forecasting in real-world supply chains. Our method combines two complementary strategies: (1) Hierarchical Ensemble (HE), which partitions training and inference by semantic levels (e.g., store, category, department) to capture localized patterns; and (2) Architectural Ensemble (AE), which integrates predictions from diverse model backbones to mitigate bias and improve stability. We conduct extensive experiments on the M5 benchmark and three external sales datasets, covering both in-domain and zero-shot forecasting. Results show that our approach consistently outperforms strong baselines, improves accuracy across hierarchical levels, and provides a simple yet effective mechanism for boosting generalization in complex forecasting environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High hopes for "Deep Medicine"? AI, economics, and the future of care</title>
<link>https://arxiv.org/abs/2507.21054</link>
<guid>https://arxiv.org/abs/2507.21054</guid>
<content:encoded><![CDATA[
arXiv:2507.21054v1 Announce Type: cross 
Abstract: In the much-celebrated book Deep Medicine, Eric Topol argues that the development of artificial intelligence for health care will lead to a dramatic shift in the culture and practice of medicine. In the next several decades, he suggests, AI will become sophisticated enough that many of the everyday tasks of physicians could be delegated to it. Topol is perhaps the most articulate advocate of the benefits of AI in medicine, but he is hardly alone in spruiking its potential to allow physicians to dedicate more of their time and attention to providing empathetic care for their patients in the future. Unfortunately, several factors suggest a radically different picture for the future of health care. Far from facilitating a return to a time of closer doctor-patient relationships, the use of medical AI seems likely to further erode therapeutic relationships and threaten professional and patient satisfaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions</title>
<link>https://arxiv.org/abs/2507.21065</link>
<guid>https://arxiv.org/abs/2507.21065</guid>
<content:encoded><![CDATA[
arXiv:2507.21065v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing</title>
<link>https://arxiv.org/abs/2507.21084</link>
<guid>https://arxiv.org/abs/2507.21084</guid>
<content:encoded><![CDATA[
arXiv:2507.21084v1 Announce Type: cross 
Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
arXiv:2507.21112v1 Announce Type: cross 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations with a focus on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate practical use cases in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classifications. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element for modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedFlex: Federated Learning for Diverse Netflix Recommendations</title>
<link>https://arxiv.org/abs/2507.21115</link>
<guid>https://arxiv.org/abs/2507.21115</guid>
<content:encoded><![CDATA[
arXiv:2507.21115v1 Announce Type: cross 
Abstract: Federated learning is a decentralized approach that enables collaborative model training across multiple devices while preserving data privacy. It has shown significant potential in various domains, including healthcare and personalized recommendation systems. However, most existing work on federated recommendation systems has focused primarily on improving accuracy, with limited attention to fairness and diversity. In this paper, we introduce FedFlex, a federated recommender system for Netflix-style TV series recommendations. FedFlex integrates two state-of-the-art matrix factorization algorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal Relevance (MMR) to re-rank items and enhance diversity. We conduct extensive experiments comparing recommendations generated by SVD and BPR algorithms. In a live two-week user study, participants received two recommendation lists: List A, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity. Participants were asked to click on the movies they were interested in watching. Our findings demonstrate that FedFlex effectively introduces diverse content, such as new genres, into recommendations without necessarily compromising user satisfaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach</title>
<link>https://arxiv.org/abs/2507.21118</link>
<guid>https://arxiv.org/abs/2507.21118</guid>
<content:encoded><![CDATA[
arXiv:2507.21118v1 Announce Type: cross 
Abstract: MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Generative AI to Enhance Synthea Module Development</title>
<link>https://arxiv.org/abs/2507.21123</link>
<guid>https://arxiv.org/abs/2507.21123</guid>
<content:encoded><![CDATA[
arXiv:2507.21123v1 Announce Type: cross 
Abstract: This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator. Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data. We demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module. While the use of LLMs in this context shows promise, we also acknowledge the challenges and limitations, such as the need for human oversight, the importance of rigorous testing and validation, and the potential for inaccuracies in LLM-generated content. The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization</title>
<link>https://arxiv.org/abs/2507.21124</link>
<guid>https://arxiv.org/abs/2507.21124</guid>
<content:encoded><![CDATA[
arXiv:2507.21124v1 Announce Type: cross 
Abstract: We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities--such as threshold-based filtering, slice extraction, and statistical analysis--through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system's adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., ``visualize the skull"). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline</title>
<link>https://arxiv.org/abs/2507.21125</link>
<guid>https://arxiv.org/abs/2507.21125</guid>
<content:encoded><![CDATA[
arXiv:2507.21125v1 Announce Type: cross 
Abstract: In an era of radical technology transformations, technology maps play a crucial role in enhancing decision making. These maps heavily rely on automated methods of technology extraction. This paper introduces Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for automated technology extraction from scientific literature. RATE combines Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation. This hybrid method results in high recall in candidate generation alongside with high precision in candidate filtering. While the pipeline is designed to be general and widely applicable, we demonstrate its use on 678 research articles focused on Brain-Computer Interfaces (BCIs) and Extended Reality (XR) as a case study. Consequently, The validated technology terms by RATE were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape. For the purpose of evaluation, a gold standard dataset of technologies in 70 selected random articles had been curated by the experts. In addition, a technology extraction model based on Bidirectional Encoder Representations of Transformers (BERT) was used as a comparative method. RATE achieved F1-score of 91.27%, Significantly outperforming BERT with F1-score of 53.73%. Our findings highlight the promise of definition-driven LLM methods for technology extraction and mapping. They also offer new insights into emerging trends within the BCI-XR field. The source code is available https://github.com/AryaAftab/RATE
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses</title>
<link>https://arxiv.org/abs/2507.21132</link>
<guid>https://arxiv.org/abs/2507.21132</guid>
<content:encoded><![CDATA[
arXiv:2507.21132v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence. This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector. Our results show that while some models exhibit sycophancy, others like o4-mini remain robust. Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</title>
<link>https://arxiv.org/abs/2507.21134</link>
<guid>https://arxiv.org/abs/2507.21134</guid>
<content:encoded><![CDATA[
arXiv:2507.21134v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTS-1 Technical Report</title>
<link>https://arxiv.org/abs/2507.21138</link>
<guid>https://arxiv.org/abs/2507.21138</guid>
<content:encoded><![CDATA[
arXiv:2507.21138v1 Announce Type: cross 
Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification</title>
<link>https://arxiv.org/abs/2507.21156</link>
<guid>https://arxiv.org/abs/2507.21156</guid>
<content:encoded><![CDATA[
arXiv:2507.21156v1 Announce Type: cross 
Abstract: The emergence of Vision Transformers (ViTs) has revolutionized computer vision, yet their effectiveness compared to traditional Convolutional Neural Networks (CNNs) in medical imaging remains under-explored. This study presents a comprehensive comparative analysis of CNN and ViT architectures across three critical medical imaging tasks: chest X-ray pneumonia detection, brain tumor classification, and skin cancer melanoma detection. We evaluated four state-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small - across datasets totaling 8,469 medical images. Our results demonstrate task-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest X-ray classification, DeiT-Small excelled at brain tumor detection with 92.16% accuracy, and EfficientNet-B0 led skin cancer classification at 81.84% accuracy. These findings provide crucial insights for practitioners selecting architectures for medical AI applications, highlighting the importance of task-specific architecture selection in clinical decision support systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity</title>
<link>https://arxiv.org/abs/2507.21159</link>
<guid>https://arxiv.org/abs/2507.21159</guid>
<content:encoded><![CDATA[
arXiv:2507.21159v1 Announce Type: cross 
Abstract: The collaborativeness of large language models (LLMs) has proven effective in natural language processing systems, holding considerable promise for healthcare development. However, it lacks explicit component selection rules, necessitating human intervention or clinical-specific validation. Moreover, existing architectures heavily rely on a predefined LLM cluster, where partial LLMs underperform in medical decision support scenarios, invalidating the collaborativeness of LLMs. To this end, we propose an adaptive cluster collaborativeness methodology involving self-diversity and cross-consistency maximization mechanisms to boost LLMs medical decision support capacity. For the self-diversity, we calculate the fuzzy matching value of pairwise outputs within an LLM as its self-diversity value, subsequently prioritizing LLMs with high self-diversity values as cluster components in a training-free manner. For the cross-consistency, we first measure cross-consistency values between the LLM with the highest self-diversity value and others, and then gradually mask out the LLM having the lowest cross-consistency value to eliminate the potential inconsistent output during the collaborative propagation. Extensive experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health, demonstrate the effectiveness of our method across physician-oriented specialties. For example, on NEJMQA, our method achieves the accuracy rate up to the publicly official passing score across all disciplines, especially achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the Obstetrics and Gynecology discipline.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues</title>
<link>https://arxiv.org/abs/2507.21161</link>
<guid>https://arxiv.org/abs/2507.21161</guid>
<content:encoded><![CDATA[
arXiv:2507.21161v1 Announce Type: cross 
Abstract: Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. In contrast to GPT-4V based methods that operate on discrete frames, BF-PIP processes uninterrupted temporal clips. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts. Without any additional training, BF-PIP achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate that combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems</title>
<link>https://arxiv.org/abs/2507.21162</link>
<guid>https://arxiv.org/abs/2507.21162</guid>
<content:encoded><![CDATA[
arXiv:2507.21162v1 Announce Type: cross 
Abstract: The increasing penetration of distributed energy resources into active distribution networks (ADNs) has made effective ADN dispatch imperative. However, the numerous newly-integrated ADN operators, such as distribution system aggregators, virtual power plant managers, and end prosumers, often lack specialized expertise in power system operation, modeling, optimization, and programming. This knowledge gap renders reliance on human experts both costly and time-intensive. To address this challenge and enable intelligent, flexible ADN dispatch, this paper proposes a large language model (LLM) powered automated modeling and optimization approach. First, the ADN dispatch problems are decomposed into sequential stages, and a multi-LLM coordination architecture is designed. This framework comprises an Information Extractor, a Problem Formulator, and a Code Programmer, tasked with information retrieval, optimization problem formulation, and code implementation, respectively. Afterwards, tailored refinement techniques are developed for each LLM agent, greatly improving the accuracy and reliability of generated content. The proposed approach features a user-centric interface that enables ADN operators to derive dispatch strategies via simple natural language queries, eliminating technical barriers and increasing efficiency. Comprehensive comparisons and end-to-end demonstrations on various test cases validate the effectiveness of the proposed architecture and methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Adversarial Point Clouds Using Diffusion Model</title>
<link>https://arxiv.org/abs/2507.21163</link>
<guid>https://arxiv.org/abs/2507.21163</guid>
<content:encoded><![CDATA[
arXiv:2507.21163v1 Announce Type: cross 
Abstract: Adversarial attack methods for 3D point cloud classification reveal the vulnerabilities of point cloud recognition models. This vulnerability could lead to safety risks in critical applications that use deep learning models, such as autonomous vehicles. To uncover the deficiencies of these models, researchers can evaluate their security through adversarial attacks. However, most existing adversarial attack methods are based on white-box attacks. While these methods achieve high attack success rates and imperceptibility, their applicability in real-world scenarios is limited. Black-box attacks, which are more meaningful in real-world scenarios, often yield poor results. This paper proposes a novel black-box adversarial example generation method that utilizes a diffusion model to improve the attack success rate and imperceptibility in the black-box setting, without relying on the internal information of the point cloud classification model to generate adversarial samples. We use a 3D diffusion model to use the compressed features of the point cloud as prior knowledge to guide the reverse diffusion process to add adversarial points to clean examples. Subsequently, its reverse process is employed to transform the distribution of other categories into adversarial points, which are then added to the point cloud.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question</title>
<link>https://arxiv.org/abs/2507.21168</link>
<guid>https://arxiv.org/abs/2507.21168</guid>
<content:encoded><![CDATA[
arXiv:2507.21168v1 Announce Type: cross 
Abstract: Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedBAP: Backdoor Defense via Benign Adversarial Perturbation in Federated Learning</title>
<link>https://arxiv.org/abs/2507.21177</link>
<guid>https://arxiv.org/abs/2507.21177</guid>
<content:encoded><![CDATA[
arXiv:2507.21177v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training while preserving data privacy, but it is highly vulnerable to backdoor attacks. Most existing defense methods in FL have limited effectiveness due to their neglect of the model's over-reliance on backdoor triggers, particularly as the proportion of malicious clients increases. In this paper, we propose FedBAP, a novel defense framework for mitigating backdoor attacks in FL by reducing the model's reliance on backdoor triggers. Specifically, first, we propose a perturbed trigger generation mechanism that creates perturbation triggers precisely matching backdoor triggers in location and size, ensuring strong influence on model outputs. Second, we utilize these perturbation triggers to generate benign adversarial perturbations that disrupt the model's dependence on backdoor triggers while forcing it to learn more robust decision boundaries. Finally, we design an adaptive scaling mechanism to dynamically adjust perturbation intensity, effectively balancing defense strength and model performance. The experimental results demonstrate that FedBAP reduces the attack success rates by 0.22%-5.34%, 0.48%-6.34%, and 97.22%-97.6% under three types of backdoor attacks, respectively. In particular, FedBAP demonstrates outstanding performance against novel backdoor attacks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers</title>
<link>https://arxiv.org/abs/2507.21186</link>
<guid>https://arxiv.org/abs/2507.21186</guid>
<content:encoded><![CDATA[
arXiv:2507.21186v1 Announce Type: cross 
Abstract: Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs</title>
<link>https://arxiv.org/abs/2507.21193</link>
<guid>https://arxiv.org/abs/2507.21193</guid>
<content:encoded><![CDATA[
arXiv:2507.21193v1 Announce Type: cross 
Abstract: Next generation Radio Access Networks (RANs) introduce programmability, intelligence, and near real-time control through intelligent controllers, enabling enhanced security within the RAN and across broader 5G/6G infrastructures. This paper presents a comprehensive survey highlighting opportunities, challenges, and research gaps for Large Language Models (LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future RAN environments. Motivated by this, we propose an LLM interpretable anomaly-based detection system for distributed denial-of-service (DDoS) attacks using multivariate time series key performance measures (KPMs), extracted from E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC). An LSTM-based model is trained to identify malicious User Equipment (UE) behavior based on these KPMs. To enhance transparency, we apply post-hoc local explainability methods such as LIME and SHAP to interpret individual predictions. Furthermore, LLMs are employed to convert technical explanations into natural-language insights accessible to non-expert users. Experimental results on real 5G network KPMs demonstrate that our framework achieves high detection accuracy (F1-score > 0.96) while delivering actionable and interpretable outputs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoGAN A Deep Generative Model for Panoramic Dental Radiographs</title>
<link>https://arxiv.org/abs/2507.21200</link>
<guid>https://arxiv.org/abs/2507.21200</guid>
<content:encoded><![CDATA[
arXiv:2507.21200v1 Announce Type: cross 
Abstract: This paper presents the development of a generative adversarial network (GAN) for synthesizing dental panoramic radiographs. Although exploratory in nature, the study aims to address the scarcity of data in dental research and education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality. The focus was on the dentoalveolar regions, other anatomical structures were cropped out. Extensive preprocessing and data cleaning were performed to standardize the inputs while preserving anatomical variability. We explored four candidate models by varying critic iterations, feature depth, and the use of denoising prior to training. A clinical expert evaluated the generated radiographs based on anatomical visibility and realism, using a 5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness. These findings provide a foundation for future work on GAN-based methods in dental imaging.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combolutional Neural Networks</title>
<link>https://arxiv.org/abs/2507.21202</link>
<guid>https://arxiv.org/abs/2507.21202</guid>
<content:encoded><![CDATA[
arXiv:2507.21202v1 Announce Type: cross 
Abstract: Selecting appropriate inductive biases is an essential step in the design of machine learning models, especially when working with audio, where even short clips may contain millions of samples. To this end, we propose the combolutional layer: a learned-delay IIR comb filter and fused envelope detector, which extracts harmonic features in the time domain. We demonstrate the efficacy of the combolutional layer on three information retrieval tasks, evaluate its computational cost relative to other audio frontends, and provide efficient implementations for training. We find that the combolutional layer is an effective replacement for convolutional layers in audio tasks where precise harmonic analysis is important, e.g., piano transcription, speaker classification, and key detection. Additionally, the combolutional layer has several other key benefits over existing frontends, namely: low parameter count, efficient CPU inference, strictly real-valued computations, and improved interpretability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An empirical comparison of some outlier detection methods with longitudinal data</title>
<link>https://arxiv.org/abs/2507.21203</link>
<guid>https://arxiv.org/abs/2507.21203</guid>
<content:encoded><![CDATA[
arXiv:2507.21203v1 Announce Type: cross 
Abstract: This note investigates the problem of detecting outliers in longitudinal data. It compares well-known methods used in official statistics with proposals from the fields of data mining and machine learning that are based on the distance between observations or binary partitioning trees. This is achieved by applying the methods to panel survey data related to different types of statistical units. Traditional methods are quite simple, enabling the direct identification of potential outliers, but they require specific assumptions. In contrast, recent methods provide only a score whose magnitude is directly related to the likelihood of an outlier being present. All the methods require the user to set a number of tuning parameters. However, the most recent methods are more flexible and sometimes more effective than traditional methods. In addition, these methods can be applied to multidimensional data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Web: Weaving the Next Web with AI Agents</title>
<link>https://arxiv.org/abs/2507.21206</link>
<guid>https://arxiv.org/abs/2507.21206</guid>
<content:encoded><![CDATA[
arXiv:2507.21206v1 Announce Type: cross 
Abstract: The emergence of AI agents powered by large language models (LLMs) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including communication protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking a Tunable Quantum Neural Network on Trapped-Ion and Superconducting Hardware</title>
<link>https://arxiv.org/abs/2507.21222</link>
<guid>https://arxiv.org/abs/2507.21222</guid>
<content:encoded><![CDATA[
arXiv:2507.21222v1 Announce Type: cross 
Abstract: We implement a quantum generalization of a neural network on trapped-ion and IBM superconducting quantum computers to classify MNIST images, a common benchmark in computer vision. The network feedforward involves qubit rotations whose angles depend on the results of measurements in the previous layer. The network is trained via simulation, but inference is performed experimentally on quantum hardware. The classical-to-quantum correspondence is controlled by an interpolation parameter, $a$, which is zero in the classical limit. Increasing $a$ introduces quantum uncertainty into the measurements, which is shown to improve network performance at moderate values of the interpolation parameter. We then focus on particular images that fail to be classified by a classical neural network but are detected correctly in the quantum network. For such borderline cases, we observe strong deviations from the simulated behavior. We attribute this to physical noise, which causes the output to fluctuate between nearby minima of the classification energy landscape. Such strong sensitivity to physical noise is absent for clear images. We further benchmark physical noise by inserting additional single-qubit and two-qubit gate pairs into the neural network circuits. Our work provides a springboard toward more complex quantum neural networks on current devices: while the approach is rooted in standard classical machine learning, scaling up such networks may prove classically non-simulable and could offer a route to near-term quantum advantage.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors</title>
<link>https://arxiv.org/abs/2507.21225</link>
<guid>https://arxiv.org/abs/2507.21225</guid>
<content:encoded><![CDATA[
arXiv:2507.21225v1 Announce Type: cross 
Abstract: Tactile sensing plays a fundamental role in enabling robots to navigate dynamic and unstructured environments, particularly in applications such as delicate object manipulation, surface exploration, and human-robot interaction. In this paper, we introduce a passive soft robotic fingertip with integrated tactile sensing, fabricated using a 3D-printed elastomer lattice with embedded air channels. This sensorization approach, termed fluidic innervation, transforms the lattice into a tactile sensor by detecting pressure changes within sealed air channels, providing a simple yet robust solution to tactile sensing in robotics. Unlike conventional methods that rely on complex materials or designs, fluidic innervation offers a simple, scalable, single-material fabrication process. We characterize the sensors' response, develop a geometric model to estimate tip displacement, and train a neural network to accurately predict contact location and contact force. Additionally, we integrate the fingertip with an admittance controller to emulate spring-like behavior, demonstrate its capability for environment exploration through tactile feedback, and validate its durability under high impact and cyclic loading conditions. This tactile sensing technique offers advantages in terms of simplicity, adaptability, and durability and opens up new opportunities for versatile robotic manipulation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Denoiser-Aided Gyrocompassing</title>
<link>https://arxiv.org/abs/2507.21245</link>
<guid>https://arxiv.org/abs/2507.21245</guid>
<content:encoded><![CDATA[
arXiv:2507.21245v1 Announce Type: cross 
Abstract: An accurate initial heading angle is essential for efficient and safe navigation across diverse domains. Unlike magnetometers, gyroscopes can provide accurate heading reference independent of the magnetic disturbances in a process known as gyrocompassing. Yet, accurate and timely gyrocompassing, using low-cost gyroscopes, remains a significant challenge in scenarios where external navigation aids are unavailable. Such challenges are commonly addressed in real-world applications such as autonomous vehicles, where size, weight, and power limitations restrict sensor quality, and noisy measurements severely degrade gyrocompassing performance. To cope with this challenge, we propose a novel diffusion denoiser-aided gyrocompass approach. It integrates a diffusion-based denoising framework with an enhanced learning-based heading estimation model. The diffusion denoiser processes raw inertial sensor signals before input to the deep learning model, resulting in accurate gyrocompassing. Experiments using both simulated and real sensor data demonstrate that our proposed approach improves gyrocompassing accuracy by 26% compared to model-based gyrocompassing and by 15% compared to other learning-driven approaches. This advancement holds particular significance for ensuring accurate and robust navigation in autonomous platforms that incorporate low-cost gyroscopes within their navigation systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale geometrical and topological learning in the analysis of soft matter collective dynamics</title>
<link>https://arxiv.org/abs/2507.21265</link>
<guid>https://arxiv.org/abs/2507.21265</guid>
<content:encoded><![CDATA[
arXiv:2507.21265v1 Announce Type: cross 
Abstract: Understanding the behavior and evolution of a dynamical many-body system by analyzing patterns in their experimentally captured images is a promising method relevant for a variety of living and non-living self-assembled systems. The arrays of moving liquid crystal skyrmions studied here are a representative example of hierarchically organized materials that exhibit complex spatiotemporal dynamics driven by multiscale processes. Joint geometric and topological data analysis (TDA) offers a powerful framework for investigating such systems by capturing the underlying structure of the data at multiple scales. In the TDA approach, we introduce the $\Psi$-function, a robust numerical topological descriptor related to both the spatiotemporal changes in the size and shape of individual topological solitons and the emergence of regions with their different spatial organization. The geometric method based on the analysis of vector fields generated from images of skyrmion ensembles offers insights into the nonlinear physical mechanisms of the system's response to external stimuli and provides a basis for comparison with theoretical predictions. The methodology presented here is very general and can provide a characterization of system behavior both at the level of individual pattern-forming agents and as a whole, allowing one to relate the results of image data analysis to processes occurring in a physical, chemical, or biological system in the real world.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical PDE solvers outperform neural PDE solvers</title>
<link>https://arxiv.org/abs/2507.21269</link>
<guid>https://arxiv.org/abs/2507.21269</guid>
<content:encoded><![CDATA[
arXiv:2507.21269v1 Announce Type: cross 
Abstract: We present DeepFDM, a differentiable finite-difference framework for learning spatially varying coefficients in time-dependent partial differential equations (PDEs). By embedding a classical forward-Euler discretization into a convolutional architecture, DeepFDM enforces stability and first-order convergence via CFL-compliant coefficient parameterizations. Model weights correspond directly to PDE coefficients, yielding an interpretable inverse-problem formulation. We evaluate DeepFDM on a benchmark suite of scalar PDEs: advection, diffusion, advection-diffusion, reaction-diffusion and inhomogeneous Burgers' equations-in one, two and three spatial dimensions. In both in-distribution and out-of-distribution tests (quantified by the Hellinger distance between coefficient priors), DeepFDM attains normalized mean-squared errors one to two orders of magnitude smaller than Fourier Neural Operators, U-Nets and ResNets; requires 10-20X fewer training epochs; and uses 5-50X fewer parameters. Moreover, recovered coefficient fields accurately match ground-truth parameters. These results establish DeepFDM as a robust, efficient, and transparent baseline for data-driven solution and identification of parametric PDEs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative imaging for radio interferometry with fast uncertainty quantification</title>
<link>https://arxiv.org/abs/2507.21270</link>
<guid>https://arxiv.org/abs/2507.21270</guid>
<content:encoded><![CDATA[
arXiv:2507.21270v1 Announce Type: cross 
Abstract: With the rise of large radio interferometric telescopes, particularly the SKA, there is a growing demand for computationally efficient image reconstruction techniques. Existing reconstruction methods, such as the CLEAN algorithm or proximal optimisation approaches, are iterative in nature, necessitating a large amount of compute. These methods either provide no uncertainty quantification or require large computational overhead to do so. Learned reconstruction methods have shown promise in providing efficient and high quality reconstruction. In this article we explore the use of generative neural networks that enable efficient approximate sampling of the posterior distribution for high quality reconstructions with uncertainty quantification. Our RI-GAN framework, builds on the regularised conditional generative adversarial network (rcGAN) framework by integrating a gradient U-Net (GU-Net) architecture - a hybrid reconstruction model that embeds the measurement operator directly into the network. This framework uses Wasserstein GANs to improve training stability in combination with regularisation terms that combat mode collapse, which are typical problems for conditional GANs. This approach takes as input the dirty image and the point spread function (PSF) of the observation and provides efficient, high-quality image reconstructions that are robust to varying visibility coverages, generalises to images with an increased dynamic range, and provides informative uncertainty quantification. Our methods provide a significant step toward computationally efficient, scalable, and uncertainty-aware imaging for next-generation radio telescopes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting VBAC Outcomes from U.S. Natality Data using Deep and Classical Machine Learning Models</title>
<link>https://arxiv.org/abs/2507.21330</link>
<guid>https://arxiv.org/abs/2507.21330</guid>
<content:encoded><![CDATA[
arXiv:2507.21330v1 Announce Type: cross 
Abstract: Accurately predicting the outcome of a trial of labor after cesarean (TOLAC) is essential for guiding prenatal counseling and minimizing delivery-related risks. This study presents supervised machine learning models for predicting vaginal birth after cesarean (VBAC) using 643,029 TOLAC cases from the CDC WONDER Natality dataset (2017-2023). After filtering for singleton births with one or two prior cesareans and complete data across 47 prenatal-period features, three classifiers were trained: logistic regression, XGBoost, and a multilayer perceptron (MLP). The MLP achieved the highest performance with an AUC of 0.7287, followed closely by XGBoost (AUC = 0.727), both surpassing the logistic regression baseline (AUC = 0.709). To address class imbalance, class weighting was applied to the MLP, and a custom loss function was implemented in XGBoost. Evaluation metrics included ROC curves, confusion matrices, and precision-recall analysis. Logistic regression coefficients highlighted maternal BMI, education, parity, comorbidities, and prenatal care indicators as key predictors. Overall, the results demonstrate that routinely collected, early-pregnancy variables can support scalable and moderately high-performing VBAC prediction models. These models offer potential utility in clinical decision support, particularly in settings lacking access to specialized intrapartum data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph neural networks for residential location choice: connection to classical logit models</title>
<link>https://arxiv.org/abs/2507.21334</link>
<guid>https://arxiv.org/abs/2507.21334</guid>
<content:encoded><![CDATA[
arXiv:2507.21334v1 Announce Type: cross 
Abstract: Researchers have adopted deep learning for classical discrete choice analysis as it can capture complex feature relationships and achieve higher predictive performance. However, the existing deep learning approaches cannot explicitly capture the relationship among choice alternatives, which has been a long-lasting focus in classical discrete choice models. To address the gap, this paper introduces Graph Neural Network (GNN) as a novel framework to analyze residential location choice. The GNN-based discrete choice models (GNN-DCMs) offer a structured approach for neural networks to capture dependence among spatial alternatives, while maintaining clear connections to classical random utility theory. Theoretically, we demonstrate that the GNN-DCMs incorporate the nested logit (NL) model and the spatially correlated logit (SCL) model as two specific cases, yielding novel algorithmic interpretation through message passing among alternatives' utilities. Empirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neural networks in predicting residential location choices among Chicago's 77 community areas. Regarding model interpretation, the GNN-DCMs can capture individual heterogeneity and exhibit spatially-aware substitution patterns. Overall, these results highlight the potential of GNN-DCMs as a unified and expressive framework for synergizing discrete choice modeling and deep learning in the complex spatial choice contexts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Relative Augmentation for Data Efficient Action Detection</title>
<link>https://arxiv.org/abs/2507.21353</link>
<guid>https://arxiv.org/abs/2507.21353</guid>
<content:encoded><![CDATA[
arXiv:2507.21353v1 Announce Type: cross 
Abstract: Adapting large Video-Language Models (VLMs) for action detection using only a few examples poses challenges like overfitting and the granularity mismatch between scene-level pre-training and required person-centric understanding. We propose an efficient adaptation strategy combining parameter-efficient tuning (LoRA) with a novel learnable internal feature augmentation. Applied within the frozen VLM backbone using FiLM, these augmentations generate diverse feature variations directly relevant to the task. Additionally, we introduce a group-weighted loss function that dynamically modulates the training contribution of each augmented sample based on its prediction divergence relative to the group average. This promotes robust learning by prioritizing informative yet reasonable augmentations. We demonstrate our method's effectiveness on complex multi-label, multi-person action detection datasets (AVA, MOMA), achieving strong mAP performance and showcasing significant data efficiency for adapting VLMs from limited examples.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Load Balancing for AI Training Workloads</title>
<link>https://arxiv.org/abs/2507.21372</link>
<guid>https://arxiv.org/abs/2507.21372</guid>
<content:encoded><![CDATA[
arXiv:2507.21372v1 Announce Type: cross 
Abstract: We investigate the performance of various load balancing algorithms for large-scale AI training workloads that are running on dedicated infrastructure. The performance of load balancing depends on both the congestion control and loss recovery algorithms, so our evaluation also sheds light on the appropriate choices for those designs as well.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reservoir Computation with Networks of Differentiating Neuron Ring Oscillators</title>
<link>https://arxiv.org/abs/2507.21377</link>
<guid>https://arxiv.org/abs/2507.21377</guid>
<content:encoded><![CDATA[
arXiv:2507.21377v1 Announce Type: cross 
Abstract: Reservoir Computing is a machine learning approach that uses the rich repertoire of complex system dynamics for function approximation. Current approaches to reservoir computing use a network of coupled integrating neurons that require a steady current to maintain activity. Here, we introduce a small world graph of differentiating neurons that are active only when there are changes in input as an alternative to integrating neurons as a reservoir computing substrate. We find the coupling strength and network topology that enable these small world networks to function as an effective reservoir. We demonstrate the efficacy of these networks in the MNIST digit recognition task, achieving comparable performance of 90.65% to existing reservoir computing approaches. The findings suggest that differentiating neurons can be a potential alternative to integrating neurons and can provide a sustainable future alternative for power-hungry AI applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascading and Proxy Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2507.21412</link>
<guid>https://arxiv.org/abs/2507.21412</guid>
<content:encoded><![CDATA[
arXiv:2507.21412v1 Announce Type: cross 
Abstract: A Membership Inference Attack (MIA) assesses how much a trained machine learning model reveals about its training data by determining whether specific query instances were included in the dataset. We classify existing MIAs into adaptive or non-adaptive, depending on whether the adversary is allowed to train shadow models on membership queries. In the adaptive setting, where the adversary can train shadow models after accessing query instances, we highlight the importance of exploiting membership dependencies between instances and propose an attack-agnostic framework called Cascading Membership Inference Attack (CMIA), which incorporates membership dependencies via conditional shadow training to boost membership inference performance.
  In the non-adaptive setting, where the adversary is restricted to training shadow models before obtaining membership queries, we introduce Proxy Membership Inference Attack (PMIA). PMIA employs a proxy selection strategy that identifies samples with similar behaviors to the query instance and uses their behaviors in shadow models to perform a membership posterior odds test for membership inference. We provide theoretical analyses for both attacks, and extensive experimental results demonstrate that CMIA and PMIA substantially outperform existing MIAs in both settings, particularly in the low false-positive regime, which is crucial for evaluating privacy risks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.21423</link>
<guid>https://arxiv.org/abs/2507.21423</guid>
<content:encoded><![CDATA[
arXiv:2507.21423v1 Announce Type: cross 
Abstract: Autonomous driving requires an understanding of the static environment from sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse multiple inputs, and a vector decoder predicts a vectorized map representation from the latent BEV grid. However, traditional map construction models provide deterministic point estimates, failing to capture uncertainty and the inherent ambiguities of real-world environments, such as occlusions and missing lane markings. We propose MapDiffusion, a novel generative approach that leverages the diffusion paradigm to learn the full distribution of possible vectorized maps. Instead of predicting a single deterministic output from learned queries, MapDiffusion iteratively refines randomly initialized queries, conditioned on a BEV latent grid, to generate multiple plausible map samples. This allows aggregating samples to improve prediction accuracy and deriving uncertainty estimates that directly correlate with scene ambiguity. Extensive experiments on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art performance in online map construction, surpassing the baseline by 5% in single-sample performance. We further show that aggregating multiple samples consistently improves performance along the ROC curve, validating the benefit of distribution modeling. Additionally, our uncertainty estimates are significantly higher in occluded areas, reinforcing their value in identifying regions with ambiguous sensor input. By modeling the full map distribution, MapDiffusion enhances the robustness and reliability of online vectorized HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles in complex environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions</title>
<link>https://arxiv.org/abs/2507.21429</link>
<guid>https://arxiv.org/abs/2507.21429</guid>
<content:encoded><![CDATA[
arXiv:2507.21429v1 Announce Type: cross 
Abstract: The convergence of gradient descent (GD) on the non-convex loss landscapes of deep neural networks (DNNs) presents a fundamental theoretical challenge. While recent work has established that GD converges to a stationary point at a sublinear rate within locally quasi-convex regions (LQCRs), this fails to explain the exponential convergence rates consistently observed in practice. In this paper, we resolve this discrepancy by proving that under a mild assumption on Neural Tangent Kernel (NTK) stability, these same regions satisfy a local Polyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally Polyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds the suboptimality gap, prove that properly initialized finite-width networks admit such regions around initialization, and establish that GD achieves linear convergence within an LPLR, providing the first finite-width guarantee that matches empirically observed rates. We validate our theory across diverse settings, from controlled experiments on fully-connected networks to modern ResNet architectures trained with stochastic methods, demonstrating that LPLR structure emerges robustly in practical deep learning scenarios. By rigorously connecting local landscape geometry to fast optimization through the NTK framework, our work provides a definitive theoretical explanation for the remarkable efficiency of gradient-based optimization in deep learning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Sample Quality with Copula Discrepancies</title>
<link>https://arxiv.org/abs/2507.21434</link>
<guid>https://arxiv.org/abs/2507.21434</guid>
<content:encoded><![CDATA[
arXiv:2507.21434v1 Announce Type: cross 
Abstract: The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern Bayesian machine learning, such as Stochastic Gradient Langevin Dynamics (SGLD), sacrifice asymptotic exactness for computational speed, creating a critical diagnostic gap: traditional sample quality measures fail catastrophically when applied to biased samplers. While powerful Stein-based diagnostics can detect distributional mismatches, they provide no direct assessment of dependence structure, often the primary inferential target in multivariate problems. We introduce the Copula Discrepancy (CD), a principled and computationally efficient diagnostic that leverages Sklar's theorem to isolate and quantify the fidelity of a sample's dependence structure independent of its marginals. Our theoretical framework provides the first structure-aware diagnostic specifically designed for the era of approximate inference. Empirically, we demonstrate that a moment-based CD dramatically outperforms standard diagnostics like effective sample size for hyperparameter selection in biased MCMC, correctly identifying optimal configurations where traditional methods fail. Furthermore, our robust MLE-based variant can detect subtle but critical mismatches in tail dependence that remain invisible to rank correlation-based approaches, distinguishing between samples with identical Kendall's tau but fundamentally different extreme-event behavior. With computational overhead orders of magnitude lower than existing Stein discrepancies, the CD provides both immediate practical value for MCMC practitioners and a theoretical foundation for the next generation of structure-aware sample quality assessment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations</title>
<link>https://arxiv.org/abs/2507.21448</link>
<guid>https://arxiv.org/abs/2507.21448</guid>
<content:encoded><![CDATA[
arXiv:2507.21448v1 Announce Type: cross 
Abstract: Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Global to Local: A Scalable Benchmark for Local Posterior Sampling</title>
<link>https://arxiv.org/abs/2507.21449</link>
<guid>https://arxiv.org/abs/2507.21449</guid>
<content:encoded><![CDATA[
arXiv:2507.21449v1 Announce Type: cross 
Abstract: Degeneracy is an inherent feature of the loss landscape of neural networks, but it is not well understood how stochastic gradient MCMC (SGMCMC) algorithms interact with this degeneracy. In particular, current global convergence guarantees for common SGMCMC algorithms rely on assumptions which are likely incompatible with degenerate loss landscapes. In this paper, we argue that this gap requires a shift in focus from global to local posterior sampling, and, as a first step, we introduce a novel scalable benchmark for evaluating the local sampling performance of SGMCMC algorithms. We evaluate a number of common algorithms, and find that RMSProp-preconditioned SGLD is most effective at faithfully representing the local geometry of the posterior distribution. Although we lack theoretical guarantees about global sampler convergence, our empirical results show that we are able to extract non-trivial local information in models with up to O(100M) parameters.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning</title>
<link>https://arxiv.org/abs/2507.21474</link>
<guid>https://arxiv.org/abs/2507.21474</guid>
<content:encoded><![CDATA[
arXiv:2507.21474v1 Announce Type: cross 
Abstract: Despite success across diverse tasks, current artificial recurrent network architectures rely primarily on implicit hidden-state memories, limiting their interpretability and ability to model long-range dependencies. In contrast, biological neural systems employ explicit, associative memory traces (i.e., engrams) strengthened through Hebbian synaptic plasticity and activated sparsely during recall. Motivated by these neurobiological insights, we introduce the Engram Neural Network (ENN), a novel recurrent architecture incorporating an explicit, differentiable memory matrix with Hebbian plasticity and sparse, attention-driven retrieval mechanisms. The ENN explicitly models memory formation and recall through dynamic Hebbian traces, improving transparency and interpretability compared to conventional RNN variants. We evaluate the ENN architecture on three canonical benchmarks: MNIST digit classification, CIFAR-10 image sequence modeling, and WikiText-103 language modeling. Our empirical results demonstrate that the ENN achieves accuracy and generalization performance broadly comparable to classical RNN, GRU, and LSTM architectures, with all models converging to similar accuracy and perplexity on the large-scale WikiText-103 task. At the same time, the ENN offers significant enhancements in interpretability through observable memory dynamics. Hebbian trace visualizations further reveal biologically plausible, structured memory formation processes, validating the potential of neuroscience-inspired mechanisms to inform the development of more interpretable and robust deep learning models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic forest transition model dynamics and parameter estimation via deep learning</title>
<link>https://arxiv.org/abs/2507.21486</link>
<guid>https://arxiv.org/abs/2507.21486</guid>
<content:encoded><![CDATA[
arXiv:2507.21486v1 Announce Type: cross 
Abstract: Forest transitions, characterized by dynamic shifts between forest, agricultural, and abandoned lands, are complex phenomena. This study developed a stochastic differential equation model to capture the intricate dynamics of these transitions. We established the existence of global positive solutions for the model and conducted numerical analyses to assess the impact of model parameters on deforestation incentives. To address the challenge of parameter estimation, we proposed a novel deep learning approach that estimates all model parameters from a single sample containing time-series observations of forest and agricultural land proportions. This innovative approach enables us to understand forest transition dynamics and deforestation trends at any future time.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifunctional physical reservoir computing in soft tensegrity robots</title>
<link>https://arxiv.org/abs/2507.21496</link>
<guid>https://arxiv.org/abs/2507.21496</guid>
<content:encoded><![CDATA[
arXiv:2507.21496v1 Announce Type: cross 
Abstract: Recent studies have demonstrated that the dynamics of physical systems can be utilized for the desired information processing under the framework of physical reservoir computing (PRC). Robots with soft bodies are examples of such physical systems, and their nonlinear body-environment dynamics can be used to compute and generate the motor signals necessary for the control of their own behavior. In this simulation study, we extend this approach to control and embed not only one but also multiple behaviors into a type of soft robot called a tensegrity robot. The resulting system, consisting of the robot and the environment, is a multistable dynamical system that converges to different attractors from varying initial conditions. Furthermore, attractor analysis reveals that there exist "untrained attractors" in the state space of the system outside the training data. These untrained attractors reflect the intrinsic properties and structures of the tensegrity robot and its interactions with the environment. The impacts of these recent findings in PRC remain unexplored in embodied AI research. We here illustrate their potential to understand various features of embodied cognition that have not been fully addressed to date.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Vectors: Monitoring and Controlling Character Traits in Language Models</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
arXiv:2507.21509v1 Announce Type: cross 
Abstract: Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Classification of User Requirements from Online Feedback -- A Replication Study</title>
<link>https://arxiv.org/abs/2507.21532</link>
<guid>https://arxiv.org/abs/2507.21532</guid>
<content:encoded><![CDATA[
arXiv:2507.21532v1 Announce Type: cross 
Abstract: Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Policy Stochasticity in Mutual Information Optimal Control of Linear Systems</title>
<link>https://arxiv.org/abs/2507.21543</link>
<guid>https://arxiv.org/abs/2507.21543</guid>
<content:encoded><![CDATA[
arXiv:2507.21543v1 Announce Type: cross 
Abstract: In recent years, mutual information optimal control has been proposed as an extension of maximum entropy optimal control. Both approaches introduce regularization terms to render the policy stochastic, and it is important to theoretically clarify the relationship between the temperature parameter (i.e., the coefficient of the regularization term) and the stochasticity of the policy. Unlike in maximum entropy optimal control, this relationship remains unexplored in mutual information optimal control. In this paper, we investigate this relationship for a mutual information optimal control problem (MIOCP) of discrete-time linear systems. After extending the result of a previous study of the MIOCP, we establish the existence of an optimal policy of the MIOCP, and then derive the respective conditions on the temperature parameter under which the optimal policy becomes stochastic and deterministic. Furthermore, we also derive the respective conditions on the temperature parameter under which the policy obtained by an alternating optimization algorithm becomes stochastic and deterministic. The validity of the theoretical results is demonstrated through numerical experiments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation</title>
<link>https://arxiv.org/abs/2507.21563</link>
<guid>https://arxiv.org/abs/2507.21563</guid>
<content:encoded><![CDATA[
arXiv:2507.21563v1 Announce Type: cross 
Abstract: Recommendation systems often suffer from data sparsity caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models (LLMs) and item textual descriptions to enrich interaction data. By few-shot prompting LLMs multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An em algorithm for quantum Boltzmann machines</title>
<link>https://arxiv.org/abs/2507.21569</link>
<guid>https://arxiv.org/abs/2507.21569</guid>
<content:encoded><![CDATA[
arXiv:2507.21569v1 Announce Type: cross 
Abstract: We develop a quantum version of the em algorithm for training quantum Boltzmann machines. The em algorithm is an information-geometric extension of the well-known expectation-maximization (EM) algorithm, offering a structured alternative to gradient-based methods with potential advantages in stability and convergence. We implement the algorithm on a semi-quantum restricted Boltzmann machine, where quantum effects are confined to the hidden layer. This structure enables analytical update rules while preserving quantum expressivity. Numerical experiments on benchmark datasets show that the proposed method achieves stable learning and outperforms gradient-based training in several cases. These results demonstrate the potential of information-geometric optimization for quantum machine learning, particularly in settings where standard methods struggle due to non-commutativity or vanishing gradients.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics</title>
<link>https://arxiv.org/abs/2507.21638</link>
<guid>https://arxiv.org/abs/2507.21638</guid>
<content:encoded><![CDATA[
arXiv:2507.21638v1 Announce Type: cross 
Abstract: The development of reinforcement learning (RL) algorithms has been largely driven by ambitious challenge tasks and benchmarks. Games have dominated RL benchmarks because they present relevant challenges, are inexpensive to run and easy to understand. While games such as Go and Atari have led to many breakthroughs, they often do not directly translate to real-world embodied applications. In recognising the need to diversify RL benchmarks and addressing complexities that arise in embodied interaction scenarios, we introduce Assistax: an open-source benchmark designed to address challenges arising in assistive robotics tasks. Assistax uses JAX's hardware acceleration for significant speed-ups for learning in physics-based simulations. In terms of open-loop wall-clock time, Assistax runs up to $370\times$ faster when vectorising training runs compared to CPU-based alternatives. Assistax conceptualises the interaction between an assistive robot and an active human patient using multi-agent RL to train a population of diverse partner agents against which an embodied robotic agent's zero-shot coordination capabilities can be tested. Extensive evaluation and hyperparameter tuning for popular continuous control RL and MARL algorithms provide reliable baselines and establish Assistax as a practical benchmark for advancing RL research for assistive robotics. The code is available at: https://github.com/assistive-autonomy/assistax.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whilter: A Whisper-based Data Filter for "In-the-Wild" Speech Corpora Using Utterance-level Multi-Task Classification</title>
<link>https://arxiv.org/abs/2507.21642</link>
<guid>https://arxiv.org/abs/2507.21642</guid>
<content:encoded><![CDATA[
arXiv:2507.21642v1 Announce Type: cross 
Abstract: Large-scale in-the-wild speech datasets have become more prevalent in recent years due to increased interest in models that can learn useful features from unlabelled data for tasks such as speech recognition or synthesis. These datasets often contain undesirable features, such as multiple speakers, non-target languages, and music, which may impact model learning. The Whilter model is proposed as a multitask solution to identify these undesirable samples. Whilter uses a Whisper encoder with an attention-based classifier to solve five diverse classification problems at once. In addition, an annotated dataset is published for a subset of two popular in-the-wild corpora. Whilter achieves F1 scores above 85% and equal error rates of 6.5% to 7.8% for three of five subtasks, outperforming a state-of-the-art BEATs classifier on speech-specific classes, with a notable decrease in processing time compared to a combination of single-task alternatives.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint Optimization and Machine Learning</title>
<link>https://arxiv.org/abs/2507.21684</link>
<guid>https://arxiv.org/abs/2507.21684</guid>
<content:encoded><![CDATA[
arXiv:2507.21684v1 Announce Type: cross 
Abstract: We present diffSPH, a novel open-source differentiable Smoothed Particle Hydrodynamics (SPH) framework developed entirely in PyTorch with GPU acceleration. diffSPH is designed centrally around differentiation to facilitate optimization and machine learning (ML) applications in Computational Fluid Dynamics~(CFD), including training neural networks and the development of hybrid models. Its differentiable SPH core, and schemes for compressible (with shock capturing and multi-phase flows), weakly compressible (with boundary handling and free-surface flows), and incompressible physics, enable a broad range of application areas. We demonstrate the framework's unique capabilities through several applications, including addressing particle shifting via a novel, target-oriented approach by minimizing physical and regularization loss terms, a task often intractable in traditional solvers. Further examples include optimizing initial conditions and physical parameters to match target trajectories, shape optimization, implementing a solver-in-the-loop setup to emulate higher-order integration, and demonstrating gradient propagation through hundreds of full simulation steps. Prioritizing readability, usability, and extensibility, this work offers a foundational platform for the CFD community to develop and deploy novel neural networks and adjoint optimization applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples</title>
<link>https://arxiv.org/abs/2507.21712</link>
<guid>https://arxiv.org/abs/2507.21712</guid>
<content:encoded><![CDATA[
arXiv:2507.21712v1 Announce Type: cross 
Abstract: This paper investigates what can be inferred about an arbitrary continuous probability distribution from a finite sample of $N$ observations drawn from it. The central finding is that the $N$ sorted sample points partition the real line into $N+1$ segments, each carrying an expected probability mass of exactly $1/(N+1)$. This non-parametric result, which follows from fundamental properties of order statistics, holds regardless of the underlying distribution's shape. This equal-probability partition yields a discrete entropy of $\log_2(N+1)$ bits, which quantifies the information gained from the sample and contrasts with Shannon's results for continuous variables. I compare this partition-based framework to the conventional ECDF and discuss its implications for robust non-parametric inference, particularly in density and tail estimation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Optimization on Tree Tensor Networks with Application in Machine Learning</title>
<link>https://arxiv.org/abs/2507.21726</link>
<guid>https://arxiv.org/abs/2507.21726</guid>
<content:encoded><![CDATA[
arXiv:2507.21726v1 Announce Type: cross 
Abstract: Tree tensor networks (TTNs) are widely used in low-rank approximation and quantum many-body simulation. In this work, we present a formal analysis of the differential geometry underlying TTNs. Building on this foundation, we develop efficient first- and second-order optimization algorithms that exploit the intrinsic quotient structure of TTNs. Additionally, we devise a backpropagation algorithm for training TTNs in a kernel learning setting. We validate our methods through numerical experiments on a representative machine learning task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum</title>
<link>https://arxiv.org/abs/2507.21728</link>
<guid>https://arxiv.org/abs/2507.21728</guid>
<content:encoded><![CDATA[
arXiv:2507.21728v1 Announce Type: cross 
Abstract: Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers (EDFAs) is essential for optimizing optical network performance, particularly as networks evolve toward multi-vendor solutions. In this work, we propose a generalized few-shot transfer learning architecture based on a Semi-Supervised Self-Normalizing Neural Network (SS-NN) that leverages internal EDFA features - such as VOA input or output power and attenuation, to improve gain spectrum prediction. Our SS-NN model employs a two-phase training strategy comprising unsupervised pre-training with noise-augmented measurements and supervised fine-tuning with a custom weighted MSE loss. Furthermore, we extend the framework with transfer learning (TL) techniques that enable both homogeneous (same-feature space) and heterogeneous (different-feature sets) model adaptation across booster, preamplifier, and ILA EDFAs. To address feature mismatches in heterogeneous TL, we incorporate a covariance matching loss to align second-order feature statistics between source and target domains. Extensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland testbeds demonstrate that the proposed approach significantly reduces the number of measurements requirements on the system while achieving lower mean absolute errors and improved error distributions compared to benchmark methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification</title>
<link>https://arxiv.org/abs/2507.21749</link>
<guid>https://arxiv.org/abs/2507.21749</guid>
<content:encoded><![CDATA[
arXiv:2507.21749v1 Announce Type: cross 
Abstract: Training neural networks can be challenging, especially as the complexity of the problem increases. Despite using wider or deeper networks, training them can be a tedious process, especially if a wrong choice of the hyperparameter is made. The learning rate is one of such crucial hyperparameters, which is usually kept static during the training process. Learning dynamics in complex systems often requires a more adaptive approach to the learning rate. This adaptability becomes crucial to effectively navigate varying gradients and optimize the learning process during the training process. In this paper, a dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the learning rate based on the loss values calculated during the training process. Experiments are conducted on problems related to physics-informed neural networks (PINNs) and image classification using multilayer perceptrons and convolutional neural networks, respectively. The results demonstrate that the proposed DLRS accelerates training and improves stability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified machine-learning framework for property prediction and time-evolution simulation of strained alloy microstructure</title>
<link>https://arxiv.org/abs/2507.21760</link>
<guid>https://arxiv.org/abs/2507.21760</guid>
<content:encoded><![CDATA[
arXiv:2507.21760v1 Announce Type: cross 
Abstract: We introduce a unified machine-learning framework designed to conveniently tackle the temporal evolution of alloy microstructures under the influence of an elastic field. This approach allows for the simultaneous extraction of elastic parameters from a short trajectory and for the prediction of further microstructure evolution under their influence. This is demonstrated by focusing on spinodal decomposition in the presence of a lattice mismatch eta, and by carrying out an extensive comparison between the ground-truth evolution supplied by phase field simulations and the predictions of suitable convolutional recurrent neural network architectures. The two tasks may then be performed subsequently into a cascade framework. Under a wide spectrum of misfit conditions, the here-presented cascade model accurately predicts eta and the full corresponding microstructure evolution, also when approaching critical conditions for spinodal decomposition. Scalability to larger computational domain sizes and mild extrapolation errors in time (for time sequences five times longer than the sampled ones during training) are demonstrated. The proposed framework is general and can be applied beyond the specific, prototypical system considered here as an example. Intriguingly, experimental videos could be used to infer unknown external parameters, prior to simulating further temporal evolution.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2507.21763</link>
<guid>https://arxiv.org/abs/2507.21763</guid>
<content:encoded><![CDATA[
arXiv:2507.21763v1 Announce Type: cross 
Abstract: We show that Generative Adversarial Networks (GANs) may be fruitfully exploited to learn stochastic dynamics, surrogating traditional models while capturing thermal fluctuations. Specifically, we showcase the application to a two-dimensional, many-particle system, focusing on surface-step fluctuations and on the related time-dependent roughness. After the construction of a dataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained to propagate stochastically the state of the system in time, allowing the generation of new sequences with a reduced computational cost. Modifications with respect to standard GANs, which facilitate convergence and increase accuracy, are discussed. The trained network is demonstrated to quantitatively reproduce equilibrium and kinetic properties, including scaling laws, with deviations of a few percent from the exact value. Extrapolation limits and future perspectives are critically discussed.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Generalization and Adaptation in Intensive Care with Anchor Regression</title>
<link>https://arxiv.org/abs/2507.21783</link>
<guid>https://arxiv.org/abs/2507.21783</guid>
<content:encoded><![CDATA[
arXiv:2507.21783v1 Announce Type: cross 
Abstract: The performance of predictive models in clinical settings often degrades when deployed in new hospitals due to distribution shifts. This paper presents a large-scale study of causality-inspired domain generalization on heterogeneous multi-center intensive care unit (ICU) data. We apply anchor regression and introduce anchor boosting, a novel, tree-based nonlinear extension, to a large dataset comprising 400,000 patients from nine distinct ICU databases. The anchor regularization consistently improves out-of-distribution performance, particularly for the most dissimilar target domains. The methods appear robust to violations of theoretical assumptions, such as anchor exogeneity. Furthermore, we propose a novel conceptual framework to quantify the utility of large external data datasets. By evaluating performance as a function of available target-domain data, we identify three regimes: (i) a domain generalization regime, where only the external model should be used, (ii) a domain adaptation regime, where refitting the external model is optimal, and (iii) a data-rich regime, where external data provides no additional value.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation</title>
<link>https://arxiv.org/abs/2507.21807</link>
<guid>https://arxiv.org/abs/2507.21807</guid>
<content:encoded><![CDATA[
arXiv:2507.21807v1 Announce Type: cross 
Abstract: Statistical learning methods for automated variable selection, such as LASSO, elastic nets, or gradient boosting, have become increasingly popular tools for building powerful prediction models. Yet, in practice, analyses are often complicated by missing data. The most widely used approach to address missingness is multiple imputation, which creates several completed datasets. However, there is an ongoing debate on how to perform model selection in the presence of multiple imputed datasets. Simple strategies, such as pooling models across datasets, have been shown to have suboptimal properties. Although more sophisticated methods exist, they are often difficult to implement and therefore not widely applied. In contrast, two recent approaches modify the regularization methods LASSO and elastic nets by defining a single loss function, resulting in a unified set of coefficients across imputations. Our key contribution is to extend this principle to the framework of component-wise gradient boosting by proposing MIBoost, a novel algorithm that employs a uniform variable-selection mechanism across imputed datasets. Simulation studies suggest that our approach yields prediction performance comparable to that of these recently proposed methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences</title>
<link>https://arxiv.org/abs/2507.21831</link>
<guid>https://arxiv.org/abs/2507.21831</guid>
<content:encoded><![CDATA[
arXiv:2507.21831v1 Announce Type: cross 
Abstract: LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representations in vision and language converge in a shared, multidimensional space of perceived similarities</title>
<link>https://arxiv.org/abs/2507.21871</link>
<guid>https://arxiv.org/abs/2507.21871</guid>
<content:encoded><![CDATA[
arXiv:2507.21871v1 Announce Type: cross 
Abstract: Humans can effortlessly describe what they see, yet establishing a shared representational format between vision and language remains a significant challenge. Emerging evidence suggests that human brain representations in both vision and language are well predicted by semantic feature spaces obtained from large language models (LLMs). This raises the possibility that sensory systems converge in their inherent ability to transform their inputs onto shared, embedding-like representational space. However, it remains unclear how such a space manifests in human behaviour. To investigate this, sixty-three participants performed behavioural similarity judgements separately on 100 natural scene images and 100 corresponding sentence captions from the Natural Scenes Dataset. We found that visual and linguistic similarity judgements not only converge at the behavioural level but also predict a remarkably similar network of fMRI brain responses evoked by viewing the natural scene images. Furthermore, computational models trained to map images onto LLM-embeddings outperformed both category-trained and AlexNet controls in explaining the behavioural similarity structure. These findings demonstrate that human visual and linguistic similarity judgements are grounded in a shared, modality-agnostic representational structure that mirrors how the visual system encodes experience. The convergence between sensory and artificial systems suggests a common capacity of how conceptual representations are formed-not as arbitrary products of first order, modality-specific input, but as structured representations that reflect the stable, relational properties of the external world.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline</title>
<link>https://arxiv.org/abs/2507.21886</link>
<guid>https://arxiv.org/abs/2507.21886</guid>
<content:encoded><![CDATA[
arXiv:2507.21886v1 Announce Type: cross 
Abstract: Pain is a complex condition affecting a large portion of the population. Accurate and consistent evaluation is essential for individuals experiencing pain, and it supports the development of effective and advanced management strategies. Automatic pain assessment systems provide continuous monitoring and support clinical decision-making, aiming to reduce distress and prevent functional decline. This study has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline that leverages respiration as the input signal and incorporates a highly efficient cross-attention transformer alongside a multi-windowing strategy. Extensive experiments demonstrate that respiration is a valuable physiological modality for pain assessment. Moreover, experiments revealed that compact and efficient models, when properly optimized, can achieve strong performance, often surpassing larger counterparts. The proposed multi-window approach effectively captures both short-term and long-term features, as well as global characteristics, thereby enhancing the model's representational capacity.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven quantum Koopman method for simulating nonlinear dynamics</title>
<link>https://arxiv.org/abs/2507.21890</link>
<guid>https://arxiv.org/abs/2507.21890</guid>
<content:encoded><![CDATA[
arXiv:2507.21890v1 Announce Type: cross 
Abstract: Quantum computation offers potential exponential speedups for simulating certain physical systems, but its application to nonlinear dynamics is inherently constrained by the requirement of unitary evolution. We propose the quantum Koopman method (QKM), a data-driven framework that bridges this gap through transforming nonlinear dynamics into linear unitary evolution in higher-dimensional observable spaces. Leveraging the Koopman operator theory to achieve a global linearization, our approach maps system states into a hierarchy of Hilbert spaces using a deep autoencoder. Within the linearized embedding spaces, the state representation is decomposed into modulus and phase components, and the evolution is governed by a set of unitary Koopman operators that act exclusively on the phase. These operators are constructed from diagonal Hamiltonians with coefficients learned from data, a structure designed for efficient implementation on quantum hardware. This architecture enables direct multi-step prediction, and the operator's computational complexity scales logarithmically with the observable space dimension. The QKM is validated across diverse nonlinear systems. Its predictions maintain relative errors below 6% for reaction-diffusion systems and shear flows, and capture key statistics in 2D turbulence. This work establishes a practical pathway for quantum-accelerated simulation of nonlinear phenomena, exploring a framework built on the synergy between deep learning for global linearization and quantum algorithms for unitary dynamics evolution.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Content Classification Approach for GitHub Repositories by the README Files</title>
<link>https://arxiv.org/abs/2507.21899</link>
<guid>https://arxiv.org/abs/2507.21899</guid>
<content:encoded><![CDATA[
arXiv:2507.21899v1 Announce Type: cross 
Abstract: GitHub is the world's most popular platform for storing, sharing, and managing code. Every GitHub repository has a README file associated with it. The README files should contain project-related information as per the recommendations of GitHub to support the usage and improvement of repositories. However, GitHub repository owners sometimes neglected these recommendations. This prevents a GitHub repository from reaching its full potential. This research posits that the comprehensiveness of a GitHub repository's README file significantly influences its adoption and utilization, with a lack of detail potentially hindering its full potential for widespread engagement and impact within the research community. Large Language Models (LLMs) have shown great performance in many text-based tasks including text classification, text generation, text summarization and text translation. In this study, an approach is developed to fine-tune LLMs for automatically classifying different sections of GitHub README files. Three encoder-only LLMs are utilized, including BERT, DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a gold-standard dataset consisting of 4226 README file sections. This approach outperforms current state-of-the-art methods and has achieved an overall F1 score of 0.98. Moreover, we have also investigated the use of Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) and shown an economical alternative to full fine-tuning without compromising much performance. The results demonstrate the potential of using LLMs in designing an automatic classifier for categorizing the content of GitHub README files. Consequently, this study contributes to the development of automated tools for GitHub repositories to improve their identifications and potential usages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Data Requirements for Sequence-Property Prediction in Copolymer Compatibilizers via Deep Neural Network Tuning</title>
<link>https://arxiv.org/abs/2507.21902</link>
<guid>https://arxiv.org/abs/2507.21902</guid>
<content:encoded><![CDATA[
arXiv:2507.21902v1 Announce Type: cross 
Abstract: Synthetic sequence-controlled polymers promise to transform polymer science by combining the chemical versatility of synthetic polymers with the precise sequence-mediated functionality of biological proteins. However, design of these materials has proven extraordinarily challenging, because they lack the massive datasets of closely related evolved molecules that accelerate design of proteins. Here we report on a new Artifical Intelligence strategy to dramatically reduce the amount of data necessary to accelerate these materials' design. We focus on data connecting the repeat-unit-sequence of a \emph{compatibilizer} molecule to its ability to reduce the interfacial tension between distinct polymer domains. The optimal sequence of these molecules, which are essential for applications such as mixed-waste polymer recycling, depends strongly on variables such as concentration and chemical details of the polymer. With current methods, this would demand an entirely distinct dataset to enable design at each condition. Here we show that a deep neural network trained on low-fidelity data for sequence/interfacial tension relations at one set of conditions can be rapidly tuned to make higher-fidelity predictions at a distinct set of conditions, requiring far less data that would ordinarily be needed. This priming-and-tuning approach should allow a single low-fidelity parent dataset to dramatically accelerate prediction and design in an entire constellation of related systems. In the long run, it may also provide an approach to bootstrapping quantitative atomistic design with AI insights from fast, coarse simulations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deepfake Detectors in the Wild</title>
<link>https://arxiv.org/abs/2507.21905</link>
<guid>https://arxiv.org/abs/2507.21905</guid>
<content:encoded><![CDATA[
arXiv:2507.21905v1 Announce Type: cross 
Abstract: Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/messlav/Deepfake-Detectors-in-the-Wild.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepGo: Predictive Directed Greybox Fuzzing</title>
<link>https://arxiv.org/abs/2507.21952</link>
<guid>https://arxiv.org/abs/2507.21952</guid>
<content:encoded><![CDATA[
arXiv:2507.21952v1 Announce Type: cross 
Abstract: The state-of-the-art DGF techniques redefine and optimize the fitness metric to reach the target sites precisely and quickly. However, optimizations for fitness metrics are mainly based on heuristic algorithms, which usually rely on historical execution information and lack foresight on paths that have not been exercised yet. Thus, those hard-to-execute paths with complex constraints would hinder DGF from reaching the targets, making DGF less efficient. In this paper, we propose DeepGo, a predictive directed grey-box fuzzer that can combine historical and predicted information to steer DGF to reach the target site via an optimal path. We first propose the path transition model, which models DGF as a process of reaching the target site through specific path transition sequences. The new seed generated by mutation would cause the path transition, and the path corresponding to the high-reward path transition sequence indicates a high likelihood of reaching the target site through it. Then, to predict the path transitions and the corresponding rewards, we use deep neural networks to construct a Virtual Ensemble Environment (VEE), which gradually imitates the path transition model and predicts the rewards of path transitions that have not been taken yet. To determine the optimal path, we develop a Reinforcement Learning for Fuzzing (RLF) model to generate the transition sequences with the highest sequence rewards. The RLF model can combine historical and predicted path transitions to generate the optimal path transition sequences, along with the policy to guide the mutation strategy of fuzzing. Finally, to exercise the high-reward path transition sequence, we propose the concept of an action group, which comprehensively optimizes the critical steps of fuzzing to realize the optimal path to reach the target efficiently.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data &amp; Activities</title>
<link>https://arxiv.org/abs/2507.21964</link>
<guid>https://arxiv.org/abs/2507.21964</guid>
<content:encoded><![CDATA[
arXiv:2507.21964v1 Announce Type: cross 
Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical direction in smart home research -- considering its impact on making HAR systems work across smart homes having diverse sensing modalities, layouts, and activities of interest. The state-of-the-art solutions along this direction are based on generating natural language descriptions of the sensor data and feeding it via a carefully crafted prompt to the LLM to perform classification. Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry several risks, including privacy invasion, reliance on an external service, and inconsistent predictions due to version changes, making a case for alternative zero-shot HAR methods that do not require prompting the LLMs. In this paper, we propose one such solution that models sensor data and activities using natural language, leveraging its embeddings to perform zero-shot classification and thereby bypassing the need to prompt the LLMs for activity predictions. The impact of our work lies in presenting a detailed case study on six datasets, highlighting how language modeling can bolster HAR systems in zero-shot recognition.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Kuramoto Oscillator Network for Dense Associative Memory</title>
<link>https://arxiv.org/abs/2507.21984</link>
<guid>https://arxiv.org/abs/2507.21984</guid>
<content:encoded><![CDATA[
arXiv:2507.21984v1 Announce Type: cross 
Abstract: Networks of phase oscillators can serve as dense associative memories if they incorporate higher-order coupling beyond the classical Kuramoto model's pairwise interactions. Here we introduce a generalized Kuramoto model with combined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling, inspired by dense Hopfield memory theory. Using mean-field theory and its dynamical approximation, we obtain a phase diagram for dense associative memory model that exhibits a tricritical point at which the continuous onset of memory retrieval is supplanted by a discontinuous, hysteretic transition. In the quartic-dominated regime, the system supports bistable phase-locked states corresponding to stored memory patterns, with a sizable energy barrier between memory and incoherent states. We analytically determine this bistable region and show that the escape time from a memory state (due to noise) grows exponentially with network size, indicating robust storage. Extending the theory to finite memory load, we show that higher-order couplings achieve superlinear scaling of memory capacity with system size, far exceeding the limit of pairwise-only oscillators. Large-scale simulations of the oscillator network confirm our theoretical predictions, demonstrating rapid pattern retrieval and robust storage of many phase patterns. These results bridge the Kuramoto synchronization with modern Hopfield memories, pointing toward experimental realization of high-capacity, analog associative memory in oscillator systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Staining and locking computer vision models without retraining</title>
<link>https://arxiv.org/abs/2507.22000</link>
<guid>https://arxiv.org/abs/2507.22000</guid>
<content:encoded><![CDATA[
arXiv:2507.22000v1 Announce Type: cross 
Abstract: We introduce new methods of staining and locking computer vision models, to protect their owners' intellectual property. Staining, also known as watermarking, embeds secret behaviour into a model which can later be used to identify it, while locking aims to make a model unusable unless a secret trigger is inserted into input images. Unlike existing methods, our algorithms can be used to stain and lock pre-trained models without requiring fine-tuning or retraining, and come with provable, computable guarantees bounding their worst-case false positive rates. The stain and lock are implemented by directly modifying a small number of the model's weights and have minimal impact on the (unlocked) model's performance. Locked models are unlocked by inserting a small `trigger patch' into the corner of the input image. We present experimental results showing the efficacy of our methods and demonstrating their practical performance on a variety of computer vision models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform</title>
<link>https://arxiv.org/abs/2507.22010</link>
<guid>https://arxiv.org/abs/2507.22010</guid>
<content:encoded><![CDATA[
arXiv:2507.22010v1 Announce Type: cross 
Abstract: In this work, we explore the structure of the embedding space of a transformer model trained for playing a particular reinforcement learning (RL) game. Specifically, we investigate how a transformer-based Proximal Policy Optimization (PPO) model embeds visual inputs in a simple environment where an agent must collect "coins" while avoiding dynamic obstacles consisting of "spotlights." By adapting Robinson et al.'s study of the volume growth transform for LLMs to the RL setting, we find that the token embedding space for our visual coin collecting game is also not a manifold, and is better modeled as a stratified space, where local dimension can vary from point to point. We further strengthen Robinson's method by proving that fairly general volume growth curves can be realized by stratified spaces. Finally, we carry out an analysis that suggests that as an RL agent acts, its latent representation alternates between periods of low local dimension, while following a fixed sub-strategy, and bursts of high local dimension, where the agent achieves a sub-goal (e.g., collecting an object) or where the environmental complexity increases (e.g., more obstacles appear). Consequently, our work suggests that the distribution of dimensions in a stratified latent space may provide a new geometric indicator of complexity for RL games.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserBench: An Interactive Gym Environment for User-Centric Agents</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
arXiv:2507.22034v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Quantum Image Processing</title>
<link>https://arxiv.org/abs/2507.22039</link>
<guid>https://arxiv.org/abs/2507.22039</guid>
<content:encoded><![CDATA[
arXiv:2507.22039v1 Announce Type: cross 
Abstract: In the era of big data and artificial intelligence, the increasing volume of data and the demand to solve more and more complex computational challenges are two driving forces for improving the efficiency of data storage, processing and analysis. Quantum image processing (QIP) is an interdisciplinary field between quantum information science and image processing, which has the potential to alleviate some of these challenges by leveraging the power of quantum computing. In this work, we compare and examine the compression properties of four different Quantum Image Representations (QImRs): namely, Tensor Network Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding (QPIE). Our simulations show that FRQI performs a higher compression of image information than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off between accuracy and memory in binary classification problems, evaluating the performance of quantum kernels based on QImRs compared to the classical linear kernel. Our results indicate that quantum kernels provide comparable classification average accuracy but require exponentially fewer resources for image storage.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical mixtures of Gaussians for combined dimensionality reduction and clustering</title>
<link>https://arxiv.org/abs/2206.04841</link>
<guid>https://arxiv.org/abs/2206.04841</guid>
<content:encoded><![CDATA[
arXiv:2206.04841v2 Announce Type: replace 
Abstract: We introduce hierarchical mixtures of Gaussians (HMoGs), which unify dimensionality reduction and clustering into a single probabilistic model. HMoGs provide closed-form expressions for the model likelihood, exact inference over latent states and cluster membership, and exact algorithms for maximum-likelihood optimization. The novel exponential family parameterization of HMoGs greatly reduces their computational complexity relative to similar model-based methods, allowing them to efficiently model hundreds of latent dimensions, and thereby capture additional structure in high-dimensional data. We demonstrate HMoGs on synthetic experiments and MNIST, and show how joint optimization of dimensionality reduction and clustering facilitates increased model performance. We also explore how sparsity-constrained dimensionality reduction can further improve clustering performance while encouraging interpretability. By bridging classical statistical modelling with the scale of modern data and compute, HMoGs offer a practical approach to high-dimensional clustering that preserves statistical rigour, interpretability, and uncertainty quantification that is often missing from embedding-based, variational, and self-supervised methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantize Once, Train Fast: Allreduce-Compatible Compression with Provable Guarantees</title>
<link>https://arxiv.org/abs/2305.18627</link>
<guid>https://arxiv.org/abs/2305.18627</guid>
<content:encoded><![CDATA[
arXiv:2305.18627v2 Announce Type: replace 
Abstract: Distributed training enables large-scale deep learning, but suffers from high communication overhead, especially as models and datasets grow. Gradient compression, particularly quantization, is a promising approach to mitigate this bottleneck. However, existing quantization schemes are often incompatible with Allreduce, the dominant communication primitive in distributed deep learning, and many prior solutions rely on heuristics without theoretical guarantees. We introduce Global-QSGD, an Allreduce-compatible gradient quantization method that leverages global norm scaling to reduce communication overhead while preserving accuracy. Global-QSGD is backed by rigorous theoretical analysis, extending standard unbiased compressor frameworks to establish formal convergence guarantees. Additionally, we develop a performance model to evaluate its impact across different hardware configurations. Extensive experiments on NVLink, PCIe, and large-scale cloud environments show that Global-QSGD accelerates distributed training by up to 3.51% over baseline quantization methods, making it a practical and efficient solution for large-scale deep learning workloads.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges</title>
<link>https://arxiv.org/abs/2406.06736</link>
<guid>https://arxiv.org/abs/2406.06736</guid>
<content:encoded><![CDATA[
arXiv:2406.06736v3 Announce Type: replace 
Abstract: The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data</title>
<link>https://arxiv.org/abs/2406.10521</link>
<guid>https://arxiv.org/abs/2406.10521</guid>
<content:encoded><![CDATA[
arXiv:2406.10521v4 Announce Type: replace 
Abstract: In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity. To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2407.15549</link>
<guid>https://arxiv.org/abs/2407.15549</guid>
<content:encoded><![CDATA[
arXiv:2407.15549v3 Announce Type: replace 
Abstract: Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Manifold Structure Using Ollivier-Ricci Curvature</title>
<link>https://arxiv.org/abs/2410.01149</link>
<guid>https://arxiv.org/abs/2410.01149</guid>
<content:encoded><![CDATA[
arXiv:2410.01149v2 Announce Type: replace 
Abstract: We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting</title>
<link>https://arxiv.org/abs/2410.03805</link>
<guid>https://arxiv.org/abs/2410.03805</guid>
<content:encoded><![CDATA[
arXiv:2410.03805v3 Announce Type: replace 
Abstract: Transformers have become the leading choice in natural language processing over other deep learning architectures. This trend has also permeated the field of time series analysis, especially for long-horizon forecasting, showcasing promising results both in performance and running time.
  In this paper, we introduce Local Attention Mechanism (LAM), an efficient attention mechanism tailored for time series analysis. This mechanism exploits the continuity properties of time series to reduce the number of attention scores computed. We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms. We also note the lack of proper datasets to evaluate long-horizon forecast models. Thus, we propose a novel set of datasets to improve the evaluation of models addressing long-horizon forecasting challenges.
  Our experimental analysis demonstrates that the vanilla transformer architecture magnified with LAM surpasses state-of-the-art models, including the vanilla attention mechanism. These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can sparse autoencoders make sense of gene expression latent variable models?</title>
<link>https://arxiv.org/abs/2410.11468</link>
<guid>https://arxiv.org/abs/2410.11468</guid>
<content:encoded><![CDATA[
arXiv:2410.11468v3 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) have lately been used to uncover interpretable latent features in large language models. By projecting dense embeddings into a much higher-dimensional and sparse space, learned features become disentangled and easier to interpret. This work explores the potential of SAEs for decomposing embeddings in complex and high-dimensional biological data. Using simulated data, it outlines the efficacy, hyperparameter landscape, and limitations of SAEs when it comes to extracting ground truth generative variables from latent space. The application to embeddings from pretrained single-cell models shows that SAEs can find and steer key biological processes and even uncover subtle biological signals that might otherwise be missed. This work further introduces scFeatureLens, an automated interpretability approach for linking SAE features and biological concepts from gene sets to enable large-scale analysis and hypothesis generation in single-cell gene expression models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks</title>
<link>https://arxiv.org/abs/2410.22296</link>
<guid>https://arxiv.org/abs/2410.22296</guid>
<content:encoded><![CDATA[
arXiv:2410.22296v5 Announce Type: replace 
Abstract: Although large language models (LLMs) have shown promise in biomolecule optimization problems, they incur heavy computational costs and struggle to satisfy precise constraints. On the other hand, specialized solvers like LaMBO-2 offer efficiency and fine-grained control but require more domain expertise. Comparing these approaches is challenging due to expensive laboratory validation and inadequate synthetic benchmarks. We address this by introducing Ehrlich functions, a synthetic test suite that captures the geometric structure of biophysical sequence optimization problems. With prompting alone, off-the-shelf LLMs struggle to optimize Ehrlich functions. In response, we propose LLOME (Language Model Optimization with Margin Expectation), a bilevel optimization routine for online black-box optimization. When combined with a novel preference learning loss, we find LLOME can not only learn to solve some Ehrlich functions, but can even perform as well as or better than LaMBO-2 on moderately difficult Ehrlich variants. However, LLMs also exhibit some likelihood-reward miscalibration and struggle without explicit rewards. Our results indicate LLMs can occasionally provide significant benefits, but specialized solvers are still competitive and incur less overhead.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HI-PMK: A Data-Dependent Kernel for Incomplete Heterogeneous Data Representation</title>
<link>https://arxiv.org/abs/2501.04300</link>
<guid>https://arxiv.org/abs/2501.04300</guid>
<content:encoded><![CDATA[
arXiv:2501.04300v3 Announce Type: replace 
Abstract: Handling incomplete and heterogeneous data remains a central challenge in real-world machine learning, where missing values may follow complex mechanisms (MCAR, MAR, MNAR) and features can be of mixed types (numerical and categorical). Existing methods often rely on imputation, which may introduce bias or privacy risks, or fail to jointly address data heterogeneity and structured missingness. We propose the \textbf{H}eterogeneous \textbf{I}ncomplete \textbf{P}robability \textbf{M}ass \textbf{K}ernel (\textbf{HI-PMK}), a novel data-dependent representation learning approach that eliminates the need for imputation. HI-PMK introduces two key innovations: (1) a probability mass-based dissimilarity measure that adapts to local data distributions across heterogeneous features (numerical, ordinal, nominal), and (2) a missingness-aware uncertainty strategy (MaxU) that conservatively handles all three missingness mechanisms by assigning maximal plausible dissimilarity to unobserved entries. Our approach is privacy-preserving, scalable, and readily applicable to downstream tasks such as classification and clustering. Extensive experiments on over 15 benchmark datasets demonstrate that HI-PMK consistently outperforms traditional imputation-based pipelines and kernel methods across a wide range of missing data settings. Code is available at: https://github.com/echoid/Incomplete-Heter-Kernel
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training</title>
<link>https://arxiv.org/abs/2501.07237</link>
<guid>https://arxiv.org/abs/2501.07237</guid>
<content:encoded><![CDATA[
arXiv:2501.07237v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Memory-Efficient Transformer-Based Model Training in AI for Science</title>
<link>https://arxiv.org/abs/2501.11847</link>
<guid>https://arxiv.org/abs/2501.11847</guid>
<content:encoded><![CDATA[
arXiv:2501.11847v2 Announce Type: replace 
Abstract: Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews transformer-based LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. This survey systematically reviews and categorizes memory-efficient pre-training techniques for large-scale transformers, including algorithm-level, system-level, and hardware-software co-optimization. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. By bridging model efficiency and scientific application needs, we hope to provide insights for scalable and cost-effective LLM training in AI for science.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</title>
<link>https://arxiv.org/abs/2502.12207</link>
<guid>https://arxiv.org/abs/2502.12207</guid>
<content:encoded><![CDATA[
arXiv:2502.12207v2 Announce Type: replace 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://github.com/LMBTough/PAR
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-branch of Attention Yields Accurate Results for Tabular Data</title>
<link>https://arxiv.org/abs/2502.12507</link>
<guid>https://arxiv.org/abs/2502.12507</guid>
<content:encoded><![CDATA[
arXiv:2502.12507v2 Announce Type: replace 
Abstract: Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Multi-Branch of Attention (MBA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A calibration test for evaluating set-based epistemic uncertainty representations</title>
<link>https://arxiv.org/abs/2502.16299</link>
<guid>https://arxiv.org/abs/2502.16299</guid>
<content:encoded><![CDATA[
arXiv:2502.16299v2 Announce Type: replace 
Abstract: The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set's predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conceptualizing Uncertainty: A Concept-based Approach to Explaining Uncertainty</title>
<link>https://arxiv.org/abs/2503.03443</link>
<guid>https://arxiv.org/abs/2503.03443</guid>
<content:encoded><![CDATA[
arXiv:2503.03443v2 Announce Type: replace 
Abstract: Uncertainty in machine learning refers to the degree of confidence or lack thereof in a model's predictions. While uncertainty quantification methods exist, explanations of uncertainty, especially in high-dimensional settings, remain an open challenge. Existing work focuses on feature attribution approaches which are restricted to local explanations. Understanding uncertainty, its origins, and characteristics on a global scale is crucial for enhancing interpretability and trust in a model's predictions. In this work, we propose to explain the uncertainty in high-dimensional data classification settings by means of concept activation vectors which give rise to local and global explanations of uncertainty. We demonstrate the utility of the generated explanations by leveraging them to refine and improve our model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQuat: Subspace-orthogonal KV Cache Quantization</title>
<link>https://arxiv.org/abs/2503.24358</link>
<guid>https://arxiv.org/abs/2503.24358</guid>
<content:encoded><![CDATA[
arXiv:2503.24358v2 Announce Type: replace 
Abstract: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compton Form Factor Extraction using Quantum Deep Neural Networks</title>
<link>https://arxiv.org/abs/2504.15458</link>
<guid>https://arxiv.org/abs/2504.15458</guid>
<content:encoded><![CDATA[
arXiv:2504.15458v2 Announce Type: replace 
Abstract: We present an extraction of Compton Form Factors (CFFs) from Deeply Virtual Compton Scattering (DVCS) experiments conducted at Thomas Jefferson National Accelerator Facility, utilizing Quantum Deep Neural Networks (QDNNs). The analysis employs the standard Belitsky, Kirchner, and M\"uller formalism at twist-two, complemented by a fitting procedure designed to minimize model dependence in a manner analogous to conventional local fits. A pseudodata extraction test of the CFFs is performed using both Classical Deep Neural Networks (CDNNs) and QDNNs, with a detailed comparative analysis. Results indicate that QDNNs can outperform CDNNs in particular cases, offering enhanced predictive accuracy and precision even with limited model complexity. Motivated by this, we develop a metric to quantify the extent of the quantum advantage based on characteristics of DVCS experimental data. These findings underscore the promising role of QDNNs in advancing future investigations into multidimensional parton distributions and hadronic physics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLAMAPIE: Proactive In-Ear Conversation Assistants</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
arXiv:2505.04066v2 Announce Type: replace 
Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An $\tilde{O}$ptimal Differentially Private Learner for Concept Classes with VC Dimension 1</title>
<link>https://arxiv.org/abs/2505.06581</link>
<guid>https://arxiv.org/abs/2505.06581</guid>
<content:encoded><![CDATA[
arXiv:2505.06581v2 Announce Type: replace 
Abstract: We present the first nearly optimal differentially private PAC learner for any concept class with VC dimension 1 and Littlestone dimension $d$. Our algorithm achieves the sample complexity of $\tilde{O}_{\varepsilon,\delta,\alpha,\delta}(\log^* d)$, nearly matching the lower bound of $\Omega(\log^* d)$ proved by Alon et al. [STOC19]. Prior to our work, the best known upper bound is $\tilde{O}(VC\cdot d^5)$ for general VC classes, as shown by Ghazi et al. [STOC21].
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.10774</link>
<guid>https://arxiv.org/abs/2505.10774</guid>
<content:encoded><![CDATA[
arXiv:2505.10774v2 Announce Type: replace 
Abstract: Time series forecasting is important for applications spanning energy markets, climate analysis, and traffic management. However, existing methods struggle to effectively integrate exogenous texts and align them with the probabilistic nature of large language models (LLMs). Current approaches either employ shallow text-time series fusion via basic prompts or rely on deterministic numerical decoding that conflict with LLMs' token-generation paradigm, which limits contextual awareness and distribution modeling. To address these limitations, we propose CAPTime, a context-aware probabilistic multimodal time series forecasting method that leverages text-informed abstraction and autoregressive LLM decoding. Our method first encodes temporal patterns using a pretrained time series encoder, then aligns them with textual contexts via learnable interactions to produce joint multimodal representations. By combining a mixture of distribution experts with frozen LLMs, we enable context-aware probabilistic forecasting while preserving LLMs' inherent distribution modeling capabilities. Experiments on diverse time series forecasting tasks demonstrate the superior accuracy and generalization of CAPTime, particularly in multimodal scenarios. Additional analysis highlights its robustness in data-scarce scenarios through hybrid probabilistic decoding.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11864</link>
<guid>https://arxiv.org/abs/2505.11864</guid>
<content:encoded><![CDATA[
arXiv:2505.11864v3 Announce Type: replace 
Abstract: As generative agents become increasingly capable, alignment of their behavior with complex human values remains a fundamental challenge. Existing approaches often simplify human intent through reduction to a scalar reward, overlooking the multi-faceted nature of human feedback. In this work, we introduce a theoretical framework for preference-based Multi-Objective Inverse Reinforcement Learning (MO-IRL), where human preferences are modeled as latent vector-valued reward functions. We formalize the problem of recovering a Pareto-optimal reward representation from noisy preference queries and establish conditions for identifying the underlying multi-objective structure. We derive tight sample complexity bounds for recovering $\epsilon$-approximations of the Pareto front and introduce a regret formulation to quantify suboptimality in this multi-objective setting. Furthermore, we propose a provably convergent algorithm for policy optimization using preference-inferred reward cones. Our results bridge the gap between practical alignment techniques and theoretical guarantees, providing a principled foundation for learning aligned behaviors in a high-dimension and value-pluralistic environment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v2 Announce Type: replace 
Abstract: Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is effective and has attracted significant attention. However, it is computationally prohibitive due to massive, data-hungry text-based reward models. By changing the data source from text to hidden states, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight technique that leverages the rich information embedded in LLM hidden states to address these issues, which operates on token-level and consists of only linear layers. Extensive experiments show that SWIFT outperforms baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training, demonstrating significant efficiency improvement. SWIFT's robust scalability, applicability to some closed-source models via logits, and ability to be combined with traditional reward models to yield further performance gains underscore its practical value.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Adopt Constraints Over Penalties in Deep Learning</title>
<link>https://arxiv.org/abs/2505.20628</link>
<guid>https://arxiv.org/abs/2505.20628</guid>
<content:encoded><![CDATA[
arXiv:2505.20628v3 Announce Type: replace 
Abstract: Recent efforts to develop trustworthy AI systems with accountability guarantees have led to widespread use of machine learning formulations incorporating external requirements, or constraints. These requirements are often enforced via penalization--adding fixed-weight terms to the task loss. We argue this approach is fundamentally ill-suited since there may be no penalty coefficient that simultaneously ensures constraint satisfaction and optimal constrained performance, i.e., that truly solves the constrained problem. Moreover, tuning these coefficients requires costly trial-and-error, incurring significant time and computational overhead. We, therefore, advocate for broader adoption of tailored constrained optimization methods--such as the Lagrangian approach, which jointly optimizes the penalization "coefficients" (the Lagrange multipliers) and the model parameters. Such methods (i) truly solve the constrained problem and do so accountably, by clearly defining feasibility and verifying when it is achieved, (ii) eliminate the need for extensive penalty tuning, and (iii) integrate seamlessly with modern deep learning pipelines.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
arXiv:2505.20734v5 Announce Type: replace 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised risk factor identification across cancer types and data modalities via explainable artificial intelligence</title>
<link>https://arxiv.org/abs/2506.12944</link>
<guid>https://arxiv.org/abs/2506.12944</guid>
<content:encoded><![CDATA[
arXiv:2506.12944v3 Announce Type: replace 
Abstract: Risk stratification is a key tool in clinical decision-making, yet current approaches often fail to translate sophisticated survival analysis into actionable clinical criteria. We present a novel method for unsupervised machine learning that directly optimizes for survival heterogeneity across patient clusters through a differentiable adaptation of the multivariate logrank statistic. Unlike most existing methods that rely on proxy metrics, our approach represents novel methodology for training any neural network architecture on any data modality to identify prognostically distinct patient groups. We thoroughly evaluate the method in simulation experiments and demonstrate its utility in practice by applying it to two distinct cancer types: analyzing laboratory parameters from multiple myeloma patients and computed tomography images from non-small cell lung cancer patients, identifying prognostically distinct patient subgroups with significantly different survival outcomes in both cases. Post-hoc explainability analyses uncover clinically meaningful features determining the group assignments which align well with established risk factors and thus lend strong weight to the methods utility. This pan-cancer, model-agnostic approach represents a valuable advancement in clinical risk stratification, enabling the discovery of novel prognostic signatures across diverse data types while providing interpretable results that promise to complement treatment personalization and clinical decision-making in oncology and beyond.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPreNets: High-Precision Neural Networks through Progressive Training</title>
<link>https://arxiv.org/abs/2506.15064</link>
<guid>https://arxiv.org/abs/2506.15064</guid>
<content:encoded><![CDATA[
arXiv:2506.15064v2 Announce Type: replace 
Abstract: Deep neural networks are powerful tools for solving nonlinear problems in science and engineering, but training highly accurate models becomes challenging as problem complexity increases. Non-convex optimization and numerous hyperparameters to tune make performance improvement difficult, and traditional approaches often prioritize minimizing mean squared error (MSE) while overlooking $L^{\infty}$ error, which is the critical focus in many applications. To address these challenges, we present a progressive framework for training and tuning high-precision neural networks (HiPreNets). Our approach refines a previously explored staged training technique for neural networks that improves an existing fully connected neural network by sequentially learning its prediction residuals using additional networks, leading to improved overall accuracy. We discuss how to take advantage of the structure of the residuals to guide the choice of loss function, number of parameters to use, and ways to introduce adaptive data sampling techniques. We validate our framework's effectiveness through several benchmark problems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning</title>
<link>https://arxiv.org/abs/2506.20031</link>
<guid>https://arxiv.org/abs/2506.20031</guid>
<content:encoded><![CDATA[
arXiv:2506.20031v2 Announce Type: replace 
Abstract: Operations in disaster response, search \& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Heterogeneous Multi-dimensional Data : A Comparative Study</title>
<link>https://arxiv.org/abs/2507.00090</link>
<guid>https://arxiv.org/abs/2507.00090</guid>
<content:encoded><![CDATA[
arXiv:2507.00090v3 Announce Type: replace 
Abstract: Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"So, Tell Me About Your Policy...": Distillation of interpretable policies from Deep Reinforcement Learning agents</title>
<link>https://arxiv.org/abs/2507.07848</link>
<guid>https://arxiv.org/abs/2507.07848</guid>
<content:encoded><![CDATA[
arXiv:2507.07848v2 Announce Type: replace 
Abstract: Recent advances in Reinforcement Learning (RL) largely benefit from the inclusion of Deep Neural Networks, boosting the number of novel approaches proposed in the field of Deep Reinforcement Learning (DRL). These techniques demonstrate the ability to tackle complex games such as Atari, Go, and other real-world applications, including financial trading. Nevertheless, a significant challenge emerges from the lack of interpretability, particularly when attempting to comprehend the underlying patterns learned, the relative importance of the state features, and how they are integrated to generate the policy's output. For this reason, in mission-critical and real-world settings, it is often preferred to deploy a simpler and more interpretable algorithm, although at the cost of performance. In this paper, we propose a novel algorithm, supported by theoretical guarantees, that can extract an interpretable policy (e.g., a linear policy) without disregarding the peculiarities of expert behavior. This result is obtained by considering the advantage function, which includes information about why an action is superior to the others. In contrast to previous works, our approach enables the training of an interpretable policy using previously collected experience. The proposed algorithm is empirically evaluated on classic control environments and on a financial trading scenario, demonstrating its ability to extract meaningful information from complex expert policies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TolerantECG: A Foundation Model for Imperfect Electrocardiogram</title>
<link>https://arxiv.org/abs/2507.09887</link>
<guid>https://arxiv.org/abs/2507.09887</guid>
<content:encoded><![CDATA[
arXiv:2507.09887v2 Announce Type: replace 
Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning</title>
<link>https://arxiv.org/abs/2507.14322</link>
<guid>https://arxiv.org/abs/2507.14322</guid>
<content:encoded><![CDATA[
arXiv:2507.14322v2 Announce Type: replace 
Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving collaborative AI, but its decentralized nature creates significant vulnerabilities to model poisoning attacks. While numerous static defenses exist, their effectiveness is highly context-dependent, often failing against adaptive adversaries or in heterogeneous data environments. This paper introduces FedStrategist, a novel meta-learning framework that reframes robust aggregation as a real-time, cost-aware control problem. We design a lightweight contextual bandit agent that dynamically selects the optimal aggregation rule from an arsenal of defenses based on real-time diagnostic metrics. Through comprehensive experiments, we demonstrate that no single static rule is universally optimal. We show that our adaptive agent successfully learns superior policies across diverse scenarios, including a ``Krum-favorable" environment and against a sophisticated "stealth" adversary designed to neutralize specific diagnostic signals. Critically, we analyze the paradoxical scenario where a non-robust baseline achieves high but compromised accuracy, and demonstrate that our agent learns a conservative policy to prioritize model integrity. Furthermore, we prove the agent's policy is controllable via a single "risk tolerance" parameter, allowing practitioners to explicitly manage the trade-off between performance and security. Our work provides a new, practical, and analyzable approach to creating resilient and intelligent decentralized AI systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction accuracy versus rescheduling flexibility in elective surgery management</title>
<link>https://arxiv.org/abs/2507.15566</link>
<guid>https://arxiv.org/abs/2507.15566</guid>
<content:encoded><![CDATA[
arXiv:2507.15566v2 Announce Type: replace 
Abstract: The availability of downstream resources plays is critical in planning the admission of elective surgery patients. The most crucial one is inpatient beds. To ensure bed availability, hospitals may use machine learning (ML) models to predict patients' length-of-stay (LOS) in the admission planning stage. However, the real value of the LOS for each patient may differ from the predicted one, potentially making the schedule infeasible. To address such infeasibilities, it is possible to implement rescheduling strategies that take advantage of operational flexibility. For example, planners may postpone admission dates, relocate patients to different wards, or even transfer patients who are already admitted among wards. A straightforward assumption is that better LOS predictions can help reduce the impact of rescheduling. However, the training process of ML models that can make such accurate predictions can be very costly. Building on previous work that proposed simulated ML for evaluating data-driven approaches, this paper explores the relationship between LOS prediction accuracy and rescheduling flexibility across various corrective policies. Specifically, we examine the most effective patient rescheduling strategies under LOS prediction errors to prevent bed overflows while optimizing resource utilization
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Beats Autoregressive in Data-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.15857</link>
<guid>https://arxiv.org/abs/2507.15857</guid>
<content:encoded><![CDATA[
arXiv:2507.15857v3 Announce Type: replace 
Abstract: Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active learning for level set estimation under input uncertainty and its extensions</title>
<link>https://arxiv.org/abs/1909.06064</link>
<guid>https://arxiv.org/abs/1909.06064</guid>
<content:encoded><![CDATA[
arXiv:1909.06064v2 Announce Type: replace-cross 
Abstract: Testing under what conditions the product satisfies the desired properties is a fundamental problem in manufacturing industry. If the condition and the property are respectively regarded as the input and the output of a black-box function, this task can be interpreted as the problem called Level Set Estimation (LSE) -- the problem of identifying input regions such that the function value is above (or below) a threshold. Although various methods for LSE problems have been developed so far, there are still many issues to be solved for their practical usage. As one of such issues, we consider the case where the input conditions cannot be controlled precisely, i.e., LSE problems under input uncertainty. We introduce a basic framework for handling input uncertainty in LSE problem, and then propose efficient methods with proper theoretical guarantees. The proposed methods and theories can be generally applied to a variety of challenges related to LSE under input uncertainty such as cost-dependent input uncertainties and unknown input uncertainties. We apply the proposed methods to artificial and real data to demonstrate the applicability and effectiveness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Misconceptions in Social Bots Research</title>
<link>https://arxiv.org/abs/2303.17251</link>
<guid>https://arxiv.org/abs/2303.17251</guid>
<content:encoded><![CDATA[
arXiv:2303.17251v4 Announce Type: replace-cross 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial attacks and defenses in explainable artificial intelligence: A survey</title>
<link>https://arxiv.org/abs/2306.06123</link>
<guid>https://arxiv.org/abs/2306.06123</guid>
<content:encoded><![CDATA[
arXiv:2306.06123v4 Announce Type: replace-cross 
Abstract: Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning (AdvML) highlight the limitations and vulnerabilities of state-of-the-art explanation methods, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This survey provides a comprehensive overview of research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We introduce a unified notation and taxonomy of methods facilitating a common ground for researchers and practitioners from the intersecting research fields of AdvML and XAI. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI). Future work should address improving explanation methods and evaluation protocols to take into account the reported safety issues.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic segmentation of SEM images of lower bainitic and tempered martensitic steels</title>
<link>https://arxiv.org/abs/2312.17251</link>
<guid>https://arxiv.org/abs/2312.17251</guid>
<content:encoded><![CDATA[
arXiv:2312.17251v2 Announce Type: replace-cross 
Abstract: This study employs deep learning techniques to segment scanning electron microscope images, enabling a quantitative analysis of carbide precipitates in lower bainite and tempered martensite steels with comparable strength. Following segmentation, carbides are investigated, and their volume percentage, size distribution, and orientations are probed within the image dataset. Our findings reveal that lower bainite and tempered martensite exhibit comparable volume percentages of carbides, albeit with a more uniform distribution of carbides in tempered martensite. Carbides in lower bainite demonstrate a tendency for better alignment than those in tempered martensite, aligning with the observations of other researchers. However, both microstructures display a scattered carbide orientation, devoid of any discernible pattern. Comparative analysis of aspect ratios and sizes of carbides in lower bainite and tempered martensite unveils striking similarities. The deep learning model achieves an impressive pixelwise accuracy of 98.0% in classifying carbide/iron matrix at the individual pixel level. The semantic segmentation derived from deep learning extends its applicability to the analysis of secondary phases in various materials, offering a time-efficient, versatile AI-powered workflow for quantitative microstructure analysis.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Super-resolution Inspired Electron Density Prediction</title>
<link>https://arxiv.org/abs/2402.12335</link>
<guid>https://arxiv.org/abs/2402.12335</guid>
<content:encoded><![CDATA[
arXiv:2402.12335v2 Announce Type: replace-cross 
Abstract: Drawing inspiration from the domain of image super-resolution, we view the electron density as a 3D grayscale image and use a convolutional residual network to transform a crude and trivially generated guess of the molecular density into an accurate ground-state quantum mechanical density. We find that this model outperforms all prior density prediction approaches. Because the input is itself a real-space density, the predictions are equivariant to molecular symmetry transformations even though the model is not constructed to be. Due to its simplicity, the model is directly applicable to unseen molecular conformations and chemical elements. We show that fine-tuning on limited new data provides high accuracy even in challenging cases of exotic elements and charge states. Our work suggests new routes to learning real-space physical quantities drawing from the established ideas of image processing.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The pitfalls of next-token prediction</title>
<link>https://arxiv.org/abs/2403.06963</link>
<guid>https://arxiv.org/abs/2403.06963</guid>
<content:encoded><![CDATA[
arXiv:2403.06963v3 Announce Type: replace-cross 
Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.
  As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.
  Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Directed Distance Fields for Ray-Based Shape Representations</title>
<link>https://arxiv.org/abs/2404.09081</link>
<guid>https://arxiv.org/abs/2404.09081</guid>
<content:encoded><![CDATA[
arXiv:2404.09081v2 Announce Type: replace-cross 
Abstract: In modern computer vision, the optimal representation of 3D shape continues to be task-dependent. One fundamental operation applied to such representations is differentiable rendering, as it enables inverse graphics approaches in learning frameworks. Standard explicit shape representations (voxels, point clouds, or meshes) are often easily rendered, but can suffer from limited geometric fidelity, among other issues. On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability. In this work, we devise Directed Distance Fields (DDFs), a novel neural shape representation that builds upon classical distance fields. The fundamental operation in a DDF maps an oriented point (position and direction) to surface visibility and depth. This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as differential geometric quantity extraction (e.g., surface normals), with only additional backward passes. Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field. We then apply DDFs to several applications, including single-shape fitting, generative modelling, and single-image 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation. Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency. We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing, for instance, which shape the field is expressing.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric Sparse Online Learning of the Koopman Operator</title>
<link>https://arxiv.org/abs/2405.07432</link>
<guid>https://arxiv.org/abs/2405.07432</guid>
<content:encoded><![CDATA[
arXiv:2405.07432v3 Announce Type: replace-cross 
Abstract: The Koopman operator provides a powerful framework for representing the dynamics of general nonlinear dynamical systems. However, existing data-driven approaches to learning the Koopman operator rely on batch data. In this work, we present a sparse online learning algorithm that learns the Koopman operator iteratively via stochastic approximation, with explicit control over model complexity and provable convergence guarantees. Specifically, we study the Koopman operator via its action on the reproducing kernel Hilbert space (RKHS), and address the mis-specified scenario where the dynamics may escape the chosen RKHS. In this mis-specified setting, we relate the Koopman operator to the conditional mean embeddings (CME) operator. We further establish both asymptotic and finite-time convergence guarantees for our learning algorithm in mis-specified setting, with trajectory-based sampling where the data arrive sequentially over time. Numerical experiments demonstrate the algorithm's capability to learn unknown nonlinear dynamics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite time analysis of distributed Q-learning</title>
<link>https://arxiv.org/abs/2405.14078</link>
<guid>https://arxiv.org/abs/2405.14078</guid>
<content:encoded><![CDATA[
arXiv:2405.14078v2 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL). In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards. In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\tilde{\mathcal{O}}\left( \min\left\{\frac{1}{\epsilon^2}\frac{t_{\text{mix}}}{(1-\gamma)^6 d_{\min}^4 } ,\frac{1}{\epsilon}\frac{\sqrt{|\gS||\gA|}}{(1-\sigma_2(\boldsymbol{W}))(1-\gamma)^4 d_{\min}^3} \right\}\right)$ under tabular lookup
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Designing Quantum Experiments with Language Models</title>
<link>https://arxiv.org/abs/2406.02470</link>
<guid>https://arxiv.org/abs/2406.02470</guid>
<content:encoded><![CDATA[
arXiv:2406.02470v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) can solve complex scientific problems beyond human capabilities, but the resulting solutions offer little insight into the underlying physical principles. One prominent example is quantum physics, where computers can discover experiments for the generation of specific quantum states, but it is unclear how finding general design concepts can be automated. Here, we address this challenge by training a transformer-based language model to create human-readable Python code, which solves an entire class of problems in a single pass. This strategy, which we call meta-design, enables scientists to gain a deeper understanding and extrapolate to larger experiments without additional optimization. To demonstrate the effectiveness of our approach, we uncover previously unknown experimental generalizations of important quantum states, e.g. from condensed matter physics. The underlying methodology of meta-design can naturally be extended to fields such as materials science or engineering.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEACON: A Bayesian Optimization Strategy for Novelty Search in Expensive Black-Box Systems</title>
<link>https://arxiv.org/abs/2406.03616</link>
<guid>https://arxiv.org/abs/2406.03616</guid>
<content:encoded><![CDATA[
arXiv:2406.03616v3 Announce Type: replace-cross 
Abstract: Novelty search (NS) refers to a class of exploration algorithms that seek to uncover diverse system behaviors through simulations or experiments. Such diversity is central to many AI-driven discovery and design tasks, including material and drug development, neural architecture search, and reinforcement learning. However, existing NS methods typically rely on evolutionary strategies and other meta-heuristics that require dense sampling of the input space, making them impractical for expensive black-box systems. In this work, we introduce BEACON, a sample-efficient, Bayesian optimization-inspired approach to NS that is tailored for settings where the input-to-behavior relationship is opaque and costly to evaluate. BEACON models this mapping using multi-output Gaussian processes (MOGPs) and selects new inputs by maximizing a novelty metric computed from posterior samples of the MOGP, effectively balancing the exploration-exploitation trade-off. By leveraging recent advances in posterior sampling and high-dimensional GP modeling, our method remains scalable to large input spaces and datasets. We evaluate BEACON across ten synthetic benchmarks and eight real-world tasks, including the design of diverse materials for clean energy applications. Our results show that BEACON significantly outperforms existing NS baselines, consistently discovering a broader set of behaviors under tight evaluation budgets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Stability Analysis of Physics-Informed Random Projection Neural Networks for ODEs</title>
<link>https://arxiv.org/abs/2408.15393</link>
<guid>https://arxiv.org/abs/2408.15393</guid>
<content:encoded><![CDATA[
arXiv:2408.15393v2 Announce Type: replace-cross 
Abstract: We present a linear stability analysis of physics-informed random projection neural networks (PI-RPNNs), for the numerical solution of {the initial value problem (IVP)} of (stiff) ODEs. We begin by proving that PI-RPNNs are uniform approximators of the solution to ODEs. We then provide a constructive proof demonstrating that PI-RPNNs offer consistent and asymptotically stable numerical schemes, thus convergent schemes. In particular, we prove that multi-collocation PI-RPNNs guarantee asymptotic stability. Our theoretical results are illustrated via numerical solutions of benchmark examples including indicative comparisons with the backward Euler method, the midpoint method, the trapezoidal rule, the 2-stage Gauss scheme, and the 2- and 3-stage Radau schemes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment</title>
<link>https://arxiv.org/abs/2409.09545</link>
<guid>https://arxiv.org/abs/2409.09545</guid>
<content:encoded><![CDATA[
arXiv:2409.09545v3 Announce Type: replace-cross 
Abstract: This paper presents a Multi-modal Emotion Recognition (MER) system designed to enhance emotion recognition accuracy in challenging acoustic conditions. Our approach combines a modified and extended Hierarchical Token-semantic Audio Transformer (HTS-AT) for multi-channel audio processing with an R(2+1)D Convolutional Neural Networks (CNN) model for video analysis. We evaluate our proposed method on a reverberated version of the Ryerson audio-visual database of emotional speech and song (RAVDESS) dataset using synthetic and real-world Room Impulse Responsess (RIRs). Our results demonstrate that integrating audio and video modalities yields superior performance compared to uni-modal approaches, especially in challenging acoustic conditions. Moreover, we show that the multimodal (audiovisual) approach that utilizes multiple microphones outperforms its single-microphone counterpart.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum enhanced stratification of Breast Cancer: exploring quantum expressivity for real omics data</title>
<link>https://arxiv.org/abs/2409.14089</link>
<guid>https://arxiv.org/abs/2409.14089</guid>
<content:encoded><![CDATA[
arXiv:2409.14089v2 Announce Type: replace-cross 
Abstract: Quantum Machine Learning (QML) is considered one of the most promising applications of Quantum Computing in the Noisy Intermediate Scale Quantum (NISQ) era for the impact it is thought to have in the near future. Although promising theoretical assumptions, the exploration of how QML could foster new discoveries in Medicine and Biology fields is still in its infancy with few examples. In this study, we aimed to assess whether Quantum Kernels (QK) could effectively classify subtypes of Breast Cancer (BC) patients on the basis of molecular characteristics. We performed an heuristic exploration of encoding configurations with different entanglement levels to determine a trade-off between kernel expressivity and performances. Our results show that QKs yield comparable clustering results with classical methods while using fewer data points, and are able to fit the data with a higher number of clusters. Additionally, we conducted the experiments on the Quantum Processing Unit (QPU) to evaluate the effect of noise on the outcome. We found that less expressive encodings showed a higher resilience to noise, indicating that the computational pipeline can be reliably implemented on the NISQ devices. Our findings suggest that QK methods show promises for application in Precision Oncology, especially in scenarios where the dataset is limited in size and a granular non-trivial stratification of complex molecular data cannot be achieved classically.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative filtering based on nonnegative/binary matrix factorization</title>
<link>https://arxiv.org/abs/2410.10381</link>
<guid>https://arxiv.org/abs/2410.10381</guid>
<content:encoded><![CDATA[
arXiv:2410.10381v4 Announce Type: replace-cross 
Abstract: Collaborative filtering generates recommendations by exploiting user-item similarities based on rating data, which often contains numerous unrated items. To predict scores for unrated items, matrix factorization techniques such as nonnegative matrix factorization (NMF) are often employed. Nonnegative/binary matrix factorization (NBMF), which is an extension of NMF, approximates a nonnegative matrix as the product of nonnegative and binary matrices. While previous studies have applied NBMF primarily to dense data such as images, this paper proposes a modified NBMF algorithm tailored for collaborative filtering with sparse data. In the modified method, unrated entries in the rating matrix are masked, enhancing prediction accuracy. Furthermore, utilizing a low-latency Ising machine in NBMF is advantageous in terms of the computation time, making the proposed method beneficial.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems</title>
<link>https://arxiv.org/abs/2411.01297</link>
<guid>https://arxiv.org/abs/2411.01297</guid>
<content:encoded><![CDATA[
arXiv:2411.01297v3 Announce Type: replace-cross 
Abstract: This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model-predictive control. Hion controllers estimate future states and develop an optimal control strategy using Pontryagin's Maximum Principle. The proposed framework, along with our Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture, allows for custom transient behavior, predictive control, and closed-loop feedback, addressing limitations of existing methods. Comparative analyses with established model-predictive controllers revealed Hion controllers' superior optimality and tracking capabilities. Optimal control strategies are also demonstrated for both linear and non-linear dynamical systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions</title>
<link>https://arxiv.org/abs/2411.17489</link>
<guid>https://arxiv.org/abs/2411.17489</guid>
<content:encoded><![CDATA[
arXiv:2411.17489v3 Announce Type: replace-cross 
Abstract: Modern reconstruction techniques can effectively model complex 3D scenes from sparse 2D views. However, automatically assessing the quality of novel views and identifying artifacts is challenging due to the lack of ground truth images and the limitations of no-reference image metrics in predicting reliable artifact maps. The absence of such metrics hinders assessment of the quality of novel views and limits the adoption of post-processing techniques, such as inpainting, to enhance reconstruction quality. To tackle this, recent work has established a new category of metrics (cross-reference), predicting image quality solely by leveraging context from alternate viewpoint captures (arXiv:2404.14409). In this work, we propose a new cross-reference metric, Puzzle Similarity, which is designed to localize artifacts in novel views. Our approach utilizes image patch statistics from the training views to establish a scene-specific distribution, later used to identify poorly reconstructed regions in the novel views. Given the lack of good measures to evaluate cross-reference methods in the context of 3D reconstruction, we collected a novel human-labeled dataset of artifact and distortion maps in unseen reconstructed views. Through this dataset, we demonstrate that our method achieves state-of-the-art localization of artifacts in novel views, correlating with human assessment, even without aligned references. We can leverage our new metric to enhance applications like automatic image restoration, guided acquisition, or 3D reconstruction from sparse inputs. Find the project page at https://nihermann.github.io/puzzlesim/ .
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Matrix Completion for Discrete Rating-Scale Data: Coping with Fake Profiles in Recommender Systems</title>
<link>https://arxiv.org/abs/2412.20802</link>
<guid>https://arxiv.org/abs/2412.20802</guid>
<content:encoded><![CDATA[
arXiv:2412.20802v2 Announce Type: replace-cross 
Abstract: Recommender systems are essential tools in the digital landscape for connecting users with content that more closely aligns with their preferences. Matrix completion is a widely used statistical framework for such systems, aiming to predict a user's preferences for items they have not yet rated by leveraging the observed ratings in a partially filled user-item rating matrix. Realistic applications of matrix completion in recommender systems must address several challenges that are too often neglected: (i) the discrete nature of rating-scale data, (ii) the presence of malicious users who manipulate the system to their advantage through the creation of fake profiles, and (iii) missing-not-at-random patterns, where users are more likely to rate items they expect to enjoy. Our goal in this paper is twofold. First, we propose a novel matrix completion method, robust discrete matrix completion (RDMC), designed specifically to handle the discrete nature of sparse rating-scale data and to remain reliable in the presence of adversarial manipulation. We evaluate RDMC through carefully designed experiments and realistic case studies. Our work therefore, secondly, offers a statistically-sound blueprint for future studies on how to evaluate matrix completion methods for recommender systems under realistic scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Kaczmarz Methods with Beyond-Krylov Convergence</title>
<link>https://arxiv.org/abs/2501.11673</link>
<guid>https://arxiv.org/abs/2501.11673</guid>
<content:encoded><![CDATA[
arXiv:2501.11673v2 Announce Type: replace-cross 
Abstract: Randomized Kaczmarz methods form a family of linear system solvers which converge by repeatedly projecting their iterates onto randomly sampled equations. While effective in some contexts, such as highly over-determined least squares, Kaczmarz methods are traditionally deemed secondary to Krylov subspace methods, since this latter family of solvers can exploit outliers in the input's singular value distribution to attain fast convergence on ill-conditioned systems.
  In this paper, we introduce Kaczmarz++, an accelerated randomized block Kaczmarz algorithm that exploits outlying singular values in the input to attain a fast Krylov-style convergence. Moreover, we show that Kaczmarz++ captures large outlying singular values provably faster than popular Krylov methods, for both over- and under-determined systems. We also develop an optimized variant for positive semidefinite systems, called CD++, demonstrating empirically that it is competitive in arithmetic operations with both CG and GMRES on a collection of benchmark problems. To attain these results, we introduce several novel algorithmic improvements to the Kaczmarz framework, including adaptive momentum acceleration, Tikhonov-regularized projections, and a memoization scheme for reusing information from previously sampled equation blocks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring Medical AI Safety: Interpretability-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data</title>
<link>https://arxiv.org/abs/2501.13818</link>
<guid>https://arxiv.org/abs/2501.13818</guid>
<content:encoded><![CDATA[
arXiv:2501.13818v2 Announce Type: replace-cross 
Abstract: Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fatal consequences in practice. Whereas a multitude of works address either the detection or mitigation of such shortcut behavior in isolation, the Reveal2Revise approach provides a comprehensive bias mitigation framework combining these steps. However, effectively addressing these biases often requires substantial labeling efforts from domain experts. In this work, we review the steps of the Reveal2Revise framework and enhance it with semi-automated interpretability-based bias annotation capabilities. This includes methods for the sample- and feature-level bias annotation, providing valuable information for bias mitigation methods to unlearn the undesired shortcut behavior. We show the applicability of the framework using four medical datasets across two modalities, featuring controlled and real-world spurious correlations caused by data artifacts. We successfully identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision Transformer models, ultimately increasing their robustness and applicability for real-world medical tasks. Our code is available at https://github.com/frederikpahde/medical-ai-safety.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review on Self-Supervised Learning for Time Series Anomaly Detection: Recent Advances and Open Challenges</title>
<link>https://arxiv.org/abs/2501.15196</link>
<guid>https://arxiv.org/abs/2501.15196</guid>
<content:encoded><![CDATA[
arXiv:2501.15196v2 Announce Type: replace-cross 
Abstract: Time series anomaly detection presents various challenges due to the sequential and dynamic nature of time-dependent data. Traditional unsupervised methods frequently encounter difficulties in generalization, often overfitting to known normal patterns observed during training and struggling to adapt to unseen normality. In response to this limitation, self-supervised techniques for time series have garnered attention as a potential solution to undertake this obstacle and enhance the performance of anomaly detectors. This paper presents a comprehensive review of the recent methods that make use of self-supervised learning for time series anomaly detection. A taxonomy is proposed to categorize these methods based on their primary characteristics, facilitating a clear understanding of their diversity within this field. The information contained in this survey, along with additional details that will be periodically updated, is available on the following GitHub repository: https://github.com/Aitorzan3/Awesome-Self-Supervised-Time-Series-Anomaly-Detection.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques</title>
<link>https://arxiv.org/abs/2501.18729</link>
<guid>https://arxiv.org/abs/2501.18729</guid>
<content:encoded><![CDATA[
arXiv:2501.18729v2 Announce Type: replace-cross 
Abstract: Attribute manipulation deals with the problem of changing individual attributes of a data point or a time series, while leaving all other aspects unaffected. This work focuses on the domain of human motion, more precisely karate movement patterns. To the best of our knowledge, it presents the first success at manipulating attributes of human motion data. One of the key requirements for achieving attribute manipulation on human motion is a suitable pose representation. Therefore, we design a novel continuous, rotation-based pose representation that enables the disentanglement of the human skeleton and the motion trajectory, while still allowing an accurate reconstruction of the original anatomy. The core idea of the manipulation approach is to use a transformer encoder for discovering high-level semantics, and a diffusion probabilistic model for modeling the remaining stochastic variations. We show that the embedding space obtained from the transformer encoder is semantically meaningful and linear. This enables the manipulation of high-level attributes, by discovering their linear direction of change in the semantic embedding space and moving the embedding along said direction. All code and data is made publicly available.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees</title>
<link>https://arxiv.org/abs/2502.01575</link>
<guid>https://arxiv.org/abs/2502.01575</guid>
<content:encoded><![CDATA[
arXiv:2502.01575v3 Announce Type: replace-cross 
Abstract: Tailoring treatments to individual needs is a central goal in fields such as medicine. A key step toward this goal is estimating Heterogeneous Treatment Effects (HTE) - the way treatments impact different subgroups. While crucial, HTE estimation is challenging with survival data, where time until an event (e.g., death) is key. Existing methods often assume complete observation, an assumption violated in survival data due to right-censoring, leading to bias and inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE estimation in survival data under no hidden confounders, combining a causal survival forest with an augmented inverse-censoring weighting estimator. However, we find it struggles under heavy censoring, which is common in rare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover, most current methods cannot handle instrumental variables, which are a crucial tool in the causal inference arsenal. We introduce Multiple Imputation for Survival Treatment Response (MISTR), a novel, general, and non-parametric method for estimating HTE in survival data. MISTR uses recursively imputed survival trees to handle censoring without directly modeling the censoring mechanism. Through extensive simulations and analysis of two real-world datasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois unemployment dataset we show that MISTR outperforms prior methods under heavy censoring in the no-hidden-confounders setting, and extends to the instrumental variable setting. To our knowledge, MISTR is the first non-parametric approach for HTE estimation with unobserved confounders via instrumental variables.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing</title>
<link>https://arxiv.org/abs/2502.03086</link>
<guid>https://arxiv.org/abs/2502.03086</guid>
<content:encoded><![CDATA[
arXiv:2502.03086v2 Announce Type: replace-cross 
Abstract: This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers</title>
<link>https://arxiv.org/abs/2502.03885</link>
<guid>https://arxiv.org/abs/2502.03885</guid>
<content:encoded><![CDATA[
arXiv:2502.03885v5 Announce Type: replace-cross 
Abstract: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 take a middle-ground approach, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).
  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt to variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh)-based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios are under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis</title>
<link>https://arxiv.org/abs/2502.05934</link>
<guid>https://arxiv.org/abs/2502.05934</guid>
<content:encoded><![CDATA[
arXiv:2502.05934v2 Announce Type: replace-cross 
Abstract: We formalize AI alignment as a multi-objective optimization problem called $\langle M,N,\varepsilon,\delta\rangle$-agreement that generalizes prior approaches with fewer assumptions, in which a set of $N$ agents (including humans) must reach approximate ($\varepsilon$) agreement across $M$ candidate objectives with probability at least $1-\delta$. Using communication complexity, we prove an information-theoretic lower bound demonstrating that once either $M$ or $N$ is large enough, no interaction or rationality can avoid intrinsic alignment overheads. This barrier establishes rigorous intrinsic limits to alignment \emph{itself}, not merely to specific methods, clarifying a crucial ``no free lunch'' principle: encoding ``all human values'' inevitably leads to misalignment, requiring future methods to explicitly manage complexity through consensus-driven reduction or prioritization of objectives. Complementing this impossibility result, we provide explicit algorithms achieving alignment under both computationally unbounded and bounded rationality with noisy messages. Even in these best-case scenarios where alignment to arbitrary precision is theoretically guaranteed, our analysis identifies three critical scalability barriers: the number of tasks ($M$), agents ($N$), and task state space size ($D$); thereby highlighting fundamental complexity-theoretic constraints and providing guidelines for safer, scalable human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKE: Steering Activations for Knowledge Editing</title>
<link>https://arxiv.org/abs/2503.01751</link>
<guid>https://arxiv.org/abs/2503.01751</guid>
<content:encoded><![CDATA[
arXiv:2503.01751v2 Announce Type: replace-cross 
Abstract: As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>($\boldsymbol{\theta}_l, \boldsymbol{\theta}_u$)-Parametric Multi-Task Optimization: Joint Search in Solution and Infinite Task Spaces</title>
<link>https://arxiv.org/abs/2503.08394</link>
<guid>https://arxiv.org/abs/2503.08394</guid>
<content:encoded><![CDATA[
arXiv:2503.08394v2 Announce Type: replace-cross 
Abstract: Multi-task optimization is typically characterized by a fixed and finite set of optimization tasks. The present paper relaxes this condition by considering a non-fixed and potentially infinite set of optimization tasks defined in a parameterized, continuous and bounded task space. We refer to this unique problem setting as parametric multi-task optimization (PMTO). Assuming the bounds of the task parameters to be ($\boldsymbol{\theta}_l$, $\boldsymbol{\theta}_u$), a novel ($\boldsymbol{\theta}_l$, $\boldsymbol{\theta}_u$)-PMTO algorithm is crafted to enable joint search over tasks and their solutions. This joint search is supported by two approximation models: (1) for mapping solutions to the objective spaces of all tasks, which provably accelerates convergence by acting as a conduit for inter-task knowledge transfers, and (2) for probabilistically mapping tasks to the solution space, which facilitates evolutionary exploration of under-explored regions of the task space. At the end of a full ($\boldsymbol{\theta}_l$, $\boldsymbol{\theta}_u$)-PMTO run, the acquired models enable rapid identification of optimized solutions for any task lying within the specified bounds. This outcome is validated on both synthetic test problems and practical case studies, with the significant real-world applicability of PMTO shown towards fast reconfiguration of robot controllers under changing task conditions. The potential of PMTO to vastly speedup the search for solutions to minimax optimization problems is also demonstrated through an example in robust engineering design.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</title>
<link>https://arxiv.org/abs/2503.16418</link>
<guid>https://arxiv.org/abs/2503.16418</guid>
<content:encoded><![CDATA[
arXiv:2503.16418v2 Announce Type: replace-cross 
Abstract: Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-CLIP : Learning EEG representations from natural language descriptions</title>
<link>https://arxiv.org/abs/2503.16531</link>
<guid>https://arxiv.org/abs/2503.16531</guid>
<content:encoded><![CDATA[
arXiv:2503.16531v2 Announce Type: replace-cross 
Abstract: Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at https://github.com/tidiane-camaret/EEGClip
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control</title>
<link>https://arxiv.org/abs/2505.03134</link>
<guid>https://arxiv.org/abs/2505.03134</guid>
<content:encoded><![CDATA[
arXiv:2505.03134v3 Announce Type: replace-cross 
Abstract: Visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. This paper presents a novel approach using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. The methodology significantly enhances image classification performance of standard CNN architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting anomalies by increasing the minority class representation. Experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision on the validation set. The most dramatic improvement was observed in ResNet50V2's overall classification accuracy, which increased from 78\% to 93\% when trained with the augmented data. This work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v5 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs</title>
<link>https://arxiv.org/abs/2506.05413</link>
<guid>https://arxiv.org/abs/2506.05413</guid>
<content:encoded><![CDATA[
arXiv:2506.05413v2 Announce Type: replace-cross 
Abstract: We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Perturbation Guidance via Attention Head Selection</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[
arXiv:2506.10978v3 Announce Type: replace-cross 
Abstract: Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: Automated Synthesis for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
arXiv:2506.15787v3 Announce Type: replace-cross 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEVLM: Parallel Encoding for Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.19651</link>
<guid>https://arxiv.org/abs/2506.19651</guid>
<content:encoded><![CDATA[
arXiv:2506.19651v3 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in multimodal understanding and generation tasks. However, their application to long video understanding remains hindered by the quadratic complexity of standard attention mechanisms. In this work, we introduce \textbf{PEVLM}, a fine-tuning-free parallel encoding method designed to enhance the prefilling efficiency of VLMs in long video scenarios. PEVLM partitions the input video into context blocks with a shared sink block, while preserving sequential position embeddings to align the attention weight distribution with that of Full-Attention. This design reduces attention complexity from $O((T \times N)^2)$ to $O(T \times N)$ where $T$ is the number of frames and $N$ the number of tokens per frame, without sacrificing accuracy. Extensive experiments across multiple state-of-the-art models and benchmarks demonstrate that PEVLM consistently outperforms existing parallel encoding approaches, achieving up to \textbf{7.47x} speedup in attention computation and reducing end-to-end latency by \textbf{40\%}. Remarkably, PEVLM not only maintains high accuracy, but in some settings even surpasses Full-Attention performance. Under strict latency constraints, it achieves substantial gains, improving accuracy from \textbf{23.26\%} to \textbf{61.03\%}. These results underscore the effectiveness of PEVLM for low-latency, long-context video understanding, making it a promising solution for real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives</title>
<link>https://arxiv.org/abs/2506.20114</link>
<guid>https://arxiv.org/abs/2506.20114</guid>
<content:encoded><![CDATA[
arXiv:2506.20114v3 Announce Type: replace-cross 
Abstract: Tree ensembles are non-parametric methods widely recognized for their accuracy and ability to capture complex interactions. While these models excel at prediction, they are difficult to interpret and may fail to uncover useful relationships in the data. We propose an estimator to extract compact sets of decision rules from tree ensembles. The extracted models are accurate and can be manually examined to reveal relationships between the predictors and the response. A key novelty of our estimator is the flexibility to jointly control the number of rules extracted and the interaction depth of each rule, which improves accuracy. We develop a tailored exact algorithm to efficiently solve optimization problems underlying our estimator and an approximate algorithm for computing regularization paths, sequences of solutions that correspond to varying model sizes. We also establish novel non-asymptotic prediction error bounds for our proposed approach, comparing it to an oracle that chooses the best data-dependent linear combination of the rules in the ensemble subject to the same complexity constraint as our estimator. The bounds illustrate that the large-sample predictive performance of our estimator is on par with that of the oracle. Through experiments, we demonstrate that our estimator outperforms existing algorithms for rule extraction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v2 Announce Type: replace-cross 
Abstract: Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer</title>
<link>https://arxiv.org/abs/2507.00683</link>
<guid>https://arxiv.org/abs/2507.00683</guid>
<content:encoded><![CDATA[
arXiv:2507.00683v5 Announce Type: replace-cross 
Abstract: The recently proposed physics-based framework by Huo and Johnson~\cite{huo2024capturing} models the attention mechanism of Large Language Models (LLMs) as an interacting two-body spin system, offering a first-principles explanation for phenomena like repetition and bias. Building on this hypothesis, we extract the complete Query-Key weight matrices from a production-grade GPT-2 model and derive the corresponding effective Hamiltonian for every attention head. From these Hamiltonians, we obtain analytic phase boundaries and logit gap criteria that predict which token should dominate the next-token distribution for a given context. A systematic evaluation on 144 heads across 20 factual-recall prompts reveals a strong negative correlation between the theoretical logit gaps and the model's empirical token rankings ($r\approx-0.70$, $p<10^{-3}$).Targeted ablations further show that suppressing the heads most aligned with the spin-bath predictions induces the anticipated shifts in output probabilities, confirming a causal link rather than a coincidental association. Taken together, our findings provide the first strong empirical evidence for the spin-bath analogy in a production-grade model. In this work, we utilize the context-field lens, which provides physics-grounded interpretability and motivates the development of novel generative models bridging theoretical condensed matter physics and artificial intelligence.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment</title>
<link>https://arxiv.org/abs/2507.11642</link>
<guid>https://arxiv.org/abs/2507.11642</guid>
<content:encoded><![CDATA[
arXiv:2507.11642v2 Announce Type: replace-cross 
Abstract: Posture-based mental state inference has significant potential in diagnosing fatigue, preventing injury, and enhancing performance across various domains. Such tools must be research-validated with large datasets before being translated into practice. Unfortunately, such vision diagnosis faces serious challenges due to the sensitivity of human subject data. To address this, we identify sports settings as a viable alternative for accumulating data from human subjects experiencing diverse emotional states. We test our hypothesis in the game of cricket and present a posture-based solution to identify human intent from activity videos. Our method achieves over 75\% F1 score and over 80\% AUC-ROC in discriminating aggressive and defensive shot intent through motion analysis. These findings indicate that posture leaks out strong signals for intent inference, even with inherent noise in the data pipeline. Furthermore, we utilize existing data statistics as weak supervision to validate our findings, offering a potential solution for overcoming data labelling limitations. This research contributes to generalizable techniques for sports analytics and also opens possibilities for applying human behavior analysis across various fields.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
arXiv:2507.12482v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have improved code generation and software automation, but remain limited by inference-time context and lack structured reasoning over code. Debugging remains unsolved despite these advances. While Claude Opus 4 and GPT-4.1 achieve >70% on code synthesis benchmarks, they perform <15% on real debugging tasks. We introduce Kodezi Chronos, a language model built specifically for debugging. Chronos combines Adaptive Graph-Guided Retrieval to navigate codebases up to 10 million lines using multi-hop traversal (92% precision, 85% recall), Persistent Debug Memory trained on 15M+ sessions, and a 7-layer architecture for iterative fix-test-refine loops. On 5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy, compared to 14.2% and 13.8% for Claude and GPT-4.1 respectively. Chronos reduces debugging time by 40% and iteration count by 65%. It resolves complex multi-file bugs involving cross-repository context and temporal reasoning. Key limitations include 23.4% success on hardware-dependent issues and 41.2% on dynamic language errors. Theoretical analysis shows O(k log d) retrieval complexity with convergence guarantees. In a human evaluation (N=50), 89% of participants preferred Chronos over baseline models. Chronos will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</title>
<link>https://arxiv.org/abs/2507.13420</link>
<guid>https://arxiv.org/abs/2507.13420</guid>
<content:encoded><![CDATA[
arXiv:2507.13420v2 Announce Type: replace-cross 
Abstract: By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification</title>
<link>https://arxiv.org/abs/2507.14116</link>
<guid>https://arxiv.org/abs/2507.14116</guid>
<content:encoded><![CDATA[
arXiv:2507.14116v2 Announce Type: replace-cross 
Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann Machines (QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of No\`e et al. (2024), who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost 70 % compared to regular annealing-based BM executions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning</title>
<link>https://arxiv.org/abs/2507.16518</link>
<guid>https://arxiv.org/abs/2507.16518</guid>
<content:encoded><![CDATA[
arXiv:2507.16518v2 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have shown impressive reasoning capabilities. However, further enhancing existing MLLMs necessitates high-quality vision-language datasets with carefully curated task complexities, which are both costly and challenging to scale. Although recent self-improving models that iteratively refine themselves offer a feasible solution, they still suffer from two core challenges: (i) most existing methods augment visual or textual data separately, resulting in discrepancies in data complexity (e.g., over-simplified diagrams paired with redundant textual descriptions); and (ii) the evolution of data and models is also separated, leading to scenarios where models are exposed to tasks with mismatched difficulty levels. To address these issues, we propose C2-Evo, an automatic, closed-loop self-improving framework that jointly evolves both training data and model capabilities. Specifically, given a base dataset and a base model, C2-Evo enhances them by a cross-modal data evolution loop and a data-model evolution loop. The former loop expands the base dataset by generating complex multimodal problems that combine structured textual sub-problems with iteratively specified geometric diagrams, while the latter loop adaptively selects the generated problems based on the performance of the base model, to conduct supervised fine-tuning and reinforcement learning alternately. Consequently, our method continuously refines its model and training data, and consistently obtains considerable performance gains across multiple mathematical reasoning benchmarks. Our code, models, and datasets will be released.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness</title>
<link>https://arxiv.org/abs/2507.14446</link>
<guid>https://arxiv.org/abs/2507.14446</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, stochastic optimization, deep learning, supply chain management, constraint coordination

Summary:
In this work, the authors explore the use of reinforcement learning (RL) combined with intervention models to efficiently solve large-scale stochastic optimization problems. By leveraging pre-trained deep learning models to simulate and compose stochastic processes, they aim to better explore the solution space. The approach is demonstrated on a real-world multi-sourcing multi-period inventory management problem in supply chain optimization, where deep RL models are used to learn and forecast stochastic supply chain processes. A constraint coordination mechanism is introduced to forecast dual costs considering cross-product constraints in the inventory network. Instead of directly modeling complex physical constraints, the approach breaks down supply chain processes into scalable DL modules, enhancing performance on real-world datasets. The authors suggest future research directions to further investigate the effectiveness of such models.<br /><br />Summary: <div>
arXiv:2507.14446v3 Announce Type: replace 
Abstract: In this work, we study how to efficiently apply reinforcement learning (RL) for solving large-scale stochastic optimization problems by leveraging intervention models. The key of the proposed methodology is to better explore the solution space by simulating and composing the stochastic processes using pre-trained deep learning (DL) models. We demonstrate our approach on a challenging real-world application, the multi-sourcing multi-period inventory management problem in supply chain optimization. In particular, we employ deep RL models for learning and forecasting the stochastic supply chain processes under a range of assumptions. Moreover, we also introduce a constraint coordination mechanism, designed to forecast dual costs given the cross-products constraints in the inventory network. We highlight that instead of directly modeling the complex physical constraints into the RL optimization problem and solving the stochastic problem as a whole, our approach breaks down those supply chain processes into scalable and composable DL modules, leading to improved performance on large real-world datasets. We also outline open problems for future research to further investigate the efficacy of such models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Origin of Self-Attention: Pairwise Affinity Matrices in Feature Selection and the Emergence of Self-Attention</title>
<link>https://arxiv.org/abs/2507.14560</link>
<guid>https://arxiv.org/abs/2507.14560</guid>
<content:encoded><![CDATA[
<div> affinity matrix, self-attention, deep learning, Inf-FS, pairwise relationships
Summary:
The paper discusses the concept of self-attention in deep learning architectures, particularly in Transformers, as a form of learning and utilizing pairwise affinity matrices to control information flow. It traces the origins of self-attention across various domains and highlights Infinite Feature Selection (Inf-FS) as a foundational approach that generalizes affinity-based weighting. Unlike Transformers, Inf-FS allows for the dynamic definition of the affinity matrix A and computes feature relevance through multi-hop propagation. The paper argues that self-attention can be viewed as a specific instance of Inf-FS, utilizing single-hop affinity computation. The fundamental structure of reasoning over pairwise relationships is common to both approaches, with distinctions lying in how the affinity matrix is defined and utilized. By placing self-attention within the context of affinity-based computation, the paper unifies different strands of machine learning research under a common mathematical foundation. <br /><br />Summary: <div>
arXiv:2507.14560v2 Announce Type: replace 
Abstract: The self-attention mechanism, now central to deep learning architectures such as Transformers, is a modern instance of a more general computational principle: learning and using pairwise affinity matrices to control how information flows through a model. This paper traces the conceptual origins of self-attention across multiple domains, including computer vision, natural language processing, and graph learning, through their shared reliance on an affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS) as a foundational approach that generalizes the idea of affinity-based weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS defines A either through domain knowledge or by learning, and computes feature relevance through multi-hop propagation over the affinity graph. From this perspective, self-attention can be seen as a special case of Inf-FS: it uses a single-hop affinity computation where A is dynamically built from token similarities. We argue that the underlying structure, reasoning over pairwise relationships, is preserved across both approaches, and the key differences lie in how the affinity matrix is defined and applied. By situating self-attention within the broader paradigm of affinity-based computation, we unify several strands of machine learning research and highlight a common mathematical foundation that underpins diverse models and tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
<div> GUI-Gaussian Grounding Rewards, reinforcement learning, graphical user interface, spatial interactions, human clicking behavior
Summary:<br /><br />Graphical User Interface (GUI) Gaussian Grounding Rewards (GUI-G$^2$) is introduced as a novel reward framework that models GUI elements as continuous Gaussian distributions, transforming sparse binary classification into dense continuous optimization. This framework incorporates Gaussian point rewards for precise localization and coverage rewards for spatial alignment, with an adaptive variance mechanism to handle diverse element scales. GUI-G$^2$ significantly outperforms the state-of-the-art method UI-TARS-72B across benchmarks, showing a 24.7% improvement on ScreenSpot-Pro. Continuous modeling enhances robustness to interface variations and generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.<br /><br />Summary: <div>
arXiv:2507.15846v3 Announce Type: replace 
Abstract: Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G$^2$, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive KalmanNet: Analyse des capacit\'es de g\'en\'eralisation d'un r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman</title>
<link>https://arxiv.org/abs/2507.14144</link>
<guid>https://arxiv.org/abs/2507.14144</guid>
<content:encoded><![CDATA[
<div> Kalman filter, neural network, dynamic systems, generalization, out-of-distribution <br />
Summary:<br />
The Recursive KalmanNet, a neural network guided by a Kalman filter, is adept at estimating state variables and error covariance in stochastic dynamic systems from noisy measurements. This network shows promising generalization capabilities, particularly in out-of-distribution scenarios where test data exhibit different temporal dynamics compared to training data. Despite lacking prior knowledge of noise characteristics, the Recursive KalmanNet is able to effectively handle such variations and accurately estimate system states. This ability to generalize to novel scenarios demonstrates the robustness and adaptability of the Recursive KalmanNet model. <div>
arXiv:2507.14144v2 Announce Type: replace-cross 
Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent neural network guided by a Kalman filter, capable of estimating the state variables and error covariance of stochastic dynamic systems from noisy measurements, without prior knowledge of the noise characteristics. This paper explores its generalization capabilities in out-of-distribution scenarios, where the temporal dynamics of the test measurements differ from those encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance pr\'ealable des caract\'eristiques des bruits. Cet article explore ses capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u les dynamiques temporelles des mesures de test diff\`erent de celles rencontr\'ees \`a l'entra\^inement.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</title>
<link>https://arxiv.org/abs/2507.14270</link>
<guid>https://arxiv.org/abs/2507.14270</guid>
<content:encoded><![CDATA[
<div> Keywords: APTx Neuron, neural computation unit, activation function, MNIST dataset, test accuracy <br />
<br />
Summary: 
The article introduces the APTx Neuron, a unified neural computation unit that combines non-linear activation and linear transformation in a single trainable expression. Derived from the APTx activation function, this neuron eliminates the need for separate activation layers, improving computational efficiency. The proposed neuron's functional form includes trainable parameters for flexibility. Validation on the MNIST dataset shows promising results, achieving up to 96.69% test accuracy in 11 epochs with 332K trainable parameters. This demonstrates the APTx Neuron's superior expressiveness and computational efficiency compared to traditional neurons. The architecture suggests a new unified neuron design paradigm with potential implications for future neural network architectures. <br /><br />Summary: <div>
arXiv:2507.14270v3 Announce Type: replace-cross 
Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both computationally efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69% test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and computational efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Artifacts in Learning Dynamical Systems</title>
<link>https://arxiv.org/abs/2507.14491</link>
<guid>https://arxiv.org/abs/2507.14491</guid>
<content:encoded><![CDATA[
<div> Learning dynamical systems from sampled data, numerical scheme, optimization problem, damped oscillatory system, reversed oscillation direction
<br />
Summary:<br />
This study addresses the impact of numerical schemes on learning dynamical systems from data points. It highlights the potential issue where a damped oscillatory system could be misidentified as having "anti-damping" and showing a reversed oscillation direction, even when fitting the data accurately. The analysis emphasizes the importance of considering the chosen numerical scheme in the optimization process, as it can significantly affect the learning outcome. By understanding these effects, researchers can improve the accuracy and reliability of identifying dynamical systems from sampled data. <div>
arXiv:2507.14491v2 Announce Type: replace-cross 
Abstract: In many applications, one needs to learn a dynamical system from its solutions sampled at a finite number of time points. The learning problem is often formulated
  as an optimization problem over a chosen function class. However, in the optimization procedure, it is necessary to employ a numerical scheme to integrate candidate dynamical systems and assess how their solutions fit the data.
  This paper reveals potentially serious effects of a chosen numerical scheme on the learning outcome. In particular, our analysis demonstrates that a damped oscillatory system may be incorrectly identified as having "anti-damping" and exhibiting a reversed oscillation direction, despite adequately fitting the given data points.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report</title>
<link>https://arxiv.org/abs/2507.16534</link>
<guid>https://arxiv.org/abs/2507.16534</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, frontier risks, E-T-C analysis, red lines, yellow lines

Summary:
The report assesses risks posed by advanced artificial intelligence models using the E-T-C analysis framework. It identifies risks in cyber offense, biological and chemical threats, persuasion and manipulation, autonomous AI research, strategic deception, self-replication, and collusion. Using red and yellow lines as thresholds, the risks are classified into green (manageable), yellow (need mitigation), and red (development suspension) zones. Recent AI models fall mostly in green and yellow zones, with no models exceeding red lines. Cyber offense and uncontrolled AI R&amp;D risks are mainly in green and yellow zones. Self-replication and strategic deception risks have some models in the yellow zone. Persuasion and manipulation risks are in the yellow zone due to their impact on human behavior. Biological and chemical risks require further assessment. The study emphasizes the need for collective action to address these AI frontier risks.<br /><br />Summary: <div>
arXiv:2507.16534v2 Announce Type: replace-cross 
Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, this report presents a comprehensive assessment of their frontier risks. Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks in seven areas: cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R\&amp;D, strategic deception and scheming, self-replication, and collusion. Guided by the "AI-$45^\circ$ Law," we evaluate these risks using "red lines" (intolerable thresholds) and "yellow lines" (early warning indicators) to define risk zones: green (manageable risk for routine deployment and continuous monitoring), yellow (requiring strengthened mitigations and controlled deployment), and red (necessitating suspension of development and/or deployment). Experimental results show that all recent frontier AI models reside in green and yellow zones, without crossing red lines. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\&amp;D risks. For self-replication, and strategic deception and scheming, most models remain in the green zone, except for certain reasoning models in the yellow zone. In persuasion and manipulation, most models are in the yellow zone due to their effective influence on humans. For biological and chemical risks, we are unable to rule out the possibility of most models residing in the yellow zone, although detailed threat modeling and in-depth assessment are required to make further claims. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning</title>
<link>https://arxiv.org/abs/2507.16802</link>
<guid>https://arxiv.org/abs/2507.16802</guid>
<content:encoded><![CDATA[
<div> Financial applications, Large Language Models, Agentar-Fin-R1, Trustworthiness, Reasoning capabilities

Summary:
Agentar-Fin-R1 series of financial large language models, optimized for financial applications, enhances reasoning capabilities and reliability. The models are based on the Qwen3 foundation and incorporate a trustworthiness assurance framework. The optimization approach includes label-guided training, multi-agent data synthesis, and data validation governance. The models outperform on financial benchmarks and general reasoning datasets, showcasing their effectiveness. The Finova evaluation benchmark assesses real-world deployment capabilities, focusing on financial reasoning and compliance verification. Experimental results demonstrate state-of-the-art performance on financial tasks. Overall, Agentar-Fin-R1 exhibits exceptional general reasoning capabilities, making it a trustworthy solution for high-stakes financial applications.

<br /><br />Summary: <div>
arXiv:2507.16802v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers</title>
<link>https://arxiv.org/abs/2507.19510</link>
<guid>https://arxiv.org/abs/2507.19510</guid>
<content:encoded><![CDATA[
<div> shift workers, urban mobility modeling, GPS data, transportation planning, transformer-based approach 

Summary:
This paper addresses the underrepresentation of shift workers in traditional transportation surveys and planning, despite comprising a significant portion of the workforce. A comparative analysis of GPS and survey data reveals stark differences in activity patterns between shift workers and standard 9-to-5 schedules. To address this bias, a novel transformer-based approach is introduced, leveraging fragmented GPS trajectory data to generate complete, behaviorally valid activity patterns for individuals working non-standard hours. The method utilizes period-aware temporal embeddings and a transition-focused loss function to capture the unique rhythms of shift workers and align with GPS data from Los Angeles County. By transforming incomplete GPS traces into comprehensive activity patterns, this approach provides transportation planners with a valuable tool for inclusive and precise transportation planning to meet the 24/7 mobility needs of urban populations. 

Summary: <div>
arXiv:2507.19510v1 Announce Type: new 
Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on shift workers, a population segment comprising 15-20% of the workforce in industrialized societies yet systematically underrepresented in traditional transportation surveys and planning. This underrepresentation is revealed in this study by a comparative analysis of GPS and survey data, highlighting stark differences between the bimodal temporal patterns of shift workers and the conventional 9-to-5 schedules recorded in surveys. To address this bias, we introduce a novel transformer-based approach that leverages fragmented GPS trajectory data to generate complete, behaviorally valid activity patterns for individuals working non-standard hours. Our method employs periodaware temporal embeddings and a transition-focused loss function specifically designed to capture the unique activity rhythms of shift workers and mitigate the inherent biases in conventional transportation datasets. Evaluation shows that the generated data achieves remarkable distributional alignment with GPS data from Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By transforming incomplete GPS traces into complete, representative activity patterns, our approach provides transportation planners with a powerful data augmentation tool to fill critical gaps in understanding the 24/7 mobility needs of urban populations, enabling precise and inclusive transportation planning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting</title>
<link>https://arxiv.org/abs/2507.19513</link>
<guid>https://arxiv.org/abs/2507.19513</guid>
<content:encoded><![CDATA[
<div> Scalar LSTM, Spatiotemporal Network, Conv3D, forecasting, 5G

Summary:<br />
- Introduces a lightweight, dual-path Spatiotemporal Network for accurate spatiotemporal traffic forecasting in 5G networks.
- Utilizes a Scalar LSTM for efficient temporal modeling and a three-layer Conv3D module for spatial feature extraction.
- Fusion layer integrates both streams for robust forecasting, improving gradient stability and convergence speed.
- Outperforms ConvLSTM baselines with a 23% reduction in Mean Absolute Error (MAE) and 30% improvement in model generalization.
- Demonstrates strong generalization to unseen regions, making it suitable for large-scale, next-generation network deployments. 

Summary: <div>
arXiv:2507.19513v1 Announce Type: new 
Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource management in 5G and beyond. However, conventional AI approaches often fail to capture the intricate spatial and temporal patterns that exist, due to e.g., the mobility of users. We introduce a lightweight, dual-path Spatiotemporal Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling and a three-layer Conv3D module for spatial feature extraction. A fusion layer integrates both streams into a cohesive representation, enabling robust forecasting. Our design improves gradient stability and convergence speed while reducing prediction error. Evaluations on real-world datasets show superior forecast performance over ConvLSTM baselines and strong generalization to unseen regions, making it well-suited for large-scale, next-generation network deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM, with a 30% improvement in model generalization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks</title>
<link>https://arxiv.org/abs/2507.19514</link>
<guid>https://arxiv.org/abs/2507.19514</guid>
<content:encoded><![CDATA[
<div> wavelet domain, spectral learning, nonlinear transformations, adaptive processing, efficient alternative<br />
<br />
Summary: 
This study introduces a novel fully spectral learning framework that operates exclusively in the wavelet domain, eliminating traditional neural layers. By applying learnable nonlinear transformations directly to wavelet coefficients and incorporating a differentiable wavelet basis selection mechanism, the model offers adaptive processing using various wavelet families. Implemented in PyTorch with 3D support, the model achieves high accuracy on 3D denoising and natural language tasks from the GLUE benchmark, comparable to Transformer baselines while using significantly fewer parameters and peak memory. The model benefits from faster convergence due to spectral sparsity priors and offers an efficient alternative to neural models by utilizing linear-time wavelet transforms and pointwise nonlinearities. This approach provides a compact, interpretable, and efficient solution for vision and language tasks, offering new avenues for model design without the need for overparameterized architectures. <br /><br />Summary: <div>
arXiv:2507.19514v1 Announce Type: new 
Abstract: We introduce a fully spectral learning framework that eliminates traditional neural layers by operating entirely in the wavelet domain. The model applies learnable nonlinear transformations, including soft-thresholding and gain-phase modulation, directly to wavelet coefficients. It also includes a differentiable wavelet basis selection mechanism, enabling adaptive processing using families such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral pipeline without spatial convolutions or attention. On synthetic 3D denoising and natural language tasks from the GLUE benchmark, including SST-2 sentiment classification, the model achieves 89.3 percent accuracy, close to a 4-layer Transformer baseline (90.1 percent), while using 72 percent fewer parameters and 58 percent less peak memory. Faster early convergence is observed due to spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix multiplications in Transformers, our approach uses linear-time wavelet transforms and pointwise nonlinearities, significantly reducing inference cost. This yields a compact, interpretable, and efficient alternative to neural models. Our results support the viability of principled spectral learning in both vision and language tasks, offering new directions for model design without overparameterized architectures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting</title>
<link>https://arxiv.org/abs/2507.19515</link>
<guid>https://arxiv.org/abs/2507.19515</guid>
<content:encoded><![CDATA[
<div> Outbreak prediction, Influenza A, deep learning models, traditional models, forecasting <br />
<br />
Summary: 
This study compares traditional models like ARIMA and ETS with deep learning models including Simple RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer for predicting Influenza A outbreaks. Deep learning models, especially the Transformer architecture, outperformed traditional models in capturing the temporal complexities associated with Influenza A data. The average testing MSE and MAE for the Transformer model were 0.0433  0.0020 and 0.1126  0.0016, respectively. These findings suggest that deep learning models can significantly enhance predictive modeling for infectious diseases, indicating a shift towards utilizing deep learning methods in public health forecasting and intervention planning strategies. Future research should focus on integrating these models into real-time forecasting and preparedness systems at an epidemic level, as well as incorporating them into existing surveillance systems.  
<br /> <div>
arXiv:2507.19515v1 Announce Type: new 
Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year, though this estimate is an improvement from years past due to improvements in sanitation, healthcare practices, and vaccination programs. In this study, we perform a comparative analysis of traditional and deep learning models to predict Influenza A outbreaks. Using historical data from January 2009 to December 2023, we compared the performance of traditional ARIMA and Exponential Smoothing(ETS) models with six distinct deep learning architectures: Simple RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear superiority of all the deep learning models, especially the state-of-the-art Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020 and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with Influenza A data, outperforming well known traditional baseline ARIMA and ETS models. These findings of this study provide evidence that state-of-the-art deep learning architectures can enhance predictive modeling for infectious diseases and indicate a more general trend toward using deep learning methods to enhance public health forecasting and intervention planning strategies. Future work should focus on how these models can be incorporated into real-time forecasting and preparedness systems at an epidemic level, and integrated into existing surveillance systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation</title>
<link>https://arxiv.org/abs/2507.19517</link>
<guid>https://arxiv.org/abs/2507.19517</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Variational Autoencoder, Bicycle Volume Estimation, Sparse Networks, Urban Planning
Summary:
The study introduces BikeVAE-GNN, a novel framework combining Graph Neural Network (GNN) with Variational Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts in sparse urban bicycling networks. The Hybrid-GNN incorporates Graph Convolutional Networks, Graph Attention Networks, and GraphSAGE to model spatial relationships effectively, while VAE generates synthetic nodes and edges to enhance estimation accuracy. BikeVAE-GNN performs regression for volume estimation and classification for traffic level categorization simultaneously. In experiments using Melbourne data with 99% count sparsity, BikeVAE-GNN outperforms baseline models, achieving a mean absolute error of 30.82 bicycles per day, 99% accuracy, and an F1-score of 0.99. Ablation studies confirm the effectiveness of Hybrid-GNN and VAE components, showcasing a significant advancement in bicycling volume estimation for sustainable urban planning.<br /><br />Summary: <div>
arXiv:2507.19517v1 Announce Type: new 
Abstract: Accurate link-level bicycle volume estimation is essential for informed urban and transport planning but it is challenged by extremely sparse count data in urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task framework augmenting a Hybrid Graph Neural Network (GNN) with Variational Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model intricate spatial relationships in sparse networks while VAE generates synthetic nodes and edges to enrich the graph structure and enhance the estimation performance. BikeVAE-GNN simultaneously performs - regression for bicycling volume estimation and classification for bicycling traffic level categorization. We demonstrate the effectiveness of BikeVAE-GNN using OpenStreetMap data and publicly available bicycle count data within the City of Melbourne - where only 141 of 15,933 road segments have labeled counts (resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN outperforms machine learning and baseline GNN models, achieving a mean absolute error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99. Ablation studies further validate the effective role of Hybrid-GNN and VAE components. Our research advances bicycling volume estimation in sparse networks using novel and state-of-the-art approaches, providing insights for sustainable bicycling infrastructures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction</title>
<link>https://arxiv.org/abs/2507.19518</link>
<guid>https://arxiv.org/abs/2507.19518</guid>
<content:encoded><![CDATA[
<div> Efficient graph matching approach, Graph Neural Networks, subgraph embeddings, large-scale circuits, time efficiency <br />
Summary: 
This paper introduces an efficient graph matching approach using Graph Neural Networks (GNNs) for subgraph matching in large-scale circuits. Traditional methods have limitations in generalizing to arbitrary target circuits and are computationally inefficient. The proposed approach utilizes GNNs to predict regions likely to contain the target circuit, incorporating various negative samples for accurate learning. It also directly extracts subgraph embeddings from the entire circuit to capture global information efficiently. Experiments show that the method outperforms existing approaches in terms of time efficiency and target region prediction, offering a scalable and effective solution for subgraph matching in large circuits. <div>
arXiv:2507.19518v1 Announce Type: new 
Abstract: Subgraph matching plays an important role in electronic design automation (EDA) and circuit verification. Traditional rule-based methods have limitations in generalizing to arbitrary target circuits. Furthermore, node-to-node matching approaches tend to be computationally inefficient, particularly for large-scale circuits. Deep learning methods have emerged as a potential solution to address these challenges, but existing models fail to efficiently capture global subgraph embeddings or rely on inefficient matching matrices, which limits their effectiveness for large circuits. In this paper, we propose an efficient graph matching approach that utilizes Graph Neural Networks (GNNs) to predict regions of high probability for containing the target circuit. Specifically, we construct various negative samples to enable GNNs to accurately learn the presence of target circuits and develop an approach to directly extracting subgraph embeddings from the entire circuit, which captures global subgraph information and addresses the inefficiency of applying GNNs to all candidate subgraphs. Extensive experiments demonstrate that our approach significantly outperforms existing methods in terms of time efficiency and target region prediction, offering a scalable and effective solution for subgraph matching in large-scale circuits.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed transfer learning for SHM via feature selection</title>
<link>https://arxiv.org/abs/2507.19519</link>
<guid>https://arxiv.org/abs/2507.19519</guid>
<content:encoded><![CDATA[
<div> SHM, transfer learning, modal assurance criterion, unsupervised learning, population-based SHM  
Summary:  
- Data scarcity and variability among structures pose challenges for training SHM systems.  
- Transfer learning can leverage information from related domains to address these challenges.  
- The Modal Assurance Criterion (MAC) is used to quantify similarities in mode shapes between structures.  
- MAC is proposed as a measure for selecting features that remain consistent across domains, particularly when structures are damaged.  
- The effectiveness of this approach is demonstrated through numerical and experimental case studies.  
<br /><br />Summary: <div>
arXiv:2507.19519v1 Announce Type: new 
Abstract: Data used for training structural health monitoring (SHM) systems are expensive and often impractical to obtain, particularly labelled data. Population-based SHM presents a potential solution to this issue by considering the available data across a population of structures. However, differences between structures will mean the training and testing distributions will differ; thus, conventional machine learning methods cannot be expected to generalise between structures. To address this issue, transfer learning (TL), can be used to leverage information across related domains. An important consideration is that the lack of labels in the target domain limits data-based metrics to quantifying the discrepancy between the marginal distributions. Thus, a prerequisite for the application of typical unsupervised TL methods is to identify suitable source structures (domains), and a set of features, for which the conditional distributions are related to the target structure. Generally, the selection of domains and features is reliant on domain expertise; however, for complex mechanisms, such as the influence of damage on the dynamic response of a structure, this task is not trivial. In this paper, knowledge of physics is leveraged to select more similar features, the modal assurance criterion (MAC) is used to quantify the correspondence between the modes of healthy structures. The MAC is shown to have high correspondence with a supervised metric that measures joint-distribution similarity, which is the primary indicator of whether a classifier will generalise between domains. The MAC is proposed as a measure for selecting a set of features that behave consistently across domains when subjected to damage, i.e. features with invariance in the conditional distributions. This approach is demonstrated on numerical and experimental case studies to verify its effectiveness in various applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves</title>
<link>https://arxiv.org/abs/2507.19520</link>
<guid>https://arxiv.org/abs/2507.19520</guid>
<content:encoded><![CDATA[
<div> Machine learning, exoplanets, discovery, models, data augmentation
Summary:<br />
- Scientists face slow exoplanet discovery rate due to manual processes.
- Machine learning (ML) is efficient and accurate in discovering exoplanets.
- ML models like logistic regression, k-nearest neighbors, and random forest show promising results in discovering exoplanets.
- Dataset from NASA's Kepler space telescope is used to train and predict models.
- Data augmentation techniques improve recall, precision, and overall generalization in exoplanet discovery. <br />Summary: <div>
arXiv:2507.19520v1 Announce Type: new 
Abstract: With manual searching processes, the rate at which scientists and astronomers discover exoplanets is slow because of inefficiencies that require an extensive time of laborious inspections. In fact, as of now there have been about only 5,000 confirmed exoplanets since the late 1900s. Recently, machine learning (ML) has proven to be extremely valuable and efficient in various fields, capable of processing massive amounts of data in addition to increasing its accuracy by learning. Though ML models for discovering exoplanets owned by large corporations (e.g. NASA) exist already, they largely depend on complex algorithms and supercomputers. In an effort to reduce such complexities, in this paper, we report the results and potential benefits of various, well-known ML models in the discovery and validation of extrasolar planets. The ML models that are examined in this study include logistic regression, k-nearest neighbors, and random forest. The dataset on which the models train and predict is acquired from NASA's Kepler space telescope. The initial results show promising scores for each model. However, potential biases and dataset imbalances necessitate the use of data augmentation techniques to further ensure fairer predictions and improved generalization. This study concludes that, in the context of searching for exoplanets, data augmentation techniques significantly improve the recall and precision, while the accuracy varies for each model.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations</title>
<link>https://arxiv.org/abs/2507.19522</link>
<guid>https://arxiv.org/abs/2507.19522</guid>
<content:encoded><![CDATA[
<div> Mathematical models, neural networks, forward problem, inverse problem, Physics-Informed Neural Network (PINN)<br />
Summary:<br />
Mathematical models in neural networks are powerful for solving complex differential equations. The forward problem predicts network output by optimizing weights and biases, while the inverse problem finds equation parameters or coefficients to model data. Physics-Informed Neural Networks (PINNs) solve both problems by injecting prior analytical information into the cost function. Prior knowledge takes the form of differential equations, with residuals minimized to effectively solve the equations. PINNs can handle sparse data without overfitting by extrapolating to fit larger trends. By embedding the solution and parameters into the loss function, PINNs optimize both the neural network weights and model parameters simultaneously. The study will develop PINNs with varying levels of complexity, starting from linear and quadratic models and then progressing to models for the heat equation and other complex differential equations. Python and the PyTorch library will be used for computational purposes. <br /> <div>
arXiv:2507.19522v1 Announce Type: new 
Abstract: Mathematical models in neural networks are powerful tools for solving complex differential equations and optimizing their parameters; that is, solving the forward and inverse problems, respectively. A forward problem predicts the output of a network for a given input by optimizing weights and biases. An inverse problem finds equation parameters or coefficients that effectively model the data. A Physics-Informed Neural Network (PINN) can solve both problems. PINNs inject prior analytical information about the data into the cost function to improve model performance outside the training set boundaries. This also allows PINNs to efficiently solve problems with sparse data without overfitting by extrapolating the model to fit larger trends in the data. The prior information we implement is in the form of differential equations. Residuals are the differences between the left-hand and right-hand sides of corresponding differential equations; PINNs minimize these residuals to effectively solve the differential equation and take advantage of prior knowledge. In this way, the solution and parameters are embedded into the loss function and optimized, allowing both the weights of the neural network and the model parameters to be found simultaneously, solving both the forward and inverse problems in the process. In this paper, we will create PINNs with residuals of varying complexity, beginning with linear and quadratic models and then expanding to fit models for the heat equation and other complex differential equations. We will mainly use Python as the computing language, using the PyTorch library to aid us in our research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models for Controllable DNA Sequence Design</title>
<link>https://arxiv.org/abs/2507.19523</link>
<guid>https://arxiv.org/abs/2507.19523</guid>
<content:encoded><![CDATA[
<div> Controllable DNA sequence design, Language models, GPT, BERT, ATGC-Gen<br />
Summary:<br />
The paper introduces ATGC-Gen, an Automated Transformer Generator for Controllable DNA sequence generation, leveraging language models like GPT and BERT. This system integrates diverse biological signals through cross-modal encoding, utilizing both decoder-only and encoder-only transformer architectures for flexible training and generation. Evaluations on tasks such as promoter and enhancer sequence design, using a new dataset based on ChIP-Seq experiments for protein binding specificity modeling, show that ATGC-Gen can produce fluent, diverse, and biologically relevant sequences. It outperforms previous methods in controllability and functional relevance, demonstrating the potential of language models in advancing programmable genomic design. Source code is available on GitHub at https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen. <br /><br />Summary: <div>
arXiv:2507.19523v1 Announce Type: new 
Abstract: We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov Arnold Network Autoencoder in Medicine</title>
<link>https://arxiv.org/abs/2507.19524</link>
<guid>https://arxiv.org/abs/2507.19524</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Neural networks, Kolmogorov Arnold Networks, Autoencoders, Cardiological signals <br />
Summary: <br />
This study introduces the Kolmogorov Arnold Networks (KAN) architecture, which improves neural network performance by incorporating learnable activation functions on the edges. It explores optimizing KAN with features like dropout regularization and benchmarking. The study also introduces the KAN Convolutional Network (KCN) that incorporates matrix convolution with KAN learning. The research benchmarks different versions of vanilla Autoencoders (AE) against their KAN counterparts using cardiological signals as input. Five classic AE tasks - reconstruction, generation, denoising, inpainting, and anomaly detection - are studied with a medical dataset containing audio signals from the stethoscope. The experiment aims to compare the performance of these AEs and KAN counterparts in various tasks to evaluate their effectiveness in analyzing cardiological signals. <br />  <div>
arXiv:2507.19524v1 Announce Type: new 
Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons (MLP) and Convolutional blocks still play a crucial role in nowadays research advancements. From a topological point of view, these architecture may be represented as graphs in which we learn the functions related to the nodes while fixed edges convey the information from the input to the output. A recent work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that reports how putting learnable activation functions on the edges of the neural network leads to better performances in multiple scenarios. Multiple studies are focusing on optimizing the KAN architecture by adding important features such as dropout regularization, Autoencoders (AE), model benchmarking and last, but not least, the KAN Convolutional Network (KCN) that introduced matrix convolution with KANs learning. This study aims to benchmark multiple versions of vanilla AEs (such as Linear, Convolutional and Variational) against their Kolmogorov-Arnold counterparts that have same or less number of parameters. Using cardiological signals as model input, a total of five different classic AE tasks were studied: reconstruction, generation, denoising, inpainting and anomaly detection. The proposed experiments uses a medical dataset \textit{AbnormalHeartbeat} that contains audio signals obtained from the stethoscope.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2507.19525</link>
<guid>https://arxiv.org/abs/2507.19525</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal large language models, electronic design automation, circuit design, MMCircuitEval

Summary:
MMCircuitEval is introduced as a benchmark for evaluating multimodal large language models (MLLMs) in Electronic Design Automation (EDA) tasks. The benchmark comprises 3614 question-answer pairs covering digital and analog circuits across various EDA stages. The QA pairs are rigorously reviewed for accuracy and relevance, categorizing them by design stage, circuit type, abilities tested, and difficulty level. Evaluation of existing LLMs using MMCircuitEval reveals performance gaps, particularly in back-end design and complex computations. The benchmark highlights the need for targeted training datasets and modeling approaches to improve MLLM performance in EDA tasks. MMCircuitEval serves as a foundational resource for integrating MLLMs into real-world circuit design workflows, providing a comprehensive assessment of model capabilities and limitations. The benchmark is publicly available at https://github.com/cure-lab/MMCircuitEval.

<br /><br />Summary: <div>
arXiv:2507.19525v1 Announce Type: new 
Abstract: The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages - ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at https://github.com/cure-lab/MMCircuitEval.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantizing Text-attributed Graphs for Semantic-Structural Integration</title>
<link>https://arxiv.org/abs/2507.19526</link>
<guid>https://arxiv.org/abs/2507.19526</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-attributed graphs, large language models (LLMs), self-supervised framework, graph learning, transfer learning

Summary: 
STAG is a novel self-supervised framework that addresses the challenges of embedding structural information into LLM-compatible formats for text-attributed graphs. The framework quantizes graph structural information into discrete tokens using a frozen codebook, employing soft assignment and KL divergence guided quantization to capture critical structural details. STAG supports true zero-shot transfer learning without requiring labeled data from the source domain, making it adaptable and efficient. Extensive experiments demonstrate state-of-the-art performance in node classification benchmarks, showcasing compatibility with different LLM architectures. STAG offers a promising solution for bridging the gap between graph learning and large language models, providing a versatile and effective approach for modeling complex relationships in diverse domains. 

<br /><br />Summary: <div>
arXiv:2507.19526v1 Announce Type: new 
Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for modeling complex relationships across diverse domains. With the rise of large language models (LLMs), there is growing interest in leveraging their capabilities for graph learning. However, current approaches face significant challenges in embedding structural information into LLM-compatible formats, requiring either computationally expensive alignment mechanisms or manual graph verbalization techniques that often lose critical structural details. Moreover, these methods typically require labeled data from source domains for effective transfer learning, significantly constraining their adaptability. We propose STAG, a novel self-supervised framework that directly quantizes graph structural information into discrete tokens using a frozen codebook. Unlike traditional quantization approaches, our method employs soft assignment and KL divergence guided quantization to address the unique challenges of graph data, which lacks natural tokenization structures. Our framework enables both LLM-based and traditional learning approaches, supporting true zero-shot transfer learning without requiring labeled data even in the source domain. Extensive experiments demonstrate state-of-the-art performance across multiple node classification benchmarks while maintaining compatibility with different LLM architectures, offering an elegant solution to bridging graph learning with LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on the application of graph data structure and graph neural network in node classification/clustering tasks</title>
<link>https://arxiv.org/abs/2507.19527</link>
<guid>https://arxiv.org/abs/2507.19527</guid>
<content:encoded><![CDATA[
<div> Keywords: graph data structures, classical graph algorithms, Graph Neural Networks, node classification, clustering <br />
<br />
Summary: 
Graph-structured data are prevalent in various domains such as social networks and knowledge graphs. This study focuses on analyzing graph data structures, classical graph algorithms, and Graph Neural Networks (GNNs) to address the challenges posed by non-Euclidean data. Through comparative experiments, GNNs demonstrate significant accuracy improvements ranging from 43% to 70% compared to traditional methods in tasks like node classification and clustering. The research also explores integration strategies between classical algorithms and GNN architectures to enhance graph representation learning research, providing theoretical guidance for future advancements in the field. <div>
arXiv:2507.19527v1 Announce Type: new 
Abstract: Graph-structured data are pervasive across domains including social networks, biological networks, and knowledge graphs. Due to their non-Euclidean nature, such data pose significant challenges to conventional machine learning methods. This study investigates graph data structures, classical graph algorithms, and Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and comparative evaluation. Through comparative experiments, we quantitatively assess performance differences between traditional algorithms and GNNs in node classification and clustering tasks. Results show GNNs achieve substantial accuracy improvements of 43% to 70% over traditional methods. We further explore integration strategies between classical algorithms and GNN architectures, providing theoretical guidance for advancing graph representation learning research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction</title>
<link>https://arxiv.org/abs/2507.19529</link>
<guid>https://arxiv.org/abs/2507.19529</guid>
<content:encoded><![CDATA[
<div> Auction, Green hydrogen, Oman, Artificial intelligence, Maintenance

Summary:
An article discussing the strategic positioning of Oman in the global decarbonisation effort through national auctions and international partnerships for green hydrogen projects. The country has launched its third auction in the Duqm region, which is vulnerable to environmental fluctuations posing risks to productivity. Due to the lack of operational data from large-scale hydrogen facilities in desert environments, an Artificial Intelligence decision support system is proposed to develop a Maintenance Pressure Index (MPI) using meteorological data to predict maintenance demands on infrastructure. This tool aims to enhance regulatory foresight and operational decision-making by predicting risk levels and future maintenance needs, enabling temporal benchmarking for infrastructure performance assessment over time. It addresses the challenge of incorporating temporal risk intelligence into auction evaluation criteria despite the absence of historical operational benchmarks. 

<br /><br />Summary: <div>
arXiv:2507.19529v1 Announce Type: new 
Abstract: As green hydrogen emerges as a major component of global decarbonisation, Oman has positioned itself strategically through national auctions and international partnerships. Following two successful green hydrogen project rounds, the country launched its third auction (R3) in the Duqm region. While this area exhibits relative geospatial homogeneity, it is still vulnerable to environmental fluctuations that pose inherent risks to productivity. Despite growing global investment in green hydrogen, operational data remains scarce, with major projects like Saudi Arabia's NEOM facility not expected to commence production until 2026, and Oman's ACME Duqm project scheduled for 2028. This absence of historical maintenance and performance data from large-scale hydrogen facilities in desert environments creates a major knowledge gap for accurate risk assessment for infrastructure planning and auction decisions. Given this data void, environmental conditions emerge as accessible and reliable proxy for predicting infrastructure maintenance pressures, because harsh desert conditions such as dust storms, extreme temperatures, and humidity fluctuations are well-documented drivers of equipment degradation in renewable energy systems. To address this challenge, this paper proposes an Artificial Intelligence decision support system that leverages publicly available meteorological data to develop a predictive Maintenance Pressure Index (MPI), which predicts risk levels and future maintenance demands on hydrogen infrastructure. This tool strengthens regulatory foresight and operational decision-making by enabling temporal benchmarking to assess and validate performance claims over time. It can be used to incorporate temporal risk intelligence into auction evaluation criteria despite the absence of historical operational benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation</title>
<link>https://arxiv.org/abs/2507.19530</link>
<guid>https://arxiv.org/abs/2507.19530</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Blood Pressure Monitoring, Uncertainty Quantification, Data Leakage Prevention, Cross-Institutional Validation

Summary:
This study introduces a comprehensive framework for predicting blood pressure using electronic health records in intensive care units. The framework addresses limitations in current machine learning approaches by implementing novel algorithmic leakage prevention, uncertainty quantification through quantile regression, and external validation between different databases. An ensemble framework utilizing Gradient Boosting, Random Forest, and XGBoost achieved clinically acceptable performance in internal validation. External validation revealed a 30% degradation in performance, particularly in patients with hypotension. Uncertainty quantification produced valid prediction intervals, enabling risk-stratified monitoring protocols. The framework's source code is publicly available for deployment in critical care settings. <div>
arXiv:2507.19530v1 Announce Type: new 
Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs) where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings</title>
<link>https://arxiv.org/abs/2507.19534</link>
<guid>https://arxiv.org/abs/2507.19534</guid>
<content:encoded><![CDATA[
<div> Keywords: Pre-trained Language Models, Prompt-tuning, Federated Learning, Dynamic Prompt Generator, Data privacy

Summary:
Federated Learning (FL) is a technique that addresses concerns around data privacy by allowing model training on user devices rather than central servers. This paper introduces Federated Dynamic Prompt Generator (FedDPG), which utilizes a dynamic prompt generator network to generate context-aware prompts based on input. This approach offers flexibility and adaptability in prompt generation while maintaining data privacy. Experiments on three NLP benchmark datasets demonstrate that FedDPG outperforms existing parameter-efficient fine-tuning methods in terms of global model performance. Additionally, FedDPG significantly reduces computation time and the number of parameters transmitted through the FL network, addressing communication and computation limitations of clients. FedDPG's dynamic prompt generation enables better task-specific adaptation compared to fixed prompts used in traditional fine-tuning methods, showcasing its effectiveness in improving model performance. 

<br /><br />Summary: <div>
arXiv:2507.19534v1 Announce Type: new 
Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance in various NLP tasks. However, traditional fine-tuning methods for leveraging PLMs for downstream tasks entail significant computational overhead. Prompt-tuning has emerged as an efficient alternative that involves prepending a limited number of parameters to the input sequence and only updating them while the PLM's parameters are frozen. However, this technique's prompts remain fixed for all inputs, reducing the model's flexibility. The Federated Learning (FL) technique has gained attention in recent years to address the growing concerns around data privacy. However, challenges such as communication and computation limitations of clients still need to be addressed. To mitigate these challenges, this paper introduces the Federated Dynamic Prompt Generator (FedDPG), which incorporates a dynamic prompt generator network to generate context-aware prompts based on the given input, ensuring flexibility and adaptability while prioritising data privacy in federated learning settings. Our experiments on three NLP benchmark datasets showcase that FedDPG outperforms the state-of-the-art parameter-efficient fine-tuning methods in terms of global model performance, and has significantly reduced the calculation time and the number of parameters to be sent through the FL network.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Learning Metallic Glass Discovery from Wikipedia</title>
<link>https://arxiv.org/abs/2507.19536</link>
<guid>https://arxiv.org/abs/2507.19536</guid>
<content:encoded><![CDATA[
<div> Keywords: materials synthesis, metallic glasses, machine learning, graph neural networks, natural language processing

Summary:
- Materials synthesis, especially for metallic glasses, is slow and costly due to the complexities involved in finding the optimal combinations of elements to resist crystallization.
- Conventional data-driven approaches for materials design face limitations due to data scarcity and simplistic material encoding.
- The proposed method utilizes sophisticated data learning from material network representations encoded from Wikipedia using a language model.
- Graph neural networks with diverse architectures are employed as recommendation systems to uncover hidden relationships among materials.
- By leveraging Wikipedia embeddings from various languages, the study evaluates the effectiveness of natural languages in materials design. 

<br /><br />Summary: The study highlights the challenges in synthesizing metallic glasses efficiently and proposes a novel approach leveraging graph neural networks and natural language processing to extract valuable insights from material network representations. By encoding node elements from Wikipedia using a language model, the research aims to enhance the predictability and generalizability of models for materials design. With the potential to explore new relationships among materials and expand the scope of materials discovery, this data-driven methodology could revolutionize the field of materials science and open up pathways for discovering innovative amorphous materials and beyond with the power of artificial intelligence. <div>
arXiv:2507.19536v1 Announce Type: new 
Abstract: Synthesizing new materials efficiently is highly demanded in various research fields. However, this process is usually slow and expensive, especially for metallic glasses, whose formation strongly depends on the optimal combinations of multiple elements to resist crystallization. This constraint renders only several thousands of candidates explored in the vast material space since 1960. Recently, data-driven approaches armed by advanced machine learning techniques provided alternative routes for intelligent materials design. Due to data scarcity and immature material encoding, the conventional tabular data is usually mined by statistical learning algorithms, giving limited model predictability and generalizability. Here, we propose sophisticated data learning from material network representations. The node elements are encoded from the Wikipedia by a language model. Graph neural networks with versatile architectures are designed to serve as recommendation systems to explore hidden relationships among materials. By employing Wikipedia embeddings from different languages, we assess the capability of natural languages in materials design. Our study proposes a new paradigm to harvesting new amorphous materials and beyond with artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swift-Sarsa: Fast and Robust Linear Control</title>
<link>https://arxiv.org/abs/2507.19539</link>
<guid>https://arxiv.org/abs/2507.19539</guid>
<content:encoded><![CDATA[
<div> Keywords: SwiftTD, True Online Sarsa, operant conditioning benchmark, on-policy reinforcement learning, linear control

Summary:
The article introduces SwiftTD, a new algorithm for TD learning that outperforms existing methods in prediction tasks. The algorithm is extended to work for control problems by combining key ideas with True Online Sarsa, resulting in Swift-Sarsa. A benchmark called the operant conditioning benchmark is proposed for linear on-policy control, challenging agents to differentiate relevant signals from noisy ones. Swift-Sarsa successfully assigns credit to relevant signals without prior knowledge of problem structure, enabling efficient learning and representation search over numerous features without degradation in performance. This advancement in reinforcement learning techniques shows promise in handling noisy inputs and improving credit assignment in control tasks.<br /><br />Summary: The article introduces SwiftTD, a superior TD learning algorithm extended to control problems with the development of Swift-Sarsa. A benchmark challenging agents to differentiate relevant signals from noise is proposed, showcasing Swift-Sarsa's ability to assign credit efficiently. The algorithm allows for parallel representation learning over numerous features without performance degradation, advancing reinforcement learning capabilities. <div>
arXiv:2507.19539v1 Announce Type: new 
Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size optimization, a bound on the effective learning rate, and step-size decay. In their experiments SwiftTD outperformed True Online TD($\lambda$) and TD($\lambda$) on a variety of prediction tasks derived from Atari games, and its performance was robust to the choice of hyper-parameters. In this extended abstract we extend SwiftTD to work for control problems. We combine the key ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the $\textit{operant conditioning benchmark}$. The key challenge in the operant conditioning benchmark is that a very small subset of input signals are relevant for decision making. The majority of the signals are noise sampled from a non-stationary distribution. To learn effectively, the agent must learn to differentiate between the relevant signals and the noisy signals, and minimize prediction errors by assigning credit to the weight parameters associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to assign credit to the relevant signals without any prior knowledge of the structure of the problem. It opens the door for solution methods that learn representations by searching over hundreds of millions of features in parallel without performance degradation due to noisy or bad features.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection</title>
<link>https://arxiv.org/abs/2507.19547</link>
<guid>https://arxiv.org/abs/2507.19547</guid>
<content:encoded><![CDATA[
<div> Keywords: Atrial Fibrillation, Deep Learning, Electrograms, Unsupervised Feature Extraction, Ablation Procedures

Summary: 
The study introduces a deep learning approach using convolutional autoencoders to extract features from atrial electrograms during atrial fibrillation (AF) ablation procedures. The framework successfully learned latent representations of atrial electrical activity from a large database of EGM recordings. These extracted features facilitated the detection of AF drivers, rotational and focal activity, and EGM entanglement. The method achieved moderate performance in identifying AF activity patterns and high discriminative performance in detecting entanglement. The proposed approach operates in real-time and can be integrated into clinical mapping systems to assist in identifying arrhythmogenic regions during ablation procedures. This work demonstrates the potential of unsupervised learning in uncovering meaningful features from intracardiac signals, providing a promising tool for improving AF ablation outcomes. 

<br /><br />Summary: <div>
arXiv:2507.19547v1 Announce Type: new 
Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet current ablation therapies, including pulmonary vein isolation, are frequently ineffective in persistent AF due to the involvement of non-pulmonary vein drivers. This study proposes a deep learning framework using convolutional autoencoders for unsupervised feature extraction from unipolar and bipolar intracavitary electrograms (EGMs) recorded during AF in ablation studies. These latent representations of atrial electrical activity enable the characterization and automation of EGM analysis, facilitating the detection of AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients, containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders successfully learned latent representations with low reconstruction loss, preserving the morphological features. The extracted embeddings allowed downstream classifiers to detect rotational and focal activity with moderate performance (AUC 0.73-0.76) and achieved high discriminative performance in identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into clinical electroanatomical mapping systems to assist in identifying arrhythmogenic regions during ablation procedures. This work highlights the potential of unsupervised learning to uncover physiologically meaningful features from intracardiac signals.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing intuitive local evolution rules for physical learning</title>
<link>https://arxiv.org/abs/2507.19561</link>
<guid>https://arxiv.org/abs/2507.19561</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Power-efficient training scheme, BEASTAL, Boundary-Enabled Adaptive State Tuning Systems, local physical rules

Summary:
BEASTAL introduces a power-efficient training scheme for physical systems called Boundary-Enabled Adaptive State Tuning Systems (BEASTS). The scheme minimizes power dissipation by externally controlling only boundary parameters. BEASTAL, the analog of the Adaline algorithm, allows autonomous learning in silico for regression and classification tasks. This approach utilizes local evolution rules, advancing previous physical learning schemes without the need for large-scale memory or complex internal architectures. BEASTAL can handle any linear task and performs best with non-linear local evolution rules. The system offers a novel way for physical systems to learn by exploiting intuitive, local physical rules, presenting a promising alternative to traditional machine learning methods in terms of power consumption and computational efficiency. 

<br /><br />Summary: <div>
arXiv:2507.19561v1 Announce Type: new 
Abstract: Machine Learning, however popular and accessible, is computationally intensive and highly power-consuming, prompting interest in alternative physical implementations of learning tasks. We introduce a training scheme for physical systems that minimize power dissipation in which only boundary parameters (i.e. inputs and outputs) are externally controlled. Using this scheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by exploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the closest analog of the Adaline algorithm for such systems. We demonstrate this autonomous learning in silico for regression and classification tasks. Our approach advances previous physical learning schemes by using intuitive, local evolution rules without requiring large-scale memory or complex internal architectures. BEASTAL can perform any linear task, achieving best performance when the local evolution rule is non-linear.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition</title>
<link>https://arxiv.org/abs/2507.19627</link>
<guid>https://arxiv.org/abs/2507.19627</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Dual Decomposition, Wasserstein Barycenter, Distribution, Support, Mixture Models <br />
Summary: 
The article introduces an efficient federated dual decomposition algorithm for calculating the Wasserstein barycenter of multiple distributions while selecting the support of the solution. This algorithm operates without accessing local data and utilizes highly aggregated information, eliminating the need for repeated solutions to mass transportation problems. The absence of matrix-vector operations results in a low complexity per iteration and excellent scalability. The algorithm's performance is compared to state-of-the-art methods on various mixture model examples, showcasing its advantages. Overall, the proposed approach provides a computationally efficient solution for determining Wasserstein barycenters, offering significant benefits in terms of simplicity, scalability, and accuracy in comparison to existing methods. <br /><br />Summary: <div>
arXiv:2507.19627v1 Announce Type: new 
Abstract: We propose an efficient federated dual decomposition algorithm for calculating the Wasserstein barycenter of several distributions, including choosing the support of the solution. The algorithm does not access local data and uses only highly aggregated information. It also does not require repeated solutions to mass transportation problems. Because of the absence of any matrix-vector operations, the algorithm exhibits a very low complexity of each iteration and significant scalability. We illustrate its virtues and compare it to the state-of-the-art methods on several examples of mixture models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Scalable Agentic AI with Heterogeneous Systems</title>
<link>https://arxiv.org/abs/2507.19635</link>
<guid>https://arxiv.org/abs/2507.19635</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, dynamic orchestration, heterogeneous compute infrastructure, MLIR, TCO optimization

Summary: 
AI agents are complex workloads that require dynamic orchestration on heterogeneous compute infrastructure. This paper presents a system design that includes planning and optimizing AI execution graphs, a compilation system based on MLIR, and a dynamic orchestration system. By leveraging a mix of CPUs and accelerators from different vendors, the system aims to optimize total cost of ownership (TCO) while meeting SLAs. Preliminary results show that a heterogeneous infrastructure can provide significant TCO benefits compared to homogeneous GPU designs. Surprisingly, a combination of older generation GPUs with newer accelerators can offer similar TCO to the latest GPU infrastructure, potentially extending the life of deployed systems. This approach opens up opportunities for efficient and scalable deployment of AI agents in various applications. 

<br /><br />Summary: <div>
arXiv:2507.19635v1 Announce Type: new 
Abstract: AI agents are emerging as a dominant workload in a wide range of applications, promising to be the vehicle that delivers the promised benefits of AI to enterprises and consumers. Unlike conventional software or static inference, agentic workloads are dynamic and structurally complex. Often these agents are directed graphs of compute and IO operations that span multi-modal data input and conversion), data processing and context gathering (e.g vector DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage, we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for dynamic orchestration of AI agent workloads on heterogeneous compute infrastructure spanning CPUs and accelerators, both from different vendors and across different performance tiers within a single vendor. The system delivers several building blocks: a framework for planning and optimizing agentic AI execution graphs using cost models that account for compute, memory, and bandwidth constraints of different HW; a MLIR based representation and compilation system that can decompose AI agent execution graphs into granular operators and generate code for different HW options; and a dynamic orchestration system that can place the granular components across a heterogeneous compute infrastructure and stitch them together while meeting an end-to-end SLA. Our design performs a systems level TCO optimization and preliminary results show that leveraging a heterogeneous infrastructure can deliver significant TCO benefits. A preliminary surprising finding is that for some workloads a heterogeneous combination of older generation GPUs with newer accelerators can deliver similar TCO as the latest generation homogenous GPU infrastructure design, potentially extending the life of deployed infrastructure.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions</title>
<link>https://arxiv.org/abs/2507.19639</link>
<guid>https://arxiv.org/abs/2507.19639</guid>
<content:encoded><![CDATA[
<div> novel loss functions, decision-making, portfolio, stock trading, artificial neural network
<br />
Summary:<br />
The article introduces four innovative loss functions designed to drive decision-making for stock trading portfolios. These functions consider potential profits or losses from buying or shorting stocks, allowing artificial neural networks to learn effective trading strategies. Despite stock market volatility, training time-series models like transformers on these loss functions led to profitable trading strategies on a portfolio of 50 S&amp;P 500 company stocks. One model, the Crossformer, consistently outperformed state-of-the-art reinforcement learning methods like PPO and DDPG, achieving returns of 51.42%, 51.04%, and 48.62% in 2021, 2022, and 2023 respectively. In comparison, the best reinforcment learning methods only achieved maximum profits of around 41%, 2.81%, and 41.58% during the same periods. The code for the study is available for access. 
<br /><br />Summary: <div>
arXiv:2507.19639v1 Announce Type: new 
Abstract: Stock trading has always been a challenging task due to the highly volatile nature of the stock market. Making sound trading decisions to generate profit is particularly difficult under such conditions. To address this, we propose four novel loss functions to drive decision-making for a portfolio of stocks. These functions account for the potential profits or losses based with respect to buying or shorting respective stocks, enabling potentially any artificial neural network to directly learn an effective trading strategy. Despite the high volatility in stock market fluctuations over time, training time-series models such as transformers on these loss functions resulted in trading strategies that generated significant profits on a portfolio of 50 different S&amp;P 500 company stocks as compared to a benchmark reinforcment learning techniques and a baseline buy and hold method. As an example, using 2021, 2022 and 2023 as three test periods, the Crossformer model adapted with our best loss function was most consistent, resulting in returns of 51.42%, 51.04% and 48.62% respectively. In comparison, the best performing state-of-the-art reinforcement learning methods, PPO and DDPG, only delivered maximum profits of around 41%, 2.81% and 41.58% for the same periods. The code is available at https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature learning is decoupled from generalization in high capacity neural networks</title>
<link>https://arxiv.org/abs/2507.19680</link>
<guid>https://arxiv.org/abs/2507.19680</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural networks, kernel methods, feature quality, feature learning, generalization <br />
Summary: <br />
Neural networks have shown impressive performance compared to kernel methods, particularly in tasks like modeling staircase functions. This advantage is attributed to their ability to learn and adapt features to better capture data, a concept referred to as feature quality. Existing theories of feature learning focus more on evaluating the strength of feature learning rather than the quality of the learned features themselves. This limitation hinders the development of comprehensive theories of generalization in neural networks. By introducing the notion of feature quality and empirically examining current theories, the study highlights the need for a more intricate understanding of feature learning processes in neural networks. This research contributes to shedding light on the mechanisms underlying the superior performance of neural networks and the importance of feature quality in enhancing their generalization capabilities. <br /> 
Summary: <div>
arXiv:2507.19680v1 Announce Type: new 
Abstract: Neural networks outperform kernel methods, sometimes by orders of magnitude, e.g. on staircase functions. This advantage stems from the ability of neural networks to learn features, adapting their hidden representations to better capture the data. We introduce a concept we call feature quality to measure this performance improvement. We examine existing theories of feature learning and demonstrate empirically that they primarily assess the strength of feature learning, rather than the quality of the learned features themselves. Consequently, current theories of feature learning do not provide a sufficient foundation for developing theories of neural network generalization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2507.19684</link>
<guid>https://arxiv.org/abs/2507.19684</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid AI, motion capture dataset, salsa dancing, interactive communication, embodied movement<br />
<br />
Summary: <br />
The article introduces CoMPAS3D, a comprehensive motion capture dataset focused on improvised salsa dancing for interactive humanoid AI. The dataset includes performances from dancers of various skill levels, annotated with detailed expert analysis. Drawing parallels between dance communication and natural language, the dataset aims to challenge AI systems in understanding and executing partner dance interactions. The provided dataset, annotations, and code support benchmark tasks for synthetic humans, such as leader or follower generation with proficiency levels and duet generation. The proposed SalsaAgent model is capable of performing these tasks and serves as a baseline for further research in socially interactive embodied AI and creative humanoid motion generation. The release of CoMPAS3D offers a unique opportunity for researchers to explore the complexities of embodied movement and communication in AI systems. <br /> <div>
arXiv:2507.19684v1 Announce Type: new 
Abstract: Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System</title>
<link>https://arxiv.org/abs/2507.19686</link>
<guid>https://arxiv.org/abs/2507.19686</guid>
<content:encoded><![CDATA[
<div> Keywords: Controller Area Network, intrusion detection, Graph Attention Networks, knowledge distillation, cybersecurity

Summary:
The paper introduces KD-GAT, an intrusion detection framework that combines Graph Attention Networks (GATs) with knowledge distillation to enhance detection accuracy in the Controller Area Network (CAN) protocol, commonly used in vehicle communication systems. The framework represents CAN traffic as graphs to capture temporal and relational patterns, with a multi-layer GAT teacher model and a compact student model trained through supervised pretraining and knowledge distillation. Experimental results on benchmark datasets show high accuracy for both teacher and student models, particularly achieving 99.97% and 99.31% accuracy on two datasets. However, challenges arise with significant class imbalance in another dataset, leading to reduced performance for both models. Addressing this imbalance is identified as an important area for future research. 

<br /><br />Summary: <div>
arXiv:2507.19686v1 Announce Type: new 
Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle communication but lacks inherent security mechanisms, making it vulnerable to cyberattacks. This paper introduces KD-GAT, an intrusion detection framework that combines Graph Attention Networks (GATs) with knowledge distillation (KD) to enhance detection accuracy while reducing computational complexity. In our approach, CAN traffic is represented as graphs using a sliding window to capture temporal and relational patterns. A multi-layer GAT with jumping knowledge aggregation acting as the teacher model, while a compact student GAT--only 6.32% the size of the teacher--is trained via a two-phase process involving supervised pretraining and knowledge distillation with both soft and hard label supervision. Experiments on three benchmark datasets--Car-Hacking, Car-Survival, and can-train-and-test demonstrate that both teacher and student models achieve strong results, with the student model attaining 99.97% and 99.31% accuracy on Car-Hacking and Car-Survival, respectively. However, significant class imbalance in can-train-and-test has led to reduced performance for both models on this dataset. Addressing this imbalance remains an important direction for future work.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology</title>
<link>https://arxiv.org/abs/2507.19697</link>
<guid>https://arxiv.org/abs/2507.19697</guid>
<content:encoded><![CDATA[
<div> Keywords: co-visitation patterns, urban planning, retail analytics, graph neural network, business semantics

Summary:
NAICS-aware GraphSAGE is introduced to predict population-scale co-visitation patterns, leveraging business taxonomy knowledge and learnable embeddings to overcome data sparsity and capture complex relationships within massive datasets. The model integrates spatial, temporal, and socioeconomic features to enhance prediction accuracy. Evaluation on a dataset of 94.9 million co-visitation records across 92,486 brands and 48 US states shows a significant improvement in performance metrics over existing baselines. The R-squared value increases by 157 percent, and the ranking quality improves by 32 percent, highlighting the effectiveness of the proposed approach in understanding and predicting human mobility patterns post-business visitation. <div>
arXiv:2507.19697v1 Announce Type: new 
Abstract: Understanding where people go after visiting one business is crucial for urban planning, retail analytics, and location-based services. However, predicting these co-visitation patterns across millions of venues remains challenging due to extreme data sparsity and the complex interplay between spatial proximity and business relationships. Traditional approaches using only geographic distance fail to capture why coffee shops attract different customer flows than fine dining restaurants, even when co-located. We introduce NAICS-aware GraphSAGE, a novel graph neural network that integrates business taxonomy knowledge through learnable embeddings to predict population-scale co-visitation patterns. Our key insight is that business semantics, captured through detailed industry codes, provide crucial signals that pure spatial models cannot explain. The approach scales to massive datasets (4.2 billion potential venue pairs) through efficient state-wise decomposition while combining spatial, temporal, and socioeconomic features in an end-to-end framework. Evaluated on our POI-Graph dataset comprising 94.9 million co-visitation records across 92,486 brands and 48 US states, our method achieves significant improvements over state-of-the-art baselines: the R-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with strong gains in ranking quality (32 percent improvement in NDCG at 10).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disjoint Generative Models</title>
<link>https://arxiv.org/abs/2507.19700</link>
<guid>https://arxiv.org/abs/2507.19700</guid>
<content:encoded><![CDATA[
<div> disjoint generative models, cross-sectional synthetic datasets, privacy, utility cost, mixed-model synthesis  
Summary:  
Disjoint generative models are proposed as a new framework for generating cross-sectional synthetic datasets. The dataset is divided into separate subsets that are fed into distinct generative models, providing increased privacy with minimal loss of utility. The results from the generative models are then combined post hoc without the need for common variables or identifiers. Case studies and examples on tabular data demonstrate the success of this approach, highlighting its effectiveness and feasibility for certain model types. Additionally, the framework allows for mixed-model synthesis, offering flexibility in model selection. The key benefits of using disjoint generative models include improved privacy protection and the potential for increased utility in data synthesis tasks.<br /><br />Summary: <div>
arXiv:2507.19700v1 Announce Type: new 
Abstract: We propose a new framework for generating cross-sectional synthetic datasets via disjoint generative models. In this paradigm, a dataset is partitioned into disjoint subsets that are supplied to separate instances of generative models. The results are then combined post hoc by a joining operation that works in the absence of common variables/identifiers. The success of the framework is demonstrated through several case studies and examples on tabular data that helps illuminate some of the design choices that one may make. The principal benefit of disjoint generative models is significantly increased privacy at only a low utility cost. Additional findings include increased effectiveness and feasibility for certain model types and the possibility for mixed-model synthesis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search</title>
<link>https://arxiv.org/abs/2507.19715</link>
<guid>https://arxiv.org/abs/2507.19715</guid>
<content:encoded><![CDATA[
<div> Keywords: vector databases, approximate nearest neighbor search, semantic compression, graph-augmented vector retrieval, submodular optimization

Summary: 
Vector databases often rely on approximate nearest neighbor search for retrieving similar vectors to a query. However, this method can result in semantically redundant outcomes, lacking diversity and contextual richness for applications like retrieval-augmented generation and multi-hop QA. To address this, a new retrieval paradigm called semantic compression is introduced, emphasizing the selection of a compact and representative set of vectors that capture broader semantic structures around a query. This approach, based on submodular optimization and information geometry principles, prioritizes coverage and diversity over traditional top-k retrieval methods. The concept is operationalized through graph-augmented vector retrieval, which incorporates semantic graphs over vector spaces for context-aware, multi-hop search. The theoretical analysis reveals the limitations of proximity-based retrieval in high-dimensional spaces and the potential enhancements introduced by graph structures for semantic coverage. This work lays the groundwork for meaning-centric vector search systems, advocating for hybrid indexing, diversity-aware querying, and structured semantic retrieval. The implementation is publicly available to support future research in this area. 

<br /><br />Summary: <div>
arXiv:2507.19715v1 Announce Type: new 
Abstract: Vector databases typically rely on approximate nearest neighbor (ANN) search to retrieve the top-k closest vectors to a query in embedding space. While effective, this approach often yields semantically redundant results, missing the diversity and contextual richness required by applications such as retrieval-augmented generation (RAG), multi-hop QA, and memory-augmented agents. We introduce a new retrieval paradigm: semantic compression, which aims to select a compact, representative set of vectors that captures the broader semantic structure around a query. We formalize this objective using principles from submodular optimization and information geometry, and show that it generalizes traditional top-k retrieval by prioritizing coverage and diversity. To operationalize this idea, we propose graph-augmented vector retrieval, which overlays semantic graphs (e.g., kNN or knowledge-based links) atop vector spaces to enable multi-hop, context-aware search. We theoretically analyze the limitations of proximity-based retrieval under high-dimensional concentration and highlight how graph structures can improve semantic coverage. Our work outlines a foundation for meaning-centric vector search systems, emphasizing hybrid indexing, diversity-aware querying, and structured semantic retrieval. We make our implementation publicly available to foster future research in this area.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning</title>
<link>https://arxiv.org/abs/2507.19737</link>
<guid>https://arxiv.org/abs/2507.19737</guid>
<content:encoded><![CDATA[
<div> DisasterMobLLM, mobility prediction, natural disasters, deep learning, urbanization<br />
Summary:<br />
The article introduces DisasterMobLLM, a framework for predicting human mobility in disaster scenarios to aid in disaster response and resource allocation. Existing models are not effective in disaster scenarios due to shifting mobility patterns. DisasterMobLLM integrates with deep learning methods using Location Language Models (LLMs) to predict mobility intentions and transfer knowledge across different disasters in cities. The framework includes an Intention Predictor, Intention Refiner, and Location Predictor to forecast intentions and map them to specific locations. Experimental results show significant improvements in prediction accuracy and immobility detection compared to baseline methods. DisasterMobLLM offers a valuable tool for predicting human mobility in disaster scenarios, enhancing early warning systems and rescue operations. The code for DisasterMobLLM is available on GitHub for implementation. <br />Summary: <div>
arXiv:2507.19737v1 Announce Type: new 
Abstract: The vulnerability of cities to natural disasters has increased with urbanization and climate change, making it more important to predict human mobility in the disaster scenarios for downstream tasks including location-based early disaster warning and pre-allocating rescue resources, etc. However, existing human mobility prediction models are mainly designed for normal scenarios, and fail to adapt to disaster scenarios due to the shift of human mobility patterns under disaster. To address this issue, we introduce \textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios that can be integrated into existing deep mobility prediction methods by leveraging LLMs to model the mobility intention and transferring the common knowledge of how different disasters affect mobility intentions between cities. This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next intention, refines it with an LLM-based Intention Refiner, and then maps the intention to an exact location using an Intention-Modulated Location Predictor. Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\% improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score of predicting immobility compared to the baselines. The code is available at https://github.com/tsinghua-fib-lab/DisasterMobLLM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling enzyme temperature stability from sequence segment perspective</title>
<link>https://arxiv.org/abs/2507.19755</link>
<guid>https://arxiv.org/abs/2507.19755</guid>
<content:encoded><![CDATA[
<div> Keywords: enzymes, thermal stability, deep learning, protein engineering, cutinase

Summary:
- Developing enzymes with specific thermal properties is crucial for various industrial and research applications.
- Experimental determination of enzyme temperature stability is time-consuming and costly.
- Limited data availability and imbalanced distributions hinder existing computational approaches for predicting enzyme thermal behavior.
- The Segment Transformer, a deep learning framework leveraging segment-level representations, achieves accurate and efficient prediction of enzyme temperature stability.
- The model demonstrates state-of-the-art performance and is validated through the successful engineering of a cutinase enzyme, improving relative activity 1.64-fold following heat treatment with minimal mutations. 

<br /><br />Summary: <div>
arXiv:2507.19755v1 Announce Type: new 
Abstract: Developing enzymes with desired thermal properties is crucial for a wide range of industrial and research applications, and determining temperature stability is an essential step in this process. Experimental determination of thermal parameters is labor-intensive, time-consuming, and costly. Moreover, existing computational approaches are often hindered by limited data availability and imbalanced distributions. To address these challenges, we introduce a curated temperature stability dataset designed for model development and benchmarking in enzyme thermal modeling. Leveraging this dataset, we present the \textit{Segment Transformer}, a novel deep learning framework that enables efficient and accurate prediction of enzyme temperature stability. The model achieves state-of-the-art performance with an RMSE of 24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33, respectively. These results highlight the effectiveness of incorporating segment-level representations, grounded in the biological observation that different regions of a protein sequence contribute unequally to thermal behavior. As a proof of concept, we applied the Segment Transformer to guide the engineering of a cutinase enzyme. Experimental validation demonstrated a 1.64-fold improvement in relative activity following heat treatment, achieved through only 17 mutations and without compromising catalytic function.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19771</link>
<guid>https://arxiv.org/abs/2507.19771</guid>
<content:encoded><![CDATA[
<div> Keywords: Structural Drawings, Civil Engineering, Generative AI, Language Model, AutoCAD

Summary: 
Structural drawings play a crucial role in communication and documentation in civil engineering, but the current manual process is labor-intensive. This study introduces a novel AI-based method using a large language model for generating structural drawings efficiently. By incorporating a retrieval-augmented generation technique, the method can extract information from natural language descriptions and produce AutoCAD drawings accurately. This approach streamlines the drawing production process, reducing the workload for engineers and enabling the direct conversion of design ideas into tangible drawings. Overall, the method enhances the productivity and effectiveness of structural engineers by automating the generation of structural drawings based on natural language descriptions. <br /><br />Summary: <div>
arXiv:2507.19771v1 Announce Type: new 
Abstract: Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines</title>
<link>https://arxiv.org/abs/2507.19803</link>
<guid>https://arxiv.org/abs/2507.19803</guid>
<content:encoded><![CDATA[
<div> Tsetlin Machine, bladder cancer, AI model, transparent, decision-support tool
Summary:<br />
Bladder cancer is a prevalent and deadly disease, with a high recurrence rate in patients diagnosed with non-muscle-invasive bladder cancer. Existing clinical tools are deemed unreliable, especially for intermediate-risk cases. In this study, researchers propose an interpretable AI model using the Tsetlin Machine (TM), which outperformed traditional methods like XGBoost and Logistic Regression. The TM achieved an impressive F1-score of 0.80 on the PHOTO trial dataset. Unlike other models, TM provides transparent, human-readable logic behind each prediction, based on crucial clinical features such as tumour count, surgeon experience, and hospital stay. This interpretability makes TM a reliable and trustworthy decision-support tool for real-world adoption. <div>
arXiv:2507.19803v1 Announce Type: new 
Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur after treatment, triggering a relentless cycle of surgeries, monitoring, and risk of progression. Clinical tools like the EORTC risk tables are outdated and unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a symbolic learner that outputs transparent, human-readable logic. Tested on the PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the exact clauses behind each prediction, grounded in clinical features like tumour count, surgeon experience, and hospital stay - offering accuracy and full transparency. This makes TM a powerful, trustworthy decision-support tool ready for real-world adoption.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking Optimization Myths in Federated Learning for Medical Image Classification</title>
<link>https://arxiv.org/abs/2507.19822</link>
<guid>https://arxiv.org/abs/2507.19822</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, decentralized model training, data privacy, edge device configurations, optimizer selection

Summary: 
Federated Learning (FL) is a collaborative approach to training models that maintains data privacy while enabling decentralized model training. However, recent FL methods have shown sensitivity to local factors such as optimizer and learning rate choices, limiting their robustness in practical applications. In this study, the impact of edge device configurations on FL performance was investigated through benchmarking on tasks related to colorectal pathology and blood cell classification. Results indicate that local optimizer and learning rate selection have a larger influence on performance than the specific FL method employed. Additionally, the number of local training epochs can either enhance or hinder convergence, depending on the FL method. These findings emphasize the importance of tailored edge-specific configurations over algorithmic complexity for achieving effective FL. 

<br /><br />Summary: <div>
arXiv:2507.19822v1 Announce Type: new 
Abstract: Federated Learning (FL) is a collaborative learning method that enables decentralized model training while preserving data privacy. Despite its promise in medical imaging, recent FL methods are often sensitive to local factors such as optimizers and learning rates, limiting their robustness in practical deployments. In this work, we revisit vanilla FL to clarify the impact of edge device configurations, benchmarking recent FL methods on colorectal pathology and blood cell classification task. We numerically show that the choice of local optimizer and learning rate has a greater effect on performance than the specific FL method. Moreover, we find that increasing local training epochs can either enhance or impair convergence, depending on the FL method. These findings indicate that appropriate edge-specific configuration is more crucial than algorithmic complexity for achieving effective FL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning</title>
<link>https://arxiv.org/abs/2507.19839</link>
<guid>https://arxiv.org/abs/2507.19839</guid>
<content:encoded><![CDATA[
<div> method, Gradient Null Space Projection, continual learning, knowledge distillation, multimodal embedding space <br />
<br />
Summary: 
The article introduces Gradient Null Space Projection (GNSP), a method for continual learning in Contrastive Language-Image Pretraining (CLIP) that prevents catastrophic forgetting and degradation of embedding alignment. GNSP efficiently projects task-specific gradients onto the null space of previous knowledge to avoid interference with past tasks. This approach maintains the generalization capability of CLIP by incorporating knowledge distillation and a modality alignment preservation loss during fine-tuning. Tested on the MTIL benchmark, the method achieves state-of-the-art performance and preserves the modality gap and cross-modal retrieval performance of CLIP. Overall, GNSP enhances the robustness of the visual-language space in CLIP throughout continual learning, making it a promising solution for maintaining zero-shot capabilities in diverse tasks. <br /> <div>
arXiv:2507.19839v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot generalization by aligning visual and textual modalities in a shared embedding space. However, when continuously fine-tuned on diverse tasks, CLIP suffers from catastrophic forgetting and degradation of its embedding alignment, undermining its zero-shot capabilities. In this work, we propose Gradient Null Space Projection (GNSP), an efficient continual learning method that projects task-specific gradients onto the null space of previously learned knowledge. This orthogonal projection mathematically prevents interference with previous tasks without relying on rehearsal or architectural modification. Furthermore, to preserve the inherent generalization property of CLIP, we introduce knowledge distillation and combine it with a modality alignment preservation loss inspired by CLIP pre-training to stabilize the structure of the multimodal embedding space during fine-tuning. On the MTIL benchmark consisting of 11 tasks, our method achieved SOTA performance on both the Average and Last key metrics. More importantly, experiments show that our method successfully maintains the original modality gap and cross-modal retrieval performance of CLIP, confirming its effectiveness in maintaining a robust visual-language space throughout the continual learning process.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets</title>
<link>https://arxiv.org/abs/2507.19844</link>
<guid>https://arxiv.org/abs/2507.19844</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, distributed energy resources, local energy market, adversarial pricing, cooperation<br />
<br />
Summary: 
This paper presents a model for coordinating prosumers with different distributed energy resources in a local energy market. The model uses a multi-agent deep deterministic policy gradient framework for real-time decision-making. An investigation of an adversarial pricing strategy using a VAE-GAN model shows that prosumers, especially those without generation capabilities, experience financial losses. This effect remains consistent across markets of various sizes. With an increase in market size, trading stabilizes, leading to improved fairness through cooperative behavior among agents. <div>
arXiv:2507.19844v1 Announce Type: new 
Abstract: This paper introduces a model for coordinating prosumers with heterogeneous distributed energy resources (DERs), participating in the local energy market (LEM) that interacts with the market-clearing entity. The proposed LEM scheme utilizes a data-driven, model-free reinforcement learning approach based on the multi-agent deep deterministic policy gradient (MADDPG) framework, enabling prosumers to make real-time decisions on whether to buy, sell, or refrain from any action while facilitating efficient coordination for optimal energy trading in a dynamic market. In addition, we investigate a price manipulation strategy using a variational auto encoder-generative adversarial network (VAE-GAN) model, which allows utilities to adjust price signals in a way that induces financial losses for the prosumers. Our results show that under adversarial pricing, heterogeneous prosumer groups, particularly those lacking generation capabilities, incur financial losses. The same outcome holds across LEMs of different sizes. As the market size increases, trading stabilizes and fairness improves through emergent cooperation among agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets</title>
<link>https://arxiv.org/abs/2507.19846</link>
<guid>https://arxiv.org/abs/2507.19846</guid>
<content:encoded><![CDATA[
<div> Keywords: resolution identification, supervised learning, NLP models, clustering, telecom datasets

Summary:
This paper addresses the challenge of resolving problem tickets in the telecom sector using machine learning techniques. The proposed solution integrates clustering, supervised learning with Latent Dirichlet Allocation (LDA), Siamese networks, and advanced NLP models to handle data drift and issues like missing data and free text resolutions. The approach includes real-time dashboard and production deployment on Kubernetes for operational efficiency. By applying clustering for resolution identification and utilizing various supervised learning methods, the solution achieves high prediction accuracy. Experiments conducted on both open-source and proprietary datasets validate the effectiveness of the approach in accurately identifying solutions for incidents or problem tickets. This research offers a robust ML-driven approach for resolution management in telecom billing and charging systems. 

<br /><br />Summary: <div>
arXiv:2507.19846v1 Announce Type: new 
Abstract: Resolution of incidents or problem tickets is a common theme in service industries in any sector, including billing and charging systems in telecom domain. Machine learning can help to identify patterns and suggest resolutions for the problem tickets, based on patterns in the historical data of the tickets. However, this process may be complicated due to a variety of phenomena such as data drift and issues such as missing data, lack of data pertaining to resolutions of past incidents, too many similar sounding resolutions due to free text and similar sounding text. This paper proposes a robust ML-driven solution employing clustering, supervised learning, and advanced NLP models to tackle these challenges effectively. Building on previous work, we demonstrate clustering-based resolution identification, supervised classification with LDA, Siamese networks, and One-shot learning, Index embedding. Additionally, we present a real-time dashboard and a highly available Kubernetes-based production deployment. Our experiments with both the open-source Bitext customer-support dataset and proprietary telecom datasets demonstrate high prediction accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforced Policy Optimization</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, multi-turn reasoning, ARPO, external tools <br />
Summary:
The paper introduces Agentic Reinforced Policy Optimization (ARPO), a new reinforcement learning algorithm designed for training multi-turn agents based on large language models (LLMs). LLMs often use external tools in real-world reasoning tasks, but current RL algorithms struggle to balance long-horizon reasoning and multi-turn tool interactions. ARPO addresses this by incorporating an adaptive rollout mechanism based on entropy, promoting exploration after tool usage. It also includes advantage attribution estimation to help LLMs understand the benefits of using tools. Experiments across various reasoning benchmarks show that ARPO outperforms other RL algorithms, achieving better results with half the tool-use budget. This makes ARPO a scalable solution for aligning LLM-based agents with dynamic environments in real-time. The code and datasets for ARPO are available on GitHub for further research and development. <br /> 
Summary: <div>
arXiv:2507.19849v1 Announce Type: new 
Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning</title>
<link>https://arxiv.org/abs/2507.19855</link>
<guid>https://arxiv.org/abs/2507.19855</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Causal World Model Induction, Causal Physics Module, Causal Intervention Loss, Physical Reasoning Tasks

Summary: 
Causal World Model Induction (CWMI) is introduced to embed a model of causal physics in Large Language Models (LLMs). It includes a Causal Physics Module (CPM) and a new training objective called Causal Intervention Loss to promote learning cause-and-effect relationships from data. By predicting outcomes of interventions, CWMI develops a robust representation of physical laws, outperforming state-of-the-art LLMs on physical reasoning tasks such as the PIQA benchmark and PhysiCa-Bench dataset. This highlights the importance of inducing a causal world model for more reliable and generalizable AI systems. <br /><br /> <div>
arXiv:2507.19855v1 Announce Type: new 
Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities, fundamentally lack an intuitive understanding of physical dynamics, which limits their effectiveness in real-world scenarios that require causal reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a novel framework designed to embed an explicit model of causal physics within an LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a new training objective called Causal Intervention Loss, encouraging the model to learn cause-and-effect relationships from multimodal data. By training the model to predict the outcomes of hypothetical interventions instead of merely capturing statistical correlations, CWMI develops a robust internal representation of physical laws. Experimental results show that CWMI significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench dataset. These findings demonstrate that inducing a causal world model is a critical step toward more reliable and generalizable AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RestoreAI - Pattern-based Risk Estimation Of Remaining Explosives</title>
<link>https://arxiv.org/abs/2507.19873</link>
<guid>https://arxiv.org/abs/2507.19873</guid>
<content:encoded><![CDATA[
<div> AI, landmine risk prediction, pattern-based, clearance efficiency, RestoreAI
Summary:
RestoreAI introduces an AI system for predicting landmine risk based on spatial patterns, aiming to improve clearance time efficiency. The system includes three instances: linear, curved, and Bayesian pattern deminers. Evaluated on real-world data, RestoreAI significantly enhances clearance efficiency, increasing the average share of cleared landmines per timestep by 14.37 percentage points and reducing the time required to locate all landmines by 24.45% compared to baseline methods. Linear and curved pattern deminers perform similarly well, indicating that linear patterns can be a viable option for risk prediction. This groundbreaking approach of utilizing AI for pattern-based risk estimation shows promising potential for enhancing landmine removal processes worldwide. 
<br /><br />Summary: <div>
arXiv:2507.19873v1 Announce Type: new 
Abstract: Landmine removal is a slow, resource-intensive process affecting over 60 countries. While AI has been proposed to enhance explosive ordnance (EO) detection, existing methods primarily focus on object recognition, with limited attention to prediction of landmine risk based on spatial pattern information. This work aims to answer the following research question: How can AI be used to predict landmine risk from landmine patterns to improve clearance time efficiency? To that effect, we introduce RestoreAI, an AI system for pattern-based risk estimation of remaining explosives. RestoreAI is the first AI system that leverages landmine patterns for risk prediction, improving the accuracy of estimating the residual risk of missing EO prior to land release. We particularly focus on the implementation of three instances of RestoreAI, respectively, linear, curved and Bayesian pattern deminers. First, the linear pattern deminer uses linear landmine patterns from a principal component analysis (PCA) for the landmine risk prediction. Second, the curved pattern deminer uses curved landmine patterns from principal curves. Finally, the Bayesian pattern deminer incorporates prior expert knowledge by using a Bayesian pattern risk prediction. Evaluated on real-world landmine data, RestoreAI significantly boosts clearance efficiency. The top-performing pattern-based deminers achieved a 14.37 percentage point increase in the average share of cleared landmines per timestep and required 24.45% less time than the best baseline deminer to locate all landmines. Interestingly, linear and curved pattern deminers showed no significant performance difference, suggesting that more efficient linear patterns are a viable option for risk prediction.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2507.19887</link>
<guid>https://arxiv.org/abs/2507.19887</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, catastrophic forgetting, neural networks, Low-Rank Adaptation, resource efficiency

Summary:
Continual learning (CL) has traditionally focused on addressing catastrophic forgetting in neural networks when learning new tasks. However, current CL methods often require retraining the entire model for each task, leading to high computational demands. In real-world scenarios with limited computational resources, this poses a significant challenge. This study introduces CLoRA, a method that leverages Low-Rank Adaptation (LoRA) for class-incremental semantic segmentation. By using a small set of model parameters across all tasks, CLoRA achieves comparable or improved performance compared to baseline methods, while significantly reducing hardware requirements for training. The results highlight the importance of evaluating CL methods not only based on task performance but also on resource efficiency, making CLoRA suitable for deployment in resource-constrained environments. 

Summary: <br /><br />Continual learning (CL) addresses catastrophic forgetting in neural networks. Current methods require retraining the entire model for each task, posing computational challenges. CLoRA uses Low-Rank Adaptation (LoRA) for semantic segmentation, achieving performance comparable to baseline methods. By utilizing a small set of parameters across tasks, CLoRA reduces hardware requirements, making it suitable for resource-constrained environments. <div>
arXiv:2507.19887v1 Announce Type: new 
Abstract: In the past, continual learning (CL) was mostly concerned with the problem of catastrophic forgetting in neural networks, that arises when incrementally learning a sequence of tasks. Current CL methods function within the confines of limited data access, without any restrictions imposed on computational resources. However, in real-world scenarios, the latter takes precedence as deployed systems are often computationally constrained. A major drawback of most CL methods is the need to retrain the entire model for each new task. The computational demands of retraining large models can be prohibitive, limiting the applicability of CL in environments with limited resources. Through CLoRA, we explore the applicability of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method for class-incremental semantic segmentation. CLoRA leverages a small set of parameters of the model and uses the same set for learning across all tasks. Results demonstrate the efficacy of CLoRA, achieving performance on par with and exceeding the baseline methods. We further evaluate CLoRA using NetScore, underscoring the need to factor in resource efficiency and evaluate CL methods beyond task performance. CLoRA significantly reduces the hardware requirements for training, making it well-suited for CL in resource-constrained environments after deployment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction</title>
<link>https://arxiv.org/abs/2507.19894</link>
<guid>https://arxiv.org/abs/2507.19894</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, privacy concerns, machine unlearning, evaluation metrics, real-world applications

Summary: 
Generative models have raised privacy concerns, leading to the development of machine unlearning techniques in this area. However, there lacks a unified framework to organize and compare existing work in Generative Model Unlearning (GenMU). This study provides a comprehensive review of GenMU, proposing an analytical framework to categorize objectives, strategies, and evaluation metrics. Connections between GenMU and related techniques are explored, highlighting practical applications. Key challenges and future research directions are identified to advance the field. The open-source materials related to this work are consistently maintained at https://github.com/caxLee/Generative-model-unlearning-survey. 

<br /><br />Summary: <div>
arXiv:2507.19894v1 Announce Type: new 
Abstract: With the rapid advancement of generative models, associated privacy concerns have attracted growing attention. To address this, researchers have begun adapting machine unlearning techniques from traditional classification models to generative settings. Although notable progress has been made in this area, a unified framework for systematically organizing and integrating existing work is still lacking. The substantial differences among current studies in terms of unlearning objectives and evaluation protocols hinder the objective and fair comparison of various approaches. While some studies focus on specific types of generative models, they often overlook the commonalities and systematic characteristics inherent in Generative Model Unlearning (GenMU). To bridge this gap, we provide a comprehensive review of current research on GenMU and propose a unified analytical framework for categorizing unlearning objectives, methodological strategies, and evaluation metrics. In addition, we explore the connections between GenMU and related techniques, including model editing, reinforcement learning from human feedback, and controllable generation. We further highlight the potential practical value of unlearning techniques in real-world applications. Finally, we identify key challenges and outline future research directions aimed at laying a solid foundation for further advancements in this field. We consistently maintain the related open-source materials at https://github.com/caxLee/Generative-model-unlearning-survey.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.19964</link>
<guid>https://arxiv.org/abs/2507.19964</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Federated Learning, Membership Inference Attack, Node Classification Tasks, Privacy Threats <br />
Summary:<br /> 
The paper discusses privacy threats in federated learning settings involving Graph Neural Networks (GNNs) and presents a systematic study on cross-client membership inference attacks against node classification tasks. Unlike previous work focusing on sample inclusion in training, this attack targets sample-to-client attribution, a unique risk in federated settings. The study introduces a general attack framework leveraging FedGNNs' behaviors to link samples to their source clients across training rounds. Results from evaluations on various graph datasets demonstrate high performance in membership inference and ownership identification. The findings reveal a new privacy threat in federated graph learning, emphasizing the leakage of client identity through structural and model-level cues. The study underscores the importance of developing attribution-robust GNNs. <br /> <div>
arXiv:2507.19964v1 Announce Type: new 
Abstract: Graph-structured data is prevalent in many real-world applications, including social networks, financial systems, and molecular biology. Graph Neural Networks (GNNs) have become the de facto standard for learning from such data due to their strong representation capabilities. As GNNs are increasingly deployed in federated learning (FL) settings to preserve data locality and privacy, new privacy threats arise from the interaction between graph structures and decentralized training. In this paper, we present the first systematic study of cross-client membership inference attacks (CC-MIA) against node classification tasks of federated GNNs (FedGNNs), where a malicious client aims to infer which client owns the given data. Unlike prior centralized-focused work that focuses on whether a sample was included in training, our attack targets sample-to-client attribution, a finer-grained privacy risk unique to federated settings. We design a general attack framework that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding proximity to link samples to their source clients across training rounds. We evaluate our attack across multiple graph datasets under realistic FL setups. Results show that our method achieves high performance on both membership inference and ownership identification. Our findings highlight a new privacy threat in federated graph learning-client identity leakage through structural and model-level cues, motivating the need for attribution-robust GNN design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training</title>
<link>https://arxiv.org/abs/2507.19968</link>
<guid>https://arxiv.org/abs/2507.19968</guid>
<content:encoded><![CDATA[
<div> SGD, Adam, first-order optimization, Dimer method, neural networks <br />
Summary: 
DEO proposes a novel framework for optimizing neural networks by utilizing the Dimer method to estimate curvature efficiently. It adapts the Dimer method to explore the loss landscape, approximating the Hessian's smallest eigenvector without the need for the full matrix. By periodically projecting the gradient onto the orthogonal subspace to the minimum curvature direction, DEO helps guide the optimizer away from saddle points and flat regions, enhancing training efficiency with non-stepwise updates. Preliminary experiments on a Transformer toy model demonstrate that DEO achieves competitive performance compared to standard first-order methods, showcasing its ability to navigate complex loss landscapes in neural network training. The work repurposes physics-inspired, first-order curvature estimation techniques to improve training efficiency in high-dimensional spaces. <br /> <div>
arXiv:2507.19968v1 Announce Type: new 
Abstract: First-order optimization methods, such as SGD and Adam, are widely used for training large-scale deep neural networks due to their computational efficiency and robust performance. However, relying solely on gradient information, these methods often struggle to navigate complex loss landscapes with flat regions, plateaus, and saddle points. Second-order methods, which use curvature information from the Hessian matrix, can address these challenges but are computationally infeasible for large models. The Dimer method, a first-order technique that constructs two closely spaced points to probe the local geometry of a potential energy surface, efficiently estimates curvature using only gradient information. Inspired by its use in molecular dynamics simulations for locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel framework to escape saddle points in neural network training. DEO adapts the Dimer method to explore a broader region of the loss landscape, approximating the Hessian's smallest eigenvector without computing the full matrix. By periodically projecting the gradient onto the subspace orthogonal to the minimum curvature direction, DEO guides the optimizer away from saddle points and flat regions, enhancing training efficiency with non-stepwise updates. Preliminary experiments on a Transformer toy model show DEO achieves competitive performance compared to standard first-order methods, improving navigation of complex loss landscapes. Our work repurposes physics-inspired, first-order curvature estimation to enhance neural network training in high-dimensional spaces.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost</title>
<link>https://arxiv.org/abs/2507.20008</link>
<guid>https://arxiv.org/abs/2507.20008</guid>
<content:encoded><![CDATA[
<div> Graph Attention Networks, XGBoost, TimesNet, fare prediction, machine learning<br />
<br />
Summary:
This study compares Graph Attention Networks (GAT), XGBoost, and TimesNet models for predicting taxi fares using a dataset of 55 million records. It analyzes both raw and denoised data to understand the impact of data quality on model performance. The evaluation considers predictive accuracy, calibration, uncertainty estimation, OOD robustness, and feature sensitivity. Pre-processing strategies like KNN imputation, Gaussian noise injection, and autoencoder-based denoising are explored. The study highlights differences between classical and deep learning models and provides practical insights for creating robust models in urban fare prediction systems. <div>
arXiv:2507.20008v1 Announce Type: new 
Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban mobility systems. This study examines three machine learning models-Graph Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive capabilities for taxi fares using a real-world dataset comprising over 55 million records. Both raw (noisy) and denoised versions of the dataset are analyzed to assess the impact of data quality on model performance. The study evaluated the models along multiple axes, including predictive accuracy, calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and feature sensitivity. We also explore pre-processing strategies, including KNN imputation, Gaussian noise injection, and autoencoder-based denoising. The study reveals critical differences between classical and deep learning models under realistic conditions, offering practical guidelines for building robust and scalable models in urban fare prediction systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging</title>
<link>https://arxiv.org/abs/2507.20016</link>
<guid>https://arxiv.org/abs/2507.20016</guid>
<content:encoded><![CDATA[
<div> Algorithm, Federated Learning, Data Heterogeneity, Generalization, Optimization<br />
<br />
Summary: 
The paper explores the impact of data heterogeneity on the generalization capability of federated learning algorithms. It introduces two novel FL algorithms, FedSWA and FedMoSWA, incorporating Stochastic Weight Averaging and momentum-based controlled weight averaging, respectively. Theoretical analysis provides convergence guarantees and generalization bounds for these algorithms, demonstrating their superiority over existing methods like FedSAM. Empirical results on CIFAR10/100 and Tiny ImageNet datasets further validate the effectiveness of FedSWA and FedMoSWA in achieving better optimization and generalization performance. The open-source code for these algorithms is available on GitHub, enabling researchers to implement and experiment with the proposed approaches. <div>
arXiv:2507.20016v1 Announce Type: new 
Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization capability is crucial for real-word applications. In this paper, we revisit the generalization problem in FL and investigate the impact of data heterogeneity on FL generalization. We find that FedSAM usually performs worse than FedAvg in the case of highly heterogeneous data, and thus propose a novel and effective federated learning algorithm with Stochastic Weight Averaging (called \texttt{FedSWA}), which aims to find flatter minima in the setting of highly heterogeneous data. Moreover, we introduce a new momentum-based stochastic controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization and generalization errors of \texttt{FedMoSWA} are smaller than those of their counterparts, including FedSAM and its variants. Empirically, experimental results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the proposed algorithms compared to their counterparts. Open source code at: https://github.com/junkangLiu0/FedSWA.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irredundant k-Fold Cross-Validation</title>
<link>https://arxiv.org/abs/2507.20048</link>
<guid>https://arxiv.org/abs/2507.20048</guid>
<content:encoded><![CDATA[
<div> Irredundant k-fold cross-validation, instance usage, dataset balance, overfitting mitigation, model comparison.
Summary:<br />
- Irredundant k-fold cross-validation ensures each instance is used exactly once for training and testing, promoting dataset balance.
- Reducing overfitting by eliminating instance repetition during training, resulting in more accurate models.
- Sharper distinctions in model comparison enabled by the balanced utilization of the dataset.
- Preserves stratification and is compatible with any classifier, maintaining flexibility in model selection.
- Experimental results show consistent performance estimates comparable to traditional k-fold cross-validation, with lower variance and reduced computational costs.<br /><br />Summary: <div>
arXiv:2507.20048v1 Announce Type: new 
Abstract: In traditional k-fold cross-validation, each instance is used ($k\!-\!1$) times for training and once for testing, leading to redundancy that lets many instances disproportionately influence the learning phase. We introduce Irredundant $k$--fold cross-validation, a novel method that guarantees each instance is used exactly once for training and once for testing across the entire validation procedure. This approach ensures a more balanced utilization of the dataset, mitigates overfitting due to instance repetition, and enables sharper distinctions in comparative model analysis. The method preserves stratification and remains model-agnostic, i.e., compatible with any classifier. Experimental results demonstrate that it delivers consistent performance estimates across diverse datasets --comparable to $k$--fold cross-validation-- while providing less optimistic variance estimates because training partitions are non-overlapping, and significantly reducing the overall computational cost.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning</title>
<link>https://arxiv.org/abs/2507.20051</link>
<guid>https://arxiv.org/abs/2507.20051</guid>
<content:encoded><![CDATA[
<div> k-nearest neighbor, log anomaly detection, unsupervised, online detection, evaluation protocol
Summary:
The article introduces the $K^4$ framework for log anomaly detection, addressing issues of speed, parsing dependency, and evaluation protocols in existing methods. $K^4 leverages k-nearest neighbor statistics to transform log embeddings into four-dimensional descriptors for efficient detection. It is unsupervised, parser-independent, and enables lightweight detectors to accurately identify anomalies without retraining. $K^4 achieves a new state-of-the-art performance with AUROC ranging from 0.995 to 0.999, outperforming baseline methods significantly. Training time is under 4 seconds, and inference can be as fast as 4 s. The framework introduces a more realistic online evaluation protocol, demonstrating its high performance in real-time anomaly detection tasks.<br /><br />Summary: <div>
arXiv:2507.20051v1 Announce Type: new 
Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on error-prone parsing, and use unrealistic evaluation protocols. We introduce $K^4$, an unsupervised and parser-independent framework for high-performance online detection. $K^4$ transforms arbitrary log embeddings into compact four-dimensional descriptors (Precision, Recall, Density, Coverage) using efficient k-nearest neighbor (k-NN) statistics. These descriptors enable lightweight detectors to accurately score anomalies without retraining. Using a more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art (AUROC: 0.995-0.999), outperforming baselines by large margins while being orders of magnitude faster, with training under 4 seconds and inference as low as 4 $\mu$s.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Can Grokking Teach Us About Learning Under Nonstationarity?</title>
<link>https://arxiv.org/abs/2507.20057</link>
<guid>https://arxiv.org/abs/2507.20057</guid>
<content:encoded><![CDATA[
<div> feature-learning dynamics, neural networks, primacy bias, grokking, non-stationary learning

Summary: 
This paper explores the challenge of primacy bias in continual learning problems, where early training data can hinder a neural network's ability to generalize on later tasks. The phenomenon of grokking, where neural networks initially memorize data before exhibiting perfect generalization, is linked to feature-learning dynamics. The study suggests that these dynamics can also aid in overwriting previously learned features. By increasing the effective learning rate, the ratio between parameter and update norms, feature-learning dynamics can be induced throughout training. This method not only accelerates grokking but also improves generalization in various scenarios, including grokking, warm-starting neural network training, and reinforcement learning tasks.<br /><br />Summary: <div>
arXiv:2507.20057v1 Announce Type: new 
Abstract: In continual learning problems, it is often necessary to overwrite components of a neural network's learned representation in response to changes in the data stream; however, neural networks often exhibit \primacy bias, whereby early training data hinders the network's ability to generalize on later tasks. While feature-learning dynamics of nonstationary learning problems are not well studied, the emergence of feature-learning dynamics is known to drive the phenomenon of grokking, wherein neural networks initially memorize their training data and only later exhibit perfect generalization. This work conjectures that the same feature-learning dynamics which facilitate generalization in grokking also underlie the ability to overwrite previous learned features as well, and methods which accelerate grokking by facilitating feature-learning dynamics are promising candidates for addressing primacy bias in non-stationary learning problems. We then propose a straightforward method to induce feature-learning dynamics as needed throughout training by increasing the effective learning rate, i.e. the ratio between parameter and update norms. We show that this approach both facilitates feature-learning and improves generalization in a variety of settings, including grokking, warm-starting neural network training, and reinforcement learning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModShift: Model Privacy via Designed Shifts</title>
<link>https://arxiv.org/abs/2507.20060</link>
<guid>https://arxiv.org/abs/2507.20060</guid>
<content:encoded><![CDATA[
arXiv:2507.20060v1 Announce Type: new 
Abstract: In this paper, shifts are introduced to preserve model privacy against an eavesdropper in federated learning. Model learning is treated as a parameter estimation problem. This perspective allows us to derive the Fisher Information matrix of the model updates from the shifted updates and drive them to singularity, thus posing a hard estimation problem for Eve. The shifts are securely shared with the central server to maintain model accuracy at the server and participating devices. A convergence test is proposed to detect if model updates have been tampered with and we show that our scheme passes this test. Numerical results show that our scheme achieves a higher model shift when compared to a noise injection scheme while requiring a lesser bandwidth secret channel.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?</title>
<link>https://arxiv.org/abs/2507.20061</link>
<guid>https://arxiv.org/abs/2507.20061</guid>
<content:encoded><![CDATA[
arXiv:2507.20061v1 Announce Type: new 
Abstract: User-generated content (UGC) on social media platforms is vulnerable to incitements and manipulations, necessitating effective regulations. To address these challenges, those platforms often deploy automated content moderators tasked with evaluating the harmfulness of UGC and filtering out content that violates established guidelines. However, such moderation inevitably gives rise to strategic responses from users, who strive to express themselves within the confines of guidelines. Such phenomena call for a careful balance between: 1. ensuring freedom of speech -- by minimizing the restriction of expression; and 2. reducing social distortion -- measured by the total amount of content manipulation. We tackle the problem of optimizing this balance through the lens of mechanism design, aiming at optimizing the trade-off between minimizing social distortion and maximizing free speech. Although determining the optimal trade-off is NP-hard, we propose practical methods to approximate the optimal solution. Additionally, we provide generalization guarantees determining the amount of finite offline data required to approximate the optimal moderator effectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Operator Learning with Optimal Transport</title>
<link>https://arxiv.org/abs/2507.20065</link>
<guid>https://arxiv.org/abs/2507.20065</guid>
<content:encoded><![CDATA[
arXiv:2507.20065v1 Announce Type: new 
Abstract: We propose integrating optimal transport (OT) into operator learning for partial differential equations (PDEs) on complex geometries. Classical geometric learning methods typically represent domains as meshes, graphs, or point clouds. Our approach generalizes discretized meshes to mesh density functions, formulating geometry embedding as an OT problem that maps these functions to a uniform density in a reference space. Compared to previous methods relying on interpolation or shared deformation, our OT-based method employs instance-dependent deformation, offering enhanced flexibility and effectiveness. For 3D simulations focused on surfaces, our OT-based neural operator embeds the surface geometry into a 2D parameterized latent space. By performing computations directly on this 2D representation of the surface manifold, it achieves significant computational efficiency gains compared to volumetric simulation. Experiments with Reynolds-averaged Navier-Stokes equations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our method achieves better accuracy and also reduces computational expenses in terms of both time and memory usage compared to existing machine learning models. Additionally, our model demonstrates significantly improved accuracy on the FlowBench dataset, underscoring the benefits of employing instance-dependent deformation for datasets with highly variable geometries.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data</title>
<link>https://arxiv.org/abs/2507.20068</link>
<guid>https://arxiv.org/abs/2507.20068</guid>
<content:encoded><![CDATA[
arXiv:2507.20068v1 Announce Type: new 
Abstract: Off-policy evaluation (OPE) methods aim to estimate the value of a new reinforcement learning (RL) policy prior to deployment. Recent advances have shown that leveraging auxiliary datasets, such as those synthesized by generative models, can improve the accuracy of these value estimates. Unfortunately, such auxiliary datasets may also be biased, and existing methods for using data augmentation for OPE in RL lack principled uncertainty quantification. In high stakes settings like healthcare, reliable uncertainty estimates are important for comparing policy value estimates. In this work, we propose two approaches to construct valid confidence intervals for OPE when using data augmentation. The first provides a confidence interval over the policy performance conditioned on a particular initial state $V^{\pi}(s_0)$-- such intervals are particularly important for human-centered applications. To do so we introduce a new conformal prediction method for high dimensional state MDPs. Second, we consider the more common task of estimating the average policy performance over many initial states; to do so we draw on ideas from doubly robust estimation and prediction powered inference. Across simulators spanning robotics, healthcare and inventory management, and a real healthcare dataset from MIMIC-IV, we find that our methods can use augmented data and still consistently produce intervals that cover the ground truth values, unlike previously proposed methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems</title>
<link>https://arxiv.org/abs/2507.20072</link>
<guid>https://arxiv.org/abs/2507.20072</guid>
<content:encoded><![CDATA[
arXiv:2507.20072v1 Announce Type: new 
Abstract: Equation discovery is a fundamental learning task for uncovering the underlying dynamics of complex systems, with wide-ranging applications in areas such as brain connectivity analysis, climate modeling, gene regulation, and physical system simulation. However, many existing approaches rely on accurate derivative estimation and are limited to first-order dynamical systems, restricting their applicability to real-world scenarios. In this work, we propose sparse equation matching (SEM), a unified framework that encompasses several existing equation discovery methods under a common formulation. SEM introduces an integral-based sparse regression method using Green's functions, enabling derivative-free estimation of differential operators and their associated driving functions in general-order dynamical systems. The effectiveness of SEM is demonstrated through extensive simulations, benchmarking its performance against derivative-based approaches. We then apply SEM to electroencephalographic (EEG) data recorded during multiple oculomotor tasks, collected from 52 participants in a brain-computer interface experiment. Our method identifies active brain regions across participants and reveals task-specific connectivity patterns. These findings offer valuable insights into brain connectivity and the underlying neural mechanisms.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection</title>
<link>https://arxiv.org/abs/2507.20078</link>
<guid>https://arxiv.org/abs/2507.20078</guid>
<content:encoded><![CDATA[
arXiv:2507.20078v1 Announce Type: new 
Abstract: Recent pre-trained transformer models achieve superior performance in various code processing objectives. However, although effective at optimizing decision boundaries, common approaches for fine-tuning them for downstream classification tasks - distance-based methods or training an additional classification head - often fail to thoroughly structure the embedding space to reflect nuanced intra-class semantic relationships. Equivalent code mutant detection is one of these tasks, where the quality of the embedding space is crucial to the performance of the models. We introduce a novel framework that integrates cross-entropy loss with a deep metric learning objective, termed Cluster Purge Loss. This objective, unlike conventional approaches, concentrates on adjusting fine-grained differences within each class, encouraging the separation of instances based on semantical equivalency to the class center using dynamically adjusted borders. Employing UniXCoder as the base model, our approach demonstrates state-of-the-art performance in the domain of equivalent mutant detection and produces a more interpretable embedding space.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feed-anywhere ANN (I) Steady Discrete $\to$ Diffusing on Graph Hidden States</title>
<link>https://arxiv.org/abs/2507.20088</link>
<guid>https://arxiv.org/abs/2507.20088</guid>
<content:encoded><![CDATA[
arXiv:2507.20088v1 Announce Type: new 
Abstract: We propose a novel framework for learning hidden graph structures from data using geometric analysis and nonlinear dynamics. Our approach: (1) Defines discrete Sobolev spaces on graphs for scalar/vector fields, establishing key functional properties; (2) Introduces gauge-equivalent nonlinear Schr\"odinger and Landau--Lifshitz dynamics with provable stable stationary solutions smoothly dependent on input data and graph weights; (3) Develops a stochastic gradient algorithm over graph moduli spaces with sparsity regularization. Theoretically, we guarantee: topological correctness (homology recovery), metric convergence (Gromov--Hausdorff), and efficient search space utilization. Our dynamics-based model achieves stronger generalization bounds than standard neural networks, with complexity dependent on the data manifold's topology.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning</title>
<link>https://arxiv.org/abs/2507.20089</link>
<guid>https://arxiv.org/abs/2507.20089</guid>
<content:encoded><![CDATA[
arXiv:2507.20089v1 Announce Type: new 
Abstract: Developing effective multimodal data fusion strategies has become increasingly essential for improving the predictive power of statistical machine learning methods across a wide range of applications, from autonomous driving to medical diagnosis. Traditional fusion methods, including early, intermediate, and late fusion, integrate data at different stages, each offering distinct advantages and limitations. In this paper, we introduce Meta Fusion, a flexible and principled framework that unifies these existing strategies as special cases. Motivated by deep mutual learning and ensemble learning, Meta Fusion constructs a cohort of models based on various combinations of latent representations across modalities, and further boosts predictive performance through soft information sharing within the cohort. Our approach is model-agnostic in learning the latent representations, allowing it to flexibly adapt to the unique characteristics of each modality. Theoretically, our soft information sharing mechanism reduces the generalization error. Empirically, Meta Fusion consistently outperforms conventional fusion strategies in extensive simulation studies. We further validate our approach on real-world applications, including Alzheimer's disease detection and neural decoding.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoTransformer: Attention without Multiplication</title>
<link>https://arxiv.org/abs/2507.20096</link>
<guid>https://arxiv.org/abs/2507.20096</guid>
<content:encoded><![CDATA[
arXiv:2507.20096v1 Announce Type: new 
Abstract: The Transformer, with its scaled dot-product attention mechanism, has become a foundational architecture in modern AI. However, this mechanism is computationally intensive and incurs substantial energy costs. We propose a new Transformer architecture EcoTransformer, in which the output context vector is constructed as the convolution of the values using a Laplacian kernel, where the distances are measured by the L1 metric between the queries and keys. Compared to dot-product based attention, the new attention score calculation is free of matrix multiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP, bioinformatics, and vision tasks, while consuming significantly less energy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graded Transformers: A Symbolic-Geometric Approach to Structured Learning</title>
<link>https://arxiv.org/abs/2507.20108</link>
<guid>https://arxiv.org/abs/2507.20108</guid>
<content:encoded><![CDATA[
arXiv:2507.20108v1 Announce Type: new 
Abstract: We introduce the Graded Transformer framework, a novel class of sequence models that embeds algebraic inductive biases through grading transformations on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we propose two architectures: the Linearly Graded Transformer (LGT) and the Exponentially Graded Transformer (EGT). These models apply parameterized scaling operators-governed by fixed or learnable grading tuples and, for EGT, exponential factors to infuse hierarchical structure into attention and representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation theorems for continuous and Sobolev functions, reduced sample complexity via effective VC dimension bounds, Lipschitz continuity of graded operations, and robustness to adversarial perturbations. A graded loss function ensures gradient stability and alignment with domain priors during optimization. By treating grades as differentiable parameters, the framework enables adaptive feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical learning and neurosymbolic reasoning, with applications spanning algebraic geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale simulations), natural language processing (e.g., syntactic parsing), biological sequence analysis (e.g., variant prediction), and emerging areas like graph neural networks and financial modeling. This work advances structured deep learning by fusing geometric and algebraic principles with attention mechanisms, offering a mathematically grounded alternative to data-driven models and paving the way for interpretable, efficient systems in complex domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning with Probing for Sequential User-Centric Selection</title>
<link>https://arxiv.org/abs/2507.20112</link>
<guid>https://arxiv.org/abs/2507.20112</guid>
<content:encoded><![CDATA[
arXiv:2507.20112v1 Announce Type: new 
Abstract: We formalize sequential decision-making with information acquisition as the probing-augmented user-centric selection (PUCS) framework, where a learner first probes a subset of arms to obtain side information on resources and rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such as ridesharing, wireless scheduling, and content recommendation, in which both resources and payoffs are initially unknown and probing is costly. For the offline setting with known distributions, we present a greedy probing algorithm with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the online setting with unknown distributions, we introduce OLPA, a stochastic combinatorial bandit algorithm that achieves a regret bound $\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound $\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic factors. Experiments on real-world data demonstrate the effectiveness of our solutions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wine Characterisation with Spectral Information and Predictive Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.20114</link>
<guid>https://arxiv.org/abs/2507.20114</guid>
<content:encoded><![CDATA[
arXiv:2507.20114v1 Announce Type: new 
Abstract: The purpose of this paper is to use absorbance data obtained by human tasting and an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the attributes of grape juice (GJ) and to classify the wine's origin, respectively. The approach combined machine learning (ML) techniques with spectroscopy to find a relatively simple way to apply them in two stages of winemaking and help improve the traditional wine analysis methods regarding sensory data and wine's origins. This new technique has overcome the disadvantages of the complex sensors by taking advantage of spectral fingerprinting technology and forming a comprehensive study of the employment of AI in the wine analysis domain. In the results, Support Vector Machine (SVM) was the most efficient and robust in both attributes and origin prediction tasks. Both the accuracy and F1 score of the origin prediction exceed 91%. The feature ranking approach found that the more influential wavelengths usually appear at the lower end of the scan range, 250 nm (nanometers) to 420 nm, which is believed to be of great help for selecting appropriate validation methods and sensors to extract wine data in future research. The knowledge of this research provides new ideas and early solutions for the wine industry or other beverage industries to integrate big data and IoT in the future, which significantly promotes the development of 'Smart Wineries'.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing</title>
<link>https://arxiv.org/abs/2507.20127</link>
<guid>https://arxiv.org/abs/2507.20127</guid>
<content:encoded><![CDATA[
arXiv:2507.20127v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning graph representations, primarily because of their message-passing mechanisms. However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or Sum without principled reasoning behind the selection. This rigidity, especially in the presence of heterophily, often leads to poor, problem dependent performance. Although some attempts address this by designing more sophisticated aggregation functions, these methods tend to rely heavily on labeled data, which is often scarce in real-world tasks. In this work, we propose a novel unsupervised framework, "Aggregation-aware Multilayer Perceptron" (AMLP), which shifts the paradigm from directly crafting aggregation functions to making MLP adaptive to aggregation. Our lightweight approach consists of two key steps: First, we utilize a graph reconstruction method that facilitates high-order grouping effects, and second, we employ a single-layer network to encode varying degrees of heterophily, thereby improving the capacity and applicability of the model. Extensive experiments on node clustering and classification demonstrate the superior performance of AMLP, highlighting its potential for diverse graph learning scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design</title>
<link>https://arxiv.org/abs/2507.20130</link>
<guid>https://arxiv.org/abs/2507.20130</guid>
<content:encoded><![CDATA[
arXiv:2507.20130v1 Announce Type: new 
Abstract: Recent advances in generative models, particularly diffusion and auto-regressive models, have revolutionized fields like computer vision and natural language processing. However, their application to structure-based drug design (SBDD) remains limited due to critical data constraints. To address the limitation of training data for models targeting SBDD tasks, we propose an evolutionary framework named MEVO, which bridges the gap between billion-scale small molecule dataset and the scarce protein-ligand complex dataset, and effectively increase the abundance of training data for generative SBDD models. MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule representation in latent space, a diffusion model for pharmacophore-guided molecule generation, and a pocket-aware evolutionary strategy for molecule optimization with physics-based scoring function. This framework efficiently generate high-affinity binders for various protein targets, validated with predicted binding affinities using free energy perturbation (FEP) methods. In addition, we showcase the capability of MEVO in designing potent inhibitors to KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with similar affinity to the known highly active inhibitor evaluated by FEP calculations. With high versatility and generalizability, MEVO offers an effective and data-efficient model for various tasks in structure-based ligand design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Awesome-OL: An Extensible Toolkit for Online Learning</title>
<link>https://arxiv.org/abs/2507.20144</link>
<guid>https://arxiv.org/abs/2507.20144</guid>
<content:encoded><![CDATA[
arXiv:2507.20144v1 Announce Type: new 
Abstract: In recent years, online learning has attracted increasing attention due to its adaptive capability to process streaming and non-stationary data. To facilitate algorithm development and practical deployment in this area, we introduce Awesome-OL, an extensible Python toolkit tailored for online learning research. Awesome-OL integrates state-of-the-art algorithm, which provides a unified framework for reproducible comparisons, curated benchmark datasets, and multi-modal visualization. Built upon the scikit-multiflow open-source infrastructure, Awesome-OL emphasizes user-friendly interactions without compromising research flexibility or extensibility. The source code is publicly available at: https://github.com/liuzy0708/Awesome-OL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASNN: Learning to Suggest Neural Architectures from Performance Distributions</title>
<link>https://arxiv.org/abs/2507.20164</link>
<guid>https://arxiv.org/abs/2507.20164</guid>
<content:encoded><![CDATA[
arXiv:2507.20164v1 Announce Type: new 
Abstract: The architecture of a neural network (NN) plays a critical role in determining its performance. However, there is no general closed-form function that maps between network structure and accuracy, making the process of architecture design largely heuristic or search-based. In this study, we propose the Architecture Suggesting Neural Network (ASNN), a model designed to learn the relationship between NN architecture and its test accuracy, and to suggest improved architectures accordingly. To train ASNN, we constructed datasets using TensorFlow-based models with varying numbers of layers and nodes. Experimental results were collected for both 2-layer and 3-layer architectures across a grid of configurations, each evaluated with 10 repeated trials to account for stochasticity. Accuracy values were treated as inputs, and architectural parameters as outputs. The trained ASNN was then used iteratively to predict architectures that yield higher performance. In both 2-layer and 3-layer cases, ASNN successfully suggested architectures that outperformed the best results found in the original training data. Repeated prediction and retraining cycles led to the discovery of architectures with improved mean test accuracies, demonstrating the model's capacity to generalize the performance-structure relationship. These results suggest that ASNN provides an efficient alternative to random search for architecture optimization, and offers a promising approach toward automating neural network design. "Parts of the manuscript, including text editing and expression refinement, were supported by OpenAI's ChatGPT. All content was reviewed and verified by the authors."
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Domain Adaptation via Importance Sampling-based Shift Correction</title>
<link>https://arxiv.org/abs/2507.20191</link>
<guid>https://arxiv.org/abs/2507.20191</guid>
<content:encoded><![CDATA[
arXiv:2507.20191v1 Announce Type: new 
Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine learning scenarios. It aims to transfer knowledge from a labeled source domain to a related unlabeled target domain, where the support set of the source label distribution subsumes the target one. Previous PDA works managed to correct the label distribution shift by weighting samples in the source domain. However, the simple reweighing technique cannot explore the latent structure and sufficiently use the labeled data, and then models are prone to over-fitting on the source domain. In this work, we propose a novel importance sampling-based shift correction (IS$^2$C) method, where new labeled data are sampled from a built sampling domain, whose label distribution is supposed to be the same as the target domain, to characterize the latent structure and enhance the generalization ability of the model. We provide theoretical guarantees for IS$^2$C by proving that the generalization error can be sufficiently dominated by IS$^2$C. In particular, by implementing sampling with the mixture distribution, the extent of shift between source and sampling domains can be connected to generalization error, which provides an interpretable way to build IS$^2$C. To improve knowledge transfer, an optimal transport-based independence criterion is proposed for conditional distribution alignment, where the computation of the criterion can be adjusted to reduce the complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive experiments on PDA benchmarks validate the theoretical results and demonstrate the effectiveness of our IS$^2$C over existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading</title>
<link>https://arxiv.org/abs/2507.20202</link>
<guid>https://arxiv.org/abs/2507.20202</guid>
<content:encoded><![CDATA[
arXiv:2507.20202v1 Announce Type: new 
Abstract: This work proposes that a vast majority of classical technical indicators in financial analysis are, in essence, special cases of neural networks with fixed and interpretable weights. It is shown that nearly all such indicators, such as moving averages, momentum-based oscillators, volatility bands, and other commonly used technical constructs, can be reconstructed topologically as modular neural network components. Technical Indicator Networks (TINs) are introduced as a general neural architecture that replicates and structurally upgrades traditional indicators by supporting n-dimensional inputs such as price, volume, sentiment, and order book data. By encoding domain-specific knowledge into neural structures, TINs modernize the foundational logic of technical analysis and propel algorithmic trading into a new era, bridging the legacy of proven indicators with the potential of contemporary AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design</title>
<link>https://arxiv.org/abs/2507.20243</link>
<guid>https://arxiv.org/abs/2507.20243</guid>
<content:encoded><![CDATA[
arXiv:2507.20243v1 Announce Type: new 
Abstract: SE(3)-based generative models have shown great promise in protein geometry modeling and effective structure design. However, the field currently lacks a modularized benchmark to enable comprehensive investigation and fair comparison of different methods. In this paper, we propose Protein-SE(3), a new benchmark based on a unified training framework, which comprises protein scaffolding tasks, integrated generative models, high-level mathematical abstraction, and diverse evaluation metrics. Recent advanced generative models designed for protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2), Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and FrameFlow) are integrated into our framework. All integrated methods are fairly investigated with the same training dataset and evaluation metrics. Furthermore, we provide a high-level abstraction of the mathematical foundations behind the generative models, enabling fast prototyping of future algorithms without reliance on explicit protein structures. Accordingly, we release the first comprehensive benchmark built upon unified training framework for SE(3)-based protein structure design, which is publicly accessible at https://github.com/BruthYU/protein-se3.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining</title>
<link>https://arxiv.org/abs/2507.20263</link>
<guid>https://arxiv.org/abs/2507.20263</guid>
<content:encoded><![CDATA[
arXiv:2507.20263v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has successfully automated the complex process of mining formulaic alpha factors, for creating interpretable and profitable investment strategies. However, existing methods are hampered by the sparse rewards given the underlying Markov Decision Process. This inefficiency limits the exploration of the vast symbolic search space and destabilizes the training process. To address this, Trajectory-level Reward Shaping (TLRS), a novel reward shaping method, is proposed. TLRS provides dense, intermediate rewards by measuring the subsequence-level similarity between partially generated expressions and a set of expert-designed formulas. Furthermore, a reward centering mechanism is introduced to reduce training variance. Extensive experiments on six major Chinese and U.S. stock indices show that TLRS significantly improves the predictive power of mined factors, boosting the Rank Information Coefficient by 9.29% over existing potential-based shaping algorithms. Notably, TLRS achieves a major leap in computational efficiency by reducing its time complexity with respect to the feature dimension from linear to constant, which is a significant improvement over distance-based baselines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Prediction-Powered Calibration via Cross-Validation</title>
<link>https://arxiv.org/abs/2507.20268</link>
<guid>https://arxiv.org/abs/2507.20268</guid>
<content:encoded><![CDATA[
arXiv:2507.20268v1 Announce Type: new 
Abstract: Calibration data are necessary to formally quantify the uncertainty of the decisions produced by an existing artificial intelligence (AI) model. To overcome the common issue of scarce calibration data, a promising approach is to employ synthetic labels produced by a (generally different) predictive model. However, fine-tuning the label-generating predictor on the inference task of interest, as well as estimating the residual bias of the synthetic labels, demand additional data, potentially exacerbating the calibration data scarcity problem. This paper introduces a novel approach that efficiently utilizes limited calibration data to simultaneously fine-tune a predictor and estimate the bias of the synthetic labels. The proposed method yields prediction sets with rigorous coverage guarantees for AI-generated decisions. Experimental results on an indoor localization problem validate the effectiveness and performance gains of our solution.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence</title>
<link>https://arxiv.org/abs/2507.20272</link>
<guid>https://arxiv.org/abs/2507.20272</guid>
<content:encoded><![CDATA[
arXiv:2507.20272v1 Announce Type: new 
Abstract: Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty estimates (e.g. pretrained models predicting only point estimates), popular approaches that operate post-hoc include Laplace's method and split conformal prediction (split-CP). However, Laplace's method can be miscalibrated when the model is misspecified and split-CP requires sample splitting, and thus comes at the expense of statistical efficiency. In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. Coupled with linearization of the network, we express the absolute residual nonconformity score as a piecewise linear function of the candidate label allowing for an efficient procedure that avoids the exhaustive search over the output space. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2507.20326</link>
<guid>https://arxiv.org/abs/2507.20326</guid>
<content:encoded><![CDATA[
arXiv:2507.20326v1 Announce Type: new 
Abstract: Polymers, composed of repeating structural units called monomers, are fundamental materials in daily life and industry. Accurate property prediction for polymers is essential for their design, development, and application. However, existing modeling approaches, which typically represent polymers by the constituent monomers, struggle to capture the whole properties of polymer, since the properties change during the polymerization process. In this study, we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training framework, which represents polymers as infinite sequences of monomers and integrates both topological and spatial information for comprehensive modeling. From the topological perspective, we generalize message passing mechanism (MPM) and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we demonstrate that applying MPM to infinite polymer sequences is equivalent to applying MPM on the induced star-linking graph of monomers. For GAM, we propose to further replace global graph attention with localized graph attention (LGA). Moreover, we show the robustness of the "star linking" strategy through Repeat and Shift Invariance Test (RSIT). Despite its robustness, "star linking" strategy exhibits limitations when monomer side chains contain ring structures, a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL) test. To overcome this issue, we propose backbone embedding to enhance the capability of MPM and LGA on infinite polymer sequences. From the spatial perspective, we extract 3D descriptors of repeating monomers to capture spatial information. Finally, we design a cross-modal fusion mechanism to unify the topological and spatial information. Experimental validation across eight diverse polymer property prediction tasks reveals that MIPS achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20335</link>
<guid>https://arxiv.org/abs/2507.20335</guid>
<content:encoded><![CDATA[
arXiv:2507.20335v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) into education presents unprecedented opportunities for scalable personalized learning. However, standard LLMs often function as generic information providers, lacking alignment with fundamental pedagogical principles such as helpfulness, student-centered personalization, and creativity cultivation. To bridge this gap, we propose EduAlign, a novel framework designed to guide LLMs toward becoming more effective and responsible educational assistants. EduAlign consists of two main stages. In the first stage, we curate a dataset of 8k educational interactions and annotate them-both manually and automatically-along three key educational dimensions: Helpfulness, Personalization, and Creativity (HPC). These annotations are used to train HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM outputs according to these educational principles. We further evaluate the consistency and reliability of this reward model. In the second stage, we leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then assess the pre- and post-finetuning models on both educational and general-domain benchmarks across the three HPC dimensions. Experimental results demonstrate that the fine-tuned model exhibits significantly improved alignment with pedagogical helpfulness, personalization, and creativity stimulation. This study presents a scalable and effective approach to aligning LLMs with nuanced and desirable educational traits, paving the way for the development of more engaging, pedagogically aligned AI tutors.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery</title>
<link>https://arxiv.org/abs/2507.20349</link>
<guid>https://arxiv.org/abs/2507.20349</guid>
<content:encoded><![CDATA[
arXiv:2507.20349v1 Announce Type: new 
Abstract: Causal discovery from observational data is challenging, especially with large datasets and complex relationships. Traditional methods often struggle with scalability and capturing global structural information. To overcome these limitations, we introduce a novel graph neural network (GNN)-based probabilistic framework that learns a probability distribution over the entire space of causal graphs, unlike methods that output a single deterministic graph. Our framework leverages a GNN that encodes both node and edge attributes into a unified graph representation, enabling the model to learn complex causal structures directly from data. The GNN model is trained on a diverse set of synthetic datasets augmented with statistical and information-theoretic measures, such as mutual information and conditional entropy, capturing both local and global data properties. We frame causal discovery as a supervised learning problem, directly predicting the entire graph structure. Our approach demonstrates superior performance, outperforming both traditional and recent non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy and scalability on synthetic and real-world datasets without further training. This probabilistic framework significantly improves causal structure learning, with broad implications for decision-making and scientific discovery across various fields.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights</title>
<link>https://arxiv.org/abs/2507.20351</link>
<guid>https://arxiv.org/abs/2507.20351</guid>
<content:encoded><![CDATA[
arXiv:2507.20351v1 Announce Type: new 
Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform the standard single-grade deep learning (SGDL) across various applications. This work aims to investigate the computational advantages of MGDL focusing on its performance in image regression, denoising, and deblurring tasks, and comparing it to SGDL. We establish convergence results for the gradient descent (GD) method applied to these models and provide mathematical insights into MGDL's improved performance. In particular, we demonstrate that MGDL is more robust to the choice of learning rate under GD than SGDL. Furthermore, we analyze the eigenvalue distributions of the Jacobian matrices associated with the iterative schemes arising from the GD iterations, offering an explanation for MGDL's enhanced training stability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wafer Defect Root Cause Analysis with Partial Trajectory Regression</title>
<link>https://arxiv.org/abs/2507.20357</link>
<guid>https://arxiv.org/abs/2507.20357</guid>
<content:encoded><![CDATA[
arXiv:2507.20357v1 Announce Type: new 
Abstract: Identifying upstream processes responsible for wafer defects is challenging due to the combinatorial nature of process flows and the inherent variability in processing routes, which arises from factors such as rework operations and random process waiting times. This paper presents a novel framework for wafer defect root cause analysis, called Partial Trajectory Regression (PTR). The proposed framework is carefully designed to address the limitations of conventional vector-based regression models, particularly in handling variable-length processing routes that span a large number of heterogeneous physical processes. To compute the attribution score of each process given a detected high defect density on a specific wafer, we propose a new algorithm that compares two counterfactual outcomes derived from partial process trajectories. This is enabled by new representation learning methods, proc2vec and route2vec. We demonstrate the effectiveness of the proposed framework using real wafer history data from the NY CREATES fab in Albany.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)</title>
<link>https://arxiv.org/abs/2507.20362</link>
<guid>https://arxiv.org/abs/2507.20362</guid>
<content:encoded><![CDATA[
arXiv:2507.20362v1 Announce Type: new 
Abstract: Location-tracking data from the Automatic Identification System, much of which is publicly available, plays a key role in a range of maritime safety and monitoring applications. However, the data suffers from missing values that hamper downstream applications. Imputing the missing values is challenging because the values of different heterogeneous attributes are updated at diverse rates, resulting in the occurrence of multi-scale dependencies among attributes. Existing imputation methods that assume similar update rates across attributes are unable to capture and exploit such dependencies, limiting their imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based Imputation Network that aims improve imputation accuracy by capturing multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale temporal features for each attribute while preserving their intrinsic heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous graph to explicitly model dependencies between heterogeneous attributes to enable more accurate imputation of missing values through graph propagation. Experimental results on two real-world datasets find that MH-GIN is capable of an average 57% reduction in imputation errors compared to state-of-the-art methods, while maintaining computational efficiency. The source code and implementation details of MH-GIN are publicly available https://github.com/hyLiu1994/MH-GIN.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis</title>
<link>https://arxiv.org/abs/2507.20364</link>
<guid>https://arxiv.org/abs/2507.20364</guid>
<content:encoded><![CDATA[
arXiv:2507.20364v1 Announce Type: new 
Abstract: How can we identify problematic upstream processes when a certain type of wafer defect starts appearing at a quality checkpoint? Given the complexity of modern semiconductor manufacturing, which involves thousands of process steps, cross-process root cause analysis for wafer defects has been considered highly challenging. This paper proposes a novel framework called Trajectory Shapley Attribution (TSA), an extension of Shapley values (SV), a widely used attribution algorithm in explainable artificial intelligence research. TSA overcomes key limitations of standard SV, including its disregard for the sequential nature of manufacturing processes and its reliance on an arbitrarily chosen reference point. We applied TSA to a good-bad wafer diagnosis task in experimental front-end-of-line processes at the NY CREATES Albany NanoTech fab, aiming to identify measurement items (serving as proxies for process parameters) most relevant to abnormal defect occurrence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning</title>
<link>https://arxiv.org/abs/2507.20369</link>
<guid>https://arxiv.org/abs/2507.20369</guid>
<content:encoded><![CDATA[
arXiv:2507.20369v1 Announce Type: new 
Abstract: Clustering is a core task in machine learning with wide-ranging applications in data mining and pattern recognition. However, its unsupervised nature makes it inherently challenging. Many existing clustering algorithms suffer from critical limitations: they often require careful parameter tuning, exhibit high computational complexity, lack interpretability, or yield suboptimal accuracy, especially when applied to large-scale datasets. In this paper, we introduce a novel clustering approach based on meta-learning. Our approach eliminates the need for parameter optimization while achieving accuracy that outperforms state-of-the-art clustering techniques. The proposed technique leverages a few pre-clustered samples to guide the clustering process for the entire dataset in a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted Transformer Network (PFN) to perform clustering. The algorithm computes attention between the pre-clustered samples and the unclustered samples, allowing it to infer cluster assignments for the entire dataset based on the learned relation. We theoretically and empirically demonstrate that, given just a few pre-clustered examples, the model can generalize to accurately cluster the rest of the dataset. Experiments on challenging benchmark datasets show that our approach can successfully cluster well-separated data without any pre-clustered samples, and significantly improves performance when a few clustered samples are provided. We show that our approach is superior to the state-of-the-art techniques. These results highlight the effectiveness and scalability of our approach, positioning it as a promising alternative to existing clustering techniques.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks</title>
<link>https://arxiv.org/abs/2507.20373</link>
<guid>https://arxiv.org/abs/2507.20373</guid>
<content:encoded><![CDATA[
arXiv:2507.20373v1 Announce Type: new 
Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for detecting black hole (BH) anomalies in communication networks. These anomalies cause packet loss without failure notifications, disrupting connectivity and leading to financial losses. WBHT combines generative modeling, sequential learning, and attention mechanisms to improve BH anomaly detection. It integrates a Wasserstein generative adversarial network with attention mechanisms for stable training and accurate anomaly identification. The model uses long-short-term memory layers to capture long-term dependencies and convolutional layers for local temporal patterns. A latent space encoding mechanism helps distinguish abnormal network behavior. Tested on real-world network data, WBHT outperforms existing models, achieving significant improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and ability to detect previously undetected anomalies make it a valuable tool for proactive network monitoring and security, especially in mission-critical networks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Set-based Implicit Likelihood Inference of Galaxy Cluster Mass</title>
<link>https://arxiv.org/abs/2507.20378</link>
<guid>https://arxiv.org/abs/2507.20378</guid>
<content:encoded><![CDATA[
arXiv:2507.20378v1 Announce Type: new 
Abstract: We present a set-based machine learning framework that infers posterior distributions of galaxy cluster masses from projected galaxy dynamics. Our model combines Deep Sets and conditional normalizing flows to incorporate both positional and velocity information of member galaxies to predict residual corrections to the $M$-$\sigma$ relation for improved interpretability. Trained on the Uchuu-UniverseMachine simulation, our approach significantly reduces scatter and provides well-calibrated uncertainties across the full mass range compared to traditional dynamical estimates.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning</title>
<link>https://arxiv.org/abs/2507.20424</link>
<guid>https://arxiv.org/abs/2507.20424</guid>
<content:encoded><![CDATA[
arXiv:2507.20424v1 Announce Type: new 
Abstract: We study centralized distributed data parallel training of deep neural networks (DNNs), aiming to improve the trade-off between communication efficiency and model performance of the local gradient methods. To this end, we revisit the flat-minima hypothesis, which suggests that models with better generalization tend to lie in flatter regions of the loss landscape. We introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and demonstrate its strong correlation with the generalization gap of DNNs. We incorporate an efficient relaxation of this measure into the distributed training objective as a lightweight regularizer that encourages workers to collaboratively seek wide minima. The regularizer exerts a pushing force that counteracts the consensus step pulling the workers together, giving rise to the Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF outperforms other communication-efficient approaches and achieves better generalization performance than local gradient methods and synchronous gradient averaging, while significantly reducing communication overhead. In addition, our loss landscape visualizations confirm the ability of DPPF to locate flatter minima. On the theoretical side, we show that DPPF guides workers to span flat valleys, with the final valley width governed by the interplay between push and pull strengths, and that its pull-push dynamics is self-stabilizing. We further provide generalization guarantees linked to the valley width and prove convergence in the non-convex setting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings</title>
<link>https://arxiv.org/abs/2507.20426</link>
<guid>https://arxiv.org/abs/2507.20426</guid>
<content:encoded><![CDATA[
arXiv:2507.20426v1 Announce Type: new 
Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that combines a residual learning-based encoder with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly from raw protein sequences. Our architecture incorporates dilated convolutions within residual blocks to mitigate vanishing gradient issues and extract rich sequence features, while capsule layers with dynamic routing capture hierarchical and spatial relationships within the learned feature space. We conducted comprehensive ablation studies comparing global and local embeddings from ProteinBERT and conventional one-hot encoding. Results show that ProteinBERT embeddings substantially outperform other representations on large datasets. Although one-hot encoding showed marginal advantages on smaller datasets, such as PDB186, it struggled to scale effectively. Extensive evaluations on four pairs of publicly available benchmark datasets demonstrate that our model consistently outperforms current state-of-the-art methods. It achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2% and 83.3%, while maintaining competitive performance on larger datasets such as PDB20000. Notably, the model maintains a well balanced sensitivity and specificity across datasets. These results demonstrate the efficacy and generalizability of integrating global protein representations with advanced deep learning architectures for reliable and scalable DBP prediction in diverse genomic contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning</title>
<link>https://arxiv.org/abs/2507.20433</link>
<guid>https://arxiv.org/abs/2507.20433</guid>
<content:encoded><![CDATA[
arXiv:2507.20433v1 Announce Type: new 
Abstract: Transfer Learning (TL) offers the potential to accelerate learning by transferring knowledge across tasks. However, it faces critical challenges such as negative transfer, domain adaptation and inefficiency in selecting solid source policies. These issues often represent critical problems in evolving domains, i.e. game development, where scenarios transform and agents must adapt. The continuous release of new agents is costly and inefficient. In this work we challenge the key issues in TL to improve knowledge transfer, agents performance across tasks and reduce computational costs. The proposed methodology, called FAST - Framework for Adaptive Similarity-based Transfer, leverages visual frames and textual descriptions to create a latent representation of tasks dynamics, that is exploited to estimate similarity between environments. The similarity scores guides our method in choosing candidate policies from which transfer abilities to simplify learning of novel tasks. Experimental results, over multiple racing tracks, demonstrate that FAST achieves competitive final performance compared to learning-from-scratch methods while requiring significantly less training steps. These findings highlight the potential of embedding-driven task similarity estimations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool</title>
<link>https://arxiv.org/abs/2507.20440</link>
<guid>https://arxiv.org/abs/2507.20440</guid>
<content:encoded><![CDATA[
arXiv:2507.20440v1 Announce Type: new 
Abstract: Multi-omics data offer unprecedented insights into complex biological systems, yet their high dimensionality, sparsity, and intricate interactions pose significant analytical challenges. Network-based approaches have advanced multi-omics research by effectively capturing biologically relevant relationships among molecular entities. While these methods are powerful for representing molecular interactions, there remains a need for tools specifically designed to effectively utilize these network representations across diverse downstream analyses. To fulfill this need, we introduce BioNeuralNet, a flexible and modular Python framework tailored for end-to-end network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural Networks (GNNs) to learn biologically meaningful low-dimensional representations from multi-omics networks, converting these complex molecular networks into versatile embeddings. BioNeuralNet supports all major stages of multi-omics network analysis, including several network construction techniques, generation of low-dimensional representations, and a broad range of downstream analytical tasks. Its extensive utilities, including diverse GNN architectures, and compatibility with established Python packages (e.g., scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick adoption. BioNeuralNet is an open-source, user-friendly, and extensively documented framework designed to support flexible and reproducible multi-omics network analysis in precision medicine.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable In-Context Learning of Nonlinear Regression with Transformers</title>
<link>https://arxiv.org/abs/2507.20443</link>
<guid>https://arxiv.org/abs/2507.20443</guid>
<content:encoded><![CDATA[
arXiv:2507.20443v1 Announce Type: new 
Abstract: The transformer architecture, which processes sequences of input tokens to produce outputs for query tokens, has revolutionized numerous areas of machine learning. A defining feature of transformers is their ability to perform previously unseen tasks using task-specific prompts without updating parameters, a phenomenon known as in-context learning (ICL). Recent research has actively explored the training dynamics behind ICL, with much of the focus on relatively simple tasks such as linear regression and binary classification. To advance the theoretical understanding of ICL, this paper investigates more complex nonlinear regression tasks, aiming to uncover how transformers acquire in-context learning capabilities in these settings. We analyze the stage-wise dynamics of attention during training: attention scores between a query token and its target features grow rapidly in the early phase, then gradually converge to one, while attention to irrelevant features decays more slowly and exhibits oscillatory behavior. Our analysis introduces new proof techniques that explicitly characterize how the nature of general non-degenerate L-Lipschitz task functions affects attention weights. Specifically, we identify that the Lipschitz constant L of nonlinear function classes as a key factor governing the convergence dynamics of transformers in ICL. Leveraging these insights, for two distinct regimes depending on whether L is below or above a threshold, we derive different time bounds to guarantee near-zero prediction error. Notably, despite the convergence time depending on the underlying task functions, we prove that query tokens consistently attend to prompt tokens with highly relevant features at convergence, demonstrating the ICL capability of transformers for unseen functions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering</title>
<link>https://arxiv.org/abs/2507.20446</link>
<guid>https://arxiv.org/abs/2507.20446</guid>
<content:encoded><![CDATA[
arXiv:2507.20446v1 Announce Type: new 
Abstract: Machine learning has been making great success in many application areas. However, for the non-expert practitioners, it is always very challenging to address a machine learning task successfully and efficiently. Finding the optimal machine learning model or the hyperparameter combination set from a large number of possible alternatives usually requires considerable expert knowledge and experience. To tackle this problem, we propose a combined Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under a unified multi-armed bandit framework to automate the model selection or the hyperparameter optimization. Specifically, BOASF consists of multiple evaluation rounds in each of which we select promising configurations for each arm using the Bayesian optimization. Then, ASF can early discard the poor-performed arms adaptively using a Gaussian UCB-based probabilistic model. Furthermore, a Softmax model is employed to adaptively allocate available resources for each promising arm that advances to the next round. The arm with a higher probability of advancing will be allocated more resources. Experimental results show that BOASF is effective for speeding up the model selection and hyperparameter optimization processes while achieving robust and better prediction performance than the existing state-of-the-art automatic machine learning methods. Moreover, BOASF achieves better anytime performance under various time budgets.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope</title>
<link>https://arxiv.org/abs/2507.20447</link>
<guid>https://arxiv.org/abs/2507.20447</guid>
<content:encoded><![CDATA[
arXiv:2507.20447v1 Announce Type: new 
Abstract: Sparse regularization is fundamental in signal processing for efficient signal recovery and feature extraction. However, it faces a fundamental dilemma: the most powerful sparsity-inducing penalties are often non-differentiable, conflicting with gradient-based optimizers that dominate the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a novel, fully differentiable sparse regularizer derived from the weakly-convex envelope framework. WEEP provides strong, unbiased sparsity while maintaining full differentiability and L-smoothness, making it natively compatible with any gradient-based optimizer. This resolves the conflict between statistical performance and computational tractability. We demonstrate superior performance compared to the L1-norm and other established non-convex sparse regularizers on challenging signal and image denoising tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations</title>
<link>https://arxiv.org/abs/2507.20453</link>
<guid>https://arxiv.org/abs/2507.20453</guid>
<content:encoded><![CDATA[
arXiv:2507.20453v1 Announce Type: new 
Abstract: Self-attention mechanisms are foundational to Transformer architectures, supporting their impressive success in a wide range of tasks. While there are many self-attention variants, their robustness to noise and spurious correlations has not been well studied. This study evaluates Softmax, Sigmoid, Linear, Doubly Stochastic, and Cosine attention within Vision Transformers under different data corruption scenarios. Through testing across the CIFAR-10, CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is the most robust. Our findings inform self-attention selection in contexts with imperfect data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling</title>
<link>https://arxiv.org/abs/2507.20459</link>
<guid>https://arxiv.org/abs/2507.20459</guid>
<content:encoded><![CDATA[
arXiv:2507.20459v1 Announce Type: new 
Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A, 185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling data as a mixture of one-dimensional Gaussians, moment-based estimation methods have proliferated. Among these methods, the generalized method of moments (GMM) improves the statistical efficiency of MM by weighting the moments appropriately. However, the computational complexity and storage complexity of MM and GMM grow exponentially with the dimension, making these methods impractical for high-dimensional data or when higher-order moments are required. Such computational bottlenecks are more severe in GMM since it additionally requires estimating a large weighting matrix. To overcome these bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a balance among statistical efficiency, computational complexity, and numerical stability. We apply DGMM to study the parameter estimation problem for weakly separated heteroscedastic low-rank Gaussian mixtures and design a computationally efficient and numerically stable algorithm that obtains the DGMM estimator without explicitly computing or storing the moment tensors. We implement the proposed algorithm and empirically validate the advantages of DGMM: in numerical studies, DGMM attains smaller estimation errors while requiring substantially shorter runtime than MM and GMM. The code and data will be available upon publication at https://github.com/liu-lzhang/dgmm.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley-Value-Based Graph Sparsification for GNN Inference</title>
<link>https://arxiv.org/abs/2507.20460</link>
<guid>https://arxiv.org/abs/2507.20460</guid>
<content:encoded><![CDATA[
arXiv:2507.20460v1 Announce Type: new 
Abstract: Graph sparsification is a key technique for improving inference efficiency in Graph Neural Networks by removing edges with minimal impact on predictions. GNN explainability methods generate local importance scores, which can be aggregated into global scores for graph sparsification. However, many explainability methods produce only non-negative scores, limiting their applicability for sparsification. In contrast, Shapley value based methods assign both positive and negative contributions to node predictions, offering a theoretically robust and fair allocation of importance by evaluating many subsets of graphs. Unlike gradient-based or perturbation-based explainers, Shapley values enable better pruning strategies that preserve influential edges while removing misleading or adversarial connections. Our approach shows that Shapley value-based graph sparsification maintains predictive performance while significantly reducing graph complexity, enhancing both interpretability and efficiency in GNN inference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Diffusion Models for Global Precipitation Map Inpainting</title>
<link>https://arxiv.org/abs/2507.20478</link>
<guid>https://arxiv.org/abs/2507.20478</guid>
<content:encoded><![CDATA[
arXiv:2507.20478v1 Announce Type: new 
Abstract: Incomplete satellite-based precipitation presents a significant challenge in global monitoring. For example, the Global Satellite Mapping of Precipitation (GSMaP) from JAXA suffers from substantial missing regions due to the orbital characteristics of satellites that have microwave sensors, and its current interpolation methods often result in spatial discontinuities. In this study, we formulate the completion of the precipitation map as a video inpainting task and propose a machine learning approach based on conditional diffusion models. Our method employs a 3D U-Net with a 3D condition encoder to reconstruct complete precipitation maps by leveraging spatio-temporal information from infrared images, latitude-longitude grids, and physical time inputs. Training was carried out on ERA5 hourly precipitation data from 2020 to 2023. We generated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps. Performance was evaluated for the calendar year 2024, and our approach produces more spatio-temporally consistent inpainted precipitation maps compared to conventional methods. These results indicate the potential to improve global precipitation monitoring using the conditional diffusion models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization</title>
<link>https://arxiv.org/abs/2507.20490</link>
<guid>https://arxiv.org/abs/2507.20490</guid>
<content:encoded><![CDATA[
arXiv:2507.20490v1 Announce Type: new 
Abstract: In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense potential in handling complex systems with high-order interactions. However, acquiring large-scale, high-quality labeled data for these models is costly, making Active Learning (AL) a critical technique. Existing Graph Active Learning (GAL) methods, when applied to hypergraphs, often rely on techniques like "clique expansion," which destroys the high-order structural information crucial to a hypergraph's success, thereby leading to suboptimal performance. To address this challenge, we introduce HIAL (Hypergraph Active Learning), a native active learning framework designed specifically for hypergraphs. We innovatively reformulate the Hypergraph Active Learning (HAL) problem as an Influence Maximization task. The core of HIAL is a dual-perspective influence function that, based on our novel "High-Order Interaction-Aware (HOI-Aware)" propagation mechanism, synergistically evaluates a node's feature-space coverage (via Magnitude of Influence, MoI) and its topological influence (via Expected Diffusion Value, EDV). We prove that this objective function is monotone and submodular, thus enabling the use of an efficient greedy algorithm with a formal (1-1/e) approximation guarantee. Extensive experiments on seven public datasets demonstrate that HIAL significantly outperforms state-of-the-art baselines in terms of performance, efficiency, generality, and robustness, establishing an efficient and powerful new paradigm for active learning on hypergraphs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning</title>
<link>https://arxiv.org/abs/2507.20498</link>
<guid>https://arxiv.org/abs/2507.20498</guid>
<content:encoded><![CDATA[
arXiv:2507.20498v1 Announce Type: new 
Abstract: Knowledge Graph (KG) reasoning, which aims to infer new facts from structured knowledge repositories, plays a vital role in Natural Language Processing (NLP) systems. Its effectiveness critically depends on constructing informative and contextually relevant reasoning paths. However, existing graph neural networks (GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting their ability to adapt to diverse linguistic contexts and semantic nuances. To address these limitations, we propose \textbf{MoKGR}, a mixture-of-experts framework that personalizes path exploration through two complementary components: (1) a mixture of length experts that adaptively selects and weights candidate path lengths according to query complexity, providing query-specific reasoning depth; and (2) a mixture of pruning experts that evaluates candidate paths from a complementary perspective, retaining the most informative paths for each query. Through comprehensive experiments on diverse benchmark, MoKGR demonstrates superior performance in both transductive and inductive settings, validating the effectiveness of personalized path exploration in KGs reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20499</link>
<guid>https://arxiv.org/abs/2507.20499</guid>
<content:encoded><![CDATA[
arXiv:2507.20499v1 Announce Type: new 
Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample efficiency in offline RL by utilizing additional offline source datasets. A key challenge is to identify and utilize source samples that are most relevant to the target domain. Existing approaches address this challenge by measuring domain gaps through domain classifiers, target transition dynamics modeling, or mutual information estimation using contrastive loss. However, these methods often require large target datasets, which is impractical in many real-world scenarios. In this work, we address cross-domain offline RL under a limited target data setting, identifying two primary challenges: (1) Dataset imbalance, which is caused by large source and small target datasets and leads to overfitting in neural network-based domain gap estimators, resulting in uninformative measurements; and (2) Partial domain overlap, where only a subset of the source data is closely aligned with the target domain. To overcome these issues, we propose DmC, a novel framework for cross-domain offline RL with limited target samples. Specifically, DmC utilizes $k$-nearest neighbor ($k$-NN) based estimation to measure domain proximity without neural network training, effectively mitigating overfitting. Then, by utilizing this domain proximity, we introduce a nearest-neighbor-guided diffusion model to generate additional source samples that are better aligned with the target domain, thus enhancing policy learning with more effective source samples. Through theoretical analysis and extensive experiments in diverse MuJoCo environments, we demonstrate that DmC significantly outperforms state-of-the-art cross-domain offline RL methods, achieving substantial performance gains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customize Multi-modal RAI Guardrails with Precedent-based predictions</title>
<link>https://arxiv.org/abs/2507.20503</link>
<guid>https://arxiv.org/abs/2507.20503</guid>
<content:encoded><![CDATA[
arXiv:2507.20503v1 Announce Type: new 
Abstract: A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on "precedents", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning</title>
<link>https://arxiv.org/abs/2507.20505</link>
<guid>https://arxiv.org/abs/2507.20505</guid>
<content:encoded><![CDATA[
arXiv:2507.20505v1 Announce Type: new 
Abstract: This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning (MPCCL) model, a novel approach for attributed graph clustering that effectively bridges critical gaps in existing methods, including long-range dependency, feature collapse, and information loss. Traditional methods often struggle to capture high-order graph features due to their reliance on low-order attribute information, while contrastive learning techniques face limitations in feature diversity by overemphasizing local neighborhood structures. Similarly, conventional graph coarsening methods, though reducing graph scale, frequently lose fine-grained structural details. MPCCL addresses these challenges through an innovative multi-scale coarsening strategy, which progressively condenses the graph while prioritizing the merging of key edges based on global node similarity to preserve essential structural information. It further introduces a one-to-many contrastive learning paradigm, integrating node embeddings with augmented graph views and cluster centroids to enhance feature diversity, while mitigating feature masking issues caused by the accumulation of high-frequency node weights during multi-scale coarsening. By incorporating a graph reconstruction loss and KL divergence into its self-supervised learning framework, MPCCL ensures cross-scale consistency of node representations. Experimental evaluations reveal that MPCCL achieves a significant improvement in clustering performance, including a remarkable 15.24% increase in NMI on the ACM dataset and notable robust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2507.20513</link>
<guid>https://arxiv.org/abs/2507.20513</guid>
<content:encoded><![CDATA[
arXiv:2507.20513v1 Announce Type: new 
Abstract: Ray tracing is a widely used technique for modeling optical systems, involving sequential surface-by-surface computations, which can be computationally intensive. We propose Ray2Ray, a novel method that leverages implicit neural representations to model optical systems with greater efficiency, eliminating the need for surface-by-surface computations in a single pass end-to-end model. Ray2Ray learns the mapping between rays emitted from a given source and their corresponding rays after passing through a given optical system in a physically accurate manner. We train Ray2Ray on nine off-the-shelf optical systems, achieving positional errors on the order of 1{\mu}m and angular deviations on the order 0.01 degrees in the estimated output rays. Our work highlights the potential of neural representations as a proxy for optical raytracer.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Learning for Sample Constrained Black-Box Optimization</title>
<link>https://arxiv.org/abs/2507.20533</link>
<guid>https://arxiv.org/abs/2507.20533</guid>
<content:encoded><![CDATA[
arXiv:2507.20533v1 Announce Type: new 
Abstract: Black box optimization (BBO) focuses on optimizing unknown functions in high-dimensional spaces. In many applications, sampling the unknown function is expensive, imposing a tight sample budget. Ongoing work is making progress on reducing the sample budget by learning the shape/structure of the function, known as kernel learning. We propose a new method to learn the kernel of a Gaussian Process. Our idea is to create a continuous kernel space in the latent space of a variational autoencoder, and run an auxiliary optimization to identify the best kernel. Results show that the proposed method, Kernel Optimized Blackbox Optimization (KOBO), outperforms state of the art by estimating the optimal at considerably lower sample budgets. Results hold not only across synthetic benchmark functions but also in real applications. We show that a hearing aid may be personalized with fewer audio queries to the user, or a generative model could converge to desirable images from limited user ratings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi K2: Open Agentic Intelligence</title>
<link>https://arxiv.org/abs/2507.20534</link>
<guid>https://arxiv.org/abs/2507.20534</guid>
<content:encoded><![CDATA[
arXiv:2507.20534v1 Announce Type: new 
Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation</title>
<link>https://arxiv.org/abs/2507.20542</link>
<guid>https://arxiv.org/abs/2507.20542</guid>
<content:encoded><![CDATA[
arXiv:2507.20542v1 Announce Type: new 
Abstract: Group fairness is important to consider in tensor decomposition to prevent discrimination based on social grounds such as gender or age. Although few works have studied group fairness in tensor decomposition, they suffer from performance degradation. To address this, we propose STAFF(Sparse Tensor Augmentation For Fairness) to improve group fairness by minimizing the gap in completion errors of different groups while reducing the overall tensor completion error. Our main idea is to augment a tensor with augmented entities including sufficient observed entries to mitigate imbalance and group bias in the sparse tensor. We evaluate \method on tensor completion with various datasets under conventional and deep learning-based tensor models. STAFF consistently shows the best trade-off between completion error and group fairness; at most, it yields 36% lower MSE and 59% lower MADE than the second-best baseline.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2507.20571</link>
<guid>https://arxiv.org/abs/2507.20571</guid>
<content:encoded><![CDATA[
arXiv:2507.20571v1 Announce Type: new 
Abstract: Due to the distributed nature of federated learning (FL), the vulnerability of the global model and the need for coordination among many client devices pose significant challenges. As a promising decentralized, scalable and secure solution, blockchain-based FL methods have attracted widespread attention in recent years. However, traditional consensus mechanisms designed for Proof of Work (PoW) similar to blockchain incur substantial resource consumption and compromise the efficiency of FL, particularly when participating devices are wireless and resource-limited. To address asynchronous client participation and data heterogeneity in FL, while limiting the additional resource overhead introduced by blockchain, we propose the Directed Acyclic Graph-based Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection algorithm that considers temporal freshness, node reachability and model accuracy, with a DAG-based trusted verification strategy. Extensive experiments on 3 benchmarking datasets against eight state-of-the-art approaches demonstrate that DAG-AFL significantly improves training efficiency and model accuracy by 22.7% and 6.5% on average, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy</title>
<link>https://arxiv.org/abs/2507.20573</link>
<guid>https://arxiv.org/abs/2507.20573</guid>
<content:encoded><![CDATA[
arXiv:2507.20573v1 Announce Type: new 
Abstract: Machine unlearning enables the removal of specific data from ML models to uphold the right to be forgotten. While approximate unlearning algorithms offer efficient alternatives to full retraining, this work reveals that they fail to adequately protect the privacy of unlearned data. In particular, these algorithms introduce implicit residuals which facilitate privacy attacks targeting at unlearned data. We observe that these residuals persist regardless of model architectures, parameters, and unlearning algorithms, exposing a new attack surface beyond conventional output-based leakage. Based on this insight, we propose the Reminiscence Attack (ReA), which amplifies the correlation between residuals and membership privacy through targeted fine-tuning processes. ReA achieves up to 1.90x and 1.12x higher accuracy than prior attacks when inferring class-wise and sample-wise membership, respectively. To mitigate such residual-induced privacy risk, we develop a dual-phase approximate unlearning framework that first eliminates deep-layer unlearned data traces and then enforces convergence stability to prevent models from "pseudo-convergence", where their outputs are similar to retrained models but still preserve unlearned residuals. Our framework works for both classification and generation tasks. Experimental evaluations confirm that our approach maintains high unlearning efficacy, while reducing the adaptive privacy attack accuracy to nearly random guess, at the computational cost of 2-12% of full retraining from scratch.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing CFD and measurement data using transfer learning</title>
<link>https://arxiv.org/abs/2507.20576</link>
<guid>https://arxiv.org/abs/2507.20576</guid>
<content:encoded><![CDATA[
arXiv:2507.20576v1 Announce Type: new 
Abstract: Aerodynamic analysis during aircraft design usually involves methods of varying accuracy and spatial resolution, which all have their advantages and disadvantages. It is therefore desirable to create data-driven models which effectively combine these advantages. Such data fusion methods for distributed quantities mainly rely on proper orthogonal decomposition as of now, which is a linear method. In this paper, we introduce a non-linear method based on neural networks combining simulation and measurement data via transfer learning. The network training accounts for the heterogeneity of the data, as simulation data usually features a high spatial resolution, while measurement data is sparse but more accurate. In a first step, the neural network is trained on simulation data to learn spatial features of the distributed quantities. The second step involves transfer learning on the measurement data to correct for systematic errors between simulation and measurement by only re-training a small subset of the entire neural network model. This approach is applied to a multilayer perceptron architecture and shows significant improvements over the established method based on proper orthogonal decomposition by producing more physical solutions near nonlinearities. In addition, the neural network provides solutions at arbitrary flow conditions, thus making the model useful for flight mechanical design, structural sizing, and certification. As the proposed training strategy is very general, it can also be applied to more complex neural network architectures in the future.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation</title>
<link>https://arxiv.org/abs/2507.20592</link>
<guid>https://arxiv.org/abs/2507.20592</guid>
<content:encoded><![CDATA[
arXiv:2507.20592v1 Announce Type: new 
Abstract: Neural Architecture Search (NAS) is challenged by the trade-off between search space exploration and efficiency, especially for complex tasks. While recent LLM-based NAS methods have shown promise, they often suffer from static search strategies and ambiguous architecture representations. We propose PhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by real-time score thresholds and a structured architecture template language for consistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS consistently discovers architectures with higher accuracy and better rank. For image classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86% while maintaining or improving accuracy. In object detection, it automatically produces YOLOv8 variants with higher mAP and lower resource cost. These results demonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS across diverse vision tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation</title>
<link>https://arxiv.org/abs/2507.20644</link>
<guid>https://arxiv.org/abs/2507.20644</guid>
<content:encoded><![CDATA[
arXiv:2507.20644v1 Announce Type: new 
Abstract: The investigation of allele frequency trajectories in populations evolving under controlled environmental pressures has become a popular approach to study evolutionary processes on the molecular level. Statistical models based on well-defined evolutionary concepts can be used to validate different hypotheses about empirical observations. Despite their popularity, classic statistical models like the Wright-Fisher model suffer from simplified assumptions such as the independence of selected loci along a chromosome and uncertainty about the parameters. Deep generative neural networks offer a powerful alternative known for the integration of multivariate dependencies and noise reduction. Due to their high data demands and challenging interpretability they have, so far, not been widely considered in the area of population genomics. To address the challenges in the area of Evolve and Resequencing experiments (E&amp;R) based on pooled sequencing (Pool-Seq) data, we introduce a deep generative neural network that aims to model a concept of evolution based on empirical observations over time. The proposed model estimates the distribution of allele frequency trajectories by embedding the observations from single nucleotide polymorphisms (SNPs) with information from neighboring loci. Evaluation on simulated E&amp;R experiments demonstrates the model's ability to capture the distribution of allele frequency trajectories and illustrates the representational power of deep generative models on the example of linkage disequilibrium (LD) estimation. Inspecting the internally learned representations enables estimating pairwise LD, which is typically inaccessible in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data high degree of LD when compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference</title>
<link>https://arxiv.org/abs/2507.20678</link>
<guid>https://arxiv.org/abs/2507.20678</guid>
<content:encoded><![CDATA[
arXiv:2507.20678v1 Announce Type: new 
Abstract: The Cholesky decomposition is a fundamental tool for solving linear systems with symmetric and positive definite matrices which are ubiquitous in linear algebra, optimization, and machine learning. Its numerical stability can be improved by introducing a pivoting strategy that iteratively permutes the rows and columns of the matrix. The order of pivoting indices determines how accurately the intermediate decomposition can reconstruct the original matrix, thus is decisive for the algorithm's efficiency in the case of early termination. Standard implementations select the next pivot from the largest value on the diagonal. In the case of Bayesian nonparametric inference, this strategy corresponds to greedy entropy maximization, which is often used in active learning and design of experiments. We explore this connection in detail and deduce novel pivoting strategies for the Cholesky decomposition. The resulting algorithms are more efficient at reducing the uncertainty over a data set, can be updated to include information about observations, and additionally benefit from a tailored implementation. We benchmark the effectiveness of the new selection strategies on two tasks important to Gaussian processes: sparse regression and inference based on preconditioned iterative solvers. Our results show that the proposed selection strategies are either on par or, in most cases, outperform traditional baselines while requiring a negligible amount of additional computation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks</title>
<link>https://arxiv.org/abs/2507.20708</link>
<guid>https://arxiv.org/abs/2507.20708</guid>
<content:encoded><![CDATA[
arXiv:2507.20708v1 Announce Type: new 
Abstract: Proving the compliance of AI algorithms has become an important challenge with the growing deployment of such algorithms for real-life applications. Inspecting possible biased behaviors is mandatory to satisfy the constraints of the regulations of the EU Artificial Intelligence's Act. Regulation-driven audits increasingly rely on global fairness metrics, with Disparate Impact being the most widely used. Yet such global measures depend highly on the distribution of the sample on which the measures are computed. We investigate first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints. Then we study how to detect such manipulation. Our analysis (i) introduces mathematically sound methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, (ii) examines how an auditee could potentially circumvent fairness inspections, and (iii) offers recommendations to help auditors detect such data manipulations. These results are validated through experiments on classical tabular datasets in bias detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI</title>
<link>https://arxiv.org/abs/2507.20714</link>
<guid>https://arxiv.org/abs/2507.20714</guid>
<content:encoded><![CDATA[
arXiv:2507.20714v1 Announce Type: new 
Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features' complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-driven Embedding Convolution</title>
<link>https://arxiv.org/abs/2507.20718</link>
<guid>https://arxiv.org/abs/2507.20718</guid>
<content:encoded><![CDATA[
arXiv:2507.20718v1 Announce Type: new 
Abstract: Text embeddings are essential components in modern NLP pipelines. While numerous embedding models have been proposed, their performance varies across domains, and no single model consistently excels across all tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble weights based on embedding uncertainty, grounded in a Bayes-optimal solution under a surrogate loss. Additionally, UEC introduces an uncertainty-aware similarity function that directly incorporates uncertainty into similarity scoring. Extensive experiments on retrieval, classification, and semantic similarity benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Hallucination Tokens Are Different from Conditional Ones</title>
<link>https://arxiv.org/abs/2507.20836</link>
<guid>https://arxiv.org/abs/2507.20836</guid>
<content:encoded><![CDATA[
arXiv:2507.20836v1 Announce Type: new 
Abstract: Hallucination, the generation of untruthful content, is one of the major concerns regarding foundational models. Detecting hallucinations at the token level is vital for real-time filtering and targeted correction, yet the variation of hallucination signals within token sequences is not fully understood. Leveraging the RAGTruth corpus with token-level annotations and reproduced logits, we analyse how these signals depend on a token's position within hallucinated spans, contributing to an improved understanding of token-level hallucination. Our results show that the first hallucinated token carries a stronger signal and is more detectable than conditional tokens. We release our analysis framework, along with code for logit reproduction and metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network</title>
<link>https://arxiv.org/abs/2507.20838</link>
<guid>https://arxiv.org/abs/2507.20838</guid>
<content:encoded><![CDATA[
arXiv:2507.20838v1 Announce Type: new 
Abstract: Due to the extensive availability of operation data, data-driven methods show strong capabilities in predicting building energy loads. Buildings with similar features often share energy patterns, reflected by spatial dependencies in their operational data, which conventional prediction methods struggle to capture. To overcome this, we propose a multi-building prediction approach using spatio-temporal graph neural networks, comprising graph representation, graph learning, and interpretation. First, a graph is built based on building characteristics and environmental factors. Next, a multi-level graph convolutional architecture with attention is developed for energy prediction. Lastly, a method interpreting the optimized graph structure is introduced. Experiments on the Building Data Genome Project 2 dataset confirm superior performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive, highlighting the method's robustness, generalization, and interpretability in capturing meaningful building similarities and spatial relationships.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Deep Clustering for Time Series Data</title>
<link>https://arxiv.org/abs/2507.20840</link>
<guid>https://arxiv.org/abs/2507.20840</guid>
<content:encoded><![CDATA[
arXiv:2507.20840v1 Announce Type: new 
Abstract: Deep clustering uncovers hidden patterns and groups in complex time series data, yet its opaque decision-making limits use in safety-critical settings. This survey offers a structured overview of explainable deep clustering for time series, collecting current methods and their real-world applications. We thoroughly discuss and compare peer-reviewed and preprint papers through application domains across healthcare, finance, IoT, and climate science. Our analysis reveals that most work relies on autoencoder and attention architectures, with limited support for streaming, irregularly sampled, or privacy-preserved series, and interpretability is still primarily treated as an add-on. To push the field forward, we outline six research opportunities: (1) combining complex networks with built-in interpretability; (2) setting up clear, faithfulness-focused evaluation metrics for unsupervised explanations; (3) building explainers that adapt to live data streams; (4) crafting explanations tailored to specific domains; (5) adding human-in-the-loop methods that refine clusters and explanations together; and (6) improving our understanding of how time series clustering models work internally. By making interpretability a primary design goal rather than an afterthought, we propose the groundwork for the next generation of trustworthy deep clustering time series analytics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</title>
<link>https://arxiv.org/abs/2507.20853</link>
<guid>https://arxiv.org/abs/2507.20853</guid>
<content:encoded><![CDATA[
arXiv:2507.20853v1 Announce Type: new 
Abstract: Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induces a set of attainable states in RL. We show that the training dynamics of a two-layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait</title>
<link>https://arxiv.org/abs/2507.20862</link>
<guid>https://arxiv.org/abs/2507.20862</guid>
<content:encoded><![CDATA[
arXiv:2507.20862v1 Announce Type: new 
Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments, including gait dysfunction, particularly in patients with freezing of gait (FOG). Current detection methods are either subjective or reliant on specialized gait analysis tools. This study aims to develop an objective, data-driven, and multi-modal classification model to detect gait dysfunction in PD patients using resting-state EEG signals combined with demographic and clinical variables. We utilized a dataset of 124 participants: 42 PD patients with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy controls. Features extracted from resting-state EEG and descriptive variables (age, education, disease duration) were used to train a novel Bi-cephalic Self-Attention Model (BiSAM). We tested three modalities: signal-only, descriptive-only, and multi-modal, across different EEG channel subsets (BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed limited performance, achieving a maximum accuracy of 55% and 68%, respectively. In contrast, the multi-modal models significantly outperformed both, with BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These results demonstrate the value of integrating EEG with objective descriptive features for robust PDFOG+ detection. This study introduces a multi-modal, attention-based architecture that objectively classifies PDFOG+ using minimal EEG channels and descriptive variables. This approach offers a scalable and efficient alternative to traditional assessments, with potential applications in routine clinical monitoring and early diagnosis of PD-related gait dysfunction.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online hierarchical partitioning of the output space in extreme multi-label data stream</title>
<link>https://arxiv.org/abs/2507.20894</link>
<guid>https://arxiv.org/abs/2507.20894</guid>
<content:encoded><![CDATA[
arXiv:2507.20894v1 Announce Type: new 
Abstract: Mining data streams with multi-label outputs poses significant challenges due to evolving distributions, high-dimensional label spaces, sparse label occurrences, and complex label dependencies. Moreover, concept drift affects not only input distributions but also label correlations and imbalance ratios over time, complicating model adaptation. To address these challenges, structured learners are categorized into local and global methods. Local methods break down the task into simpler components, while global methods adapt the algorithm to the full output space, potentially yielding better predictions by exploiting label correlations. This work introduces iHOMER (Incremental Hierarchy Of Multi-label Classifiers), an online multi-label learning framework that incrementally partitions the label space into disjoint, correlated clusters without relying on predefined hierarchies. iHOMER leverages online divisive-agglomerative clustering based on \textit{Jaccard} similarity and a global tree-based learner driven by a multivariate \textit{Bernoulli} process to guide instance partitioning. To address non-stationarity, it integrates drift detection mechanisms at both global and local levels, enabling dynamic restructuring of label partitions and subtrees. Experiments across 23 real-world datasets show iHOMER outperforms 5 state-of-the-art global baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local baselines, such as binary relevance transformations of kNN, EFDT, ARF, and ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for online multi-label classification.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling User Behavior from Adaptive Surveys with Supplemental Context</title>
<link>https://arxiv.org/abs/2507.20919</link>
<guid>https://arxiv.org/abs/2507.20919</guid>
<content:encoded><![CDATA[
arXiv:2507.20919v1 Announce Type: new 
Abstract: Modeling user behavior is critical across many industries where understanding preferences, intent, or decisions informs personalization, targeting, and strategic outcomes. Surveys have long served as a classical mechanism for collecting such behavioral data due to their interpretability, structure, and ease of deployment. However, surveys alone are inherently limited by user fatigue, incomplete responses, and practical constraints on their length making them insufficient for capturing user behavior. In this work, we present LANTERN (Late-Attentive Network for Enriched Response Modeling), a modular architecture for modeling user behavior by fusing adaptive survey responses with supplemental contextual signals. We demonstrate the architectural value of maintaining survey primacy through selective gating, residual connections and late fusion via cross-attention, treating survey data as the primary signal while incorporating external modalities only when relevant. LANTERN outperforms strong survey-only baselines in multi-label prediction of survey responses. We further investigate threshold sensitivity and the benefits of selective modality reliance through ablation and rare/frequent attribute analysis. LANTERN's modularity supports scalable integration of new encoders and evolving datasets. This work provides a practical and extensible blueprint for behavior modeling in survey-centric applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction</title>
<link>https://arxiv.org/abs/2507.20925</link>
<guid>https://arxiv.org/abs/2507.20925</guid>
<content:encoded><![CDATA[
arXiv:2507.20925v1 Announce Type: new 
Abstract: Given the vastness of chemical space and the ongoing emergence of previously uncharacterized proteins, zero-shot compound-protein interaction (CPI) prediction better reflects the practical challenges and requirements of real-world drug development. Although existing methods perform adequately during certain CPI tasks, they still face the following challenges: (1) Representation learning from local or complete protein sequences often overlooks the complex interdependencies between subsequences, which are essential for predicting spatial structures and binding properties. (2) Dependence on large-scale or scarce multimodal protein datasets demands significant training data and computational resources, limiting scalability and efficiency. To address these challenges, we propose a novel approach that pretrains protein representations for CPI prediction tasks using subsequence reordering, explicitly capturing the dependencies between protein subsequences. Furthermore, we apply length-variable protein augmentation to ensure excellent pretraining performance on small training datasets. To evaluate the model's effectiveness and zero-shot learning ability, we combine it with various baseline methods. The results demonstrate that our approach can improve the baseline model's performance on the CPI task, especially in the challenging zero-shot scenario. Compared to existing pre-training models, our model demonstrates superior performance, particularly in data-scarce scenarios where training samples are limited. Our implementation is available at https://github.com/Hoch-Zhang/PSRP-CPI.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy</title>
<link>https://arxiv.org/abs/2507.20929</link>
<guid>https://arxiv.org/abs/2507.20929</guid>
<content:encoded><![CDATA[
arXiv:2507.20929v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of $10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a perceived precision ceiling that limits their adoption in engineering applications. We break through this barrier with a hybrid Fourier-neural architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2 error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and \(15-500\times\) better than traditional numerical methods. Our approach synergistically combines a truncated Fourier series capturing dominant modal behavior with a deep neural network providing adaptive residual corrections. A systematic harmonic optimization study revealed a counter-intuitive discovery: exactly 10 harmonics yield optimal performance, with accuracy catastrophically degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing enable stable ultra-precision convergence. GPU-accelerated implementation achieves sub-30-minute training despite fourth-order derivative complexity. By addressing 12 critical gaps in existing approaches-from architectural rigidity to optimization landscapes-this work demonstrates that ultra-precision is achievable through proper design, opening new paradigms for scientific computing where machine learning can match or exceed traditional numerical methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Persona-Driven Reasoning in Language Models via Activation Patching</title>
<link>https://arxiv.org/abs/2507.20936</link>
<guid>https://arxiv.org/abs/2507.20936</guid>
<content:encoded><![CDATA[
arXiv:2507.20936v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery</title>
<link>https://arxiv.org/abs/2507.20954</link>
<guid>https://arxiv.org/abs/2507.20954</guid>
<content:encoded><![CDATA[
arXiv:2507.20954v1 Announce Type: new 
Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for modeling high-dimensional dynamical systems and/or spatiotemporal data from dynamical system snapshot observations. PySHRED is a Python package that implements SHRED and several of its major extensions, including for robust sensing, reduced order modeling and physics discovery. In this paper, we introduce the version 1.0 release of PySHRED, which includes data preprocessors and a number of cutting-edge SHRED methods specifically designed to handle real-world data that may be noisy, multi-scale, parameterized, prohibitively high-dimensional, and strongly nonlinear. The package is easy to install, thoroughly-documented, supplemented with extensive code examples, and modularly-structured to support future additions. The entire codebase is released under the MIT license and is available at https://github.com/pyshred-dev/pyshred.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes</title>
<link>https://arxiv.org/abs/2507.20967</link>
<guid>https://arxiv.org/abs/2507.20967</guid>
<content:encoded><![CDATA[
arXiv:2507.20967v1 Announce Type: new 
Abstract: The rise of graph-structured data has driven interest in graph learning and synthetic data generation. While successful in text and image domains, synthetic graph generation remains challenging -- especially for real-world graphs with complex, heterogeneous schemas. Existing research has focused mostly on homogeneous structures with simple attributes, limiting their usefulness and relevance for application domains requiring semantic fidelity.
  In this research, we introduce ProvCreator, a synthetic graph framework designed for complex heterogeneous graphs with high-dimensional node and edge attributes. ProvCreator formulates graph synthesis as a sequence generation task, enabling the use of transformer-based large language models. It features a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph structure and attributes, 2. efficiently compresses large graphs for contextual modeling, and 3. supports end-to-end, learnable graph generation.
  To validate our research, we evaluate ProvCreator on two challenging domains: system provenance graphs in cybersecurity and knowledge graphs from IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate dependencies between structure and semantics, enabling the generation of realistic and privacy-aware synthetic datasets.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation</title>
<link>https://arxiv.org/abs/2507.20968</link>
<guid>https://arxiv.org/abs/2507.20968</guid>
<content:encoded><![CDATA[
arXiv:2507.20968v1 Announce Type: new 
Abstract: Domain shift poses a fundamental challenge in time series analysis, where models trained on source domain often fail dramatically when applied in target domain with different yet similar distributions. While current unsupervised domain adaptation (UDA) methods attempt to align cross-domain feature distributions, they typically treat features as indivisible entities, ignoring their intrinsic compositions that governs domain adaptation. We introduce DARSD, a novel UDA framework with theoretical explainability that explicitly realizes UDA tasks from the perspective of representation space decomposition. Our core insight is that effective domain adaptation requires not just alignment, but principled disentanglement of transferable knowledge from mixed representations. DARSD consists three synergistic components: (I) An adversarial learnable common invariant basis that projects original features into a domain-invariant subspace while preserving semantic content; (II) A prototypical pseudo-labeling mechanism that dynamically separates target features based on confidence, hindering error accumulation; (III) A hybrid contrastive optimization strategy that simultaneously enforces feature clustering and consistency while mitigating emerging distribution gaps. Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR, HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms, achieving optimal performance in 35 out of 53 cross-domain scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2507.20973</link>
<guid>https://arxiv.org/abs/2507.20973</guid>
<content:encoded><![CDATA[
arXiv:2507.20973v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</title>
<link>https://arxiv.org/abs/2507.20984</link>
<guid>https://arxiv.org/abs/2507.20984</guid>
<content:encoded><![CDATA[
arXiv:2507.20984v1 Announce Type: new 
Abstract: While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Treatment Effect Estimation from Unstructured Data</title>
<link>https://arxiv.org/abs/2507.20993</link>
<guid>https://arxiv.org/abs/2507.20993</guid>
<content:encoded><![CDATA[
arXiv:2507.20993v1 Announce Type: new 
Abstract: Existing methods for estimating personalized treatment effects typically rely on structured covariates, limiting their applicability to unstructured data. Yet, leveraging unstructured data for causal inference has considerable application potential, for instance in healthcare, where clinical notes or medical images are abundant. To this end, we first introduce an approximate 'plug-in' method trained directly on the neural representations of unstructured data. However, when these fail to capture all confounding information, the method may be subject to confounding bias. We therefore introduce two theoretically grounded estimators that leverage structured measurements of the confounders during training, but allow estimating personalized treatment effects purely from unstructured inputs, while avoiding confounding bias. When these structured measurements are only available for a non-representative subset of the data, these estimators may suffer from sampling bias. To address this, we further introduce a regression-based correction that accounts for the non-uniform sampling, assuming the sampling mechanism is known or can be well-estimated. Our experiments on two benchmark datasets show that the plug-in method, directly trainable on large unstructured datasets, achieves strong empirical performance across all settings, despite its simplicity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition</title>
<link>https://arxiv.org/abs/2507.20997</link>
<guid>https://arxiv.org/abs/2507.20997</guid>
<content:encoded><![CDATA[
arXiv:2507.20997v1 Announce Type: new 
Abstract: In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20999</link>
<guid>https://arxiv.org/abs/2507.20999</guid>
<content:encoded><![CDATA[
arXiv:2507.20999v1 Announce Type: new 
Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by "Thinking, Fast and Slow," which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different "subregions" of an LLM's parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability</title>
<link>https://arxiv.org/abs/2507.21004</link>
<guid>https://arxiv.org/abs/2507.21004</guid>
<content:encoded><![CDATA[
arXiv:2507.21004v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their black-box nature limits deployment in high-stakes domains requiring transparency. We introduce Compositional Function Networks (CFNs), a novel framework that builds inherently interpretable models by composing elementary mathematical functions with clear semantics. Unlike existing interpretable approaches that are limited to simple additive structures, CFNs support diverse compositional patterns -- sequential, parallel, and conditional -- enabling complex feature interactions while maintaining transparency. A key innovation is that CFNs are fully differentiable, allowing efficient training through standard gradient descent. We demonstrate CFNs' versatility across multiple domains, from symbolic regression to image classification with deep hierarchical networks. Our empirical evaluation shows CFNs achieve competitive performance against black-box models (96.24% accuracy on CIFAR-10) while outperforming state-of-the-art interpretable models like Explainable Boosting Machines. By combining the hierarchical expressiveness and efficient training of deep learning with the intrinsic interpretability of well-defined mathematical functions, CFNs offer a powerful framework for applications where both performance and accountability are paramount.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions</title>
<link>https://arxiv.org/abs/2507.21016</link>
<guid>https://arxiv.org/abs/2507.21016</guid>
<content:encoded><![CDATA[
arXiv:2507.21016v1 Announce Type: new 
Abstract: Predicting cognition from neuroimaging data in healthy individuals offers insights into the neural mechanisms underlying cognitive abilities, with potential applications in precision medicine and early detection of neurological and psychiatric conditions. This study systematically benchmarked classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN)) for cognitive prediction using Resting-state (RS), Working Memory, and Language task fMRI data from the Human Connectome Project Young Adult dataset.
  Our results, based on R2 scores, Pearson correlation coefficient, and mean absolute error, revealed that task-based fMRI, eliciting neural responses directly tied to cognition, outperformed RS fMRI in predicting cognitive behavior. Among the methods compared, a GNN combining structural connectivity (SC) and functional connectivity (FC) consistently achieved the highest performance across all fMRI modalities; however, its advantage over KRR using FC alone was not statistically significant. The TGNN, designed to model temporal dynamics with SC as a prior, performed competitively with FC-based approaches for task-fMRI but struggled with RS data, where its performance aligned with the lower-performing GNN that directly used fMRI time-series data as node features. These findings emphasize the importance of selecting appropriate model architectures and feature representations to fully leverage the spatial and temporal richness of neuroimaging data.
  This study highlights the potential of multimodal graph-aware DL models to combine SC and FC for cognitive prediction, as well as the promise of Transformer-based approaches for capturing temporal dynamics. By providing a comprehensive comparison of models, this work serves as a guide for advancing brain-behavior modeling using fMRI, SC and DL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming</title>
<link>https://arxiv.org/abs/2507.21021</link>
<guid>https://arxiv.org/abs/2507.21021</guid>
<content:encoded><![CDATA[
arXiv:2507.21021v1 Announce Type: new 
Abstract: This study proposes a behavior-specific filtering method to improve behavior classification accuracy in Precision Livestock Farming. While traditional filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%, they apply uniform processing to all behaviors. In contrast, the proposed behavior-specific filtering method combines Wavelet Denoising with a Low Pass Filter, tailored to active and inactive pig behaviors, and achieved a peak accuracy of 94.73%. These results highlight the effectiveness of behavior-specific filtering in enhancing animal behavior monitoring, supporting better health management and farm efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Using the Shapley Value for Anomaly Localization: A Statistical Investigation</title>
<link>https://arxiv.org/abs/2507.21023</link>
<guid>https://arxiv.org/abs/2507.21023</guid>
<content:encoded><![CDATA[
arXiv:2507.21023v1 Announce Type: new 
Abstract: Recent publications have suggested using the Shapley value for anomaly localization for sensor data systems. Using a reasonable mathematical anomaly model for full control, experiments indicate that using a single fixed term in the Shapley value calculation achieves a lower complexity anomaly localization test, with the same probability of error, as a test using the Shapley value for all cases tested. A proof demonstrates these conclusions must be true for all independent observation cases. For dependent observation cases, no proof is available.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Performance of Factorization Machine with Annealing under Limited Training Data</title>
<link>https://arxiv.org/abs/2507.21024</link>
<guid>https://arxiv.org/abs/2507.21024</guid>
<content:encoded><![CDATA[
arXiv:2507.21024v1 Announce Type: new 
Abstract: Black-box (BB) optimization problems aim to identify an input that minimizes the output of a function (the BB function) whose input-output relationship is unknown. Factorization machine with annealing (FMA) is a promising approach to this task, employing a factorization machine (FM) as a surrogate model to iteratively guide the solution search via an Ising machine. Although FMA has demonstrated strong optimization performance across various applications, its performance often stagnates as the number of optimization iterations increases. One contributing factor to this stagnation is the growing number of data points in the dataset used to train FM. It is hypothesized that as more data points are accumulated, the contribution of newly added data points becomes diluted within the entire dataset, thereby reducing their impact on improving the prediction accuracy of FM. To address this issue, we propose a novel method for sequential dataset construction that retains at most a specified number of the most recently added data points. This strategy is designed to enhance the influence of newly added data points on the surrogate model. Numerical experiments demonstrate that the proposed FMA achieves lower-cost solutions with fewer BB function evaluations compared to the conventional FMA.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding</title>
<link>https://arxiv.org/abs/2507.21037</link>
<guid>https://arxiv.org/abs/2507.21037</guid>
<content:encoded><![CDATA[
arXiv:2507.21037v1 Announce Type: new 
Abstract: Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key non-invasive brain-computer interface (BCI) paradigm for controlling external systems, has been significantly advanced by deep learning. However, MI-EEG decoding remains challenging due to substantial inter-subject variability and limited labeled target data, which necessitate costly calibration for new users. Many existing multi-source domain adaptation (MSDA) methods indiscriminately incorporate all available source domains, disregarding the large inter-subject differences in EEG signals, which leads to negative transfer and excessive computational costs. Moreover, while many approaches focus on feature distribution alignment, they often neglect the explicit dependence between features and decision-level outputs, limiting their ability to preserve discriminative structures. To address these gaps, we propose a novel MSDA framework that leverages a pretrained large Brain Foundation Model (BFM) for dynamic and informed source subject selection, ensuring only relevant sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS) and Conditional CS (CCS) divergences to jointly perform feature-level and decision-level alignment, enhancing domain invariance while maintaining class discriminability. Extensive evaluations on two benchmark MI-EEG datasets demonstrate that our framework outperforms a broad range of state-of-the-art baselines. Additional experiments with a large source pool validate the scalability and efficiency of BFM-guided selection, which significantly reduces training time without sacrificing performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements</title>
<link>https://arxiv.org/abs/2507.21040</link>
<guid>https://arxiv.org/abs/2507.21040</guid>
<content:encoded><![CDATA[
arXiv:2507.21040v1 Announce Type: new 
Abstract: We propose a probabilistic interpretation of transformers as unrolled inference steps assuming a probabilistic Laplacian Eigenmaps model from the ProbDR framework. Our derivation shows that at initialisation, transformers perform "linear" dimensionality reduction. We also show that within the transformer block, a graph Laplacian term arises from our arguments, rather than an attention matrix (which we interpret as an adjacency matrix). We demonstrate that simply subtracting the identity from the attention matrix (and thereby taking a graph diffusion step) improves validation performance on a language model and a simple vision transformer.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2507.21049</link>
<guid>https://arxiv.org/abs/2507.21049</guid>
<content:encoded><![CDATA[
arXiv:2507.21049v1 Announce Type: new 
Abstract: Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Policy Gradients</title>
<link>https://arxiv.org/abs/2507.21053</link>
<guid>https://arxiv.org/abs/2507.21053</guid>
<content:encoded><![CDATA[
arXiv:2507.21053v1 Announce Type: new 
Abstract: Flow-based generative models, including diffusion models, excel at modeling continuous distributions in high-dimensional spaces. In this work, we introduce Flow Policy Optimization (FPO), a simple on-policy reinforcement learning algorithm that brings flow matching into the policy gradient framework. FPO casts policy optimization as maximizing an advantage-weighted ratio computed from the conditional flow matching loss, in a manner compatible with the popular PPO-clip framework. It sidesteps the need for exact likelihood computation while preserving the generative capabilities of flow-based models. Unlike prior approaches for diffusion-based reinforcement learning that bind training to a specific sampling method, FPO is agnostic to the choice of diffusion or flow integration at both training and inference time. We show that FPO can train diffusion-style policies from scratch in a variety of continuous control tasks. We find that flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media</title>
<link>https://arxiv.org/abs/2507.19511</link>
<guid>https://arxiv.org/abs/2507.19511</guid>
<content:encoded><![CDATA[
arXiv:2507.19511v1 Announce Type: cross 
Abstract: The rising prevalence of mental health disorders necessitates the development of robust, automated tools for early detection and monitoring. Recent advances in Natural Language Processing (NLP), particularly transformer-based architectures, have demonstrated significant potential in text analysis. This study provides a comprehensive evaluation of state-of-the-art transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term Memory (LSTM) based approaches using different text embedding techniques for mental health disorder classification on Reddit. We construct a large annotated dataset, validating its reliability through statistical judgmental analysis and topic modeling. Experimental results demonstrate the superior performance of transformer models over traditional deep-learning approaches. RoBERTa achieved the highest classification performance, with a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set. Notably, LSTM models augmented with BERT embeddings proved highly competitive, achieving F1 scores exceeding 94% on the external dataset while requiring significantly fewer computational resources. These findings highlight the effectiveness of transformer-based models for real-time, scalable mental health monitoring. We discuss the implications for clinical applications and digital mental health interventions, offering insights into the capabilities and limitations of state-of-the-art NLP methodologies in mental disorder detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables</title>
<link>https://arxiv.org/abs/2507.19521</link>
<guid>https://arxiv.org/abs/2507.19521</guid>
<content:encoded><![CDATA[
arXiv:2507.19521v1 Announce Type: cross 
Abstract: The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with synthesized intents and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. Next, we propose several LLM-based schema editing techniques. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Then we demonstrate that our editing techniques can further improve schemas generated by these methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Behavioural Cloning and Reinforcement Learning for Spacecraft Guidance and Control Networks</title>
<link>https://arxiv.org/abs/2507.19535</link>
<guid>https://arxiv.org/abs/2507.19535</guid>
<content:encoded><![CDATA[
arXiv:2507.19535v1 Announce Type: cross 
Abstract: Guidance & control networks (G&amp;CNETs) provide a promising alternative to on-board guidance and control (G&amp;C) architectures for spacecraft, offering a differentiable, end-to-end representation of the guidance and control architecture. When training G&amp;CNETs, two predominant paradigms emerge: behavioural cloning (BC), which mimics optimal trajectories, and reinforcement learning (RL), which learns optimal behaviour through trials and errors. Although both approaches have been adopted in G&amp;CNET related literature, direct comparisons are notably absent. To address this, we conduct a systematic evaluation of BC and RL specifically for training G&amp;CNETs on continuous-thrust spacecraft trajectory optimisation tasks. We introduce a novel RL training framework tailored to G&amp;CNETs, incorporating decoupled action and control frequencies alongside reward redistribution strategies to stabilise training and to provide a fair comparison. Our results show that BC-trained G&amp;CNETs excel at closely replicating expert policy behaviour, and thus the optimal control structure of a deterministic environment, but can be negatively constrained by the quality and coverage of the training dataset. In contrast RL-trained G&amp;CNETs, beyond demonstrating a superior adaptability to stochastic conditions, can also discover solutions that improve upon suboptimal expert demonstrations, sometimes revealing globally optimal strategies that eluded the generation of training samples.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian symbolic regression: Automated equation discovery from a physicists' perspective</title>
<link>https://arxiv.org/abs/2507.19540</link>
<guid>https://arxiv.org/abs/2507.19540</guid>
<content:encoded><![CDATA[
arXiv:2507.19540v1 Announce Type: cross 
Abstract: Symbolic regression automates the process of learning closed-form mathematical models from data. Standard approaches to symbolic regression, as well as newer deep learning approaches, rely on heuristic model selection criteria, heuristic regularization, and heuristic exploration of model space. Here, we discuss the probabilistic approach to symbolic regression, an alternative to such heuristic approaches with direct connections to information theory and statistical physics. We show how the probabilistic approach establishes model plausibility from basic considerations and explicit approximations, and how it provides guarantees of performance that heuristic approaches lack. We also discuss how the probabilistic approach compels us to consider model ensembles, as opposed to single models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Sustainability Model Cards</title>
<link>https://arxiv.org/abs/2507.19559</link>
<guid>https://arxiv.org/abs/2507.19559</guid>
<content:encoded><![CDATA[
arXiv:2507.19559v1 Announce Type: cross 
Abstract: The growth of machine learning (ML) models and associated datasets triggers a consequent dramatic increase in energy costs for the use and training of these models. In the current context of environmental awareness and global sustainability concerns involving ICT, Green AI is becoming an important research topic. Initiatives like the AI Energy Score Ratings are a good example. Nevertheless, these benchmarking attempts are still to be integrated with existing work on Quality Models and Service-Level Agreements common in other, more mature, ICT subfields. This limits the (automatic) analysis of this model energy descriptions and their use in (semi)automatic model comparison, selection, and certification processes. We aim to leverage the concept of quality models and merge it with existing ML model reporting initiatives and Green/Frugal AI proposals to formalize a Sustainable Quality Model for AI/ML models. As a first step, we propose a new Domain-Specific Language to precisely define the sustainability aspects of an ML model (including the energy costs for its different tasks). This information can then be exported as an extended version of the well-known Model Cards initiative while, at the same time, being formal enough to be input of any other model description automatic process.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography</title>
<link>https://arxiv.org/abs/2507.19565</link>
<guid>https://arxiv.org/abs/2507.19565</guid>
<content:encoded><![CDATA[
arXiv:2507.19565v1 Announce Type: cross 
Abstract: The past decade's "cryoEM revolution" has produced exponential growth in high-resolution structural data through advances in cryogenic electron microscopy (cryoEM) and tomography (cryoET). Deep learning integration into structural proteomics workflows addresses longstanding challenges including low signal-to-noise ratios, preferred orientation artifacts, and missing-wedge problems that historically limited efficiency and scalability. This review examines AI applications across the entire cryoEM pipeline, from automated particle picking using convolutional neural networks (Topaz, crYOLO, CryoSegNet) to computational solutions for preferred orientation bias (spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In cryoET, tools like IsoNet employ U-Net architectures for simultaneous missing-wedge correction and noise reduction, while TomoNet streamlines subtomogram averaging through AI-driven particle detection. The workflow culminates with automated atomic model building using sophisticated tools like ModelAngelo, DeepTracer, and CryoREAD that translate density maps into interpretable biological structures. These AI-enhanced approaches have achieved near-atomic resolution reconstructions with minimal manual intervention, resolved previously intractable datasets suffering from severe orientation bias, and enabled successful application to diverse biological systems from HIV virus-like particles to in situ ribosomal complexes. As deep learning evolves, particularly with large language models and vision transformers, the future promises sophisticated automation and accessibility in structural biology, potentially revolutionizing our understanding of macromolecular architecture and function.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</title>
<link>https://arxiv.org/abs/2507.19568</link>
<guid>https://arxiv.org/abs/2507.19568</guid>
<content:encoded><![CDATA[
arXiv:2507.19568v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?</title>
<link>https://arxiv.org/abs/2507.19575</link>
<guid>https://arxiv.org/abs/2507.19575</guid>
<content:encoded><![CDATA[
arXiv:2507.19575v1 Announce Type: cross 
Abstract: Data scarcity is a major challenge in medical imaging, particularly for deep learning models. While data pooling (combining datasets from multiple sources) and data addition (adding more data from a new dataset) have been shown to enhance model performance, they are not without complications. Specifically, increasing the size of the training dataset through pooling or addition can induce distributional shifts, negatively affecting downstream model performance, a phenomenon known as the "Data Addition Dilemma". While the traditional i.i.d. assumption may not hold in multi-source contexts, assuming exchangeability across datasets provides a more practical framework for data pooling. In this work, we investigate medical image segmentation under these conditions, drawing insights from causal frameworks to propose a method for controlling foreground-background feature discrepancies across all layers of deep networks. This approach improves feature representations, which are crucial in data-addition scenarios. Our method achieves state-of-the-art segmentation performance on histopathology and ultrasound images across five datasets, including a novel ultrasound dataset that we have curated and contributed. Qualitative results demonstrate more refined and accurate segmentation maps compared to prominent baselines across three model architectures. The code will be available on Github.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning</title>
<link>https://arxiv.org/abs/2507.19586</link>
<guid>https://arxiv.org/abs/2507.19586</guid>
<content:encoded><![CDATA[
arXiv:2507.19586v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?</title>
<link>https://arxiv.org/abs/2507.19598</link>
<guid>https://arxiv.org/abs/2507.19598</guid>
<content:encoded><![CDATA[
arXiv:2507.19598v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State evolution beyond first-order methods I: Rigorous predictions and finite-sample guarantees</title>
<link>https://arxiv.org/abs/2507.19611</link>
<guid>https://arxiv.org/abs/2507.19611</guid>
<content:encoded><![CDATA[
arXiv:2507.19611v1 Announce Type: cross 
Abstract: We develop a toolbox for exact analysis of iterative algorithms on a class of high-dimensional nonconvex optimization problems with random data. While prior work has shown that low-dimensional statistics of (generalized) first-order methods can be predicted by a deterministic recursion known as state evolution, our focus is on developing such a prediction for a more general class of algorithms. We provide a state evolution for any method whose iterations are given by (possibly interleaved) first-order and saddle point updates, showing two main results. First, we establish a rigorous state evolution prediction that holds even when the updates are not coordinate-wise separable. Second, we establish finite-sample guarantees bounding the deviation of the empirical updates from the established state evolution. In the process, we develop a technical toolkit that may prove useful in related problems. One component of this toolkit is a general Hilbert space lifting technique to prove existence and uniqueness of a convenient parameterization of the state evolution. Another component of the toolkit combines a generic application of Bolthausen's conditioning method with a sequential variant of Gordon's Gaussian comparison inequality, and provides additional ingredients that enable a general finite-sample analysis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Reinforcement Learning by Adaptive Non-local Observables</title>
<link>https://arxiv.org/abs/2507.19629</link>
<guid>https://arxiv.org/abs/2507.19629</guid>
<content:encoded><![CDATA[
arXiv:2507.19629v1 Announce Type: cross 
Abstract: Hybrid quantum-classical frameworks leverage quantum computing for machine learning; however, variational quantum circuits (VQCs) are limited by the need for local measurements. We introduce an adaptive non-local observable (ANO) paradigm within VQCs for quantum reinforcement learning (QRL), jointly optimizing circuit parameters and multi-qubit measurements. The ANO-VQC architecture serves as the function approximator in Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms. On multiple benchmark tasks, ANO-VQC agents outperform baseline VQCs. Ablation studies reveal that adaptive measurements enhance the function space without increasing circuit depth. Our results demonstrate that adaptive multi-qubit observables can enable practical quantum advantages in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street network sub-patterns and travel mode</title>
<link>https://arxiv.org/abs/2507.19648</link>
<guid>https://arxiv.org/abs/2507.19648</guid>
<content:encoded><![CDATA[
arXiv:2507.19648v1 Announce Type: cross 
Abstract: Urban morphology has long been recognized as a factor shaping human mobility, yet comparative and formal classifications of urban form across metropolitan areas remain limited. Building on theoretical principles of urban structure and advances in unsupervised learning, we systematically classified the built environment of nine U.S. metropolitan areas using structural indicators such as density, connectivity, and spatial configuration. The resulting morphological types were linked to mobility patterns through descriptive statistics, marginal effects estimation, and post hoc statistical testing. Here we show that distinct urban forms are systematically associated with different mobility behaviors, such as reticular morphologies being linked to significantly higher public transport use (marginal effect = 0.49) and reduced car dependence (-0.41), while organic forms are associated with increased car usage (0.44), and substantial declines in public transport (-0.47) and active mobility (-0.30). These effects are statistically robust (p < 1e-19), highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices. Our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research. These results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments</title>
<link>https://arxiv.org/abs/2507.19653</link>
<guid>https://arxiv.org/abs/2507.19653</guid>
<content:encoded><![CDATA[
arXiv:2507.19653v1 Announce Type: cross 
Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links in central Rome. We use a real measurement set of 1,664 user-equipments (UEs) and six nominal base-station (BS) sites. Using these fixed positions we systematically vary the main simulation parameters, including path depth, diffuse/specular/refraction flags, carrier frequency, as well as antenna's properties like its altitude, radiation pattern, and orientation. Simulator fidelity is scored for each base station via Spearman correlation between measured and simulated powers, and by a fingerprint-based k-nearest-neighbor localization algorithm using RSSI-based fingerprints. Across all experiments, solver hyper-parameters are having immaterial effect on the chosen metrics. On the contrary, antenna locations and orientations prove decisive. By simple greedy optimization we improve the Spearman correlation by 5% to 130% for various base stations, while kNN-based localization error using only simulated data as reference points is decreased by one-third on real-world samples, while staying twice higher than the error with purely real data. Precise geometry and credible antenna models are therefore necessary but not sufficient; faithfully capturing the residual urban noise remains an open challenge for transferable, high-fidelity outdoor RF simulation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices</title>
<link>https://arxiv.org/abs/2507.19663</link>
<guid>https://arxiv.org/abs/2507.19663</guid>
<content:encoded><![CDATA[
arXiv:2507.19663v1 Announce Type: cross 
Abstract: Solder joint reliability related to failures due to thermomechanical loading is a critically important yet physically complex engineering problem. As a result, simulated behavior is oftentimes computationally expensive. In an increasingly data-driven world, the usage of efficient data-driven design schemes is a popular choice. Among them, Bayesian optimization (BO) with Gaussian process regression is one of the most important representatives. The authors argue that computational savings can be obtained from exploiting thorough surrogate modeling and selecting a design candidate based on multiple acquisition functions. This is feasible due to the relatively low computational cost, compared to the expensive simulation objective. This paper addresses the shortcomings in the adjacent literature by providing and implementing a novel heuristic framework to perform BO with adaptive hyperparameters across the various optimization iterations. Adaptive BO is subsequently compared to regular BO when faced with synthetic objective minimization problems. The results show the efficiency of adaptive BO when compared any worst-performing regular Bayesian schemes. As an engineering use case, the solder joint reliability problem is tackled by minimizing the accumulated non-linear creep strain under a cyclic thermal load. Results show that adaptive BO outperforms regular BO by 3% on average at any given computational budget threshold, critically saving half of the computational expense budget. This practical result underlines the methodological potential of the adaptive Bayesian data-driven methodology to achieve better results and cut optimization-related expenses. Lastly, in order to promote the reproducibility of the results, the data-driven implementations are made available on an open-source basis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges</title>
<link>https://arxiv.org/abs/2507.19672</link>
<guid>https://arxiv.org/abs/2507.19672</guid>
<content:encoded><![CDATA[
arXiv:2507.19672v1 Announce Type: cross 
Abstract: Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks</title>
<link>https://arxiv.org/abs/2507.19702</link>
<guid>https://arxiv.org/abs/2507.19702</guid>
<content:encoded><![CDATA[
arXiv:2507.19702v1 Announce Type: cross 
Abstract: Identifying influential nodes in complex networks is a critical task with a wide range of applications across different domains. However, existing approaches often face trade-offs between accuracy and computational efficiency. To address these challenges, we propose 1D-CGS, a lightweight and effective hybrid model that integrates the speed of one-dimensional convolutional neural networks (1D-CNN) with the topological representation power of GraphSAGE for efficient node ranking. The model uses a lightweight input representation built on two straightforward and significant topological features: node degree and average neighbor degree. These features are processed through 1D convolutions to extract local patterns, followed by GraphSAGE layers to aggregate neighborhood information. We formulate the node ranking task as a regression problem and use the Susceptible-Infected-Recovered (SIR) model to generate ground truth influence scores. 1D-CGS is initially trained on synthetic networks generated by the Barabasi-Albert model and then applied to real world networks for identifying influential nodes. Experimental evaluations on twelve real world networks demonstrate that 1D-CGS significantly outperforms traditional centrality measures and recent deep learning models in ranking accuracy, while operating in very fast runtime. The proposed model achieves an average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard Similarity over the best performing deep learning baselines. It also achieves an average Monotonicity Index (MI) score 0.99 and produces near perfect rank distributions, indicating highly unique and discriminative rankings. Furthermore, all experiments confirm that 1D-CGS operates in a highly reasonable time, running significantly faster than existing deep learning methods, making it suitable for large scale applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19712</link>
<guid>https://arxiv.org/abs/2507.19712</guid>
<content:encoded><![CDATA[
arXiv:2507.19712v1 Announce Type: cross 
Abstract: In this paper, we explore mission assignment and task offloading in an Open Radio Access Network (Open RAN)-based intelligent transportation system (ITS), where autonomous vehicles leverage mobile edge computing for efficient processing. Existing studies often overlook the intricate interdependencies between missions and the costs associated with offloading tasks to edge servers, leading to suboptimal decision-making. To bridge this gap, we introduce Oranits, a novel system model that explicitly accounts for mission dependencies and offloading costs while optimizing performance through vehicle cooperation. To achieve this, we propose a twofold optimization approach. First, we develop a metaheuristic-based evolutionary computing algorithm, namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline for one-slot optimization. Second, we design an enhanced reward-based deep reinforcement learning (DRL) framework, referred to as the Multi-agent Double Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and multi-action selection mechanisms, significantly reducing mission assignment time and improving adaptability over baseline methods. Extensive simulations reveal that CGG-ARO improves the number of completed missions and overall benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN achieves even greater improvements of 11.0% in terms of mission completions and 12.5% in terms of the overall benefit. These results highlight the effectiveness of Oranits in enabling faster, more adaptive, and more efficient task processing in dynamic ITS environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2507.19718</link>
<guid>https://arxiv.org/abs/2507.19718</guid>
<content:encoded><![CDATA[
arXiv:2507.19718v1 Announce Type: cross 
Abstract: Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare</title>
<link>https://arxiv.org/abs/2507.19726</link>
<guid>https://arxiv.org/abs/2507.19726</guid>
<content:encoded><![CDATA[
arXiv:2507.19726v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are widely used in various application domains. Healthcare is one of such domains where KGs are intensively used, due to the high requirement for knowledge accuracy and interconnected nature of healthcare data. However, KGs storing general factual information often lack the ability to account for important contexts of the knowledge such as the status of specific patients, which are crucial in precision healthcare. Meanwhile, electronic health records (EHRs) provide rich personal data, including various diagnoses and medications, which provide natural contexts for general KGs. In this paper, we propose HypKG, a framework that integrates patient information from EHRs into KGs to generate contextualized knowledge representations for accurate healthcare predictions. Using advanced entity-linking techniques, we connect relevant knowledge from general KGs with patient information from EHRs, and then utilize a hypergraph model to "contextualize" the knowledge with the patient information. Finally, we employ hypergraph transformers guided by downstream prediction tasks to jointly learn proper contextualized representations for both KGs and patients, fully leveraging existing knowledge in KGs and patient contexts in EHRs. In experiments using a large biomedical KG and two real-world EHR datasets, HypKG demonstrates significant improvements in healthcare prediction tasks across multiple evaluation metrics. Additionally, by integrating external contexts, HypKG can learn to adjust the representations of entities and relations in KG, potentially improving the quality and real-world utility of knowledge.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Metabolic-Imaging Integrated Model for Prognostic Prediction in Colorectal Liver Metastases</title>
<link>https://arxiv.org/abs/2507.19734</link>
<guid>https://arxiv.org/abs/2507.19734</guid>
<content:encoded><![CDATA[
arXiv:2507.19734v1 Announce Type: cross 
Abstract: Prognostic evaluation in patients with colorectal liver metastases (CRLM) remains challenging due to suboptimal accuracy of conventional clinical models. This study developed and validated a robust machine learning model for predicting postoperative recurrence risk. Preliminary ensemble models achieved exceptionally high performance (AUC $>$ 0.98) but incorporated postoperative features, introducing data leakage risks. To enhance clinical applicability, we restricted input variables to preoperative baseline clinical parameters and radiomic features from contrast-enhanced CT imaging, specifically targeting recurrence prediction at 3, 6, and 12 months postoperatively. The 3-month recurrence prediction model demonstrated optimal performance with an AUC of 0.723 in cross-validation. Decision curve analysis revealed that across threshold probabilities of 0.55-0.95, the model consistently provided greater net benefit than "treat-all" or "treat-none" strategies, supporting its utility in postoperative surveillance and therapeutic decision-making. This study successfully developed a robust predictive model for early CRLM recurrence with confirmed clinical utility. Importantly, it highlights the critical risk of data leakage in clinical prognostic modeling and proposes a rigorous framework to mitigate this issue, enhancing model reliability and translational value in real-world settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19742</link>
<guid>https://arxiv.org/abs/2507.19742</guid>
<content:encoded><![CDATA[
arXiv:2507.19742v1 Announce Type: cross 
Abstract: Particle filter-based 2D-SLAM is widely used in indoor localization tasks due to its efficiency. However, indoor environments such as long straight corridors can cause severe degeneracy problems in SLAM. In this paper, we use Proximal Policy Optimization (PPO) to train an adaptive degeneracy optimization agent (DOA) to address degeneracy problem. We propose a systematic methodology to address three critical challenges in traditional supervised learning frameworks: (1) data acquisition bottlenecks in degenerate dataset, (2) inherent quality deterioration of training samples, and (3) ambiguity in annotation protocol design. We design a specialized reward function to guide the agent in developing perception capabilities for degenerate environments. Using the output degeneracy factor as a reference weight, the agent can dynamically adjust the contribution of different sensors to pose optimization. Specifically, the observation distribution is shifted towards the motion model distribution, with the step size determined by a linear interpolation formula related to the degeneracy factor. In addition, we employ a transfer learning module to endow the agent with generalization capabilities across different environments and address the inefficiency of training in degenerate environments. Finally, we conduct ablation studies to demonstrate the rationality of our model design and the role of transfer learning. We also compare the proposed DOA with SOTA methods to prove its superior degeneracy detection and optimization capabilities across various environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenBlowUp: Resolving Representational Singularities in LLM Token Spaces via Monoidal Transformations</title>
<link>https://arxiv.org/abs/2507.19747</link>
<guid>https://arxiv.org/abs/2507.19747</guid>
<content:encoded><![CDATA[
arXiv:2507.19747v1 Announce Type: cross 
Abstract: Recent work has provided compelling evidence challenging the foundational manifold hypothesis for the token embedding spaces of Large Language Models (LLMs). These findings reveal the presence of geometric singularities around polysemous tokens, which can lead to representational instability. Existing methodologies, which presuppose a smooth data manifold, are ill-equipped to address such intrinsic structural flaws. In this paper, we formalize this problem in the language of scheme theory and propose a rigorous resolution by applying the scheme-theoretic blow-up at each singular point. This procedure replaces a singular point in the ambient affine scheme with its exceptional divisor, which we identify as a canonical geometric space -- a projective space of directions -- that houses the disambiguated semantic meanings of the token. This process of ``representational desingularization'' constructs a new geometric landscape for embeddings. We prove a formal theorem guaranteeing the geometric regularization of this new space, showing that the original pathologies are resolved. Finally, we outline the architectural implications of our framework, arguing for a paradigm shift from static look-ups to dynamic, geometrically-grounded computation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery</title>
<link>https://arxiv.org/abs/2507.19759</link>
<guid>https://arxiv.org/abs/2507.19759</guid>
<content:encoded><![CDATA[
arXiv:2507.19759v1 Announce Type: cross 
Abstract: The microphysical properties of ice crystals are important because they significantly alter the radiative properties and spatiotemporal distributions of clouds, which in turn strongly affect Earth's climate. However, it is challenging to measure key properties of ice crystals, such as mass or morphological features. Here, we present a framework for predicting three-dimensional (3D) microphysical properties of ice crystals from in situ two-dimensional (2D) imagery. First, we computationally generate synthetic ice crystals using 3D modeling software along with geometric parameters estimated from the 2021 Ice Cryo-Encapsulation Balloon (ICEBall) field campaign. Then, we use synthetic crystals to train machine learning (ML) models to predict effective density ($\rho_{e}$), effective surface area ($A_e$), and number of bullets ($N_b$) from synthetic rosette imagery. When tested on unseen synthetic images, we find that our ML models can predict microphysical properties with high accuracy. For $\rho_{e}$ and $A_e$, respectively, our best-performing single view models achieved $R^2$ values of 0.99 and 0.98. For $N_b$, our best single view model achieved a balanced accuracy and F1 score of 0.91. We also quantify the marginal prediction improvements from incorporating a second view. A stereo view ResNet-18 model reduced RMSE by 40% for both $\rho_e$ and $A_e$, relative to a single view ResNet-18 model. For $N_b$, we find that a stereo view ResNet-18 model improved the F1 score by 8%. This work provides a novel ML-driven framework for estimating ice microphysical properties from in situ imagery, which will allow for downstream constraints on microphysical parameterizations, such as the mass-size relationship.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bag of Coins: A Statistical Probe into Neural Confidence Structures</title>
<link>https://arxiv.org/abs/2507.19774</link>
<guid>https://arxiv.org/abs/2507.19774</guid>
<content:encoded><![CDATA[
arXiv:2507.19774v1 Announce Type: cross 
Abstract: Modern neural networks, despite their high accuracy, often produce poorly calibrated confidence scores, limiting their reliability in high-stakes applications. Existing calibration methods typically post-process model outputs without interrogating the internal consistency of the predictions themselves. In this work, we introduce a novel, non-parametric statistical probe, the Bag-of-Coins (BoC) test, that examines the internal consistency of a classifier's logits. The BoC test reframes confidence estimation as a frequentist hypothesis test: does the model's top-ranked class win 1-v-1 contests against random competitors at a rate consistent with its own stated softmax probability? When applied to modern deep learning architectures, this simple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs), the BoC output serves as a state-of-the-art confidence score, achieving near-perfect calibration with an ECE of 0.0212, an 88% improvement over a temperature-scaled baseline. Conversely, on Convolutional Neural Networks (CNNs) like ResNet, the probe reveals a deep inconsistency between the model's predictions and its internal logit structure, a property missed by traditional metrics. We posit that BoC is not merely a calibration method, but a new diagnostic tool for understanding and exposing the differing ways that popular architectures represent uncertainty.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation</title>
<link>https://arxiv.org/abs/2507.19781</link>
<guid>https://arxiv.org/abs/2507.19781</guid>
<content:encoded><![CDATA[
arXiv:2507.19781v1 Announce Type: cross 
Abstract: Self-supervised learning has revolutionized representation learning in vision and language, but remains underexplored for hyperspectral imagery (HSI), where the sequential structure of spectral bands offers unique opportunities. In this work, we propose Spectral Band Permutation Prediction (SpecBPP), a novel self-supervised learning framework that leverages the inherent spectral continuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a model to recover the correct order of shuffled spectral segments, encouraging global spectral understanding. We implement a curriculum-based training strategy that progressively increases permutation difficulty to manage the factorial complexity of the permutation space. Applied to Soil Organic Carbon (SOC) estimation using EnMAP satellite data, our method achieves state-of-the-art results, outperforming both masked autoencoder (MAE) and joint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled samples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19, significantly surpassing traditional and self-supervised benchmarks. Our results demonstrate that spectral order prediction is a powerful pretext task for hyperspectral understanding, opening new avenues for scientific representation learning in remote sensing and beyond.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures</title>
<link>https://arxiv.org/abs/2507.19787</link>
<guid>https://arxiv.org/abs/2507.19787</guid>
<content:encoded><![CDATA[
arXiv:2507.19787v1 Announce Type: cross 
Abstract: The dynamic mode decomposition (DMD) is a data-driven approach that extracts the dominant features from spatiotemporal data. In this work, we introduce sparse-mode DMD, a new variant of the optimized DMD framework that specifically leverages sparsity-promoting regularization in order to approximate DMD modes which have localized spatial structure. The algorithm maintains the noise-robust properties of optimized DMD while disambiguating between modes which are spatially local versus global in nature. In many applications, such modes are associated with discrete and continuous spectra respectively, thus allowing the algorithm to explicitly construct, in an unsupervised manner, the distinct portions of the spectrum. We demonstrate this by analyzing synthetic and real-world systems, including examples from optical waveguides, quantum mechanics, and sea surface temperature data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning</title>
<link>https://arxiv.org/abs/2507.19795</link>
<guid>https://arxiv.org/abs/2507.19795</guid>
<content:encoded><![CDATA[
arXiv:2507.19795v1 Announce Type: cross 
Abstract: Major advancements in the capabilities of computer vision models have been primarily fueled by rapid expansion of datasets, model parameters, and computational budgets, leading to ever-increasing demands on computational infrastructure. However, as these models are deployed in increasingly diverse and resource-constrained environments, there is a pressing need for architectures that can deliver high performance while requiring fewer computational resources.
  This dissertation focuses on architectural principles through which models can achieve increased performance while reducing their computational demands. We discuss strides towards this goal through three directions. First, we focus on data ingress and egress, investigating how information may be passed into and retrieved from our core neural processing units. This ensures that our models make the most of available data, allowing smaller architectures to become more performant. Second, we investigate modifications to the core neural architecture, applied to restricted attention in vision transformers. This section explores how removing uniform context windows in restricted attention increases the expressivity of the underlying neural architecture. Third, we explore the natural structures of Normalizing Flows and how we can leverage these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures can increase the efficiency of machine learning algorithms, allowing them to become smaller, faster, and cheaper.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing and Mitigating Repetitions in Trip Recommendation</title>
<link>https://arxiv.org/abs/2507.19798</link>
<guid>https://arxiv.org/abs/2507.19798</guid>
<content:encoded><![CDATA[
arXiv:2507.19798v1 Announce Type: cross 
Abstract: Trip recommendation has emerged as a highly sought-after service over the past decade. Although current studies significantly understand human intention consistency, they struggle with undesired repetitive outcomes that need resolution. We make two pivotal discoveries using statistical analyses and experimental designs: (1) The occurrence of repetitions is intricately linked to the models and decoding strategies. (2) During training and decoding, adding perturbations to logits can reduce repetition. Motivated by these observations, we introduce AR-Trip (Anti Repetition for Trip Recommendation), which incorporates a cycle-aware predictor comprising three mechanisms to avoid duplicate Points-of-Interest (POIs) and demonstrates their effectiveness in alleviating repetition. Experiments on four public datasets illustrate that AR-Trip successfully mitigates repetition issues while enhancing precision.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Materials Discovery with Valence Constrained Design in Generative Modeling</title>
<link>https://arxiv.org/abs/2507.19799</link>
<guid>https://arxiv.org/abs/2507.19799</guid>
<content:encoded><![CDATA[
arXiv:2507.19799v1 Announce Type: cross 
Abstract: Diffusion-based deep generative models have emerged as powerful tools for inverse materials design. Yet, many existing approaches overlook essential chemical constraints such as oxidation state balance, which can lead to chemically invalid structures. Here we introduce CrysVCD (Crystal generator with Valence-Constrained Design), a modular framework that integrates chemical rules directly into the generative process. CrysVCD first employs a transformer-based elemental language model to generate valence-balanced compositions, followed by a diffusion model to generate crystal structures. The valence constraint enables orders-of-magnitude more efficient chemical valence checking, compared to pure data-driven approaches with post-screening. When fine-tuned on stability metrics, CrysVCD achieves 85% thermodynamic stability and 68% phonon stability. Moreover, CrysVCD supports conditional generation of functional materials, enabling discovery of candidates such as high thermal conductivity semiconductors and high-$\kappa$ dielectric compounds. Designed as a general-purpose plugin, CrysVCD can be integrated into diverse generative pipeline to promote chemical validity, offering a reliable, scientifically grounded path for materials discovery.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence-based protein-protein interaction prediction and its applications in drug discovery</title>
<link>https://arxiv.org/abs/2507.19805</link>
<guid>https://arxiv.org/abs/2507.19805</guid>
<content:encoded><![CDATA[
arXiv:2507.19805v1 Announce Type: cross 
Abstract: Aberrant protein-protein interactions (PPIs) underpin a plethora of human diseases, and disruption of these harmful interactions constitute a compelling treatment avenue. Advances in computational approaches to PPI prediction have closely followed progress in deep learning and natural language processing. In this review, we outline the state-of the-art for sequence-based PPI prediction methods and explore their impact on target identification and drug discovery. We begin with an overview of commonly used training data sources and techniques used to curate these data to enhance the quality of the training set. Subsequently, we survey various PPI predictor types, including traditional similarity-based approaches, and deep learning-based approaches with a particular emphasis on the transformer architecture. Finally, we provide examples of PPI prediction in systems-level proteomics analyses, target identification, and design of therapeutic peptides and antibodies. We also take the opportunity to showcase the potential of PPI-aware drug discovery models in accelerating therapeutic development.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</title>
<link>https://arxiv.org/abs/2507.19823</link>
<guid>https://arxiv.org/abs/2507.19823</guid>
<content:encoded><![CDATA[
arXiv:2507.19823v1 Announce Type: cross 
Abstract: Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Deep Learning and Handcrafted Feature Fusion for Mammographic Breast Cancer Classification</title>
<link>https://arxiv.org/abs/2507.19843</link>
<guid>https://arxiv.org/abs/2507.19843</guid>
<content:encoded><![CDATA[
arXiv:2507.19843v1 Announce Type: cross 
Abstract: Automated breast cancer classification from mammography remains a significant challenge due to subtle distinctions between benign and malignant tissue. In this work, we present a hybrid framework combining deep convolutional features from a ResNet-50 backbone with handcrafted descriptors and transformer-based embeddings. Using the CBIS-DDSM dataset, we benchmark our ResNet-50 baseline (AUC: 78.1%) and demonstrate that fusing handcrafted features with deep ResNet-50 and DINOv2 features improves AUC to 79.6% (setup d1), with a peak recall of 80.5% (setup d1) and highest F1 score of 67.4% (setup d1). Our experiments show that handcrafted features not only complement deep representations but also enhance performance beyond transformer-based embeddings. This hybrid fusion approach achieves results comparable to state-of-the-art methods while maintaining architectural simplicity and computational efficiency, making it a practical and effective solution for clinical decision support.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization</title>
<link>https://arxiv.org/abs/2507.19858</link>
<guid>https://arxiv.org/abs/2507.19858</guid>
<content:encoded><![CDATA[
arXiv:2507.19858v1 Announce Type: cross 
Abstract: Multi-source CT-scan classification suffers from domain shifts that impair cross-source generalization. While preprocessing pipelines combining Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) have shown empirical success, the mechanisms underlying their domain robustness remain underexplored. This study analyzes how this input-space standardization manages the trade-off between local discriminability and cross-source generalization. The SSFL++ and KDS pipeline performs spatial and temporal standardization to reduce inter-source variance, effectively mapping disparate inputs into a consistent target space. This preemptive alignment mitigates domain shift and simplifies the learning task for network optimization. Experimental validation demonstrates consistent improvements across architectures, proving the benefits stem from the preprocessing itself. The approach's effectiveness was validated by securing first place in a competitive challenge, supporting input-space standardization as a robust and practical solution for multi-institutional medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Informed Machine Learning for Chaotic Systems</title>
<link>https://arxiv.org/abs/2507.19861</link>
<guid>https://arxiv.org/abs/2507.19861</guid>
<content:encoded><![CDATA[
arXiv:2507.19861v1 Announce Type: cross 
Abstract: Learning the behaviour of chaotic systems remains challenging due to instability in long-term predictions and difficulties in accurately capturing invariant statistical properties. While quantum machine learning offers a promising route to efficiently capture physical properties from high-dimensional data, its practical deployment is hindered by current hardware noise and limited scalability. We introduce a quantum-informed machine learning framework for learning partial differential equations, with an application focus on chaotic systems. A quantum circuit Born machine is employed to learn the invariant properties of chaotic dynamical systems, achieving substantial memory efficiency by representing these complex physical statistics with a compact set of trainable circuit parameters. This approach reduces the data storage requirement by over two orders of magnitude compared to the raw simulation data. The resulting statistical quantum-informed prior is then incorporated into a Koopman-based auto-regressive model to address issues such as gradient vanishing or explosion, while maintaining long-term statistical fidelity. The framework is evaluated on three representative systems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and turbulent channel flow. In all cases, the quantum-informed model achieves superior performance compared to its classical counterparts without quantum priors. This hybrid architecture offers a practical route for learning dynamical systems using near-term quantum hardware.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control. II: Non-Penalty Approach</title>
<link>https://arxiv.org/abs/2507.19895</link>
<guid>https://arxiv.org/abs/2507.19895</guid>
<content:encoded><![CDATA[
arXiv:2507.19895v1 Announce Type: cross 
Abstract: This work is a companion paper of [8], where the distributed linear-quadratic problem with fixed communication topology (DFT-LQ) and the sparse feedback LQ problem (SF-LQ) are formulated into a nonsmooth and nonconvex optimization problem with affine constraints. Moreover, a penalty approach is considered in \cite{feng-part1}, and the PALM (proximal alternating linearized minimization) algorithm is studied with convergence and complexity analysis. In this paper, we aim to address the inherent drawbacks of the penalty approach, such as the challenge of tuning the penalty parameter and the risk of introducing spurious stationary points. Specifically, we first reformulate the SF-LQ problem and the DFT-LQ problem from an epi-composition function perspective, aiming to solve the constrained problem directly. Then, from a theoretical viewpoint, we revisit the alternating direction method of multipliers (ADMM) and establish its convergence to the set of cluster points under certain assumptions. When these assumptions do not hold, we can effectively utilize alternative approaches combining subgradient descent with Difference-of-Convex relaxation methods. In summary, our results enable the direct design of group-sparse feedback gains with theoretical guarantees, without resorting to convex surrogates, restrictive structural assumptions, or penalty formulations that incorporate constraints into the cost function.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Insight: Visualizing Thompson Sampling for Verification and XAI</title>
<link>https://arxiv.org/abs/2507.19898</link>
<guid>https://arxiv.org/abs/2507.19898</guid>
<content:encoded><![CDATA[
arXiv:2507.19898v1 Announce Type: cross 
Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a ``black box'', hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Fine-tuning Large Language Models on Automated Program Repair</title>
<link>https://arxiv.org/abs/2507.19909</link>
<guid>https://arxiv.org/abs/2507.19909</guid>
<content:encoded><![CDATA[
arXiv:2507.19909v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems</title>
<link>https://arxiv.org/abs/2507.19936</link>
<guid>https://arxiv.org/abs/2507.19936</guid>
<content:encoded><![CDATA[
arXiv:2507.19936v1 Announce Type: cross 
Abstract: This paper investigates joint channel estimation and positioning in near-field sparse extra-large multiple-input multiple-output (XL-MIMO) orthogonal frequency division multiplexing (OFDM) systems. To achieve cooperative gains between channel estimation and positioning, we propose a deep learning-based two-stage framework comprising positioning and channel estimation. In the positioning stage, the user's coordinates are predicted and utilized in the channel estimation stage, thereby enhancing the accuracy of channel estimation. Within this framework, we propose a U-shaped Mamba architecture for channel estimation and positioning, termed as CP-Mamba. This network integrates the strengths of the Mamba model with the structural advantages of U-shaped convolutional networks, enabling effective capture of local spatial features and long-range temporal dependencies of the channel. Numerical simulation results demonstrate that the proposed two-stage approach with CP-Mamba architecture outperforms existing baseline methods. Moreover, sparse arrays (SA) exhibit significantly superior performance in both channel estimation and positioning accuracy compared to conventional compact arrays.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations</title>
<link>https://arxiv.org/abs/2507.19947</link>
<guid>https://arxiv.org/abs/2507.19947</guid>
<content:encoded><![CDATA[
arXiv:2507.19947v1 Announce Type: cross 
Abstract: Fusing information from human observations can help robots overcome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework requires a grounded likelihood representing the uncertainty of human inputs. This paper presents a Feature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning relevant map image features and their relationships with spatial relation semantics. The model is trained as a probability estimator to capture aleatoric uncertainty in human language using three-stage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative sensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware fusion of heterogeneous human language observations and robot sensor measurements, achieving significant improvements in human-robot collaborative task performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions</title>
<link>https://arxiv.org/abs/2507.19970</link>
<guid>https://arxiv.org/abs/2507.19970</guid>
<content:encoded><![CDATA[
arXiv:2507.19970v1 Announce Type: cross 
Abstract: Medical image analysis plays a pivotal role in the early diagnosis of diseases such as skin lesions. However, the scarcity of data and the class imbalance significantly hinder the performance of deep learning models. We propose a novel method that leverages the pretrained Stable Diffusion-2.0 model to generate high-quality synthetic skin lesion images and corresponding segmentation masks. This approach augments training datasets for classification and segmentation tasks. We adapt Stable Diffusion-2.0 through domain-specific Low-Rank Adaptation (LoRA) fine-tuning and joint optimization of multi-objective loss functions, enabling the model to simultaneously generate clinically relevant images and segmentation masks conditioned on textual descriptions in a single step. Experimental results show that the generated images, validated by FID scores, closely resemble real images in quality. A hybrid dataset combining real and synthetic data markedly enhances the performance of classification and segmentation models, achieving substantial improvements in accuracy and F1-score of 8% to 15%, with additional positive gains in other key metrics such as the Dice coefficient and IoU. Our approach offers a scalable solution to address the challenges of medical imaging data, contributing to improved accuracy and reliability in diagnosing rare diseases.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A roadmap for AI in robotics</title>
<link>https://arxiv.org/abs/2507.19975</link>
<guid>https://arxiv.org/abs/2507.19975</guid>
<content:encoded><![CDATA[
arXiv:2507.19975v1 Announce Type: cross 
Abstract: AI technologies, including deep learning, large-language models have gone from one breakthrough to the other. As a result, we are witnessing growing excitement in robotics at the prospect of leveraging the potential of AI to tackle some of the outstanding barriers to the full deployment of robots in our daily lives. However, action and sensing in the physical world pose greater and different challenges than analysing data in isolation. As the development and application of AI in robotic products advances, it is important to reflect on which technologies, among the vast array of network architectures and learning models now available in the AI field, are most likely to be successfully applied to robots; how they can be adapted to specific robot designs, tasks, environments; which challenges must be overcome. This article offers an assessment of what AI for robotics has achieved since the 1990s and proposes a short- and medium-term research roadmap listing challenges and promises. These range from keeping up-to-date large datasets, representatives of a diversity of tasks robots may have to perform, and of environments they may encounter, to designing AI algorithms tailored specifically to robotics problems but generic enough to apply to a wide range of applications and transfer easily to a variety of robotic platforms. For robots to collaborate effectively with humans, they must predict human behavior without relying on bias-based profiling. Explainability and transparency in AI-driven robot control are not optional but essential for building trust, preventing misuse, and attributing responsibility in accidents. We close on what we view as the primary long-term challenges, that is, to design robots capable of lifelong learning, while guaranteeing safe deployment and usage, and sustainable computational costs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extreme value theory for singular subspace estimation in the matrix denoising model</title>
<link>https://arxiv.org/abs/2507.19978</link>
<guid>https://arxiv.org/abs/2507.19978</guid>
<content:encoded><![CDATA[
arXiv:2507.19978v1 Announce Type: cross 
Abstract: This paper studies fine-grained singular subspace estimation in the matrix denoising model where a deterministic low-rank signal matrix is additively perturbed by a stochastic matrix of Gaussian noise. We establish that the maximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned difference between the leading sample and population singular vectors approaches the Gumbel distribution in the large-matrix limit, under suitable signal-to-noise conditions and after appropriate centering and scaling. We apply our novel asymptotic distributional theory to test hypotheses of low-rank signal structure encoded in the leading singular vectors and their corresponding principal subspace. We provide de-biased estimators for the corresponding nuisance signal singular values and show that our proposed plug-in test statistic has desirable properties. Notably, compared to using the Frobenius norm subspace distance, our test statistic based on the two-to-infinity norm has higher power to detect structured alternatives that differ from the null in only a few matrix entries or rows. Our main results are obtained by a novel synthesis of and technical analysis involving entrywise matrix perturbation analysis, extreme value theory, saddle point approximation methods, and random matrix theory. Our contributions complement the existing literature for matrix denoising focused on minimaxity, mean squared error analysis, unitarily invariant distances between subspaces, component-wise asymptotic distributional theory, and row-wise uniform error bounds. Numerical simulations illustrate our main results and demonstrate the robustness properties of our testing procedure to non-Gaussian noise distributions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Analytics Using Tensor Unified Linear Comparative Analysis</title>
<link>https://arxiv.org/abs/2507.19988</link>
<guid>https://arxiv.org/abs/2507.19988</guid>
<content:encoded><![CDATA[
arXiv:2507.19988v1 Announce Type: cross 
Abstract: Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors' essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA's functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Vocal-Conditioned Music Generation via Soft Alignment Attention and Latent Diffusion</title>
<link>https://arxiv.org/abs/2507.19991</link>
<guid>https://arxiv.org/abs/2507.19991</guid>
<content:encoded><![CDATA[
arXiv:2507.19991v1 Announce Type: cross 
Abstract: We present a lightweight latent diffusion model for vocal-conditioned musical accompaniment generation that addresses critical limitations in existing music AI systems. Our approach introduces a novel soft alignment attention mechanism that adaptively combines local and global temporal dependencies based on diffusion timesteps, enabling efficient capture of multi-scale musical structure. Operating in the compressed latent space of a pre-trained variational autoencoder, the model achieves a 220 times parameter reduction compared to state-of-the-art systems while delivering 52 times faster inference. Experimental evaluation demonstrates competitive performance with only 15M parameters, outperforming OpenAI Jukebox in production quality and content unity while maintaining reasonable musical coherence. The ultra-lightweight architecture enables real-time deployment on consumer hardware, making AI-assisted music creation accessible for interactive applications and resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation</title>
<link>https://arxiv.org/abs/2507.20021</link>
<guid>https://arxiv.org/abs/2507.20021</guid>
<content:encoded><![CDATA[
arXiv:2507.20021v1 Announce Type: cross 
Abstract: Large language models (LLMs) are often credited with recent leaps in ObjectGoal Navigation, yet the extent to which they improve planning remains unclear. We revisit this question on the HM3D-v1 validation split. First, we strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary GLEE detector and Intuition saliency map, and replace them with a simple Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000 validation episodes, outperforming all previous training-free baselines. Second, we add a lightweight language prior (SHF); on a 200-episode subset this yields a further +2% Success and +0.9% SPL while shortening paths by five steps on average. Qualitative trajectories confirm the trend: InstructNav back-tracks and times-out, DWFE reaches the goal after a few islands, and SHF follows an almost straight route. Our results indicate that frontier geometry, not emergent LLM reasoning, drives most reported gains, and suggest that metric-aware prompts or offline semantic graphs are necessary before attributing navigation success to "LLM intelligence."
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Audio Classification by Transitioning from Zero- to Few-Shot</title>
<link>https://arxiv.org/abs/2507.20036</link>
<guid>https://arxiv.org/abs/2507.20036</guid>
<content:encoded><![CDATA[
arXiv:2507.20036v1 Announce Type: cross 
Abstract: State-of-the-art audio classification often employs a zero-shot approach, which involves comparing audio embeddings with embeddings from text describing the respective audio class. These embeddings are usually generated by neural networks trained through contrastive learning to align audio and text representations. Identifying the optimal text description for an audio class is challenging, particularly when the class comprises a wide variety of sounds. This paper examines few-shot methods designed to improve classification accuracy beyond the zero-shot approach. Specifically, audio embeddings are grouped by class and processed to replace the inherently noisy text embeddings. Our results demonstrate that few-shot classification typically outperforms the zero-shot baseline.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Deep Learning-based Respiratory Sound Analysis with Frequency Selection and Attention Mechanism</title>
<link>https://arxiv.org/abs/2507.20052</link>
<guid>https://arxiv.org/abs/2507.20052</guid>
<content:encoded><![CDATA[
arXiv:2507.20052v1 Announce Type: cross 
Abstract: Accurate classification of respiratory sounds requires deep learning models that effectively capture fine-grained acoustic features and long-range temporal dependencies. Convolutional Neural Networks (CNNs) are well-suited for extracting local time-frequency patterns but are limited in modeling global context. In contrast, transformer-based models can capture long-range dependencies, albeit with higher computational demands. To address these limitations, we propose a compact CNN-Temporal Self-Attention (CNN-TSA) network that integrates lightweight self-attention into an efficient CNN backbone. Central to our approach is a Frequency Band Selection (FBS) module that suppresses noisy and non-informative frequency regions, substantially improving accuracy and reducing FLOPs by up to 50%. We also introduce age-specific models to enhance robustness across diverse patient groups. Evaluated on the SPRSound-2022/2023 and ICBHI-2017 lung sound datasets, CNN-TSA with FBS sets new benchmarks on SPRSound and achieves state-of-the-art performance on ICBHI, all with a significantly smaller computational footprint. Furthermore, integrating FBS into an existing transformer baseline yields a new record on ICBHI, confirming FBS as an effective drop-in enhancement. These results demonstrate that our framework enables reliable, real-time respiratory sound analysis suitable for deployment in resource-constrained settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers</title>
<link>https://arxiv.org/abs/2507.20058</link>
<guid>https://arxiv.org/abs/2507.20058</guid>
<content:encoded><![CDATA[
arXiv:2507.20058v1 Announce Type: cross 
Abstract: Predicting Parkinson's Disease (PD) progression is crucial, and voice biomarkers offer a non-invasive method for tracking symptom severity (UPDRS scores) through telemonitoring. Analyzing this longitudinal data is challenging due to within-subject correlations and complex, nonlinear patient-specific progression patterns. This study benchmarks LMMs against two advanced hybrid approaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021), which embeds a neural network within a GLMM structure, and the Neural Mixed Effects (NME) model (Wortwein 2023), allowing nonlinear subject-specific parameters throughout the network. Using the Oxford Parkinson's telemonitoring voice dataset, we evaluate these models' performance in predicting Total UPDRS to offer practical guidance for PD research and clinical applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2507.20059</link>
<guid>https://arxiv.org/abs/2507.20059</guid>
<content:encoded><![CDATA[
arXiv:2507.20059v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v1 Announce Type: cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding</title>
<link>https://arxiv.org/abs/2507.20110</link>
<guid>https://arxiv.org/abs/2507.20110</guid>
<content:encoded><![CDATA[
arXiv:2507.20110v1 Announce Type: cross 
Abstract: Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering</title>
<link>https://arxiv.org/abs/2507.20133</link>
<guid>https://arxiv.org/abs/2507.20133</guid>
<content:encoded><![CDATA[
arXiv:2507.20133v1 Announce Type: cross 
Abstract: Generative AI can now synthesize strikingly realistic images from text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts that win higher preference scores can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine distance between the original prompt and winning candidate in embedding space, softly down-weighting training signals that would otherwise reward semantically mismatched prompts. We provide the first analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps learned prompts within a provably bounded neighborhood of the original text. On three standard text-to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented with semantic weighting should become the new standard for prompt-optimization studies and lay the groundwork for broader, semantics-aware preference optimization in language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20150</link>
<guid>https://arxiv.org/abs/2507.20150</guid>
<content:encoded><![CDATA[
arXiv:2507.20150v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Multi-Task Learning for Rare Conversions in Ad Tech</title>
<link>https://arxiv.org/abs/2507.20161</link>
<guid>https://arxiv.org/abs/2507.20161</guid>
<content:encoded><![CDATA[
arXiv:2507.20161v1 Announce Type: cross 
Abstract: We present a Multi-Task Learning (MTL) approach for improving predictions for rare (e.g., <1%) conversion events in online advertising. The conversions are classified into "rare" or "frequent" types based on historical statistics. The model learns shared representations across all signals while specializing through separate task towers for each type. The approach was tested and fully deployed to production, demonstrating consistent improvements in both offline (0.69% AUC lift) and online KPI performance metric (2% Cost per Action reduction).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis</title>
<link>https://arxiv.org/abs/2507.20189</link>
<guid>https://arxiv.org/abs/2507.20189</guid>
<content:encoded><![CDATA[
arXiv:2507.20189v1 Announce Type: cross 
Abstract: Methamphetamine dependence poses a significant global health challenge, yet its assessment and the evaluation of treatments like repetitive transcranial magnetic stimulation (rTMS) frequently depend on subjective self-reports, which may introduce uncertainties. While objective neuroimaging modalities such as electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) offer alternatives, their individual limitations and the reliance on conventional, often hand-crafted, feature extraction can compromise the reliability of derived biomarkers. To overcome these limitations, we propose NeuroCLIP, a novel deep learning framework integrating simultaneously recorded EEG and fNIRS data through a progressive learning strategy. This approach offers a robust and trustworthy biomarker for methamphetamine addiction. Validation experiments show that NeuroCLIP significantly improves discriminative capabilities among the methamphetamine-dependent individuals and healthy controls compared to models using either EEG or only fNIRS alone. Furthermore, the proposed framework facilitates objective, brain-based evaluation of rTMS treatment efficacy, demonstrating measurable shifts in neural patterns towards healthy control profiles after treatment. Critically, we establish the trustworthiness of the multimodal data-driven biomarker by showing its strong correlation with psychometrically validated craving scores. These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP offers enhanced robustness and reliability over single-modality approaches, providing a valuable tool for addiction neuroscience research and potentially improving clinical assessments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation</title>
<link>https://arxiv.org/abs/2507.20284</link>
<guid>https://arxiv.org/abs/2507.20284</guid>
<content:encoded><![CDATA[
arXiv:2507.20284v1 Announce Type: cross 
Abstract: As the use of artificial intelligence rapidly increases, the development of trustworthy artificial intelligence has become important. However, recent studies have shown that deep neural networks are susceptible to learn spurious correlations present in datasets. To improve the reliability, we propose a simple yet effective framework called controllable feature whitening. We quantify the linear correlation between the target and bias features by the covariance matrix, and eliminate it through the whitening module. Our results systemically demonstrate that removing the linear correlations between features fed into the last linear classifier significantly mitigates the bias, while avoiding the need to model intractable higher-order dependencies. A particular advantage of the proposed method is that it does not require regularization terms or adversarial learning, which often leads to unstable optimization in practice. Furthermore, we show that two fairness criteria, demographic parity and equalized odds, can be effectively handled by whitening with the re-weighted covariance matrix. Consequently, our method controls the trade-off between the utility and fairness of algorithms by adjusting the weighting coefficient. Finally, we validate that our method outperforms existing approaches on four benchmark datasets: Corrupted CIFAR-10, Biased FFHQ, WaterBirds, and Celeb-A.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalized Parameter Tuning in Coherent Ising Machines: A Portfolio-Based Approach</title>
<link>https://arxiv.org/abs/2507.20295</link>
<guid>https://arxiv.org/abs/2507.20295</guid>
<content:encoded><![CDATA[
arXiv:2507.20295v1 Announce Type: cross 
Abstract: Coherent Ising Machines (CIMs) have recently gained attention as a promising computing model for solving combinatorial optimization problems. In particular, the Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution quality, but its performance is highly sensitive to a large number of hyperparameters, making efficient tuning essential. In this study, we present an algorithm portfolio approach for hyperparameter tuning in CIMs employing Chaotic Amplitude Control with momentum (CACm) algorithm. Our method incorporates multiple search strategies, enabling flexible and effective adaptation to the characteristics of the hyperparameter space. Specifically, we propose two representative tuning methods, Method A and Method B. Method A optimizes each hyperparameter sequentially with a fixed total number of trials, while Method B prioritizes hyperparameters based on initial evaluations before applying Method A in order. Performance evaluations were conducted on the Supercomputer "Flow" at Nagoya University, using planted Wishart instances and Time to Solution (TTS) as the evaluation metric. Compared to the baseline performance with best-known hyperparameters, Method A achieved up to 1.47x improvement, and Method B achieved up to 1.65x improvement. These results demonstrate the effectiveness of the algorithm portfolio approach in enhancing the tuning process for CIMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies</title>
<link>https://arxiv.org/abs/2507.20312</link>
<guid>https://arxiv.org/abs/2507.20312</guid>
<content:encoded><![CDATA[
arXiv:2507.20312v1 Announce Type: cross 
Abstract: Scientific and data science applications are becoming increasingly complex, with growing computational and memory demands. Modern high performance computing (HPC) systems provide high parallelism and heterogeneity across nodes, devices, and cores. To achieve good performance, effective scheduling and load balancing techniques are essential. Parallel programming frameworks such as OpenMP now offer a variety of advanced scheduling algorithms to support diverse applications and platforms. This creates an instance of the scheduling algorithm selection problem, which involves identifying the most suitable algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling algorithms in OpenMP. We propose and evaluate expert-based and reinforcement learning (RL)-based methods, and conduct a detailed performance analysis across six applications and three systems. Our results show that RL methods are capable of learning high-performing scheduling decisions, although they require significant exploration, with the choice of reward function playing a key role. Expert-based methods, in contrast, rely on prior knowledge and involve less exploration, though they may not always identify the optimal algorithm for a specific application-system pair. By combining expert knowledge with RL-based learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling algorithms during execution is both viable and beneficial for OpenMP applications. The approach can also be extended to MPI-based programs, enabling optimization of scheduling decisions across multiple levels of parallelism.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Blessing and Curse of Dimensionality in Safety Alignment</title>
<link>https://arxiv.org/abs/2507.20333</link>
<guid>https://arxiv.org/abs/2507.20333</guid>
<content:encoded><![CDATA[
arXiv:2507.20333v1 Announce Type: cross 
Abstract: The focus on safety alignment in large language models (LLMs) has increased significantly due to their widespread adoption across different domains. The scale of LLMs play a contributing role in their success, and the growth in parameter count follows larger hidden dimensions. In this paper, we hypothesize that while the increase in dimensions has been a key advantage, it may lead to emergent problems as well. These problems emerge as the linear structures in the activation space can be exploited, in the form of activation engineering, to circumvent its safety alignment. Through detailed visualizations of linear subspaces associated with different concepts, such as safety, across various model scales, we show that the curse of high-dimensional representations uniquely impacts LLMs. Further substantiating our claim, we demonstrate that projecting the representations of the model onto a lower dimensional subspace can preserve sufficient information for alignment while avoiding those linear structures. Empirical results confirm that such dimensional reduction significantly reduces susceptibility to jailbreaking through representation engineering. Building on our empirical validations, we provide theoretical insights into these linear jailbreaking methods relative to a model's hidden dimensions. Broadly speaking, our work posits that the high dimensions of a model's internal representations can be both a blessing and a curse in safety alignment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of $\theta$-Expectations</title>
<link>https://arxiv.org/abs/2507.20353</link>
<guid>https://arxiv.org/abs/2507.20353</guid>
<content:encoded><![CDATA[
arXiv:2507.20353v1 Announce Type: cross 
Abstract: The canonical theory of stochastic calculus under ambiguity, founded on sub-additivity, is insensitive to non-convex uncertainty structures, leading to an identifiability impasse. This paper develops a mathematical framework for an identifiable calculus sensitive to non-convex geometry. We introduce the $\theta$-BSDE, a class of backward stochastic differential equations where the driver is determined by a pointwise maximization over a primitive, possibly non-convex, uncertainty set. The system's tractability is predicated not on convexity, but on a global analytic hypothesis: the existence of a unique and globally Lipschitz maximizer map for the driver function. Under this hypothesis, which carves out a tractable class of models, we establish well-posedness via a fixed-point argument. For a distinct, geometrically regular class of models, we prove a result of independent interest: under non-degeneracy conditions from Malliavin calculus, the maximizer is unique along any solution path, ensuring the model's internal consistency. We clarify the fundamental logical gap between this pathwise property and the global regularity required by our existence proof. The resulting valuation operator defines a dynamically consistent expectation, and we establish its connection to fully nonlinear PDEs via a Feynman-Kac formula.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20382</link>
<guid>https://arxiv.org/abs/2507.20382</guid>
<content:encoded><![CDATA[
arXiv:2507.20382v1 Announce Type: cross 
Abstract: Loco-manipulation of quadrupedal robots has broadened robotic applications, but using legs as manipulators often compromises locomotion, while mounting arms complicates the system. To mitigate this issue, we introduce bipedalism for quadrupedal robots, thus freeing the front legs for versatile interactions with the environment. We propose a risk-adaptive distributional Reinforcement Learning (RL) framework designed for quadrupedal robots walking on their hind legs, balancing worst-case conservativeness with optimal performance in this inherently unstable task. During training, the adaptive risk preference is dynamically adjusted based on the uncertainty of the return, measured by the coefficient of variation of the estimated return distribution. Extensive experiments in simulation show our method's superior performance over baselines. Real-world deployment on a Unitree Go2 robot further demonstrates the versatility of our policy, enabling tasks like cart pushing, obstacle probing, and payload transport, while showcasing robustness against challenging dynamics and external disturbances.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General Framework for Estimating Preferences Using Response Time Data</title>
<link>https://arxiv.org/abs/2507.20403</link>
<guid>https://arxiv.org/abs/2507.20403</guid>
<content:encoded><![CDATA[
arXiv:2507.20403v1 Announce Type: cross 
Abstract: We propose a general methodology for recovering preference parameters from data on choices and response times. Our methods yield estimates with fast ($1/n$ for $n$ data points) convergence rates when specialized to the popular Drift Diffusion Model (DDM), but are broadly applicable to generalizations of the DDM as well as to alternative models of decision making that make use of response time data. The paper develops an empirical application to an experiment on intertemporal choice, showing that the use of response times delivers predictive accuracy and matters for the estimation of economically relevant parameters.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?</title>
<link>https://arxiv.org/abs/2507.20419</link>
<guid>https://arxiv.org/abs/2507.20419</guid>
<content:encoded><![CDATA[
arXiv:2507.20419v1 Announce Type: cross 
Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis</title>
<link>https://arxiv.org/abs/2507.20454</link>
<guid>https://arxiv.org/abs/2507.20454</guid>
<content:encoded><![CDATA[
arXiv:2507.20454v1 Announce Type: cross 
Abstract: Visual autoregressive modeling, based on the next-scale prediction paradigm, exhibits notable advantages in image quality and model scalability over traditional autoregressive and diffusion models. It generates images by progressively refining resolution across multiple stages. However, the computational overhead in high-resolution stages remains a critical challenge due to the substantial number of tokens involved. In this paper, we introduce SparseVAR, a plug-and-play acceleration framework for next-scale prediction that dynamically excludes low-frequency tokens during inference without requiring additional training. Our approach is motivated by the observation that tokens in low-frequency regions have a negligible impact on image quality in high-resolution stages and exhibit strong similarity with neighboring tokens. Additionally, we observe that different blocks in the next-scale prediction model focus on distinct regions, with some concentrating on high-frequency areas. SparseVAR leverages these insights by employing lightweight MSE-based metrics to identify low-frequency tokens while preserving the fidelity of excluded regions through a small set of uniformly sampled anchor tokens. By significantly reducing the computational cost while maintaining high image generation quality, SparseVAR achieves notable acceleration in both HART and Infinity. Specifically, SparseVAR achieves up to a 2 times speedup with minimal quality degradation in Infinity-2B.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator Inference Aware Quadratic Manifolds with Isotropic Reduced Coordinates for Nonintrusive Model Reduction</title>
<link>https://arxiv.org/abs/2507.20463</link>
<guid>https://arxiv.org/abs/2507.20463</guid>
<content:encoded><![CDATA[
arXiv:2507.20463v1 Announce Type: cross 
Abstract: Quadratic manifolds for nonintrusive reduced modeling are typically trained to minimize the reconstruction error on snapshot data, which means that the error of models fitted to the embedded data in downstream learning steps is ignored. In contrast, we propose a greedy training procedure that takes into account both the reconstruction error on the snapshot data and the prediction error of reduced models fitted to the data. Because our procedure learns quadratic manifolds with the objective of achieving accurate reduced models, it avoids oscillatory and other non-smooth embeddings that can hinder learning accurate reduced models. Numerical experiments on transport and turbulent flow problems show that quadratic manifolds trained with the proposed greedy approach lead to reduced models with up to two orders of magnitude higher accuracy than quadratic manifolds trained with respect to the reconstruction error alone.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building crypto portfolios with agentic AI</title>
<link>https://arxiv.org/abs/2507.20468</link>
<guid>https://arxiv.org/abs/2507.20468</guid>
<content:encoded><![CDATA[
arXiv:2507.20468v1 Announce Type: cross 
Abstract: The rapid growth of crypto markets has opened new opportunities for investors, but at the same time exposed them to high volatility. To address the challenge of managing dynamic portfolios in such an environment, this paper presents a practical application of a multi-agent system designed to autonomously construct and evaluate crypto-asset allocations. Using data on daily frequencies of the ten most capitalized cryptocurrencies from 2020 to 2025, we compare two automated investment strategies. These are a static equal weighting strategy and a rolling-window optimization strategy, both implemented to maximize the evaluation metrics of the Modern Portfolio Theory (MPT), such as Expected Return, Sharpe and Sortino ratios, while minimizing volatility. Each step of the process is handled by dedicated agents, integrated through a collaborative architecture in Crew AI. The results show that the dynamic optimization strategy achieves significantly better performance in terms of risk-adjusted returns, both in-sample and out-of-sample. This highlights the benefits of adaptive techniques in portfolio management, particularly in volatile markets such as cryptocurrency markets. The following methodology proposed also demonstrates how multi-agent systems can provide scalable, auditable, and flexible solutions in financial automation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading</title>
<link>https://arxiv.org/abs/2507.20474</link>
<guid>https://arxiv.org/abs/2507.20474</guid>
<content:encoded><![CDATA[
arXiv:2507.20474v1 Announce Type: cross 
Abstract: Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reputation Scoring in DeFi: zScore-Based Wallet Ranking from Liquidity and Trading Signals</title>
<link>https://arxiv.org/abs/2507.20494</link>
<guid>https://arxiv.org/abs/2507.20494</guid>
<content:encoded><![CDATA[
arXiv:2507.20494v1 Announce Type: cross 
Abstract: As decentralized finance (DeFi) evolves, distinguishing between user behaviors - liquidity provision versus active trading - has become vital for risk modeling and on-chain reputation. We propose a behavioral scoring framework for Uniswap that assigns two complementary scores: a Liquidity Provision Score that assesses strategic liquidity contributions, and a Swap Behavior Score that reflects trading intent, volatility exposure, and discipline. The scores are constructed using rule-based blueprints that decompose behavior into volume, frequency, holding time, and withdrawal patterns. To handle edge cases and learn feature interactions, we introduce a deep residual neural network with densely connected skip blocks inspired by the U-Net architecture. We also incorporate pool-level context such as total value locked (TVL), fee tiers, and pool size, allowing the system to differentiate similar user behaviors across pools with varying characteristics. Our framework enables context-aware and scalable DeFi user scoring, supporting improved risk assessment and incentive design. Experiments on Uniswap v3 data show its usefulness for user segmentation and protocol-aligned reputation systems. Although we refer to our metric as zScore, it is independently developed and methodologically different from the cross-protocol system proposed by Udupi et al. Our focus is on role-specific behavioral modeling within Uniswap using blueprint logic and supervised learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
arXiv:2507.20520v1 Announce Type: cross 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Differentially Private Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2507.20560</link>
<guid>https://arxiv.org/abs/2507.20560</guid>
<content:encoded><![CDATA[
arXiv:2507.20560v1 Announce Type: cross 
Abstract: Privacy preservation in machine learning, particularly through Differentially Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data analysis. However, existing statistical inference methods for SGD predominantly focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This paper first bridges this gap by establishing the asymptotic properties of SGD under the randomized rule and extending these results to DP-SGD. For the output of DP-SGD, we show that the asymptotic variance decomposes into statistical, sampling, and privacy-induced components. Two methods are proposed for constructing valid confidence intervals: the plug-in method and the random scaling method. We also perform extensive numerical analysis, which shows that the proposed confidence intervals achieve nominal coverage rates while maintaining privacy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A note on the Artstein-Avidan-Milman's generalized Legendre transforms</title>
<link>https://arxiv.org/abs/2507.20577</link>
<guid>https://arxiv.org/abs/2507.20577</guid>
<content:encoded><![CDATA[
arXiv:2507.20577v1 Announce Type: cross 
Abstract: Artstein-Avidan and Milman [Annals of mathematics (2009), (169):661-674] characterized invertible reverse-ordering transforms on the space of lower-semi-continuous extended real-valued convex functions as affine deformations of the ordinary Legendre transform. In this note, we prove that all those generalized Legendre transforms on functions correspond to the ordinary Legendre transform on dually corresponding affine-deformed functions. That is, generalized convex conjugates are convex conjugates of affine-deformed functions. We conclude this note by sketching how this result can be interpreted from the lens of information geometry.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing and Scaling fMRI Features for Brain-Behavior Prediction</title>
<link>https://arxiv.org/abs/2507.20601</link>
<guid>https://arxiv.org/abs/2507.20601</guid>
<content:encoded><![CDATA[
arXiv:2507.20601v1 Announce Type: cross 
Abstract: Predicting behavioral variables from neuroimaging modalities such as magnetic resonance imaging (MRI) has the potential to allow the development of neuroimaging biomarkers of mental and neurological disorders. A crucial processing step to this aim is the extraction of suitable features. These can differ in how well they predict the target of interest, and how this prediction scales with sample size and scan time. Here, we compare nine feature subtypes extracted from resting-state functional MRI recordings for behavior prediction, ranging from regional measures of functional activity to functional connectivity (FC) and metrics derived with graph signal processing (GSP), a principled approach for the extraction of structure-informed functional features. We study 979 subjects from the Human Connectome Project Young Adult dataset, predicting summary scores for mental health, cognition, processing speed, and substance use, as well as age and sex. The scaling properties of the features are investigated for different combinations of sample size and scan time. FC comes out as the best feature for predicting cognition, age, and sex. Graph power spectral density is the second best for predicting cognition and age, while for sex, variability-based features show potential as well. When predicting sex, the low-pass graph filtered coupled FC slightly outperforms the simple FC variant. None of the other targets were predicted significantly. The scaling results point to higher performance reserves for the better-performing features. They also indicate that it is important to balance sample size and scan time when acquiring data for prediction studies. The results confirm FC as a robust feature for behavior prediction, but also show the potential of GSP and variability-based measures. We discuss the implications for future prediction studies in terms of strategies for acquisition and sample composition.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression</title>
<link>https://arxiv.org/abs/2507.20613</link>
<guid>https://arxiv.org/abs/2507.20613</guid>
<content:encoded><![CDATA[
arXiv:2507.20613v1 Announce Type: cross 
Abstract: Large multimodal models (LMMs) have advanced significantly by integrating visual encoders with extensive language models, enabling robust reasoning capabilities. However, compressing LMMs for deployment on edge devices remains a critical challenge. In this work, we propose an adaptive search algorithm that optimizes sparsity and KV cache compression to enhance LMM efficiency. Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers, using model performance as the optimization objective. This approach uniquely combines pruning with key-value cache quantization and incorporates a fast pruning technique that eliminates the need for additional fine-tuning or weight adjustments, achieving efficient compression without compromising accuracy. Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and 13B, demonstrate our method superiority over state-of-the-art techniques such as SparseGPT and Wanda across various compression levels. Notably, our framework automatic allocation of KV cache compression resources sets a new standard in LMM optimization, delivering memory efficiency without sacrificing much performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards trustworthy AI in materials mechanics through domain-guided attention</title>
<link>https://arxiv.org/abs/2507.20658</link>
<guid>https://arxiv.org/abs/2507.20658</guid>
<content:encoded><![CDATA[
arXiv:2507.20658v1 Announce Type: cross 
Abstract: Ensuring the trustworthiness and robustness of deep learning models remains a fundamental challenge, particularly in high-stakes scientific applications. In this study, we present a framework called attention-guided training that combines explainable artificial intelligence techniques with quantitative evaluation and domain-specific priors to guide model attention. We demonstrate that domain specific feedback on model explanations during training can enhance the model's generalization capabilities. We validate our approach on the task of semantic crack tip segmentation in digital image correlation data which is a key application in the fracture mechanical characterization of materials. By aligning model attention with physically meaningful stress fields, such as those described by Williams' analytical solution, attention-guided training ensures that the model focuses on physically relevant regions. This finally leads to improved generalization and more faithful explanations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection</title>
<link>https://arxiv.org/abs/2507.20666</link>
<guid>https://arxiv.org/abs/2507.20666</guid>
<content:encoded><![CDATA[
arXiv:2507.20666v1 Announce Type: cross 
Abstract: This paper proposes a method for generating machine-type-specific anomalies to evaluate the relative performance of unsupervised anomalous sound detection (UASD) systems across different machine types, even in the absence of real anomaly sound data. Conventional keyword-based data augmentation methods often produce unrealistic sounds due to their reliance on manually defined labels, limiting scalability as machine types and anomaly patterns diversify. Advanced audio generative models, such as MIMII-Gen, show promise but typically depend on anomalous training data, making them less effective when diverse anomalous examples are unavailable. To address these limitations, we propose a novel synthesis approach leveraging large language models (LLMs) to interpret textual descriptions of faults and automatically select audio transformation functions, converting normal machine sounds into diverse and plausible anomalous sounds. We validate this approach by evaluating a UASD system trained only on normal sounds from five machine types, using both real and synthetic anomaly data. Experimental results reveal consistent trends in relative detection difficulty across machine types between synthetic and real anomalies. This finding supports our hypothesis and highlights the effectiveness of the proposed LLM-based synthesis approach for relative evaluation of UASD systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games</title>
<link>https://arxiv.org/abs/2507.20670</link>
<guid>https://arxiv.org/abs/2507.20670</guid>
<content:encoded><![CDATA[
arXiv:2507.20670v1 Announce Type: cross 
Abstract: Understanding and predicting player movement in multiplayer games is crucial for achieving use cases such as player-mimicking bot navigation, preemptive bot control, strategy recommendation, and real-time player behavior analytics. However, the complex environments allow for a high degree of navigational freedom, and the interactions and team-play between players require models that make effective use of the available heterogeneous input data. This paper presents a multimodal architecture for predicting future player locations on a dynamic time horizon, using a U-Net-based approach for calculating endpoint location probability heatmaps, conditioned using a multimodal feature encoder. The application of a multi-head attention mechanism for different groups of features allows for communication between agents. In doing so, the architecture makes efficient use of the multimodal game state including image inputs, numerical and categorical features, as well as dynamic game data. Consequently, the presented technique lays the foundation for various downstream tasks that rely on future player positions such as the creation of player-predictive bot behavior or player anomaly detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Value Systems of Societies from Preferences</title>
<link>https://arxiv.org/abs/2507.20728</link>
<guid>https://arxiv.org/abs/2507.20728</guid>
<content:encoded><![CDATA[
arXiv:2507.20728v1 Announce Type: cross 
Abstract: Aligning AI systems with human values and the value-based preferences of various stakeholders (their value systems) is key in ethical AI. In value-aware AI systems, decision-making draws upon explicit computational representations of individual values (groundings) and their aggregation into value systems. As these are notoriously difficult to elicit and calibrate manually, value learning approaches aim to automatically derive computational models of an agent's values and value system from demonstrations of human behaviour. Nonetheless, social science and humanities literature suggest that it is more adequate to conceive the value system of a society as a set of value systems of different groups, rather than as the simple aggregation of individual value systems. Accordingly, here we formalize the problem of learning the value systems of societies and propose a method to address it based on heuristic deep clustering. The method learns socially shared value groundings and a set of diverse value systems representing a given society by observing qualitative value-based preferences from a sample of agents. We evaluate the proposal in a use case with real data about travelling decisions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Self-Taught Faithfulness Evaluators</title>
<link>https://arxiv.org/abs/2507.20752</link>
<guid>https://arxiv.org/abs/2507.20752</guid>
<content:encoded><![CDATA[
arXiv:2507.20752v1 Announce Type: cross 
Abstract: The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank</title>
<link>https://arxiv.org/abs/2507.20753</link>
<guid>https://arxiv.org/abs/2507.20753</guid>
<content:encoded><![CDATA[
arXiv:2507.20753v1 Announce Type: cross 
Abstract: In e-commerce recommender and search systems, tree-based models, such as LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks. Despite their effectiveness and widespread adoption in industry, the debate continues whether deep neural networks (DNNs) can outperform traditional tree-based models in this domain. To contribute to this discussion, we systematically benchmark DNNs against our production-grade LambdaMART model. We evaluate multiple DNN architectures and loss functions on a proprietary dataset from OTTO and validate our findings through an 8-week online A/B test. The results show that a simple DNN architecture outperforms a strong tree-based baseline in terms of total clicks and revenue, while achieving parity in total units sold.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach</title>
<link>https://arxiv.org/abs/2507.20796</link>
<guid>https://arxiv.org/abs/2507.20796</guid>
<content:encoded><![CDATA[
arXiv:2507.20796v1 Announce Type: cross 
Abstract: Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Bias in Perceiving Dimensionality Reduction Projections</title>
<link>https://arxiv.org/abs/2507.20805</link>
<guid>https://arxiv.org/abs/2507.20805</guid>
<content:encoded><![CDATA[
arXiv:2507.20805v1 Announce Type: cross 
Abstract: Selecting the dimensionality reduction technique that faithfully represents the structure is essential for reliable visual communication and analytics. In reality, however, practitioners favor projections for other attractions, such as aesthetics and visual saliency, over the projection's structural faithfulness, a bias we define as visual interestingness. In this research, we conduct a user study that (1) verifies the existence of such bias and (2) explains why the bias exists. Our study suggests that visual interestingness biases practitioners' preferences when selecting projections for analysis, and this bias intensifies with color-encoded labels and shorter exposure time. Based on our findings, we discuss strategies to mitigate bias in perceiving and interpreting DR projections.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Flow Matching is Particle Swarm Optimization?</title>
<link>https://arxiv.org/abs/2507.20810</link>
<guid>https://arxiv.org/abs/2507.20810</guid>
<content:encoded><![CDATA[
arXiv:2507.20810v1 Announce Type: cross 
Abstract: This paper preliminarily investigates the duality between flow matching in generative models and particle swarm optimization (PSO) in evolutionary computation. Through theoretical analysis, we reveal the intrinsic connections between these two approaches in terms of their mathematical formulations and optimization mechanisms: the vector field learning in flow matching shares similar mathematical expressions with the velocity update rules in PSO; both methods follow the fundamental framework of progressive evolution from initial to target distributions; and both can be formulated as dynamical systems governed by ordinary differential equations. Our study demonstrates that flow matching can be viewed as a continuous generalization of PSO, while PSO provides a discrete implementation of swarm intelligence principles. This duality understanding establishes a theoretical foundation for developing novel hybrid algorithms and creates a unified framework for analyzing both methods. Although this paper only presents preliminary discussions, the revealed correspondences suggest several promising research directions, including improving swarm intelligence algorithms based on flow matching principles and enhancing generative models using swarm intelligence concepts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View</title>
<link>https://arxiv.org/abs/2507.20871</link>
<guid>https://arxiv.org/abs/2507.20871</guid>
<content:encoded><![CDATA[
arXiv:2507.20871v1 Announce Type: cross 
Abstract: Native AI support is a key objective in the evolution of 6G networks, with Federated Learning (FL) emerging as a promising paradigm. FL allows decentralized clients to collaboratively train an AI model without directly sharing their data, preserving privacy. Clients train local models on private data and share model updates, which a central server aggregates to refine the global model and redistribute it for the next iteration. However, client data heterogeneity slows convergence and reduces model accuracy, and frequent client participation imposes communication and computational burdens. To address these challenges, we propose \textit{FedABC}, an innovative client selection algorithm designed to take a long-term view in managing data heterogeneity and optimizing client participation. Inspired by attention mechanisms, \textit{FedABC} prioritizes informative clients by evaluating both model similarity and each model's unique contributions to the global model. Moreover, considering the evolving demands of the global model, we formulate an optimization problem to guide \textit{FedABC} throughout the training process. Following the ``later-is-better" principle, \textit{FedABC} adaptively adjusts the client selection threshold, encouraging greater participation in later training stages. Extensive simulations on CIFAR-10 demonstrate that \textit{FedABC} significantly outperforms existing approaches in model accuracy and client participation efficiency, achieving comparable performance with 32\% fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher accuracy with 2\% fewer clients than the state-of-the-art. This work marks a step toward deploying FL in heterogeneous, resource-constrained environments, thereby supporting native AI capabilities in 6G networks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2507.20872</link>
<guid>https://arxiv.org/abs/2507.20872</guid>
<content:encoded><![CDATA[
arXiv:2507.20872v1 Announce Type: cross 
Abstract: Alzheimer's disease affects over 55 million people worldwide and is projected to more than double by 2050, necessitating rapid, accurate, and scalable diagnostics. However, existing approaches are limited because they cannot achieve clinically acceptable accuracy, generalization across datasets, robustness to missing modalities, and explainability all at the same time. This inability to satisfy all these requirements simultaneously undermines their reliability in clinical settings. We propose OmniBrain, a multimodal framework that integrates brain MRI, radiomics, gene expression, and clinical data using a unified model with cross-attention and modality dropout. OmniBrain achieves $92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior multimodal approaches. Explainability analyses highlight neuropathologically relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a robust, interpretable, and practical solution for real-world Alzheimer's diagnosis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testbed and Software Architecture for Enhancing Security in Industrial Private 5G Networks</title>
<link>https://arxiv.org/abs/2507.20873</link>
<guid>https://arxiv.org/abs/2507.20873</guid>
<content:encoded><![CDATA[
arXiv:2507.20873v1 Announce Type: cross 
Abstract: In the era of Industry 4.0, the growing need for secure and efficient communication systems has driven the development of fifth-generation (5G) networks characterized by extremely low latency, massive device connectivity and high data transfer speeds. However, the deployment of 5G networks presents significant security challenges, requiring advanced and robust solutions to counter increasingly sophisticated cyber threats. This paper proposes a testbed and software architecture to strengthen the security of Private 5G Networks, particularly in industrial communication environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2507.20930</link>
<guid>https://arxiv.org/abs/2507.20930</guid>
<content:encoded><![CDATA[
arXiv:2507.20930v1 Announce Type: cross 
Abstract: Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate Conformal Prediction via Conformalized Gaussian Scoring</title>
<link>https://arxiv.org/abs/2507.20941</link>
<guid>https://arxiv.org/abs/2507.20941</guid>
<content:encoded><![CDATA[
arXiv:2507.20941v1 Announce Type: cross 
Abstract: While achieving exact conditional coverage in conformal prediction is unattainable without making strong, untestable regularity assumptions, the promise of conformal prediction hinges on finding approximations to conditional guarantees that are realizable in practice. A promising direction for obtaining conditional dependence for conformal sets--in particular capturing heteroskedasticity--is through estimating the conditional density $\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this vein has focused on nonconformity scores based on the empirical cumulative distribution function (CDF). Such scores are, however, computationally costly, typically requiring expensive sampling methods. To avoid the need for sampling, we observe that the CDF-based score reduces to a Mahalanobis distance in the case of Gaussian scores, yielding a closed-form expression that can be directly conformalized. Moreover, the use of a Gaussian-based score opens the door to a number of extensions of the basic conformal method; in particular, we show how to construct conformal sets with missing output values, refine conformal sets as partial information about $Y$ becomes available, and construct conformal sets on transformations of the output space. Finally, empirical results indicate that our approach produces conformal sets that more closely approximate conditional coverage in multivariate settings compared to alternative methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Langevin Diffusions with Density-dependent Temperature</title>
<link>https://arxiv.org/abs/2507.20958</link>
<guid>https://arxiv.org/abs/2507.20958</guid>
<content:encoded><![CDATA[
arXiv:2507.20958v1 Announce Type: cross 
Abstract: In the context of non-convex optimization, we let the temperature of a Langevin diffusion to depend on the diffusion's own density function. The rationale is that the induced density reveals to some extent the landscape imposed by the non-convex function to be minimized, such that a density-dependent temperature can provide location-wise random perturbation that may better react to, for instance, the location and depth of local minimizers. As the Langevin dynamics is now self-regulated by its own density, it forms a mean-field stochastic differential equation (SDE) of the Nemytskii type, distinct from the standard McKean-Vlasov equations. Relying on Wasserstein subdifferential calculus, we first show that the corresponding (nonlinear) Fokker-Planck equation has a unique solution. Next, a weak solution to the SDE is constructed from the solution to the Fokker-Planck equation, by Trevisan's superposition principle. As time goes to infinity, we further show that the density induced by the SDE converges to an invariant distribution, which admits an explicit formula in terms of the Lambert $W$ function.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Safety Values for Provably Corrigible Agents</title>
<link>https://arxiv.org/abs/2507.20964</link>
<guid>https://arxiv.org/abs/2507.20964</guid>
<content:encoded><![CDATA[
arXiv:2507.20964v1 Announce Type: cross 
Abstract: We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \emph{learned} to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating \emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island'' where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for today's LLM assistants and future autonomous systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL</title>
<link>https://arxiv.org/abs/2507.20966</link>
<guid>https://arxiv.org/abs/2507.20966</guid>
<content:encoded><![CDATA[
arXiv:2507.20966v1 Announce Type: cross 
Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user mobility necessitates updating the set of serving access points to maintain the user-centric clustering. Such updates are typically performed through handoff (HO) operations; however, frequent HOs lead to overheads associated with the allocation and release of resources. This paper presents a deep reinforcement learning (DRL)-based solution to predict and manage these connections for mobile users. Our solution employs the Soft Actor-Critic algorithm, with continuous action space representation, to train a deep neural network to serve as the HO policy. We present a novel proposition for a reward function that integrates a HO penalty in order to balance the attainable rate and the associated overhead related to HOs. We develop two variants of our system; the first one uses mobility direction-assisted (DA) observations that are based on the user movement pattern, while the second one uses history-assisted (HA) observations that are based on the history of the large-scale fading (LSF). Simulation results show that our DRL-based continuous action space approach is more scalable than discrete space counterpart, and that our derived HO policy automatically learns to gather HOs in specific time slots to minimize the overhead of initiating HOs. Our solution can also operate in real time with a response time less than 0.4 ms.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Adaptive Conformal Inference for Operator Models</title>
<link>https://arxiv.org/abs/2507.20975</link>
<guid>https://arxiv.org/abs/2507.20975</guid>
<content:encoded><![CDATA[
arXiv:2507.20975v1 Announce Type: cross 
Abstract: Operator models are regression algorithms for functional data and have become a key tool for emulating large-scale dynamical systems. Recent advances in deep neural operators have dramatically improved the accuracy and scalability of operator modeling, but lack an inherent notion of predictive uncertainty. We introduce Local Spectral Conformal Inference (LSCI), a new framework for locally adaptive, distribution-free uncertainty quantification for neural operator models. LSCI uses projection-based depth scoring and localized conformal inference to generate function-valued prediction sets with statistical guarantees. We prove approximate finite-sample marginal coverage under local exchangeability, and demonstrate significant gains in adaptivity and coverage across synthetic and real-world operator learning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs</title>
<link>https://arxiv.org/abs/2507.20977</link>
<guid>https://arxiv.org/abs/2507.20977</guid>
<content:encoded><![CDATA[
arXiv:2507.20977v1 Announce Type: cross 
Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of program repair. Recent studies show that large language models (LLMs) outperform traditional techniques, extending their success beyond code generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands" such as training-data leakage or perfect fault localization -- that let an LLM reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by deliberately adding errors to the reported vulnerability location in the prompt. If LLMs merely regurgitate memorized fixes, both small and large localization errors should yield the same number of correct patches, because any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans benchmarks after shifting the fault location by n lines from the ground truth. A first LLM generates a patch, a second LLM reviews it, and we validate the result with regression and proof-of-vulnerability tests. Finally, we manually audit a sample of patches and estimate the error rate with the Agresti-Coull-Wilson method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</title>
<link>https://arxiv.org/abs/2507.21018</link>
<guid>https://arxiv.org/abs/2507.21018</guid>
<content:encoded><![CDATA[
arXiv:2507.21018v1 Announce Type: cross 
Abstract: Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis</title>
<link>https://arxiv.org/abs/2507.21035</link>
<guid>https://arxiv.org/abs/2507.21035</guid>
<content:encoded><![CDATA[
arXiv:2507.21035v1 Announce Type: cross 
Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2305.15612</link>
<guid>https://arxiv.org/abs/2305.15612</guid>
<content:encoded><![CDATA[
arXiv:2305.15612v5 Announce Type: replace 
Abstract: Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of efficiently finding a global optimum of an expensive-to-evaluate black-box function. In general, a probabilistic regression model is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based methods, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, supervised classifiers are employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy are prone to be overconfident for known knowledge on global solution candidates. Supposing that we have access to unlabeled points, e.g., predefined fixed-size pools, we propose density ratio estimation-based Bayesian optimization with semi-supervised learning to solve this challenge. Finally, we show the empirical results of our methods and several baseline methods in two distinct scenarios with unlabeled point sampling and a fixed-size pool, and analyze the validity of our methods in diverse experiments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization</title>
<link>https://arxiv.org/abs/2309.10370</link>
<guid>https://arxiv.org/abs/2309.10370</guid>
<content:encoded><![CDATA[
arXiv:2309.10370v3 Announce Type: replace 
Abstract: In this paper, we approach the problem of cost (loss) minimization in underparametrized shallow ReLU networks through the explicit construction of upper bounds which appeal to the structure of classification data, without use of gradient descent. A key focus is on elucidating the geometric structure of approximate and precise minimizers. We consider an $\mathcal{L}^2$ cost function, input space $\mathbb{R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size that can be arbitrarily large. We prove an upper bound on the minimum of the cost function of order $O(\delta_P)$ where $\delta_P$ measures the signal-to-noise ratio of training data. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function, and show that the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes a particular $Q$-dimensional subspace in the input space ${\mathbb R}^M$. We comment on the characterization of the global minimum of the cost function in the given context.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task</title>
<link>https://arxiv.org/abs/2310.09336</link>
<guid>https://arxiv.org/abs/2310.09336</guid>
<content:encoded><![CDATA[
arXiv:2310.09336v5 Announce Type: replace 
Abstract: Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden "emergence" due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding capabilities and compositionality in generative models from a data-centric perspective.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints</title>
<link>https://arxiv.org/abs/2311.13349</link>
<guid>https://arxiv.org/abs/2311.13349</guid>
<content:encoded><![CDATA[
arXiv:2311.13349v3 Announce Type: replace 
Abstract: Deep learning models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models that are not capable to adapt at runtime. In this work, we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS leverages structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieves computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) taking advantage of data cache by re-arranging the order of operations in REDS computational graph. REDS supports conventional deep networks frequently deployed on the edge and provides computational benefits even for small and simple networks. We evaluate REDS on eight benchmark architectures trained on the Visual Wake Words, Google Speech Commands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four off-the-shelf mobile and embedded hardware platforms. We provide a theoretical result and empirical evidence demonstrating REDS' outstanding performance in terms of submodels' test set accuracy, and demonstrate an adaptation time in response to dynamic resource constraints of under 40$\mu$s, utilizing a fully-connected network on Arduino Nano 33 BLE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks</title>
<link>https://arxiv.org/abs/2311.18718</link>
<guid>https://arxiv.org/abs/2311.18718</guid>
<content:encoded><![CDATA[
arXiv:2311.18718v4 Announce Type: replace 
Abstract: Deep learning succeeds by doing hierarchical feature learning, yet tuning hyper-parameters (HP) such as initialization scales, learning rates etc., only give indirect control over this behavior. In this paper, we introduce a key notion to predict and control feature learning: the angle $\theta_\ell$ between the feature updates and the backward pass (at layer index $\ell$). We show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple and general \emph{feature speed formula} in terms of this angle $\theta_\ell$, the loss decay, and the magnitude of the backward pass. This angle $\theta_\ell$ is controlled by the conditioning of the layer-to-layer Jacobians and at random initialization, it is determined by the spectrum of a certain kernel, which coincides with the Neural Tangent Kernel when $\ell=\text{depth}$. Given $\theta_\ell$, the feature speed formula provides us with rules to adjust HPs (scales and learning rates) so as to satisfy certain dynamical properties, such as feature learning and loss decay. We investigate the implications of our approach for ReLU MLPs and ResNets in the large width-then-depth limit. Relying on prior work, we show that in ReLU MLPs with iid initialization, the angle degenerates with depth as $\cos(\theta_\ell)=\Theta(1/\sqrt{\ell})$. In contrast, ResNets with branch scale $O(1/\sqrt{\text{depth}})$ maintain a non-degenerate angle $\cos(\theta_\ell)=\Theta(1)$. We use these insights to recover key properties of known HP scalings and also to introduce a new HP scaling for large depth ReLU MLPs with favorable theoretical properties.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark</title>
<link>https://arxiv.org/abs/2312.09857</link>
<guid>https://arxiv.org/abs/2312.09857</guid>
<content:encoded><![CDATA[
arXiv:2312.09857v3 Announce Type: replace 
Abstract: Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to train models for unlabeled target data. Despite extensive research in domains like computer vision and natural language processing, UDA remains underexplored for time series data, which has widespread real-world applications ranging from medicine and manufacturing to earth observation and human activity recognition. Our paper addresses this gap by introducing a comprehensive benchmark for evaluating UDA techniques for time series classification, with a focus on deep learning methods. We provide seven new benchmark datasets covering various domain shifts and temporal dynamics, facilitating fair and standardized UDA method assessments with state of the art neural network backbones (e.g. Inception) for time series data. This benchmark offers insights into the strengths and limitations of the evaluated approaches while preserving the unsupervised nature of domain adaptation, making it directly applicable to practical problems. Our paper serves as a vital resource for researchers and practitioners, advancing domain adaptation solutions for time series data and fostering innovation in this critical field. The implementation code of this benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Data Poisoning on Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2402.08290</link>
<guid>https://arxiv.org/abs/2402.08290</guid>
<content:encoded><![CDATA[
arXiv:2402.08290v4 Announce Type: replace 
Abstract: Counterfactual explanations are a widely used approach for examining the predictions of black-box systems. They can offer the opportunity for computational recourse by suggesting actionable changes on how to alter the input to obtain a different (i.e., more favorable) system output. However, recent studies have pointed out their susceptibility to various forms of manipulation.
  This work studies the vulnerability of counterfactual explanations to data poisoning. We formally introduce and investigate data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, a sub-group of instances, or globally for all instances. In this context, we formally introduce and characterize data poisonings, from which we derive and investigate a general data poisoning mechanism. We demonstrate the impact of such data poisoning in the critical real-world application of explaining event detections in water distribution networks. Additionally, we conduct an extensive empirical evaluation, demonstrating that state-of-the-art counterfactual generation methods and toolboxes are vulnerable to such data poisoning. Furthermore, we find that existing defense methods fail to detect those poisonous samples.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the rates of convergence for learning with convolutional neural networks</title>
<link>https://arxiv.org/abs/2403.16459</link>
<guid>https://arxiv.org/abs/2403.16459</guid>
<content:encoded><![CDATA[
arXiv:2403.16459v3 Announce Type: replace 
Abstract: We study approximation and learning capacities of convolutional neural networks (CNNs) with one-side zero-padding and multiple channels. Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives new analysis on the covering number of feed-forward neural networks with CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than the existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates for classification are minimax optimal in some common settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of Global Feature Effect Explanations</title>
<link>https://arxiv.org/abs/2406.09069</link>
<guid>https://arxiv.org/abs/2406.09069</guid>
<content:encoded><![CDATA[
arXiv:2406.09069v2 Announce Type: replace 
Abstract: We study the robustness of global post-hoc explanations for predictive models trained on tabular data. Effects of predictor features in black-box supervised learning are an essential diagnostic tool for model debugging and scientific discovery in applied sciences. However, how vulnerable they are to data and model perturbations remains an open research question. We introduce several theoretical bounds for evaluating the robustness of partial dependence plots and accumulated local effects. Our experimental results with synthetic and real-world datasets quantify the gap between the best and worst-case scenarios of (mis)interpreting machine learning predictions globally.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuSemSlice: Towards Effective DNN Model Maintenance via Neuron-level Semantic Slicing</title>
<link>https://arxiv.org/abs/2407.20281</link>
<guid>https://arxiv.org/abs/2407.20281</guid>
<content:encoded><![CDATA[
arXiv:2407.20281v2 Announce Type: replace 
Abstract: Deep Neural networks (DNNs), extensively applied across diverse disciplines, are characterized by their integrated and monolithic architectures, setting them apart from conventional software systems. This architectural difference introduces particular challenges to maintenance tasks, such as model restructure (e.g., model compression), re-adaptation (e.g., fitting new samples), and incremental development (e.g., continual knowledge accumulation). Prior research addresses these challenges by identifying task-critical neuron layers, and dividing neural networks into semantically-similar sequential modules. However, such layer-level approaches fail to precisely identify and manipulate neuron-level semantic components, restricting their applicability to finer-grained model maintenance tasks. In this work, we implement NeuSemSlice, a novel framework that introduces the semantic slicing technique to effectively identify critical neuron-level semantic components in DNN models for semantic-aware model maintenance tasks. Specifically, semantic slicing identifies, categorizes and merges critical neurons across different categories and layers according to their semantic similarity, enabling their flexibility and effectiveness in the subsequent tasks. For semantic-aware model maintenance tasks, we provide a series of novel strategies based on semantic slicing to enhance NeuSemSlice. They include semantic components (i.e., critical neurons) preservation for model restructure, critical neuron tuning for model re-adaptation, and non-critical neuron training for model incremental development. A thorough evaluation has demonstrated that NeuSemSlice significantly outperforms baselines in all three tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models</title>
<link>https://arxiv.org/abs/2408.08554</link>
<guid>https://arxiv.org/abs/2408.08554</guid>
<content:encoded><![CDATA[
arXiv:2408.08554v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their practical application is constrained by substantial memory and computational demands. Post-training quantization (PTQ) is considered an effective method to accelerate LLM inference. Despite its growing popularity in LLM model compression, PTQ deployment faces two major challenges. First, low-bit quantization leads to performance degradation. Second, restricted by the limited integer computing unit type on GPUs, quantized matrix operations with different precisions cannot be effectively accelerated. To address these issues, we introduce a novel arbitrary-bit quantization algorithm and inference framework, ABQ-LLM. It achieves superior performance across various quantization settings and enables efficient arbitrary-precision quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1) a distribution correction method for transformer blocks to mitigate distribution differences caused by full quantization of weights and activations, improving performance at low bit-widths. (2) the bit balance strategy to counteract performance degradation from asymmetric distribution issues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization acceleration framework that reconstructs the quantization matrix multiplication of arbitrary precision combinations based on BTC (Binary TensorCore) equivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM can convert each component bit width gain into actual acceleration gain, maximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8 quantization configuration on LLaMA-7B model, it achieved a WikiText2 perplexity of 7.59 (2.17$\downarrow $ vs 9.76 in AffineQuant). Compared to SmoothQuant, we realized 1.6$\times$ acceleration improvement and 2.7$\times$ memory compression gain.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models</title>
<link>https://arxiv.org/abs/2408.10631</link>
<guid>https://arxiv.org/abs/2408.10631</guid>
<content:encoded><![CDATA[
arXiv:2408.10631v2 Announce Type: replace 
Abstract: Large language models (LLMs) have seen substantial growth, necessitating efficient model pruning techniques. Existing post-training pruning methods primarily measure weight importance in converged dense models, often overlooking changes in weight significance during the pruning process, leading to performance degradation. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, facilitating global performance optimization. We are the first to employ the product of weights and gradients as a pruning metric in the context of LLM post-training pruning. This enables accurate identification of weight importance in massive models and significantly reduces computational complexity compared to methods using secondorder information. Our experiments show that LLM-Barber efficiently prunes models from LLaMA and OPT families (7B to 13B) on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistent Backdoor Attacks in Continual Learning</title>
<link>https://arxiv.org/abs/2409.13864</link>
<guid>https://arxiv.org/abs/2409.13864</guid>
<content:encoded><![CDATA[
arXiv:2409.13864v2 Announce Type: replace 
Abstract: Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Neural Networks for Modularity aids Interpretability</title>
<link>https://arxiv.org/abs/2409.15747</link>
<guid>https://arxiv.org/abs/2409.15747</guid>
<content:encoded><![CDATA[
arXiv:2409.15747v2 Announce Type: replace 
Abstract: An approach to improve network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We find pretrained models to be highly unclusterable and thus train models to be more modular using an ``enmeshment loss'' function that encourages the formation of non-interacting clusters. Using automated interpretability measures, we show that our method finds clusters that learn different, disjoint, and smaller circuits for CIFAR-10 labels. Our approach provides a promising direction for making neural networks easier to interpret.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Representation Condition Improves Equivariant Molecule Generation</title>
<link>https://arxiv.org/abs/2410.03655</link>
<guid>https://arxiv.org/abs/2410.03655</guid>
<content:encoded><![CDATA[
arXiv:2410.03655v4 Announce Type: replace 
Abstract: Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed. Code is available at https://github.com/GraphPKU/GeoRCG.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</title>
<link>https://arxiv.org/abs/2410.08979</link>
<guid>https://arxiv.org/abs/2410.08979</guid>
<content:encoded><![CDATA[
arXiv:2410.08979v5 Announce Type: replace 
Abstract: Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a "temporal recall" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Furthermore, we compare SRL with model-based online planning, showing that SRL achieves comparable FAS while leveraging the same model during training that online planners use for planning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Networks Reveal Spatial Domains from Single-cell Transcriptomics Data</title>
<link>https://arxiv.org/abs/2410.19868</link>
<guid>https://arxiv.org/abs/2410.19868</guid>
<content:encoded><![CDATA[
arXiv:2410.19868v2 Announce Type: replace 
Abstract: The task of spatial clustering of transcriptomics data is of paramount importance. It enables the classification of tissue samples into diverse subpopulations of cells, which, in turn, facilitates the analysis of the biological functions of clusters, tissue reconstruction, and cell-cell interactions. Many approaches leverage gene expressions, spatial locations, and histological images to detect spatial domains; however, Graph Neural Networks (GNNs) as state of the art models suffer from a limitation in the assumption of pairwise connections between nodes. In the case of domain detection in spatial transcriptomics, some cells are found to be not directly related. Still, they are grouped as the same domain, which shows the incapability of GNNs for capturing implicit connections among the cells.
  While graph edges connect only two nodes, hyperedges connect an arbitrary number of nodes along their edges, which lets Hypergraph Neural Networks (HGNNs) capture and utilize richer and more complex structural information than traditional GNNs. We use autoencoders to address the limitation of not having the actual labels, which are well-suited for unsupervised learning. Our model has demonstrated exceptional performance, achieving the highest iLISI score of 1.843 compared to other methods. This score indicates the greatest diversity of cell types identified by our method. Furthermore, our model outperforms other methods in downstream clustering, achieving the highest ARI values of 0.51 and Leiden score of 0.60.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytic Continual Test-Time Adaptation for Multi-Modality Corruption</title>
<link>https://arxiv.org/abs/2410.22373</link>
<guid>https://arxiv.org/abs/2410.22373</guid>
<content:encoded><![CDATA[
arXiv:2410.22373v2 Announce Type: replace 
Abstract: Test-Time Adaptation (TTA) enables pre-trained models to bridge the gap between source and target datasets using unlabeled test data, addressing domain shifts caused by corruptions like weather changes, noise, or sensor malfunctions in test time. Multi-Modal Continual Test-Time Adaptation (MM-CTTA), as an extension of standard TTA, further allows models to handle multi-modal inputs and adapt to continuously evolving target domains. However, MM-CTTA faces critical challenges such as catastrophic forgetting and reliability bias, which are rarely addressed effectively under multi-modal corruption scenarios. In this paper, we propose a novel approach, Multi-modality Dynamic Analytic Adapter (MDAA), to tackle MM-CTTA tasks. MDAA introduces analytic learning,a closed-form training technique,through Analytic Classifiers (ACs) to mitigate catastrophic forgetting. Furthermore, we design the Dynamic Late Fusion Mechanism (DLFM) to dynamically select and integrate reliable information from different modalities. Extensive experiments show that MDAA achieves state-of-the-art performance across the proposed tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does equivariance matter at scale?</title>
<link>https://arxiv.org/abs/2410.23179</link>
<guid>https://arxiv.org/abs/2410.23179</guid>
<content:encoded><![CDATA[
arXiv:2410.23179v2 Announce Type: replace 
Abstract: Given large datasets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syno: Structured Synthesis for Neural Operators</title>
<link>https://arxiv.org/abs/2410.23745</link>
<guid>https://arxiv.org/abs/2410.23745</guid>
<content:encoded><![CDATA[
arXiv:2410.23745v2 Announce Type: replace 
Abstract: The desires for better prediction accuracy and higher execution performance in neural networks never end. Neural architecture search (NAS) and tensor compilers are two popular techniques to optimize these two goals, but they are both limited to composing or optimizing existing manually designed operators rather than coming up with completely new designs. In this work, we explore the less studied direction of neural operator synthesis, which aims to automatically and efficiently discover novel neural operators with better accuracy and/or speed. We develop an end-to-end framework Syno, to realize practical neural operator synthesis. Syno makes use of a novel set of fine-grained primitives defined on tensor dimensions, which ensure various desired properties to ease model training, and also enable expression canonicalization techniques to avoid redundant candidates during search. Syno further adopts a novel guided synthesis flow to obtain valid operators matched with the specified input/output dimension sizes, and leverages efficient stochastic tree search algorithms to quickly explore the design space. We demonstrate that Syno discovers better operators with average speedups of $1.37\times$ to $2.06\times$ on various hardware and compiler choices, while keeping less than 1% accuracy loss even on NAS-optimized models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lagrangian neural networks for nonholonomic mechanics</title>
<link>https://arxiv.org/abs/2411.00110</link>
<guid>https://arxiv.org/abs/2411.00110</guid>
<content:encoded><![CDATA[
arXiv:2411.00110v2 Announce Type: replace 
Abstract: Lagrangian Neural Networks (LNNs) are a powerful tool for addressing physical systems, particularly those governed by conservation laws. LNNs can parametrize the Lagrangian of a system to predict trajectories with nearly conserved energy. These techniques have proven effective in unconstrained systems as well as those with holonomic constraints. In this work, we adapt LNN techniques to mechanical systems with nonholonomic constraints. We test our approach on some well-known examples with nonholonomic constraints, showing that incorporating these restrictions into the neural network's learning improves not only trajectory estimation accuracy but also ensures adherence to constraints and exhibits better energy behavior compared to the unconstrained counterpart.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Trusted Multi-view Classification Framework with Hierarchical Opinion Aggregation</title>
<link>https://arxiv.org/abs/2411.03713</link>
<guid>https://arxiv.org/abs/2411.03713</guid>
<content:encoded><![CDATA[
arXiv:2411.03713v2 Announce Type: replace 
Abstract: Recently, multi-view learning has witnessed a considerable interest on the research of trusted decision-making. Previous methods are mainly inspired from an important paper published by Han et al. in 2021, which formulates a Trusted Multi-view Classification (TMC) framework that aggregates evidence from different views based on Dempster's combination rule. All these methods only consider inter-view aggregation, yet lacking exploitation of intra-view information. In this paper, we propose a generalized trusted multi-view classification framework with hierarchical opinion aggregation. This hierarchical framework includes a two-phase aggregation process: the intra-view and inter-view aggregation hierarchies. In the intra aggregation, we assume that each view is comprised of common information shared with other views, as well as its specific information. We then aggregate both the common and specific information. This aggregation phase is useful to eliminate the feature noise inherent to view itself, thereby improving the view quality. In the inter-view aggregation, we design an attention mechanism at the evidence level to facilitate opinion aggregation from different views. To the best of our knowledge, this is one of the pioneering efforts to formulate a hierarchical aggregation framework in the trusted multi-view learning domain. Extensive experiments show that our model outperforms some state-of art trust-related baselines. One can access the source code on https://github.com/lshi91/GTMC-HOA.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Discrete Representation in Sparse Mixture of Experts</title>
<link>https://arxiv.org/abs/2411.19402</link>
<guid>https://arxiv.org/abs/2411.19402</guid>
<content:encoded><![CDATA[
arXiv:2411.19402v2 Announce Type: replace 
Abstract: Sparse mixture of experts (SMoE) is an effective solution for scaling up model capacity without increasing the computational costs. A crucial component of SMoE is the router, responsible for directing the input to relevant experts; however, it also presents a major weakness, leading to routing inconsistencies and representation collapse issues. Instead of fixing the router like previous works, we propose an alternative that assigns experts to input via indirection, which employs the discrete representation of input that points to the expert. The discrete representations are learnt via vector quantization, resulting in a new architecture dubbed Vector-Quantized Mixture of Experts (VQMoE). We provide theoretical support and empirical evidence demonstrating the VQMoE's ability to overcome the challenges present in traditional routers. Through extensive evaluations on both large language models and vision tasks for pre-training and fine-tuning, we show that VQMoE achieves a 28% improvement in robustness compared to other SMoE routing methods, while maintaining strong performance in fine-tuning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Risk Control via Prediction-Powered Inference</title>
<link>https://arxiv.org/abs/2412.11174</link>
<guid>https://arxiv.org/abs/2412.11174</guid>
<content:encoded><![CDATA[
arXiv:2412.11174v2 Announce Type: replace 
Abstract: The risk-controlling prediction sets (RCPS) framework is a general tool for transforming the output of any machine learning model to design a predictive rule with rigorous error rate control. The key idea behind this framework is to use labeled hold-out calibration data to tune a hyper-parameter that affects the error rate of the resulting prediction rule. However, the limitation of such a calibration scheme is that with limited hold-out data, the tuned hyper-parameter becomes noisy and leads to a prediction rule with an error rate that is often unnecessarily conservative. To overcome this sample-size barrier, we introduce a semi-supervised calibration procedure that leverages unlabeled data to rigorously tune the hyper-parameter without compromising statistical validity. Our procedure builds upon the prediction-powered inference framework, carefully tailoring it to risk-controlling tasks. We demonstrate the benefits and validity of our proposal through two real-data experiments: few-shot image classification and early time series classification.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference</title>
<link>https://arxiv.org/abs/2412.17560</link>
<guid>https://arxiv.org/abs/2412.17560</guid>
<content:encoded><![CDATA[
arXiv:2412.17560v4 Announce Type: replace 
Abstract: Model compression has emerged as a mainstream solution to reduce memory usage and computational overhead. This paper presents Group Quantization and Sparse Acceleration (GQSA), a novel compression technique tailored for LLMs. Traditional methods typically focus exclusively on either quantization or sparsification, but relying on a single strategy often results in significant performance loss at high compression rates. In contrast, GQSA integrates quantization and sparsification in a tightly coupled manner, leveraging GPU-friendly structured group sparsity and quantization for efficient acceleration. Building upon system-algorithm co-design principles, we propose a two-stage sparse optimization strategy that ensures the performance superiority of the compressed model. On the engine side, we introduce a "task-centric" parallel strategy, which, to the best of our knowledge, is the first application in the domain of sparse computing. Compared to the traditional 2:4 sparse method, the GQSA offers a more flexible and adjustable sparsity rate, as well as a higher weight compression rate, and is efficiently compatible with weight-only quantization methods. Experimental results demonstrate that, under the GQSA W4S50% compression setting, the model's accuracy surpasses that of both 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA outperforms W2 by 1.26$\times$ and 2:4 pruning by 2.35$\times$ in terms of speed.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Growing Neural Networks: Dynamic Evolution through Gradient Descent</title>
<link>https://arxiv.org/abs/2501.18012</link>
<guid>https://arxiv.org/abs/2501.18012</guid>
<content:encoded><![CDATA[
arXiv:2501.18012v2 Announce Type: replace 
Abstract: In contrast to conventional artificial neural networks, which are structurally static, we present two approaches for evolving small networks into larger ones during training. The first method employs an auxiliary weight that directly controls network size, while the second uses a controller-generated mask to modulate neuron participation. Both approaches optimize network size through the same gradient-descent algorithm that updates the network's weights and biases. We evaluate these growing networks on nonlinear regression and classification tasks, where they consistently outperform static networks of equivalent final size. We then explore the hyperparameter space of these networks to find associated scaling relations relative to their static counterparts. Our results suggest that starting small and growing naturally may be preferable to simply starting large, particularly as neural networks continue to grow in size and energy consumption.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation</title>
<link>https://arxiv.org/abs/2501.19364</link>
<guid>https://arxiv.org/abs/2501.19364</guid>
<content:encoded><![CDATA[
arXiv:2501.19364v2 Announce Type: replace 
Abstract: Multivariate Time Series Imputation (MTSI) is crucial for many applications, such as healthcare monitoring and traffic management, where incomplete data can compromise decision-making. Existing state-of-the-art methods, like Denoising Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy; however, they suffer from significant computational costs and are notably time-consuming due to their iterative nature. In this work, we propose CoSTI, an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI employs Consistency Training to achieve comparable imputation quality to DDPMs while drastically reducing inference times, making it more suitable for real-time applications. We evaluate CoSTI across multiple datasets and missing data scenarios, demonstrating up to a 98% reduction in imputation time with performance on par with diffusion-based models. This work bridges the gap between efficiency and accuracy in generative imputation tasks, providing a scalable solution for handling missing data in critical spatio-temporal systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2502.01416</link>
<guid>https://arxiv.org/abs/2502.01416</guid>
<content:encoded><![CDATA[
arXiv:2502.01416v3 Announce Type: replace 
Abstract: The Schr\"odinger Bridge (SB) is a powerful framework for solving generative modeling tasks such as unpaired domain translation. Most SB-related research focuses on continuous data space $\mathbb{R}^{D}$ and leaves open theoretical and algorithmic questions about applying SB methods to discrete data, e.g, on finite spaces $\mathbb{S}^{D}$. Notable examples of such sets $\mathbb{S}$ are codebooks of vector-quantized (VQ) representations of modern autoencoders, tokens in texts, categories of atoms in molecules, etc. In this paper, we provide a theoretical and algorithmic foundation for solving SB in discrete spaces using the recently introduced Iterative Markovian Fitting (IMF) procedure. Specifically, we theoretically justify the convergence of discrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop a practical computational algorithm for SB, which we call Categorical Schr\"odinger Bridge Matching (CSBM). We show the performance of CSBM via a series of experiments with synthetic data and VQ representations of images. The code of CSBM is available at https://github.com/gregkseno/csbm.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Untrained Machine Learning for Anomaly Detection by using 3D Point Cloud Data</title>
<link>https://arxiv.org/abs/2502.03876</link>
<guid>https://arxiv.org/abs/2502.03876</guid>
<content:encoded><![CDATA[
arXiv:2502.03876v3 Announce Type: replace 
Abstract: Anomaly detection based on 3D point cloud data is an important research problem and receives more and more attention recently. Untrained anomaly detection based on only one sample is an emerging research problem motivated by real manufacturing industries such as personalized manufacturing where only one sample can be collected without any additional labels and historical datasets. Identifying anomalies accurately based on one 3D point cloud sample is a critical challenge in both industrial applications and the field of machine learning. This paper aims to provide a formal definition of the untrained anomaly detection problem based on 3D point cloud data, discuss the differences between untrained anomaly detection and current unsupervised anomaly detection problems. Unlike trained unsupervised learning, untrained unsupervised learning does not rely on any data, including unlabeled data. Instead, they leverage prior knowledge about the surfaces and anomalies.
  We propose three complementary methodological frameworks: the Latent Variable Inference Framework that employs probabilistic modeling to distinguish anomalies; the Decomposition Framework that separates point clouds into reference, anomaly, and noise components through sparse learning; and the Local Geometry Framework that leverages neighborhood information for anomaly identification. Experimental results demonstrate that untrained methods achieve competitive detection performance while offering significant computational advantages, demonstrating up to a 15-fold increase in execution speed. The proposed methods provide viable solutions for scenarios with extreme data scarcity, addressing critical challenges in personalized manufacturing and healthcare applications where collecting multiple samples or historical data is infeasible.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Histogram-based Outlier Score (EHBOS)</title>
<link>https://arxiv.org/abs/2502.05719</link>
<guid>https://arxiv.org/abs/2502.05719</guid>
<content:encoded><![CDATA[
arXiv:2502.05719v2 Announce Type: replace 
Abstract: Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Analysis of Discrete-Time Stochastic Interpolants</title>
<link>https://arxiv.org/abs/2502.09130</link>
<guid>https://arxiv.org/abs/2502.09130</guid>
<content:encoded><![CDATA[
arXiv:2502.09130v2 Announce Type: replace 
Abstract: The stochastic interpolant framework offers a powerful approach for constructing generative models based on ordinary differential equations (ODEs) or stochastic differential equations (SDEs) to transform arbitrary data distributions. However, prior analyses of this framework have primarily focused on the continuous-time setting, assuming a perfect solution of the underlying equations. In this work, we present the first discrete-time analysis of the stochastic interpolant framework, where we introduce an innovative discrete-time sampler and derive a finite-time upper bound on its distribution estimation error. Our result provides a novel quantification of how different factors, including the distance between source and target distributions and estimation accuracy, affect the convergence rate and also offers a new principled way to design efficient schedules for convergence acceleration. Finally, numerical experiments are conducted on the discrete-time sampler to corroborate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NestQuant: Nested Lattice Quantization for Matrix Products and LLMs</title>
<link>https://arxiv.org/abs/2502.09720</link>
<guid>https://arxiv.org/abs/2502.09720</guid>
<content:encoded><![CDATA[
arXiv:2502.09720v3 Announce Type: replace 
Abstract: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent works have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot (8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation benchmarks confirm uniform superiority of NestQuant.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference learning made easy: Everything should be understood through win rate</title>
<link>https://arxiv.org/abs/2502.10505</link>
<guid>https://arxiv.org/abs/2502.10505</guid>
<content:encoded><![CDATA[
arXiv:2502.10505v2 Announce Type: replace 
Abstract: Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioned Inexact Stochastic ADMM for Deep Model</title>
<link>https://arxiv.org/abs/2502.10784</link>
<guid>https://arxiv.org/abs/2502.10784</guid>
<content:encoded><![CDATA[
arXiv:2502.10784v3 Announce Type: replace 
Abstract: The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (\textbf{P}reconditioned \textbf{I}nexact \textbf{S}tochastic \textbf{A}lternating Direction Method of Multipliers), which enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables PISA to tackle the challenge of data heterogeneity effectively. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate its superior numerical performance compared to various state-of-the-art optimizers.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Combinatorial Semi-bandits with Graph Feedback</title>
<link>https://arxiv.org/abs/2502.18826</link>
<guid>https://arxiv.org/abs/2502.18826</guid>
<content:encoded><![CDATA[
arXiv:2502.18826v5 Announce Type: replace 
Abstract: In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\widetilde{\Theta}(S\sqrt{T}+\sqrt{\alpha ST})$, where $S$ is the size of the combinatorial decisions and $\alpha$ is the independence number of $G$. This result interpolates between the known regrets $\widetilde\Theta(S\sqrt{T})$ under full information (i.e., $G$ is complete) and $\widetilde\Theta(\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations. We also show that online stochastic mirror descent (OSMD) that only realizes convexified actions in expectation is suboptimal. In addition, we describe the problem of \emph{combinatorial semi-bandits with general capacity} and apply our results to derive an improved regret upper bound, which may be of independent interest.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite-Surface-Area Machine-Learning Models for Reservoir Storage Estimation: Regime-Sensitive Evaluation and Operational Deployment at Loskop Dam, South Africa</title>
<link>https://arxiv.org/abs/2502.19989</link>
<guid>https://arxiv.org/abs/2502.19989</guid>
<content:encoded><![CDATA[
arXiv:2502.19989v3 Announce Type: replace 
Abstract: Reliable daily estimates of reservoir storage are pivotal for water allocation and drought response decisions in semiarid regions. Conventional rating curves at Loskop Dam, the primary storage on South Africa's Olifants River, have become increasingly uncertain owing to sedimentation and episodic drawdown. A 40 year Digital Earth Africa (DEA) surface area archive (1984-2024) fused with gauged water levels to develop data driven volume predictors that operate under a maximum 9.14%, a 90 day drawdown constraint. Four nested feature sets were examined: (i) raw water area, (ii) +a power law "calculated volume" proxy, (iii) +six river geometry metrics, and (iv) +full supply elevation. Five candidate algorithms, Gradient Boosting (GB), Random Forest (RF), Ridge (RI), Lasso (LA) and Elastic Net (EN), were tuned using a 20 draw random search and assessed with a five fold Timeseries Split to eliminate look ahead bias. Prediction errors were decomposed into two regimes: Low (<250 x 10^6 cubic meters) and High (>250 x 10^6 cubic meters) storage regimes. Ridge regression achieved the lowest cross validated RMSE (12.3 x 10^6 cubic meters), outperforming GB by 16% and RF by 7%. In regime terms, Ridge was superior in the Low band (18.0 ver. 22.7 MCM for GB) and tied RF in the High band (~12 MCM). In sample diagnostics showed GB's apparent dominance (6.8-5.4 MCM) to be an artefact of overfitting. A Ridge meta stacked ensemble combining GB, RF, and Ridge reduced full series RMSE to ~ 11 MCM (~ 3% of live capacity). We recommend (i) GB retrained daily for routine operations, (ii) Ridge for drought early warning, and (iii) the stacked blend for all weather dashboards. Quarterly rolling retraining and regime specific metrics are advised to maintain operational accuracy below the 5% threshold mandated by the Department of Water and Sanitation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Open-world Continual Learning under the Constraints of Scarce Labeled Data</title>
<link>https://arxiv.org/abs/2502.20974</link>
<guid>https://arxiv.org/abs/2502.20974</guid>
<content:encoded><![CDATA[
arXiv:2502.20974v2 Announce Type: replace 
Abstract: Open-world continual learning (OWCL) adapts to sequential tasks with open samples, learning knowledge incrementally while preventing forgetting. However, existing OWCL still requires a large amount of labeled data for training, which is often impractical in real-world applications. Given that new categories/entities typically come with limited annotations and are in small quantities, a more realistic situation is OWCL with scarce labeled data, i.e., few-shot training samples. Hence, this paper investigates the problem of open-world few-shot continual learning (OFCL), challenging in (i) learning unbounded tasks without forgetting previous knowledge and avoiding overfitting, (ii) constructing compact decision boundaries for open detection with limited labeled data, and (iii) transferring knowledge about knowns and unknowns and even update the unknowns to knowns once the labels of open samples are learned. In response, we propose a novel OFCL framework that integrates three key components: (1) an instance-wise token augmentation (ITA) that represents and enriches sample representations with additional knowledge, (2) a margin-based open boundary (MOB) that supports open detection with new tasks emerge over time, and (3) an adaptive knowledge space (AKS) that endows unknowns with knowledge for the updating from unknowns to knowns. Finally, extensive experiments show that the proposed OFCL framework outperforms all baselines remarkably with practical importance and reproducibility. The source code is released at https://github.com/liyj1201/OFCL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax Optimal Reinforcement Learning with Quasi-Optimism</title>
<link>https://arxiv.org/abs/2503.00810</link>
<guid>https://arxiv.org/abs/2503.00810</guid>
<content:encoded><![CDATA[
arXiv:2503.00810v3 Announce Type: replace 
Abstract: In our quest for a reinforcement learning (RL) algorithm that is both practical and provably optimal, we introduce EQO (Exploration via Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids reliance on empirical variances and employs a simple bonus term proportional to the inverse of the state-action visit count. Central to EQO is the concept of quasi-optimism, where estimated values need not be fully optimistic, allowing for a simpler yet effective exploration strategy. The algorithm achieves the sharpest known regret bound for tabular RL under the mildest assumptions, proving that fast convergence can be attained with a practical and computationally efficient approach. Empirical evaluations demonstrate that EQO consistently outperforms existing algorithms in both regret performance and computational efficiency, providing the best of both theoretical soundness and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Unlearn while Retaining: Combating Gradient Conflicts in Machine Unlearning</title>
<link>https://arxiv.org/abs/2503.06339</link>
<guid>https://arxiv.org/abs/2503.06339</guid>
<content:encoded><![CDATA[
arXiv:2503.06339v2 Announce Type: replace 
Abstract: Machine Unlearning has recently garnered significant attention, aiming to selectively remove knowledge associated with specific data while preserving the model's performance on the remaining data. A fundamental challenge in this process is balancing effective unlearning with knowledge retention, as naive optimization of these competing objectives can lead to conflicting gradients, hindering convergence and degrading overall performance. To address this issue, we propose Learning to Unlearn while Retaining, aimed to mitigate gradient conflicts between unlearning and retention objectives. Our approach strategically avoids conflicts through an implicit gradient regularization mechanism that emerges naturally within the proposed framework. This prevents conflicting gradients between unlearning and retention, leading to effective unlearning while preserving the model's utility. We validate our approach across both discriminative and generative tasks, demonstrating its effectiveness in achieving unlearning without compromising performance on remaining data. Our results highlight the advantages of avoiding such gradient conflicts, outperforming existing methods that fail to account for these interactions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pitfalls of Imitation Learning when Actions are Continuous</title>
<link>https://arxiv.org/abs/2503.09722</link>
<guid>https://arxiv.org/abs/2503.09722</guid>
<content:encoded><![CDATA[
arXiv:2503.09722v4 Announce Type: replace 
Abstract: We study the problem of imitating an expert demonstrator in a discrete-time, continuous state-and-action control system. We show that, even if the dynamics satisfy a control-theoretic property called exponential stability (i.e. the effects of perturbations decay exponentially quickly), and the expert is smooth and deterministic, any smooth, deterministic imitator policy necessarily suffers error on execution that is exponentially larger, as a function of problem horizon, than the error under the distribution of expert training data. Our negative result applies to any algorithm which learns solely from expert data, including both behavior cloning and offline-RL algorithms, unless the algorithm produces highly "improper" imitator policies--those which are non-smooth, non-Markovian, or which exhibit highly state-dependent stochasticity--or unless the expert trajectory distribution is sufficiently "spread." We provide experimental evidence of the benefits of these more complex policy parameterizations, explicating the benefits of today's popular policy parameterizations in robot learning (e.g. action-chunking and diffusion policies). We also establish a host of complementary negative and positive results for imitation in control systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture-Aware Minimization (A$^2$M): How to Find Flat Minima in Neural Architecture Search</title>
<link>https://arxiv.org/abs/2503.10404</link>
<guid>https://arxiv.org/abs/2503.10404</guid>
<content:encoded><![CDATA[
arXiv:2503.10404v2 Announce Type: replace 
Abstract: Neural Architecture Search (NAS) has become an essential tool for designing effective and efficient neural networks. In this paper, we investigate the geometric properties of neural architecture spaces commonly used in differentiable NAS methods, specifically NAS-Bench-201 and DARTS. By defining flatness metrics such as neighborhoods and loss barriers along paths in architecture space, we reveal locality and flatness characteristics analogous to the well-known properties of neural network loss landscapes in weight space. In particular, we find that highly accurate architectures cluster together in flat regions, while suboptimal architectures remain isolated, unveiling the detailed geometrical structure of the architecture search landscape. Building on these insights, we propose Architecture-Aware Minimization (A$^2$M), a novel analytically derived algorithmic framework that explicitly biases, for the first time, the gradient of differentiable NAS methods towards flat minima in architecture space. A$^2$M consistently improves generalization over state-of-the-art DARTS-based algorithms on benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet16-120, across both NAS-Bench-201 and DARTS search spaces. Notably, A$^2$M is able to increase the test accuracy, on average across different differentiable NAS methods, by +3.60\% on CIFAR-10, +4.60\% on CIFAR-100, and +3.64\% on ImageNet16-120, demonstrating its superior effectiveness in practice. A$^2$M can be easily integrated into existing differentiable NAS frameworks, offering a versatile tool for future research and applications in automated machine learning. We open-source our code at https://github.com/AI-Tech-Research-Lab/AsquaredM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research</title>
<link>https://arxiv.org/abs/2503.12730</link>
<guid>https://arxiv.org/abs/2503.12730</guid>
<content:encoded><![CDATA[
arXiv:2503.12730v4 Announce Type: replace 
Abstract: Mechanistic interpretability research faces a gap between analyzing simple circuits in toy tasks and discovering features in large models. To bridge this gap, we propose text-to-SQL generation as an ideal task to study, as it combines the formal structure of toy tasks with real-world complexity. We introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL operations, and train models ranging from 33M to 1B parameters to establish a comprehensive testbed for interpretability. We apply multiple complementary interpretability techniques, including Edge Attribution Patching and Sparse Autoencoders, to identify minimal circuits and components supporting SQL generation. We compare circuits for different SQL subskills, evaluating their minimality, reliability, and identifiability. Finally, we conduct a layerwise logit lens analysis to reveal how models compose SQL queries across layers: from intent recognition to schema resolution to structured generation. Our work provides a robust framework for probing and comparing interpretability methods in a structured, progressively complex setting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and Biomarker Identification using Multi-Omics Data</title>
<link>https://arxiv.org/abs/2503.22939</link>
<guid>https://arxiv.org/abs/2503.22939</guid>
<content:encoded><![CDATA[
arXiv:2503.22939v3 Announce Type: replace 
Abstract: The integration of heterogeneous multi-omics datasets at a systems level remains a central challenge for developing analytical and computational models in precision cancer diagnostics. This paper introduces Multi-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes messenger-RNA, micro-RNA sequences, and DNA methylation samples together with Protein-Protein Interaction (PPI) networks for cancer classification across 31 different cancer types. The proposed approach combines differential gene expression with DESeq2, Linear Models for Microarray (LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression to reduce multi-omics data dimensionality while preserving relevant biological features. The model architecture is based on the Kolmogorov-Arnold theorem principle and uses trainable univariate functions to enhance interpretability and feature analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits low experimental variability in comparison to related deep learning-based models. The biomarkers identified by MOGKAN were validated as cancer-related markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. By integrating multi-omics data with graph-based deep learning, our proposed approach demonstrates robust predictive performance and interpretability with potential to enhance the translation of complex multi-omics data into clinically actionable cancer diagnostics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Low-complexity Structured Neural Network to Realize States of Dynamical Systems</title>
<link>https://arxiv.org/abs/2503.23697</link>
<guid>https://arxiv.org/abs/2503.23697</guid>
<content:encoded><![CDATA[
arXiv:2503.23697v2 Announce Type: replace 
Abstract: Data-driven learning is rapidly evolving and places a new perspective on realizing state-space dynamical systems. However, dynamical systems derived from nonlinear ordinary differential equations (ODEs) suffer from limitations in computational efficiency. Thus, this paper stems from data-driven learning to advance states of dynamical systems utilizing a structured neural network (StNN). The proposed learning technique also seeks to identify an optimal, low-complexity operator to solve dynamical systems, the so-called Hankel operator, derived from time-delay measurements. Thus, we utilize the StNN based on the Hankel operator to solve dynamical systems as an alternative to existing data-driven techniques. We show that the proposed StNN reduces the number of parameters and computational complexity compared with the conventional neural networks and also with the classical data-driven techniques, such as Sparse Identification of Nonlinear Dynamics (SINDy) and Hankel Alternative view of Koopman (HAVOK), which is commonly known as delay-Dynamic Mode Decomposition(DMD) or Hankel-DMD. More specifically, we present numerical simulations to solve dynamical systems utilizing the StNN based on the Hankel operator beginning from the fundamental Lotka-Volterra model, where we compare the StNN with the LEarning Across Dynamical Systems (LEADS), and extend our analysis to highly nonlinear and chaotic Lorenz systems, comparing the StNN with conventional neural networks, SINDy, and HAVOK. Hence, we show that the proposed StNN paves the way for realizing state-space dynamical systems with a low-complexity learning algorithm, enabling prediction and understanding of future states.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.04691</link>
<guid>https://arxiv.org/abs/2504.04691</guid>
<content:encoded><![CDATA[
arXiv:2504.04691v2 Announce Type: replace 
Abstract: Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17s to 5.09s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction</title>
<link>https://arxiv.org/abs/2504.05059</link>
<guid>https://arxiv.org/abs/2504.05059</guid>
<content:encoded><![CDATA[
arXiv:2504.05059v2 Announce Type: replace 
Abstract: Accurate vehicle trajectory prediction is critical for safe and efficient autonomous driving, especially in mixed traffic environments when both human-driven and autonomous vehicles co-exist. However, uncertainties introduced by inherent driving behaviors--such as acceleration, deceleration, and left and right maneuvers--pose significant challenges for reliable trajectory prediction. We introduce a Maneuver-Intention-Aware Transformer (MIAT) architecture, which integrates a maneuver intention awareness control mechanism with spatiotemporal interaction modeling to enhance long-horizon trajectory predictions. We systematically investigate the impact of varying awareness of maneuver intention on both short- and long-horizon trajectory predictions. Evaluated on the real-world NGSIM dataset and benchmarked against various transformer- and LSTM-based methods, our approach achieves an improvement of up to 4.7% in short-horizon predictions and a 1.6% in long-horizon predictions compared to other intention-aware benchmark methods. Moreover, by leveraging intention awareness control mechanism, MIAT realizes an 11.1% performance boost in long-horizon predictions, with a modest drop in short-horizon performance. The source code and datasets are available at https://github.com/cpraskoti/MIAT.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCA: A Parametric ReLU Composite Activation Function</title>
<link>https://arxiv.org/abs/2504.08994</link>
<guid>https://arxiv.org/abs/2504.08994</guid>
<content:encoded><![CDATA[
arXiv:2504.08994v2 Announce Type: replace 
Abstract: Activation functions have been shown to affect the performance of deep neural networks significantly. While the Rectified Linear Unit (ReLU) remains the dominant choice in practice, the optimal activation function for deep neural networks remains an open research question. In this paper, we propose a novel parametric activation function, ReCA, based on ReLU, which has been shown to outperform all baselines on state-of-the-art datasets using different complex neural network architectures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v3 Announce Type: replace 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate in vitro vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks</title>
<link>https://arxiv.org/abs/2505.02022</link>
<guid>https://arxiv.org/abs/2505.02022</guid>
<content:encoded><![CDATA[
arXiv:2505.02022v2 Announce Type: replace 
Abstract: Nanobodies -- single-domain antibody fragments derived from camelid heavy-chain-only antibodies -- exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models -- including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs -- in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dragonfly: a modular deep reinforcement learning library</title>
<link>https://arxiv.org/abs/2505.03778</link>
<guid>https://arxiv.org/abs/2505.03778</guid>
<content:encoded><![CDATA[
arXiv:2505.03778v2 Announce Type: replace 
Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in order to ease experimentation and developments. It relies on a json serialization that allows to swap building blocks and perform parameter sweep, while minimizing code maintenance. Some of its features are specifically designed for CPU-intensive environments, such as numerical simulations. Its performance on standard agents using common benchmarks compares favorably with the literature.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guide your favorite protein sequence generative model</title>
<link>https://arxiv.org/abs/2505.04823</link>
<guid>https://arxiv.org/abs/2505.04823</guid>
<content:encoded><![CDATA[
arXiv:2505.04823v3 Announce Type: replace 
Abstract: Generative machine learning models on sequences are transforming protein engineering. However, no principled framework exists for conditioning these models on auxiliary information, such as experimental data, in a plug-and-play manner. Herein, we present ProteinGuide -- a principled and general method for conditioning -- by unifying a broad class of protein generative models under a single framework. We demonstrate the applicability of ProteinGuide by guiding two protein generative models, ProteinMPNN and ESM3, to generate amino acid and structure token sequences, conditioned on several user-specified properties such as enhanced stability, enzyme classes, and CATH-labeled folds. We also used ProteinGuide with inverse folding models and our own experimental assay to design adenine base editor sequences for high activity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance</title>
<link>https://arxiv.org/abs/2505.07004</link>
<guid>https://arxiv.org/abs/2505.07004</guid>
<content:encoded><![CDATA[
arXiv:2505.07004v3 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Searching Expandable Architectures for Incremental Learning</title>
<link>https://arxiv.org/abs/2505.10457</link>
<guid>https://arxiv.org/abs/2505.10457</guid>
<content:encoded><![CDATA[
arXiv:2505.10457v2 Announce Type: replace 
Abstract: Incremental learning is a machine learning paradigm where a model learns from a sequential stream of tasks. This setting poses a key challenge: balancing plasticity (learning new tasks) and stability (preserving past knowledge). Neural Architecture Search (NAS), a branch of AutoML, automates the design of the architecture of Deep Neural Networks and has shown success in static settings. However, existing NAS-based approaches to incremental learning often rely on expanding the model at every task, making them impractical in resource-constrained environments. In this work, we introduce SEAL, a NAS-based framework tailored for data-incremental learning, a scenario where disjoint data samples arrive sequentially and are not stored for future access. SEAL adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric. Stability is preserved through cross-distillation training after each expansion step. The NAS component jointly searches for both the architecture and the optimal expansion policy. Experiments across multiple benchmarks demonstrate that SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods. These results highlight the promise of combining NAS and selective expansion for efficient, adaptive learning in incremental scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
arXiv:2505.18227v2 Announce Type: replace 
Abstract: In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</title>
<link>https://arxiv.org/abs/2505.18300</link>
<guid>https://arxiv.org/abs/2505.18300</guid>
<content:encoded><![CDATA[
arXiv:2505.18300v3 Announce Type: replace 
Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\boldsymbol{\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target $\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.23017</link>
<guid>https://arxiv.org/abs/2505.23017</guid>
<content:encoded><![CDATA[
arXiv:2505.23017v3 Announce Type: replace 
Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Analytic Gradients in Provably Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01665</link>
<guid>https://arxiv.org/abs/2506.01665</guid>
<content:encoded><![CDATA[
arXiv:2506.01665v2 Announce Type: replace 
Abstract: The deployment of autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research that aims to provide such guarantees using safeguards. These safeguards should be integrated during training to reduce the sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance from fewer environment interactions. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them with a state-of-the-art learning algorithm and a differentiable simulation. Using numerical experiments on three control tasks, we evaluate how different safeguards affect learning. The results demonstrate safeguarded training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2506.05713</link>
<guid>https://arxiv.org/abs/2506.05713</guid>
<content:encoded><![CDATA[
arXiv:2506.05713v2 Announce Type: replace 
Abstract: Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient fine-tuning technique for adapting large foundation models, yet it often locks adapters into suboptimal minima near their initialization. This hampers model generalization and limits downstream operators such as adapter merging and pruning. Here, we propose CoTo, a progressive training strategy that gradually increases adapters' activation probability over the course of fine-tuning. By stochastically deactivating adapters, CoTo encourages more balanced optimization and broader exploration of the loss landscape. We provide a theoretical analysis showing that CoTo promotes layer-wise dropout stability and linear mode connectivity, and we adopt a cooperative-game approach to quantify each adapter's marginal contribution. Extensive experiments demonstrate that CoTo consistently boosts single-task performance, enhances multi-task merging accuracy, improves pruning robustness, and reduces training overhead, all while remaining compatible with diverse LoRA variants. Code is available at https://github.com/zwebzone/coto.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy</title>
<link>https://arxiv.org/abs/2506.09091</link>
<guid>https://arxiv.org/abs/2506.09091</guid>
<content:encoded><![CDATA[
arXiv:2506.09091v3 Announce Type: replace 
Abstract: We introduce an optimization framework for variational inference based on the coupled free energy, extending variational inference techniques to account for the curved geometry of the coupled exponential family. This family includes important heavy-tailed distributions such as the generalized Pareto and the Student's t. By leveraging the coupled free energy, which is equal to the coupled evidence lower bound (ELBO) of the inverted probabilities, we improve the accuracy and robustness of the learned model. The coupled generalization of Fisher Information metric and the affine connection. The method is applied to the design of a coupled variational autoencoder (CVAE). By using the coupling for both the distributions and cost functions, the reconstruction metric is derived to still be the mean-square average loss with modified constants. The novelty comes from sampling the heavy-tailed latent distribution with its associated coupled probability, which has faster decaying tails. The result is the ability to train a model robust against severe outliers, while assuring that the training process is stable. The Wasserstein-2 or Fr\'echet Inception Distance of the reconstructed CelebA images shows the CVAE has a 3\% improvement over the VAE after 5 epochs of training.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation</title>
<link>https://arxiv.org/abs/2506.10235</link>
<guid>https://arxiv.org/abs/2506.10235</guid>
<content:encoded><![CDATA[
arXiv:2506.10235v2 Announce Type: replace 
Abstract: Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications. However, its circuit formulation is inefficient due to O(|V |2) token length and suffers from low precision sensitivity to numeric inputs. In this work, we introduce LaMAGIC2, a succinct float-input canonical formulation with identifier (SFCI) for language model-based analog topology generation. SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to O(|V |), and enhancing numeric precision sensitivity for better performance under tight tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a prior method. LaMAGIC2 also exhibits better transferability for circuits with more vertices with up to 58.5% improvement. These advancements establish LaMAGIC2 as a robust framework for analog topology generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Free Probabilistic Framework for Analyzing the Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2506.16550</link>
<guid>https://arxiv.org/abs/2506.16550</guid>
<content:encoded><![CDATA[
arXiv:2506.16550v2 Announce Type: replace 
Abstract: We present a formal operator-theoretic framework for analyzing Transformer-based language models using free probability theory. By modeling token embeddings and attention mechanisms as self-adjoint operators in a tracial \( W^* \)-probability space, we reinterpret attention as non-commutative convolution and describe representation propagation via free additive convolution. This leads to a spectral dynamic system interpretation of deep Transformers. We derive entropy-based generalization bounds under freeness assumptions and provide insight into positional encoding, spectral evolution, and representational complexity. This work offers a principled, though theoretical, perspective on structural dynamics in large language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Model Integration with Open World Temporal Logic for Process Automation</title>
<link>https://arxiv.org/abs/2506.17776</link>
<guid>https://arxiv.org/abs/2506.17776</guid>
<content:encoded><![CDATA[
arXiv:2506.17776v2 Announce Type: replace 
Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models capable of extracting structured information from diverse and complex data sources. However, a significant challenge lies in translating these perceptual or extractive outputs into actionable, reasoned decisions within complex operational workflows. To address these challenges, this paper introduces a novel approach that integrates the outputs from various machine learning models directly with the PyReason framework, an open-world temporal logic programming reasoning engine. PyReason's foundation in generalized annotated logic allows for the seamless incorporation of real-valued outputs (e.g., probabilities, confidence scores) from diverse ML models, treating them as truth intervals within its logical framework. Crucially, PyReason provides mechanisms, implemented in Python, to continuously poll ML model outputs, convert them into logical facts, and dynamically recompute the minimal model, ensuring real-tine adaptive decision-making. Furthermore, its native support for temporal reasoning, knowledge graph integration, and fully explainable interface traces enables sophisticated analysis over time-sensitive process data and existing organizational knowledge. By combining the strengths of perception and extraction from ML models with the logical deduction and transparency of PyReason, we aim to create a powerful system for automating complex processes. This integration finds utility across numerous domains, including manufacturing, healthcare, and business operations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions</title>
<link>https://arxiv.org/abs/2507.02087</link>
<guid>https://arxiv.org/abs/2507.02087</guid>
<content:encoded><![CDATA[
arXiv:2507.02087v2 Announce Type: replace 
Abstract: The use of large language models (LLMs) in hiring promises to streamline candidate screening, but it also raises serious concerns regarding accuracy and algorithmic bias where sufficient safeguards are not in place. In this work, we benchmark several state-of-the-art foundational LLMs - including models from OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our proprietary domain-specific hiring model (Match Score) for job candidate matching. We evaluate each model's predictive accuracy (ROC AUC, Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis across declared gender, race, and intersectional subgroups). Our experiments on a dataset of roughly 10,000 real-world recent candidate-job pairs show that Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs 0.77) and achieves significantly more equitable outcomes across demographic groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957 (near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the intersectionals, respectively). We discuss why pretraining biases may cause LLMs with insufficient safeguards to propagate societal biases in hiring scenarios, whereas a bespoke supervised model can more effectively mitigate these biases. Our findings highlight the importance of domain-specific modeling and bias auditing when deploying AI in high-stakes domains such as hiring, and caution against relying on off-the-shelf LLMs for such tasks without extensive fairness safeguards. Furthermore, we show with empirical evidence that there shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a well-designed algorithm can achieve both accuracy in hiring and fairness in outcomes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tractable Representation Learning with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2507.04385</link>
<guid>https://arxiv.org/abs/2507.04385</guid>
<content:encoded><![CDATA[
arXiv:2507.04385v2 Announce Type: replace 
Abstract: Probabilistic circuits (PCs) are powerful probabilistic models that enable exact and tractable inference, making them highly suitable for probabilistic reasoning and inference tasks. While dominant in neural networks, representation learning with PCs remains underexplored, with prior approaches relying on external neural embeddings or activation-based encodings. To address this gap, we introduce autoencoding probabilistic circuits (APCs), a novel framework leveraging the tractability of PCs to model probabilistic embeddings explicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining embedding representations through tractable probabilistic inference. The PC encoder allows the framework to natively handle arbitrary missing data and is seamlessly integrated with a neural decoder in a hybrid, end-to-end trainable architecture enabled by differentiable sampling. Our empirical evaluation demonstrates that APCs outperform existing PC-based autoencoding methods in reconstruction quality, generate embeddings competitive with, and exhibit superior robustness in handling missing data compared to neural autoencoders. These results highlight APCs as a powerful and flexible representation learning method that exploits the probabilistic inference capabilities of PCs, showing promising directions for robust inference, out-of-distribution detection, and knowledge distillation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critiques of World Models</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
arXiv:2507.05169v3 Announce Type: replace 
Abstract: World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning</title>
<link>https://arxiv.org/abs/2507.06821</link>
<guid>https://arxiv.org/abs/2507.06821</guid>
<content:encoded><![CDATA[
arXiv:2507.06821v3 Announce Type: replace 
Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays a significant role in human-computer interaction (HCI) in recent years. Since different discrete emotions may exist at the same time, compared with single-class emotion recognition, emotion distribution learning (EDL) that identifies a mixture of basic emotions has gradually emerged as a trend. However, existing EDL methods face challenges in mining the heterogeneity among multiple modalities. Besides, rich semantic correlations across arbitrary basic emotions are not fully exploited. In this paper, we propose a multi-modal emotion distribution learning framework, named HeLo, aimed at fully exploring the heterogeneity and complementary information in multi-modal emotional data and label correlation within mixed basic emotions. Specifically, we first adopt cross-attention to effectively fuse the physiological data. Then, an optimal transport (OT)-based heterogeneity mining module is devised to mine the interaction and heterogeneity between the physiological and behavioral representations. To facilitate label correlation learning, we introduce a learnable label embedding optimized by correlation matrix alignment. Finally, the learnable label embeddings and label correlation matrices are integrated with the multi-modal representations through a novel label correlation-driven cross-attention mechanism for accurate emotion distribution learning. Experimental results on two publicly available datasets demonstrate the superiority of our proposed method in emotion distribution learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction</title>
<link>https://arxiv.org/abs/2507.09061</link>
<guid>https://arxiv.org/abs/2507.09061</guid>
<content:encoded><![CDATA[
arXiv:2507.09061v2 Announce Type: replace 
Abstract: We study the problem of imitating an expert demonstrator in a continuous state-and-action dynamical system. While imitation learning in discrete settings such as autoregressive language modeling has seen immense success and popularity in recent years, imitation in physical settings such as autonomous driving and robot learning has proven comparably more complex due to the compounding errors problem, often requiring elaborate set-ups to perform stably. Recent work has demonstrated that even in benign settings, exponential compounding errors are unavoidable when learning solely from expert-controlled trajectories, suggesting the need for more advanced policy parameterizations or data augmentation. To this end, we present minimal interventions that provably mitigate compounding errors in continuous state-and-action imitation learning. When the system is open-loop stable, we prescribe "action chunking," i.e., predicting and playing sequences of actions in open-loop; when the system is possibly unstable, we prescribe "noise injection," i.e., adding noise during expert demonstrations. These interventions align with popular choices in modern robot learning, though the benefits we derive are distinct from the effects they were designed to target. Our results draw insights and tools from both control theory and reinforcement learning; however, our analysis reveals novel considerations that do not naturally arise when either literature is considered in isolation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime</title>
<link>https://arxiv.org/abs/2507.11274</link>
<guid>https://arxiv.org/abs/2507.11274</guid>
<content:encoded><![CDATA[
arXiv:2507.11274v2 Announce Type: replace 
Abstract: We study population convergence guarantees of stochastic gradient descent (SGD) for smooth convex objectives in the interpolation regime, where the noise at optimum is zero or near zero. The behavior of the last iterate of SGD in this setting -- particularly with large (constant) stepsizes -- has received growing attention in recent years due to implications for the training of over-parameterized models, as well as to analyzing forgetting in continual learning and to understanding the convergence of the randomized Kaczmarz method for solving linear systems. We establish that after $T$ steps of SGD on $\beta$-smooth convex loss functions with stepsize $0 < \eta < 2/\beta$, the last iterate exhibits expected excess risk $\widetilde{O}(\frac{1}{\eta (2-\beta \eta) T^{1-\beta\eta/2}} + \frac{\eta}{(2-\beta\eta)^2} T^{\beta\eta/2} \sigma_\star^2)$, where $\sigma_\star^2$ denotes the variance of the stochastic gradients at the optimum. In particular, for a well-tuned stepsize we obtain a near optimal $\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate, extending the results of Varre et al. (2021) beyond least squares regression; and when $\sigma_\star=0$ we obtain a rate of $\smash{O(1/\sqrt T)}$ with $\eta=1/\beta$, improving upon the best-known $\smash{O(T^{-1/4})}$ rate recently established by Evron et al. (2025) in the special case of realizable linear regression.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v2 Announce Type: replace 
Abstract: Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce Video Diffusion for Action Reasoning (Vidar), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), Vidar generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Completion with Nearly Linear Samples Given Weak Side Information</title>
<link>https://arxiv.org/abs/2007.00736</link>
<guid>https://arxiv.org/abs/2007.00736</guid>
<content:encoded><![CDATA[
arXiv:2007.00736v4 Announce Type: replace-cross 
Abstract: Tensor completion exhibits an interesting computational-statistical gap in terms of the number of samples needed to perform tensor estimation. While there are only $\Theta(tn)$ degrees of freedom in a $t$-order tensor with $n^t$ entries, the best known polynomial time algorithm requires $O(n^{t/2})$ samples in order to guarantee consistent estimation. In this paper, we show that weak side information is sufficient to reduce the sample complexity to $O(n)$. The side information consists of a weight vector for each of the modes which is not orthogonal to any of the latent factors along that mode; this is significantly weaker than assuming noisy knowledge of the subspaces. We provide an algorithm that utilizes this side information to produce a consistent estimator with $O(n^{1+\kappa})$ samples for any small constant $\kappa > 0$. We also provide experiments on both synthetic and real-world datasets that validate our theoretical insights.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of high-dimensional data with spiked covariance matrix structure</title>
<link>https://arxiv.org/abs/2110.01950</link>
<guid>https://arxiv.org/abs/2110.01950</guid>
<content:encoded><![CDATA[
arXiv:2110.01950v2 Announce Type: replace-cross 
Abstract: We study the classification problem for high-dimensional data with $n$ observations on $p$ features where the $p \times p$ covariance matrix $\Sigma$ exhibits a spiked eigenvalues structure and the vector $\zeta$, given by the difference between the whitened mean vectors, is sparse with sparsity at most $s$. We propose an adaptive classifier (adaptive with respect to the sparsity $s$) that first performs dimension reduction on the feature vectors prior to classification in the dimensionally reduced space, i.e., the classifier whitened the data, then screen the features by keeping only those corresponding to the $s$ largest coordinates of $\zeta$ and finally apply Fisher linear discriminant on the selected features. Leveraging recent results on entrywise matrix perturbation bounds for covariance matrices, we show that the resulting classifier is Bayes optimal whenever $n \rightarrow \infty$ and $s \sqrt{n^{-1} \ln p} \rightarrow 0$. Experimental results on real and synthetic data sets indicate that the proposed classifier is competitive with existing state-of-the-art methods while also selecting a smaller number of features.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Validation Approach to Over-parameterized Matrix and Image Recovery</title>
<link>https://arxiv.org/abs/2209.10675</link>
<guid>https://arxiv.org/abs/2209.10675</guid>
<content:encoded><![CDATA[
arXiv:2209.10675v3 Announce Type: replace-cross 
Abstract: This paper studies the problem of recovering a low-rank matrix from several noisy random linear measurements. We consider the setting where the rank of the ground-truth matrix is unknown a priori and use an objective function built from a rank-overspecified factored representation of the matrix variable, where the global optimal solutions overfit and do not correspond to the underlying ground truth. We then solve the associated nonconvex problem using gradient descent with small random initialization. We show that as long as the measurement operators satisfy the restricted isometry property (RIP) with its rank parameter scaling with the rank of the ground-truth matrix rather than scaling with the overspecified matrix rank, gradient descent iterations are on a particular trajectory towards the ground-truth matrix and achieve nearly information-theoretically optimal recovery when it is stopped appropriately. We then propose an efficient stopping strategy based on the common hold-out method and show that it detects a nearly optimal estimator provably. Moreover, experiments show that the proposed validation approach can also be efficiently used for image restoration with deep image prior, which over-parameterizes an image with a deep network.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCK: an Algorithm for Learning Nonparametric Differential Equations via Multivariate Occupation Kernel Functions</title>
<link>https://arxiv.org/abs/2306.10189</link>
<guid>https://arxiv.org/abs/2306.10189</guid>
<content:encoded><![CDATA[
arXiv:2306.10189v4 Announce Type: replace-cross 
Abstract: Learning a nonparametric system of ordinary differential equations from trajectories in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations often scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach, the multivariate occupation kernel method (MOCK), using the implicit formulation provided by vector-valued reproducing kernel Hilbert spaces. The solution for the vector field relies on multivariate occupation kernel functions associated with the trajectories and scales linearly with the dimension of the state space. We validate through experiments on a variety of simulated and real datasets ranging from 2 to 1024 dimensions. MOCK outperforms all other comparators on 3 of the 9 datasets on full trajectory prediction and 4 out of the 9 datasets on next-point prediction.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning unitaries with quantum statistical queries</title>
<link>https://arxiv.org/abs/2310.02254</link>
<guid>https://arxiv.org/abs/2310.02254</guid>
<content:encoded><![CDATA[
arXiv:2310.02254v3 Announce Type: replace-cross 
Abstract: We propose several algorithms for learning unitary operators from quantum statistical queries with respect to their Choi-Jamiolkowski state. Quantum statistical queries capture the capabilities of a learner with limited quantum resources, which receives as input only noisy estimates of expected values of measurements. Our approach leverages quantum statistical queries to estimate the Fourier mass of a unitary on a subset of Pauli strings, generalizing previous techniques developed for uniform quantum examples. Specifically, we show that the celebrated quantum Goldreich-Levin algorithm can be implemented with quantum statistical queries, whereas the prior version of the algorithm involves oracle access to the unitary and its inverse. As an application, we prove that quantum Boolean functions with constant total influence or with constant degree are efficiently learnable in our model. Moreover, we prove that $\mathcal{O}(\log n)$-juntas are efficiently learnable and constant-depth circuits are learnable query-efficiently with quantum statistical queries. On the other hand, all previous algorithms for these tasks demand significantly greater resources, such as oracle access to the unitary or direct access to the Choi-Jamiolkowski state. We also demonstrate that, despite these positive results, quantum statistical queries lead to an exponentially larger query complexity for certain tasks, compared to separable measurements to the Choi-Jamiolkowski state. In particular, we show an exponential lower bound for learning a class of phase-oracle unitaries and a double exponential lower bound for testing the unitarity of channels. Taken together, our results indicate that quantum statistical queries offer a unified framework for various unitary learning tasks, with potential applications in quantum machine learning, many-body physics and benchmarking of near-term devices.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Random Reshuffling Method for Nonsmooth Nonconvex Finite-sum Optimization</title>
<link>https://arxiv.org/abs/2312.01047</link>
<guid>https://arxiv.org/abs/2312.01047</guid>
<content:encoded><![CDATA[
arXiv:2312.01047v3 Announce Type: replace-cross 
Abstract: Random reshuffling techniques are prevalent in large-scale applications, such as training neural networks. While the convergence and acceleration effects of random reshuffling-type methods are fairly well understood in the smooth setting, much less studies seem available in the nonsmooth case. In this work, we design a new normal map-based proximal random reshuffling (norm-PRR) method for nonsmooth nonconvex finite-sum problems. We show that norm-PRR achieves the iteration complexity ${\cal O}(n^{-1/3}T^{-2/3})$ where $n$ denotes the number of component functions $f(\cdot,i)$ and $T$ counts the total number of iterations. This improves the currently known complexity bounds for this class of problems by a factor of $n^{-1/3}$ in terms of the number of gradient evaluations. Additionally, we prove that norm-PRR converges linearly under the (global) Polyak-{\L}ojasiewicz condition and in the interpolation setting. We further complement these non-asymptotic results and provide an in-depth analysis of the asymptotic properties of norm-PRR. Specifically, under the (local) Kurdyka-{\L}ojasiewicz inequality, the whole sequence of iterates generated by norm-PRR is shown to converge to a single stationary point. Moreover, we derive last-iterate convergence rates that can match those in the smooth, strongly convex setting. Finally, numerical experiments are performed on nonconvex classification tasks to illustrate the efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2402.02339</link>
<guid>https://arxiv.org/abs/2402.02339</guid>
<content:encoded><![CDATA[
arXiv:2402.02339v2 Announce Type: replace-cross 
Abstract: Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on a projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of the pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a latent state. Projection loss is then employed to ensure the generated poses are well aligned in 2D space for high-quality optimization. Furthermore, we utilize the uncertainty of each joint to determine how much each joint is allowed for optimization. The effectiveness and superiority of the proposed framework are validated through extensive experiments on challenging datasets: Human3.6M, MPI-INF-3DHP, and 3DPW. Notably, our approach outperforms the previous best result by a large margin of 5.5\% on Human3.6M. Code is available at \href{https://github.com/xiu-cs/UAO-Pose3D}{https://github.com/xiu-cs/UAO-Pose3D}.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning</title>
<link>https://arxiv.org/abs/2403.14410</link>
<guid>https://arxiv.org/abs/2403.14410</guid>
<content:encoded><![CDATA[
arXiv:2403.14410v2 Announce Type: replace-cross 
Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy. We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.8\% and 18.9\% in H-score on VisDA, respectively. GLC++ enhances the novel category clustering accuracy of GLC by 4.1\% in open-set scenarios on Office-Home. Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies. The code is available at https://github.com/ispc-lab/GLC-plus.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent neural network wave functions for Rydberg atom arrays on kagome lattice</title>
<link>https://arxiv.org/abs/2405.20384</link>
<guid>https://arxiv.org/abs/2405.20384</guid>
<content:encoded><![CDATA[
arXiv:2405.20384v2 Announce Type: replace-cross 
Abstract: Rydberg atom array experiments have demonstrated the ability to act as powerful quantum simulators, preparing strongly-correlated phases of matter which are challenging to study for conventional computer simulations. A key direction has been the implementation of interactions on frustrated geometries, in an effort to prepare exotic many-body states such as spin liquids and glasses. In this paper, we apply two-dimensional recurrent neural network (RNN) wave functions to study the ground states of Rydberg atom arrays on the kagome lattice. We implement an annealing scheme to find the RNN variational parameters in regions of the phase diagram where exotic phases may occur, corresponding to rough optimization landscapes. For Rydberg atom array Hamiltonians studied previously on the kagome lattice, our RNN ground states show no evidence of exotic spin liquid or emergent glassy behavior. In the latter case, we argue that the presence of a non-zero Edwards-Anderson order parameter is an artifact of the long autocorrelations times experienced with quantum Monte Carlo (QMC) simulations, and we show that autocorrelations can be systematically reduced by increasing numerical effort. This result emphasizes the utility of autoregressive models, such as RNNs, in conjunction with QMC, to explore Rydberg atom array physics on frustrated lattices and beyond.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</title>
<link>https://arxiv.org/abs/2406.00222</link>
<guid>https://arxiv.org/abs/2406.00222</guid>
<content:encoded><![CDATA[
arXiv:2406.00222v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as disambiguation -- when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarification questions. Under task-specific settings, high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO), that enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under in data-efficient tuning scenarios, even when there is no action label available, using multiple real-world conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for complex SQL generation towards data analysis agents. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful Differentiable Reasoning with Reshuffled Region-based Embeddings</title>
<link>https://arxiv.org/abs/2406.09529</link>
<guid>https://arxiv.org/abs/2406.09529</guid>
<content:encoded><![CDATA[
arXiv:2406.09529v2 Announce Type: replace-cross 
Abstract: Knowledge graph (KG) embedding methods learn geometric representations of entities and relations to predict plausible missing knowledge. These representations are typically assumed to capture rule-like inference patterns. However, our theoretical understanding of which inference patterns can be captured remains limited. Ideally, KG embedding methods should be expressive enough such that for any set of rules, there exist relation embeddings that exactly capture these rules. This principle has been studied within the framework of region-based embeddings, but existing models are severely limited in the kinds of rule bases that can be captured. We argue that this stems from the fact that entity embeddings are only compared in a coordinate-wise fashion. As an alternative, we propose RESHUFFLE, a simple model based on ordering constraints that can faithfully capture a much larger class of rule bases than existing approaches. Most notably, RESHUFFLE can capture bounded inference w.r.t. arbitrary sets of closed path rules. The entity embeddings in our framework can be learned by a Graph Neural Network (GNN), which effectively acts as a differentiable rule base.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2TEA: An Agentic AI Designer for Discovery with Generative Evolutionary Multitasking</title>
<link>https://arxiv.org/abs/2406.14917</link>
<guid>https://arxiv.org/abs/2406.14917</guid>
<content:encoded><![CDATA[
arXiv:2406.14917v3 Announce Type: replace-cross 
Abstract: This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask Evolutionary Algorithm, representing the first agentic AI designer of its kind operating with generative evolutionary multitasking (GEM). LLM2TEA enables the crossbreeding of solutions from multiple domains, fostering novel solutions that transcend disciplinary boundaries. Of particular interest is the ability to discover designs that are both novel and conforming to real-world physical specifications. LLM2TEA comprises an LLM to generate genotype samples from text prompts describing target objects, a text-to-3D generative model to produce corresponding phenotypes, a classifier to interpret its semantic representations, and a computational simulator to assess its physical properties. Novel LLM-based multitask evolutionary operators are introduced to guide the search towards high-performing, practically viable designs. Experimental results in conceptual design optimization validate the effectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of novel designs over the current text-to-3D baseline. Moreover, over 73% of the generated designs outperform the top 1% of designs produced by the text-to-3D baseline in terms of physical performance. The designs produced by LLM2TEA are not only aesthetically creative but also functional in real-world contexts. Several of these designs have been successfully 3D printed, demonstrating the ability of our approach to transform AI-generated outputs into tangible, physical designs. These designs underscore the potential of LLM2TEA as a powerful tool for complex design optimization and discovery, capable of producing novel and physically viable designs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Shallow Ritz Method For 1D Diffusion-Reaction Problems</title>
<link>https://arxiv.org/abs/2407.01496</link>
<guid>https://arxiv.org/abs/2407.01496</guid>
<content:encoded><![CDATA[
arXiv:2407.01496v4 Announce Type: replace-cross 
Abstract: This paper studies the shallow Ritz method for solving one-dimensional diffusion-reaction problems. The method is capable of improving the order of approximation for non-smooth problems. By following a similar approach to the one presented in [9], we present a damped block Newton (dBN) method to achieve nearly optimal order of approximation. The dBN method optimizes the Ritz functional by alternating between the linear and non-linear parameters of the shallow ReLU neural network (NN). For diffusion-reaction problems, new difficulties arise: (1) for the linear parameters, the mass matrix is dense and even more ill-conditioned than the stiffness matrix, and (2) for the non-linear parameters, the Hessian matrix is dense and may be singular. This paper addresses these challenges, resulting in a dBN method with computational cost of ${\cal O}(n)$.
  The ideas presented for diffusion-reaction problems can also be applied to least-squares approximation problems. For both applications, starting with the non-linear parameters as a uniform partition, numerical experiments show that the dBN method moves the mesh points to nearly optimal locations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRL-AdaPart: DRL-Driven Adaptive STAR-RIS Partitioning for Fair and Frugal Resource Utilization</title>
<link>https://arxiv.org/abs/2407.06868</link>
<guid>https://arxiv.org/abs/2407.06868</guid>
<content:encoded><![CDATA[
arXiv:2407.06868v2 Announce Type: replace-cross 
Abstract: In this work, we propose a method for efficient resource utilization of simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) elements to ensure fair and high data rates. We introduce a subsurface assignment variable that determines the number of STAR-RIS elements allocated to each user and maximizes the sum of the data rates by jointly optimizing the phase shifts of the STAR-RIS and the subsurface assignment variables using an appropriately tailored deep reinforcement learning (DRL) algorithm. The proposed DRL method is also compared with a Dinkelbach algorithm and the designed hybrid DRL approach. A penalty term is incorporated into the DRL model to enhance resource utilization by intelligently deactivating STAR-RIS elements when not required. The proposed DRL method can achieve fair and high data rates for static and mobile users while ensuring efficient resource utilization through extensive simulations. Using the proposed DRL method, up to 27% and 21% of STAR-RIS elements can be deactivated in static and mobile scenarios, respectively, without affecting performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference</title>
<link>https://arxiv.org/abs/2408.06003</link>
<guid>https://arxiv.org/abs/2408.06003</guid>
<content:encoded><![CDATA[
arXiv:2408.06003v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) inference becomes resource-intensive, prompting a shift toward low-bit model weights to reduce the memory footprint and improve efficiency. Such low-bit LLMs necessitate the mixed-precision matrix multiplication (mpGEMM), an important yet underexplored operation involving the multiplication of lower-precision weights with higher-precision activations. Off-the-shelf hardware does not support this operation natively, leading to indirect, thus inefficient, dequantization-based implementations.
  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and find that a conventional LUT implementation fails to achieve the promised gains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor Core, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core differentiates itself from conventional LUT designs through: 1) software-based optimizations to minimize table precompute overhead and weight reinterpretation to reduce table storage; 2) a LUT-based Tensor Core hardware design with an elongated tiling shape to maximize table reuse and a bit-serial design to support diverse precision combinations in mpGEMM; 3) a new instruction set and compilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly outperforms existing pure software LUT implementations and achieves a 1.44$\times$ improvement in compute density and energy efficiency compared to previous state-of-the-art LUT-based accelerators.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</title>
<link>https://arxiv.org/abs/2408.13276</link>
<guid>https://arxiv.org/abs/2408.13276</guid>
<content:encoded><![CDATA[
arXiv:2408.13276v4 Announce Type: replace-cross 
Abstract: For the problem of reconstructing a low-rank matrix from a few linear measurements, two classes of algorithms have been widely studied in the literature: convex approaches based on nuclear norm minimization, and non-convex approaches that use factorized gradient descent. Under certain statistical model assumptions, it is known that nuclear norm minimization recovers the ground truth as soon as the number of samples scales linearly with the number of degrees of freedom of the ground-truth. In contrast, while non-convex approaches are computationally less expensive, existing recovery guarantees assume that the number of samples scales at least quadratically with the rank $r$ of the ground-truth matrix. In this paper, we close this gap by showing that the non-convex approaches can be as efficient as nuclear norm minimization in terms of sample complexity. Namely, we consider the problem of reconstructing a positive semidefinite matrix from a few Gaussian measurements. We show that factorized gradient descent with spectral initialization converges to the ground truth at a linear rate as soon as the number of samples scales with $ \Omega (rd\kappa^2)$, where $d$ is the dimension, and $\kappa$ is the condition number of the ground truth matrix. This improves the previous rank-dependence in the sample complexity of non-convex matrix factorization from quadratic to linear. Furthermore, we extend our theory to the noisy setting, where we show that with noisy measurements, factorized gradient descent with spectral initialization converges to the minimax optimal error up to a factor linear in $\kappa$. Our proof relies on a probabilistic decoupling argument, where we show that the gradient descent iterates are only weakly dependent on the individual entries of the measurement matrices. We expect that our proof technique is of independent interest for other non-convex problems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RF Challenge: The Data-Driven Radio Frequency Signal Separation Challenge</title>
<link>https://arxiv.org/abs/2409.08839</link>
<guid>https://arxiv.org/abs/2409.08839</guid>
<content:encoded><![CDATA[
arXiv:2409.08839v3 Announce Type: replace-cross 
Abstract: We address the critical problem of interference rejection in radio-frequency (RF) signals using a data-driven approach that leverages deep-learning methods. A primary contribution of this paper is the introduction of the RF Challenge, which is a publicly available, diverse RF signal dataset for data-driven analyses of RF signal problems. Specifically, we adopt a simplified signal model for developing and analyzing interference rejection algorithms. For this signal model, we introduce a set of carefully chosen deep learning architectures, incorporating key domain-informed modifications alongside traditional benchmark solutions to establish baseline performance metrics for this intricate, ubiquitous problem. Through extensive simulations involving eight different signal mixture types, we demonstrate the superior performance (in some cases, by two orders of magnitude) of architectures such as UNet and WaveNet over traditional methods like matched filtering and linear minimum mean square error estimation. Our findings suggest that the data-driven approach can yield scalable solutions, in the sense that the same architectures may be similarly trained and deployed for different types of signals. Moreover, these findings further corroborate the promising potential of deep learning algorithms for enhancing communication systems, particularly via interference mitigation. This work also includes results from an open competition based on the RF Challenge, hosted at the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP'24).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
<link>https://arxiv.org/abs/2409.12059</link>
<guid>https://arxiv.org/abs/2409.12059</guid>
<content:encoded><![CDATA[
arXiv:2409.12059v5 Announce Type: replace-cross 
Abstract: Current research efforts are focused on enhancing the thinking and reasoning capability of large language model (LLM) by prompting, data-driven emergence and inference-time computation. In this study, we consider stimulating language model's thinking and cognitive abilities from a modular perspective, which mimics the human brain architecture. We select a specific intermediate attention layer with newly implemented language heads. We conduct dual-layer fine-tuning by annotated (query, thought, answer) samples and show that the intermediate layer can also learn to decode fluent and reasonable language tokens. A two-pass inference mechanism is designed to generate thoughts then formal responses. The entire framework is called modularized thinking language model (MeTHanol) which can enhance LLM's cognitive behaviors as indicated by Theory of Mind (ToM) and Vignette-based experiments. Case studies also show that MeTHanol can plan and self-reflect and generate human-like thoughts and answers, even on unseen and open-domain tasks. MeTHanol can also adapt to a personalized prompt and behave as the specified character. Our study holds promise for significant cognitive gains from a modular perspective. Our code, model and data are available at https://bachozean.github.io/methanol-page
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Finite Space Mean-Field Type Games</title>
<link>https://arxiv.org/abs/2409.18152</link>
<guid>https://arxiv.org/abs/2409.18152</guid>
<content:encoded><![CDATA[
arXiv:2409.18152v3 Announce Type: replace-cross 
Abstract: Mean field type games (MFTGs) describe Nash equilibria between large coalitions: each coalition consists of a continuum of cooperative agents who maximize the average reward of their coalition while interacting non-cooperatively with a finite number of other coalitions. Although the theory has been extensively developed, we are still lacking efficient and scalable computational methods. Here, we develop reinforcement learning methods for such games in a finite space setting with general dynamics and reward functions. We start by proving that the MFTG solution yields approximate Nash equilibria in finite-size coalition games. We then propose two algorithms. The first is based on the quantization of mean-field spaces and Nash Q-learning. We provide convergence and stability analysis. We then propose a deep reinforcement learning algorithm, which can scale to larger spaces. Numerical experiments in 4 environments with mean-field distributions of dimension up to $200$ show the scalability and efficiency of the proposed method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating the Design Choice of Probability Paths in Flow Matching for Forecasting</title>
<link>https://arxiv.org/abs/2410.03229</link>
<guid>https://arxiv.org/abs/2410.03229</guid>
<content:encoded><![CDATA[
arXiv:2410.03229v3 Announce Type: replace-cross 
Abstract: Flow matching has recently emerged as a powerful paradigm for generative modeling and has been extended to probabilistic time series forecasting in latent spaces. However, the impact of the specific choice of probability path model on forecasting performance remains under-explored. In this work, we demonstrate that forecasting spatio-temporal data with flow matching is highly sensitive to the selection of the probability path model. Motivated by this insight, we propose a novel probability path model designed to improve forecasting performance. Our empirical results across various dynamical system benchmarks show that our model achieves faster convergence during training and improved predictive performance compared to existing probability path models. Importantly, our approach is efficient during inference, requiring only a few sampling steps. This makes our proposed model practical for real-world applications and opens new avenues for probabilistic forecasting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification and estimation for matrix time series CP-factor models</title>
<link>https://arxiv.org/abs/2410.05634</link>
<guid>https://arxiv.org/abs/2410.05634</guid>
<content:encoded><![CDATA[
arXiv:2410.05634v3 Announce Type: replace-cross 
Abstract: We propose a new method for identifying and estimating the CP-factor models for matrix time series. Unlike the generalized eigenanalysis-based method of Chang et al. (2023) for which the convergence rates of the associated estimators may suffer from small eigengaps as the asymptotic theory is based on some matrix perturbation analysis, the proposed new method enjoys faster convergence rates which are free from any eigengaps. It achieves this by turning the problem into a joint diagonalization of several matrices whose elements are determined by a basis of a linear system, and by choosing the basis carefully to avoid near co-linearity (see Proposition 5 and Section 4.3). Furthermore, unlike Chang et al. (2023) which requires the two factor loading matrices to be full-ranked, the proposed new method can handle rank-deficient factor loading matrices. Illustration with both simulated and real matrix time series data shows the advantages of the proposed new method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation</title>
<link>https://arxiv.org/abs/2410.19745</link>
<guid>https://arxiv.org/abs/2410.19745</guid>
<content:encoded><![CDATA[
arXiv:2410.19745v2 Announce Type: replace-cross 
Abstract: Deep learning has proven to be a highly effective tool for a wide range of applications, significantly when leveraging the power of multi-loss functions to optimize performance on multiple criteria simultaneously. However, optimal selection and weighting loss functions in deep learning tasks can significantly influence model performance, yet manual tuning of these functions is often inefficient and inflexible. We propose a novel framework called dynamic memory fusion for adaptive multi-loss function penalizing in real-time to address this. This framework leverages historical loss values data to dynamically adjust the weighting of multiple loss functions throughout the training process. Additionally, this framework integrates an auxiliary loss function to enhance model performance in the early stages. To further research horizons, we introduce the class-balanced dice loss function, designed to address class imbalance by prioritizing underrepresented classes. Experiments on breast ultrasound datasets demonstrate that the framework improves segmentation performance across various metrics. These results demonstrate the effectiveness of our proposed framework in ensuring that the model dynamically adjusts its focus to prioritize the most relevant criteria, leading to improved performance in evolving environments. The source code for our proposed methodology is publicly available on GitHub.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Wrong with Perplexity for Long-context Language Modeling?</title>
<link>https://arxiv.org/abs/2410.23771</link>
<guid>https://arxiv.org/abs/2410.23771</guid>
<content:encoded><![CDATA[
arXiv:2410.23771v5 Announce Type: replace-cross 
Abstract: Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has proven unreliable for assessing long-context capabilities. The underlying cause of this limitation has remained unclear. In this work, we provide a comprehensive explanation for this issue. We find that PPL overlooks key tokens, which are essential for long-context understanding, by averaging across all tokens and thereby obscuring the true performance of models in long-context scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that focuses on key tokens by employing a long-short context contrastive method to identify them. Our experiments demonstrate that LongPPL strongly correlates with performance on various long-context benchmarks (e.g., Pearson correlation of -0.96), significantly outperforming traditional PPL in predictive accuracy. Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a re-weighting strategy for fine-tuning that prioritizes key tokens, leading to consistent improvements across diverse benchmarks. In summary, these contributions offer deeper insights into the limitations of PPL and present effective solutions for accurately evaluating and enhancing the long-context capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing generalization in high energy physics using white-box adversarial attacks</title>
<link>https://arxiv.org/abs/2411.09296</link>
<guid>https://arxiv.org/abs/2411.09296</guid>
<content:encoded><![CDATA[
arXiv:2411.09296v3 Announce Type: replace-cross 
Abstract: Machine learning is becoming increasingly popular in the context of particle physics. Supervised learning, which uses labeled Monte Carlo (MC) simulations, remains one of the most widely used methods for discriminating signals beyond the Standard Model. However, this paper suggests that supervised models may depend excessively on artifacts and approximations from Monte Carlo simulations, potentially limiting their ability to generalize well to real data. This study aims to enhance the generalization properties of supervised models by reducing the sharpness of local minima. It reviews the application of four distinct white-box adversarial attacks in the context of classifying Higgs boson decay signals. The attacks are divided into weight-space attacks and feature-space attacks. To study and quantify the sharpness of different local minima, this paper presents two analysis methods: gradient ascent and reduced Hessian eigenvalue analysis. The results show that white-box adversarial attacks significantly improve generalization performance, albeit with increased computational complexity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everything is a Video: Unifying Modalities through Next-Frame Prediction</title>
<link>https://arxiv.org/abs/2411.10503</link>
<guid>https://arxiv.org/abs/2411.10503</guid>
<content:encoded><![CDATA[
arXiv:2411.10503v2 Announce Type: replace-cross 
Abstract: Multimodal learning, which involves integrating information from various modalities such as text, images, audio, and video, is pivotal for numerous complex tasks like visual question answering, cross-modal retrieval, and caption generation. Traditional approaches rely on modality-specific encoders and late fusion techniques, which can hinder scalability and flexibility when adapting to new tasks or modalities. To address these limitations, we introduce a novel framework that extends the concept of task reformulation beyond natural language processing (NLP) to multimodal learning. We propose to reformulate diverse multimodal tasks into a unified next-frame prediction problem, allowing a single model to handle different modalities without modality-specific components. This method treats all inputs and outputs as sequential frames in a video, enabling seamless integration of modalities and effective knowledge transfer across tasks. Our approach is evaluated on a range of tasks, including text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text, demonstrating the model's ability to generalize across modalities with minimal adaptation. We show that task reformulation can significantly simplify multimodal model design across various tasks, laying the groundwork for more generalized multimodal foundation models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Open Source Framework for Genomic Variant Calling</title>
<link>https://arxiv.org/abs/2411.11513</link>
<guid>https://arxiv.org/abs/2411.11513</guid>
<content:encoded><![CDATA[
arXiv:2411.11513v2 Announce Type: replace-cross 
Abstract: Variant calling is a fundamental task in genomic research, essential for detecting genetic variations such as single nucleotide polymorphisms (SNPs) and insertions or deletions (indels). This paper presents an enhancement to DeepChem, a widely used open-source drug discovery framework, through the integration of DeepVariant. In particular, we introduce a variant calling pipeline that leverages DeepVariant's convolutional neural network (CNN) architecture to improve the accuracy and reliability of variant detection. The implemented pipeline includes stages for realignment of sequencing reads, candidate variant detection, and pileup image generation, followed by variant classification using a modified Inception v3 model. Our work adds a modular and extensible variant calling framework to the DeepChem framework and enables future work integrating DeepChem's drug discovery infrastructure more tightly with bioinformatics pipelines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaRCE: Probabilistic and Reconstruction-based Competency Estimation for CNN-based Image Classification</title>
<link>https://arxiv.org/abs/2411.16715</link>
<guid>https://arxiv.org/abs/2411.16715</guid>
<content:encoded><![CDATA[
arXiv:2411.16715v3 Announce Type: replace-cross 
Abstract: Convolutional neural networks (CNNs) are extremely popular and effective for image classification tasks but tend to be overly confident in their predictions. Various works have sought to quantify uncertainty associated with these models, detect out-of-distribution (OOD) inputs, or identify anomalous regions in an image, but limited work has sought to develop a holistic approach that can accurately estimate perception model confidence across various sources of uncertainty. We develop a probabilistic and reconstruction-based competency estimation (PaRCE) method and compare it to existing approaches for uncertainty quantification and OOD detection. We find that our method can best distinguish between correctly classified, misclassified, and OOD samples with anomalous regions, as well as between samples with visual image modifications resulting in high, medium, and low prediction accuracy. We describe how to extend our approach for anomaly localization tasks and demonstrate the ability of our approach to distinguish between regions in an image that are familiar to the perception model from those that are unfamiliar. We find that our method generates interpretable scores that most reliably capture a holistic notion of perception model confidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Low-Rank Scaled Dot-product Attention</title>
<link>https://arxiv.org/abs/2412.03214</link>
<guid>https://arxiv.org/abs/2412.03214</guid>
<content:encoded><![CDATA[
arXiv:2412.03214v4 Announce Type: replace-cross 
Abstract: Transformers are widely used for their ability to capture data relations in sequence processing, with great success for a wide range of static tasks. However, the computational and memory footprint of their main component, i.e., the Scaled Dot-product Attention, is commonly overlooked. This makes their adoption in applications involving stream data processing with constraints in response latency, computational and memory resources infeasible. Some works have proposed methods to lower the computational cost of Transformers, i.e. low-rank approximations, sparsity in attention, and efficient formulations for Continual Inference. In this paper, we introduce a new formulation of the Scaled Dot-product Attention based on the Nystr\"om approximation that is suitable for Continual Inference. In experiments on Online Audio Classification and Online Action Detection tasks, the proposed Continual Scaled Dot-product Attention can lower the number of operations by up to three orders of magnitude compared to the original Transformers while retaining the predictive performance of competing models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Targeted Data Poisoning against Varying Physical Objects</title>
<link>https://arxiv.org/abs/2412.03908</link>
<guid>https://arxiv.org/abs/2412.03908</guid>
<content:encoded><![CDATA[
arXiv:2412.03908v2 Announce Type: replace-cross 
Abstract: Targeted data poisoning (TDP) aims to compromise the model's prediction on a specific (test) target by perturbing a small subset of training data. Existing work on TDP has focused on an overly ideal threat model in which the same image sample of the target is used during both poisoning and inference stages. However, in the real world, a target object often appears in complex variations due to changes of physical settings such as viewpoint, background, and lighting conditions. In this work, we take the first step toward understanding the real-world threats of TDP by studying its generalizability across varying physical conditions. In particular, we observe that solely optimizing gradient directions, as adopted by the best previous TDP method, achieves limited generalization. To address this limitation, we propose optimizing both the gradient direction and magnitude for more generalizable gradient matching, thereby leading to higher poisoning success rates. For instance, our method outperforms the state of the art by 19.49% when poisoning CIFAR-10 images targeting multi-view cars.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask prior-guided denoising diffusion improves inverse protein folding</title>
<link>https://arxiv.org/abs/2412.07815</link>
<guid>https://arxiv.org/abs/2412.07815</guid>
<content:encoded><![CDATA[
arXiv:2412.07815v2 Announce Type: replace-cross 
Abstract: Inverse protein folding generates valid amino acid sequences that can fold into a desired protein structure, with recent deep-learning advances showing strong potential and competitive performance. However, challenges remain, such as predicting elements with high structural uncertainty, including disordered regions. To tackle such low-confidence residue prediction, we propose a Mask-prior-guided denoising Diffusion (MapDiff) framework that accurately captures both structural information and residue interactions for inverse protein folding. MapDiff is a discrete diffusion probabilistic model that iteratively generates amino acid sequences with reduced noise, conditioned on a given protein backbone. To incorporate structural information and residue interactions, we develop a graph-based denoising network with a mask-prior pre-training strategy. Moreover, in the generative process, we combine the denoising diffusion implicit model with Monte-Carlo dropout to reduce uncertainty. Evaluation on four challenging sequence design benchmarks shows that MapDiff substantially outperforms state-of-the-art methods. Furthermore, the in silico sequences generated by MapDiff closely resemble the physico-chemical and structural characteristics of native proteins across different protein families and architectures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dark side of the forces: assessing non-conservative force models for atomistic machine learning</title>
<link>https://arxiv.org/abs/2412.11569</link>
<guid>https://arxiv.org/abs/2412.11569</guid>
<content:encoded><![CDATA[
arXiv:2412.11569v5 Announce Type: replace-cross 
Abstract: The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, has revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency, and that energy conservation can be learned during training. This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Given the difficulty in monitoring and correcting the lack of energy conservation, direct forces should be used with great care. We show that the best approach to exploit the acceleration they afford is to use them in conjunction with conservative forces. A model can be pre-trained efficiently on direct forces, then fine-tuned using backpropagation. At evaluation time, both force types can be used together to avoid unphysical effects while still benefitting almost entirely from the computational efficiency of direct forces.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models</title>
<link>https://arxiv.org/abs/2412.15748</link>
<guid>https://arxiv.org/abs/2412.15748</guid>
<content:encoded><![CDATA[
arXiv:2412.15748v2 Announce Type: replace-cross 
Abstract: Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2412.18988</link>
<guid>https://arxiv.org/abs/2412.18988</guid>
<content:encoded><![CDATA[
arXiv:2412.18988v2 Announce Type: replace-cross 
Abstract: This paper expands the cascaded network branch of the autoencoder-based multi-task learning (MTL) framework for dynamic facial expression recognition, namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder module, which is based on the Vision Transformer (ViT) architecture and employs the decoder concept of Transformer to reconstruct the multi-head attention module. The decoder output from the previous task serves as the query (Q), representing local dynamic features, while the Video Masked Autoencoder (VideoMAE) shared encoder output acts as both the key (K) and value (V), representing global dynamic features. This setup facilitates interaction between global and local dynamic features across related tasks. Additionally, this proposal aims to alleviate overfitting of complex large model. We utilize autoencoder-based multi-task cascaded learning approach to explore the impact of dynamic face detection and dynamic face landmark on dynamic facial expression recognition, which enhances the model's generalization ability. After we conduct extensive ablation experiments and comparison with state-of-the-art (SOTA) methods on various public datasets for dynamic facial expression recognition, the robustness of the MTCAE-DFER model and the effectiveness of global-local dynamic feature interaction among related tasks have been proven.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact</title>
<link>https://arxiv.org/abs/2412.19124</link>
<guid>https://arxiv.org/abs/2412.19124</guid>
<content:encoded><![CDATA[
arXiv:2412.19124v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has emerged as a promising paradigm in medical imaging, addressing the chronic challenge of limited labeled data in healthcare settings. While SSL has shown impressive results, existing studies in the medical domain are often limited in scope, focusing on specific datasets or modalities, or evaluating only isolated aspects of model performance. This fragmented evaluation approach poses a significant challenge, as models deployed in critical medical settings must not only achieve high accuracy but also demonstrate robust performance and generalizability across diverse datasets and varying conditions. To address this gap, we present a comprehensive evaluation of SSL methods within the medical domain, with a particular focus on robustness and generalizability. Using the MedMNIST dataset collection as a standardized benchmark, we evaluate 8 major SSL methods across 11 different medical datasets. Our study provides an in-depth analysis of model performance in both in-domain scenarios and the detection of out-of-distribution (OOD) samples, while exploring the effect of various initialization strategies, model architectures, and multi-domain pre-training. We further assess the generalizability of SSL methods through cross-dataset evaluations and the in-domain performance with varying label proportions (1%, 10%, and 100%) to simulate real-world scenarios with limited supervision. We hope this comprehensive benchmark helps practitioners and researchers make more informed decisions when applying SSL methods to medical applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet Losses for Remote Sensing Image Super-Resolution</title>
<link>https://arxiv.org/abs/2501.01460</link>
<guid>https://arxiv.org/abs/2501.01460</guid>
<content:encoded><![CDATA[
arXiv:2501.01460v3 Announce Type: replace-cross 
Abstract: In recent years, deep neural networks, including Convolutional Neural Networks, Transformers, and State Space Models, have achieved significant progress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing SR methods typically overlook the complementary relationship between global and local dependencies. These methods either focus on capturing local information or prioritize global information, which results in models that are unable to effectively capture both global and local features simultaneously. Moreover, their computational cost becomes prohibitive when applied to large-scale RSIs. To address these challenges, we introduce the novel application of Receptance Weighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies with linear complexity. To simultaneously model global and local features, we propose the Global-Detail dual-branch structure, GDSR, which performs SR by paralleling RWKV and convolutional operations to handle large-scale RSIs. Furthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an intermediary between the two branches to bridge their complementary roles. In addition, we propose the Dual-Group Multi-Scale Wavelet Loss, a wavelet-domain constraint mechanism via dual-group subband strategy and cross-resolution frequency alignment for enhanced reconstruction fidelity in RSI-SR. Extensive experiments under two degradation methods on several benchmarks, including AID, UCMerced, and RSSRD-QH, demonstrate that GSDR outperforms the state-of-the-art Transformer-based method HAT by an average of 0.09 dB in PSNR, while using only 63% of its parameters and 51% of its FLOPs, achieving an inference speed 3.2 times faster.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Back Home: A Computer Vision Solution to Seashell Identification for Ecological Restoration</title>
<link>https://arxiv.org/abs/2501.04873</link>
<guid>https://arxiv.org/abs/2501.04873</guid>
<content:encoded><![CDATA[
arXiv:2501.04873v3 Announce Type: replace-cross 
Abstract: Illegal souvenir collection strips an estimated five tonnes of seashells from Costa Rica's beaches each year. Yet, once these specimens are seized, their coastal origin -- Pacific or Caribbean -- cannot be verified easily due to the lack of information, preventing their return when confiscated by local authorities. To solve this issue, we introduce BackHome19K, the first large-scale image corpus (19{,}058 photographs, 516 species) annotated with coast-level labels, and propose a lightweight pipeline that infers provenance in real time on a mobile-grade CPU. A trained anomaly filter pre-screens uploads, increasing robustness to user-generated noise. On a held-out test set, the classifier attains 86.3\% balanced accuracy, while the filter rejects 93\% of 180 out-of-domain objects with zero false negatives. Deployed as a web application, the system has already processed 70{,}000 shells for wildlife officers in under three seconds per image, enabling confiscated specimens to be safely repatriated to their native ecosystems. The dataset is available at https://huggingface.co/datasets/FIFCO/BackHome19K
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative Dependence as a toolbox for machine learning : review and new developments</title>
<link>https://arxiv.org/abs/2502.07285</link>
<guid>https://arxiv.org/abs/2502.07285</guid>
<content:encoded><![CDATA[
arXiv:2502.07285v2 Announce Type: replace-cross 
Abstract: Negative dependence is becoming a key driver in advancing learning capabilities beyond the limits of traditional independence. Recent developments have evidenced support towards negatively dependent systems as a learning paradigm in a broad range of fundamental machine learning challenges including optimization, sampling, dimensionality reduction and sparse signal recovery, often surpassing the performance of current methods based on statistical independence. The most popular negatively dependent model has been that of determinantal point processes (DPPs), which have their origins in quantum theory. However, other models, such as perturbed lattice models, strongly Rayleigh measures, zeros of random functions have gained salience in various learning applications. In this article, we review this burgeoning field of research, as it has developed over the past two decades or so. We also present new results on applications of DPPs to the parsimonious representation of neural networks. In the limited scope of the article, we mostly focus on aspects of this area to which the authors contributed over the recent years, including applications to Monte Carlo methods, coresets and stochastic gradient descent, stochastic networks, signal processing and connections to quantum computation. However, starting from basics of negative dependence for the uninitiated reader, extensive references are provided to a broad swath of related developments which could not be covered within our limited scope. While existing works and reviews generally focus on specific negatively dependent models (e.g. DPPs), a notable feature of this article is that it addresses negative dependence as a machine learning methodology as a whole. In this vein, it covers within its span an array of negatively dependent models and their applications well beyond DPPs, thereby putting forward a very general and rather unique perspective.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying number theory with deep learning: a case study with the M\"obius and squarefree indicator functions</title>
<link>https://arxiv.org/abs/2502.10335</link>
<guid>https://arxiv.org/abs/2502.10335</guid>
<content:encoded><![CDATA[
arXiv:2502.10335v2 Announce Type: replace-cross 
Abstract: Building on work of Charton, we train small transformer models to calculate the M\"{o}bius function $\mu(n)$ and the squarefree indicator function $\mu^2(n)$. The models attain nontrivial predictive power. We apply a mixture of additional models and feature scoring to give a theoretical explanation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</title>
<link>https://arxiv.org/abs/2503.04036</link>
<guid>https://arxiv.org/abs/2503.04036</guid>
<content:encoded><![CDATA[
arXiv:2503.04036v3 Announce Type: replace-cross 
Abstract: Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization during pretraining, while overlooking challenges that arise in other stages of the LLM lifecycle, such as the risk of watermark filtering during data preprocessing and verification difficulties due to API-only access. To address these challenges, we propose a novel data watermarking approach that injects plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks' density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain effective after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the similarity of bandwidth-tuned quantum kernels and classical kernels</title>
<link>https://arxiv.org/abs/2503.05602</link>
<guid>https://arxiv.org/abs/2503.05602</guid>
<content:encoded><![CDATA[
arXiv:2503.05602v3 Announce Type: replace-cross 
Abstract: Quantum kernels (QK) are widely used in quantum machine learning applications; yet, their potential to surpass classical machine learning methods on classical datasets remains uncertain. This limitation can be attributed to the exponential concentration phenomenon, which can impair generalization. A common strategy to alleviate this is bandwidth tuning, which involves rescaling data points in the quantum model to improve generalization. In this work, we numerically demonstrate that optimal bandwidth tuning results in QKs that closely resemble radial basis function (RBF) kernels, leading to a lack of quantum advantage over classical methods. Moreover, we reveal that the size of optimal bandwidth tuning parameters further simplifies QKs, causing them to behave like polynomial kernels, corresponding to a low-order Taylor approximation of a RBF kernel. We thoroughly investigate this for fidelity quantum kernels and projected quantum kernels using various data encoding circuits across several classification datasets. We provide numerical evidence and derive a simple analytical model that elucidates how bandwidth tuning influences key quantities in classification tasks. Overall, our findings shed light on the mechanisms that render QK methods classically tractable.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are ECGs enough? Deep learning classification of pulmonary embolism using electrocardiograms</title>
<link>https://arxiv.org/abs/2503.08960</link>
<guid>https://arxiv.org/abs/2503.08960</guid>
<content:encoded><![CDATA[
arXiv:2503.08960v2 Announce Type: replace-cross 
Abstract: Pulmonary embolism is a leading cause of out of hospital cardiac arrest that requires fast diagnosis. While computed tomography pulmonary angiography is the standard diagnostic tool, it is not always accessible. Electrocardiography is an essential tool for diagnosing mul- tiple cardiac anomalies, as it is affordable, fast and available in many settings. However, the availability of public ECG datasets, specially for PE, is limited and, in practice, these datasets tend to be small, making it essential to optimize learning strategies. In this study, we investigate the performance of multiple neural networks in order to assess the impact of various approaches. Moreover, we check whether these practices enhance model generalization when transfer learning is used to translate infor- mation learned in larger ECG datasets, such as PTB-XL, CPSC18 and MedalCare-XL, to a smaller, more challenging dataset for PE. By lever- aging transfer learning, we analyze the extent to which we can improve learning efficiency and predictive performance on limited data. Code available at https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Learning over Arbitrary Topology: Linear Speed-Up with Polynomial Transient Time</title>
<link>https://arxiv.org/abs/2503.16123</link>
<guid>https://arxiv.org/abs/2503.16123</guid>
<content:encoded><![CDATA[
arXiv:2503.16123v2 Announce Type: replace-cross 
Abstract: We study a distributed learning problem in which $n$ agents, each with potentially heterogeneous local data, collaboratively minimize the sum of their local cost functions via peer-to-peer communication. We propose a novel algorithm, \emph{Spanning Tree Push-Pull} (STPP), which employs two spanning trees extracted from a general communication graph to distribute both model parameters and stochastic gradients. Unlike prior approaches that rely heavily on spectral gap properties, STPP leverages a more flexible topological characterization, enabling robust information flow and efficient updates. Theoretically, we prove that STPP achieves linear speedup and polynomial transient iteration complexity -- up to $\mathcal{O}(n^7)$ for smooth nonconvex objectives and $\tilde{\mathcal{O}}(n^3)$ for smooth strongly convex objectives -- under arbitrary network topologies. Moreover, compared with existing methods, STPP achieves faster convergence rates on sparse and non-regular topologies (e.g., directed rings) and reduces communication overhead on dense networks (e.g., static exponential graphs). Numerical experiments further demonstrate the strong performance of STPP across various graph architectures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</title>
<link>https://arxiv.org/abs/2503.16421</link>
<guid>https://arxiv.org/abs/2503.16421</guid>
<content:encoded><![CDATA[
arXiv:2503.16421v2 Announce Type: replace-cross 
Abstract: Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aether: Geometric-Aware Unified World Modeling</title>
<link>https://arxiv.org/abs/2503.18945</link>
<guid>https://arxiv.org/abs/2503.18945</guid>
<content:encoded><![CDATA[
arXiv:2503.18945v3 Announce Type: replace-cross 
Abstract: The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates zero-shot synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Notably, even without real-world data, its reconstruction performance is comparable with or even better than that of domain-specific models. Additionally, Aether employs camera trajectories as geometry-informed action spaces, enabling effective action-conditioned prediction and visual planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaved Multitask Learning with Energy Modulated Learning Progress</title>
<link>https://arxiv.org/abs/2504.00707</link>
<guid>https://arxiv.org/abs/2504.00707</guid>
<content:encoded><![CDATA[
arXiv:2504.00707v2 Announce Type: replace-cross 
Abstract: As humans learn new skills and apply their existing knowledge while maintaining previously learned information, "continual learning" in machine learning aims to incorporate new data while retaining and utilizing past knowledge. However, existing machine learning methods often does not mimic human learning where tasks are intermixed due to individual preferences and environmental conditions. Humans typically switch between tasks instead of completely mastering one task before proceeding to the next. To explore how human-like task switching can enhance learning efficiency, we propose a multi task learning architecture that alternates tasks based on task-agnostic measures such as "learning progress" and "neural computational energy expenditure". To evaluate the efficacy of our method, we run several systematic experiments by using a set of effect-prediction tasks executed by a simulated manipulator robot. The experiments show that our approach surpasses random interleaved and sequential task learning in terms of average learning accuracy. Moreover, by including energy expenditure in the task switching logic, our approach can still perform favorably while reducing neural energy expenditure.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</title>
<link>https://arxiv.org/abs/2504.02263</link>
<guid>https://arxiv.org/abs/2504.02263</guid>
<content:encoded><![CDATA[
arXiv:2504.02263v4 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity. However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs.
  We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization. Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model</title>
<link>https://arxiv.org/abs/2504.12039</link>
<guid>https://arxiv.org/abs/2504.12039</guid>
<content:encoded><![CDATA[
arXiv:2504.12039v2 Announce Type: replace-cross 
Abstract: Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as Vision Transformer (ViT) and State-Space Model (SSM) architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: https://github.com/lab-emi/AIRHAR.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization: A Close Look at Books</title>
<link>https://arxiv.org/abs/2504.12549</link>
<guid>https://arxiv.org/abs/2504.12549</guid>
<content:encoded><![CDATA[
arXiv:2504.12549v2 Announce Type: replace-cross 
Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the "prefix-prompting" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars</title>
<link>https://arxiv.org/abs/2504.17562</link>
<guid>https://arxiv.org/abs/2504.17562</guid>
<content:encoded><![CDATA[
arXiv:2504.17562v2 Announce Type: replace-cross 
Abstract: The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v2 Announce Type: replace-cross 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback e.g. "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own" into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs</title>
<link>https://arxiv.org/abs/2505.14699</link>
<guid>https://arxiv.org/abs/2505.14699</guid>
<content:encoded><![CDATA[
arXiv:2505.14699v2 Announce Type: replace-cross 
Abstract: The automatic analysis of document layouts in digital-born PDF documents remains a challenging problem due to the heterogeneous arrangement of textual and nontextual elements and the imprecision of the textual metadata in the Portable Document Format. In this work, we benchmark Graph Neural Network (GNN) architectures for the task of fine-grained layout classification of text blocks from digital native documents. We introduce two graph construction structures: a k-closest-neighbor graph and a fully connected graph, and generate node features via pre-trained text and vision models, thus avoiding manual feature engineering. Three experimental frameworks are evaluated: single-modality (text or visual), concatenated multimodal, and dual-branch multimodal. We evaluated four foundational GNN models and compared them with the baseline. Our experiments are specifically conducted on a rich dataset of public affairs documents that includes more than 20 sources (e.g., regional and national-level official gazettes), 37K PDF documents, with 441K pages in total. Our results demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a dual-branch configuration achieves the highest per-class and overall accuracy, outperforming the baseline in some sources. These findings confirm the importance of local layout relationships and multimodal fusion exploited through GNNs for the analysis of native digital document layouts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v4 Announce Type: replace-cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards</title>
<link>https://arxiv.org/abs/2505.16789</link>
<guid>https://arxiv.org/abs/2505.16789</guid>
<content:encoded><![CDATA[
arXiv:2505.16789v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) gain popularity, their vulnerability to adversarial attacks emerges as a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can inadvertently introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Vulnerability, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity across multiple experimental datasets. We then evaluate the adversarial robustness of these fine-tuned models, analyzing persona shifts and interpretability traits to understand how dataset factors contribute to attack success rates. Lastly, we explore causal relationships that offer new insights into adversarial defense strategies, highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_vulnerability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
arXiv:2505.17067v3 Announce Type: replace-cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Person Interaction Generation from Two-Person Motion Priors</title>
<link>https://arxiv.org/abs/2505.17860</link>
<guid>https://arxiv.org/abs/2505.17860</guid>
<content:encoded><![CDATA[
arXiv:2505.17860v2 Announce Type: replace-cross 
Abstract: Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset</title>
<link>https://arxiv.org/abs/2505.20788</link>
<guid>https://arxiv.org/abs/2505.20788</guid>
<content:encoded><![CDATA[
arXiv:2505.20788v2 Announce Type: replace-cross 
Abstract: Wearable human activity recognition has been shown to benefit from the inclusion of acoustic data, as the sounds around a person often contain valuable context. However, due to privacy concerns, it is usually not ethically feasible to record and save microphone data from the device, since the audio could, for instance, also contain private conversations. Rather, the data should be processed locally, which in turn requires processing power and consumes energy on the wearable device. One special use case of contextual information that can be utilized to augment special tasks in human activity recognition is water flow detection, which can, e.g., be used to aid wearable hand washing detection. We created a new label called tap water for the recently released HD-Epic data set, creating 717 hand-labeled annotations of tap water flow, based on existing annotations of the water class. We analyzed the relation of tap water and water in the dataset and additionally trained and evaluated two lightweight classifiers to evaluate the newly added label class, showing that the new class can be learned more easily.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGNIS: A Robust Neural Network Framework for Constrained Parameter Estimation in Archimedean Copulas</title>
<link>https://arxiv.org/abs/2505.22518</link>
<guid>https://arxiv.org/abs/2505.22518</guid>
<content:encoded><![CDATA[
arXiv:2505.22518v3 Announce Type: replace-cross 
Abstract: Classical estimators, the cornerstones of statistical inference, face insurmountable challenges when applied to important emerging classes of Archimedean copulas. These models exhibit pathological properties, including numerically unstable densities, non-monotonic parameter-to-dependence mappings, and vanishingly small likelihood gradients, rendering methods like Maximum Likelihood (MLE) and Method of Moments (MoM) inconsistent or computationally infeasible. We introduce IGNIS, a unified neural estimation framework that sidesteps these barriers by learning a direct, robust mapping from data-driven dependency measures to the underlying copula parameter theta. IGNIS utilizes a multi-input architecture and a theory-guided output layer (softplus(z) + 1) to automatically enforce the domain constraint theta_hat >= 1. Trained and validated on four families (Gumbel, Joe, and the numerically challenging A1/A2), IGNIS delivers accurate and stable estimates for real-world financial and health datasets, demonstrating its necessity for reliable inference in modern, complex dependence models where traditional methods fail.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing</title>
<link>https://arxiv.org/abs/2505.23145</link>
<guid>https://arxiv.org/abs/2505.23145</guid>
<content:encoded><![CDATA[
arXiv:2505.23145v4 Announce Type: replace-cross 
Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose {\em FlowAlign}, a novel inversion-free flow-based framework for consistent image editing with optimal control-based trajectory control. Specifically, FlowAlign introduces source similarity at the terminal point as a regularization term to promote smoother and more consistent trajectories during the editing process. Notably, our terminal point regularization is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highliting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Physical Reasoning with the PHYSICS Dataset</title>
<link>https://arxiv.org/abs/2506.00022</link>
<guid>https://arxiv.org/abs/2506.00022</guid>
<content:encoded><![CDATA[
arXiv:2506.00022v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint modeling for learning decision-making dynamics in behavioral experiments</title>
<link>https://arxiv.org/abs/2506.02394</link>
<guid>https://arxiv.org/abs/2506.02394</guid>
<content:encoded><![CDATA[
arXiv:2506.02394v2 Announce Type: replace-cross 
Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality, is associated with reward-processing abnormalities and concentration issues. Motivated by the probabilistic reward task from the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel framework that integrates the reinforcement learning (RL) model and drift-diffusion model (DDM) to jointly analyze reward-based decision-making with response times. To account for emerging evidence suggesting that decision-making may alternate between multiple interleaved strategies, we model latent state switching using a hidden Markov model (HMM). In the ''engaged'' state, decisions follow an RL-DDM, simultaneously capturing reward processing, decision dynamics, and temporal structure. In contrast, in the ''lapsed'' state, decision-making is modeled using a simplified DDM, where specific parameters are fixed to approximate random guessing with equal probability. The proposed method is implemented using a computationally efficient generalized expectation-maximization (EM) algorithm with forward-backward procedures. Through extensive numerical studies, we demonstrate that our proposed method outperforms competing approaches across various reward-generating distributions, under both strategy-switching and non-switching scenarios, as well as in the presence of input perturbations. When applied to the EMBARC study, our framework reveals that MDD patients exhibit lower overall engagement than healthy controls and experience longer decision times when they do engage. Additionally, we show that neuroimaging measures of brain activities are associated with decision-making characteristics in the ''engaged'' state but not in the ''lapsed'' state, providing evidence of brain-behavior association specific to the ''engaged'' state.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Before Filtering: Real-Time Hardware Learning at the Detector Level</title>
<link>https://arxiv.org/abs/2506.11981</link>
<guid>https://arxiv.org/abs/2506.11981</guid>
<content:encoded><![CDATA[
arXiv:2506.11981v2 Announce Type: replace-cross 
Abstract: Advances in sensor technology and automation have ushered in an era of data abundance, where the ability to identify and extract relevant information in real time has become increasingly critical. Traditional filtering approaches, which depend on a priori knowledge, often struggle to adapt to dynamic or unanticipated data features. Machine learning offers a compelling alternative-particularly when training can occur directly at or near the detector. This paper presents a digital hardware architecture designed for real-time neural network training, specifically optimized for high-throughput data ingestion. The design is described in an implementation-independent manner, with detailed analysis of each architectural component and their performance implications. Through system parameterization, the study explores trade-offs between processing speed, model complexity, and hardware resource utilization. Practical examples illustrate how these parameters affect applicability across various use cases. A proof-of-concept implementation on an FPGA demonstrates in-situ training, confirming that computational accuracy is preserved relative to conventional software-based approaches. Moreover, resource estimates indicate that current-generation FPGAs can train networks of approximately 3,500 neurons per chip. The architecture is both scalable and adaptable, representing a significant advancement toward integrating learning directly within detector systems and enabling a new class of extreme-edge, real-time information processing.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Deep Lagrangian Networks for Model Predictive Control</title>
<link>https://arxiv.org/abs/2506.15249</link>
<guid>https://arxiv.org/abs/2506.15249</guid>
<content:encoded><![CDATA[
arXiv:2506.15249v3 Announce Type: replace-cross 
Abstract: Controlling a robot based on physics-consistent dynamic models, such as Deep Lagrangian Networks (DeLaN), can improve the generalizability and interpretability of the resulting behavior. However, in complex environments, the number of objects to potentially interact with is vast, and their physical properties are often uncertain. This complexity makes it infeasible to employ a single global model. Therefore, we need to resort to online system identification of context-aware models that capture only the currently relevant aspects of the environment. While physical principles such as the conservation of energy may not hold across varying contexts, ensuring physical plausibility for any individual context-aware model can still be highly desirable, particularly when using it for receding horizon control methods such as model predictive control (MPC). Hence, in this work, we extend DeLaN to make it context-aware, combine it with a recurrent network for online system identification, and integrate it with an MPC for adaptive, physics-consistent control. We also combine DeLaN with a residual dynamics model to leverage the fact that a nominal model of the robot is typically available. We evaluate our method on a 7-DOF robot arm for trajectory tracking under varying loads. Our method reduces the end-effector tracking error by 39%, compared to a 21% improvement achieved by a baseline that uses an extended Kalman filter.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Safety Shielding for Imperfect-Perception Agents</title>
<link>https://arxiv.org/abs/2506.17275</link>
<guid>https://arxiv.org/abs/2506.17275</guid>
<content:encoded><![CDATA[
arXiv:2506.17275v2 Announce Type: replace-cross 
Abstract: We consider the problem of safe control in discrete autonomous agents that use learned components for imperfect perception (or more generally, state estimation) from high-dimensional observations. We propose a shield construction that provides run-time safety guarantees under perception errors by restricting the actions available to an agent, modeled as a Markov decision process, as a function of the state estimates. Our construction uses conformal prediction for the perception component, which guarantees that for each observation, the predicted set of estimates includes the actual state with a user-specified probability. The shield allows an action only if it is allowed for all the estimates in the predicted set, resulting in local safety. We also articulate and prove a global safety property of existing shield constructions for perfect-perception agents bounding the probability of reaching unsafe states if the agent always chooses actions prescribed by the shield. We illustrate our approach with a case-study of an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First-Order Sparse Convex Optimization: Better Rates with Sparse Updates</title>
<link>https://arxiv.org/abs/2506.19075</link>
<guid>https://arxiv.org/abs/2506.19075</guid>
<content:encoded><![CDATA[
arXiv:2506.19075v2 Announce Type: replace-cross 
Abstract: In was recently established that for convex optimization problems with a sparse optimal solution (may it be entry-wise sparsity or matrix rank-wise sparsity) it is possible to have linear convergence rates which depend on an improved mixed-norm condition number of the form $\frac{\beta_1{}s}{\alpha_2}$, where $\beta_1$ is the $\ell_1$-Lipchitz continuity constant of the gradient, $\alpha_2$ is the $\ell_2$-quadratic growth constant, and $s$ is the sparsity of the optimal solution. However, beyond the improved convergence rate, these methods are unable to leverage the sparsity of optimal solutions towards improving also the runtime of each iteration, which may still be prohibitively high for high-dimensional problems. In this work, we establish that linear convergence rates which depend on this improved condition number can be obtained using only sparse updates, which may result in overall significantly improved running times. Moreover, our methods are considerably easier to implement.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prover Agent: An Agent-based Framework for Formal Mathematical Proofs</title>
<link>https://arxiv.org/abs/2506.19923</link>
<guid>https://arxiv.org/abs/2506.19923</guid>
<content:encoded><![CDATA[
arXiv:2506.19923v2 Announce Type: replace-cross 
Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 86.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present case studies illustrating how these generated lemmas contribute to solving challenging problems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization</title>
<link>https://arxiv.org/abs/2506.20056</link>
<guid>https://arxiv.org/abs/2506.20056</guid>
<content:encoded><![CDATA[
arXiv:2506.20056v2 Announce Type: replace-cross 
Abstract: Photonic device development (PDD) has achieved remarkable success in designing and implementing new devices for controlling light across various wavelengths, scales, and applications, including telecommunications, imaging, sensing, and quantum information processing. PDD is an iterative, five-step process that consists of: i) deriving device behavior from design parameters, ii) simulating device performance, iii) finding the optimal candidate designs from simulations, iv) fabricating the optimal device, and v) measuring device performance. Classically, all these steps involve Bayesian optimization, material science, control theory, and direct physics-driven numerical methods. However, many of these techniques are computationally intractable, monetarily costly, or difficult to implement at scale. In addition, PDD suffers from large optimization landscapes, uncertainties in structural or optical characterization, and difficulties in implementing robust fabrication processes. However, the advent of machine learning over the past decade has provided novel, data-driven strategies for tackling these challenges, including surrogate estimators for speeding up computations, generative modeling for noisy measurement modeling and data augmentation, reinforcement learning for fabrication, and active learning for experimental physical discovery. In this review, we present a comprehensive perspective on these methods to enable machine-learning-assisted PDD (ML-PDD) for efficient design optimization with powerful generative models, fast simulation and characterization modeling under noisy measurements, and reinforcement learning for fabrication. This review will provide researchers from diverse backgrounds with valuable insights into this emerging topic, fostering interdisciplinary efforts to accelerate the development of complex photonic devices and systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps</title>
<link>https://arxiv.org/abs/2507.01397</link>
<guid>https://arxiv.org/abs/2507.01397</guid>
<content:encoded><![CDATA[
arXiv:2507.01397v2 Announce Type: replace-cross 
Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps. Current research aims to address this constraint by directly predicting HD map elements from onboard sensors and reasoning about the relationships between the predicted map and traffic elements. Despite recent advancements, the coherent online construction of HD maps remains a challenging endeavor, as it necessitates modeling the high complexity of road topologies in a unified and consistent manner. To address this challenge, we propose a coherent approach to predict lane segments and their corresponding topology, as well as road boundaries, all by leveraging prior map information represented by commonly available standard-definition (SD) maps. We propose a network architecture, which leverages hybrid lane segment encodings comprising prior information and denoising techniques to enhance training stability and performance. Furthermore, we facilitate past frames for temporal consistency. Our experimental evaluation demonstrates that our approach outperforms previous methods by a large margin, highlighting the benefits of our modeling scheme.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?</title>
<link>https://arxiv.org/abs/2507.04632</link>
<guid>https://arxiv.org/abs/2507.04632</guid>
<content:encoded><![CDATA[
arXiv:2507.04632v3 Announce Type: replace-cross 
Abstract: Recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs). The optimization process often requires numerous iterations to achieve satisfactory performance, resulting in high computational costs due to the need for frequent prompt evaluations under intensive LLM interactions and repeated policy updates. Appropriate online prompt selection methods reduce iteration steps by prioritizing informative prompts during training, while the pipeline's reliance on exhaustive prompt evaluation and subset selection for optimization still incurs substantial computational overhead due to frequent LLM inference calls. Distinguished from these direct evaluate-then-select schemes, this work investigates iterative approximate evaluation for arbitrary prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions. Technically, MoPPS models each prompt's success rate as a latent variable, performs streaming Bayesian inference, and employs posterior sampling in a constructed multi-armed bandit machine, enabling sample efficient and adaptive prompt selection. Extensive experiments across mathematics, planning, and vision-based geometry tasks show that MoPPS reliably predicts prompt difficulty and accelerates training with significantly reduced LLM rollouts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Manual Annotation: A Human-AI Collaborative Framework for Medical Image Segmentation Using Only "Better or Worse" Expert Feedback</title>
<link>https://arxiv.org/abs/2507.05815</link>
<guid>https://arxiv.org/abs/2507.05815</guid>
<content:encoded><![CDATA[
arXiv:2507.05815v2 Announce Type: replace-cross 
Abstract: Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel hands-free Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback -- simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Adaptations for Vision AutoRegressive Model</title>
<link>https://arxiv.org/abs/2507.11441</link>
<guid>https://arxiv.org/abs/2507.11441</guid>
<content:encoded><![CDATA[
arXiv:2507.11441v2 Announce Type: replace-cross 
Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at https://github.com/sprintml/finetuning_var_dp.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoftPipe: A Soft-Guided Reinforcement Learning Framework for Automated Data Preparation</title>
<link>https://arxiv.org/abs/2507.13710</link>
<guid>https://arxiv.org/abs/2507.13710</guid>
<content:encoded><![CDATA[
arXiv:2507.13710v2 Announce Type: replace-cross 
Abstract: Data preparation is a foundational yet notoriously challenging component of the machine learning lifecycle, characterized by a vast combinatorial search space. While reinforcement learning (RL) offers a promising direction, state-of-the-art methods suffer from a critical limitation: to manage the search space, they rely on rigid ``hard constraints'' that prematurely prune the search space and often preclude optimal solutions. To address this, we introduce SoftPipe, a novel RL framework that replaces these constraints with a flexible ``soft guidance'' paradigm. SoftPipe formulates action selection as a Bayesian inference problem. A high-level strategic prior, generated by a Large Language Model (LLM), probabilistically guides exploration. This prior is combined with empirical estimators from two sources through a collaborative process: a fine-grained quality score from a supervised Learning-to-Rank (LTR) model and a long-term value estimate from the agent's Q-function. Through extensive experiments on 18 diverse datasets, we demonstrate that SoftPipe achieves up to a 13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
arXiv:2507.14111v4 Announce Type: replace-cross 
Abstract: The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.
  CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.
  The capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models</title>
<link>https://arxiv.org/abs/2506.19037</link>
<guid>https://arxiv.org/abs/2506.19037</guid>
<content:encoded><![CDATA[
<div> Masked diffusion language models (MDLMs), Dilated Unmasking Scheduler (DUS), non-autoregressive text generation, parallel unmasking strategies, planner-model-free method.<br />
<br />
Summary: Masked diffusion language models promise fast, non-autoregressive text generation, but existing samplers can reduce to slow, autoregressive behavior. The Dilated Unmasking Scheduler (DUS) proposes a planner-model-free method that partitions sequence positions into non-adjacent dilated groups to unmask them in parallel, minimizing joint entropy gain. DUS outperforms confidence-based planners across math, code, and general-knowledge benchmarks, without modifying the denoiser, revealing the true speed-quality frontier of MDLMs. <div>
arXiv:2506.19037v3 Announce Type: replace-cross 
Abstract: Masked diffusion language models (MDLMs) promise fast, non-autoregressive text generation, yet existing samplers, which pick tokens to unmask based on model confidence, ignore interactions when unmasking multiple positions in parallel and effectively reduce to slow, autoregressive behavior. We propose the Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free method that partitions sequence positions into non-adjacent dilated groups and unmasked them in parallel so as to minimize an upper bound on joint entropy gain at each denoising step. By explicitly trading off the number of network calls against generation quality, DUS recovers most of the performance lost under traditional parallel unmasking strategies. Across math (GSM8K, MATH500), code (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS outperforms confidence-based planners, without modifying the underlying denoiser, and reveals the true speed-quality frontier of MDLMs.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</title>
<link>https://arxiv.org/abs/2507.04790</link>
<guid>https://arxiv.org/abs/2507.04790</guid>
<content:encoded><![CDATA[
<div> Motion planning, autonomous robot driving, trajectory datasets, domain adaptation, ensemble learning<br />
<br />
Summary:<br />
Motion planning is crucial for autonomous robot driving. However, effectively utilizing trajectory datasets for a target domain is challenging due to differences in agent interactions and environments. Conventional approaches like domain adaptation and ensemble learning face issues such as domain imbalance and high computational costs. To overcome these challenges, the Interaction-Merged Motion Planning (IMMP) approach is proposed. IMMP involves pre-merging to capture agent behaviors and interactions from different domains and merging to create an adaptable model transferring diverse interactions efficiently. The method is tested on planning benchmarks, showing superior performance to conventional methods. <div>
arXiv:2507.04790v3 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance</title>
<link>https://arxiv.org/abs/2507.18654</link>
<guid>https://arxiv.org/abs/2507.18654</guid>
<content:encoded><![CDATA[
<div> Diffusion models, guidance mechanism, inverse problems, piecewise guidance scheme, image restoration<br />
<br />
Summary:<br />
Diffusion models, equipped with a guidance mechanism, can efficiently solve inverse problems by using a piecewise guidance scheme. This framework balances computational efficiency and accuracy, adapting to various inverse problems without the need for retraining. The method explicitly considers measurement noise in the reconstruction process. Experimental results on image restoration tasks like inpainting and super-resolution show the effectiveness of the proposed framework. Using a class conditional diffusion model, the framework reduces inference time by 25% for inpainting and 23% and 24% for 4x and 8x super-resolution tasks, respectively, compared to the baseline, with only minimal loss in PSNR and SSIM. <div>
arXiv:2507.18654v1 Announce Type: new 
Abstract: Diffusion models are powerful tools for sampling from high-dimensional distributions by progressively transforming pure noise into structured data through a denoising process. When equipped with a guidance mechanism, these models can also generate samples from conditional distributions. In this paper, a novel diffusion-based framework is introduced for solving inverse problems using a piecewise guidance scheme. The guidance term is defined as a piecewise function of the diffusion timestep, facilitating the use of different approximations during high-noise and low-noise phases. This design is shown to effectively balance computational efficiency with the accuracy of the guidance term. Unlike task-specific approaches that require retraining for each problem, the proposed method is problem-agnostic and readily adaptable to a variety of inverse problems. Additionally, it explicitly incorporates measurement noise into the reconstruction process. The effectiveness of the proposed framework is demonstrated through extensive experiments on image restoration tasks, specifically image inpainting and super-resolution. Using a class conditional diffusion model for recovery, compared to the \pgdm baseline, the proposed framework achieves a reduction in inference time of \(25\%\) for inpainting with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\) and \(8\times\) super-resolution tasks, respectively, while incurring only negligible loss in PSNR and SSIM.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs</title>
<link>https://arxiv.org/abs/2507.18668</link>
<guid>https://arxiv.org/abs/2507.18668</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Knowledge Tracing, Computational Efficiency, Subgraph-based Approach, Resource Efficiency

Summary:
Dual Graph Attention-based Knowledge Tracing (DGAKT) is introduced as a solution to the computationally expensive problem of utilizing large graphs and long learning sequences in online learning. DGAKT leverages high-order information from subgraphs representing student-exercise-KC relationships, using a subgraph-based approach to enhance computational efficiency. By processing only relevant subgraphs for each target interaction, DGAKT reduces memory and computational requirements compared to global graph models. Experimental results show that DGAKT outperforms existing KT models and sets a new standard in resource efficiency. This addresses a critical need in online learning that has been largely overlooked by previous knowledge tracing approaches. 

<br /><br />Summary: <div>
arXiv:2507.18668v1 Announce Type: new 
Abstract: The rise of online learning has led to the development of various knowledge tracing (KT) methods. However, existing methods have overlooked the problem of increasing computational cost when utilizing large graphs and long learning sequences. To address this issue, we introduce Dual Graph Attention-based Knowledge Tracing (DGAKT), a graph neural network model designed to leverage high-order information from subgraphs representing student-exercise-KC relationships. DGAKT incorporates a subgraph-based approach to enhance computational efficiency. By processing only relevant subgraphs for each target interaction, DGAKT significantly reduces memory and computational requirements compared to full global graph models. Extensive experimental results demonstrate that DGAKT not only outperforms existing KT models but also sets a new standard in resource efficiency, addressing a critical need that has been largely overlooked by prior KT approaches.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling</title>
<link>https://arxiv.org/abs/2507.18671</link>
<guid>https://arxiv.org/abs/2507.18671</guid>
<content:encoded><![CDATA[
<div> inductive, fine-grained, upcycle, Mixtures-of-Experts, reasoning <br />
Summary: <br />
The article introduces Innovator, a model that addresses the issue of catastrophic forgetting in large language models (LLMs) during continued pretraining on scientific data. Innovator uses a Mixtures-of-Experts model with specialized experts for different scientific disciplines and a shared expert for general tasks. It follows a four-stage upcycle training paradigm including scientific expert induction, fine-grained expert splitting, science-aware routing, and generalist-scientist integration. Innovator extends a pre-existing model with additional parameters and achieves significant improvements in 30 scientific tasks while maintaining performance in general tasks. Additionally, Innovator-Reason, a variant of Innovator, demonstrates enhanced reasoning capabilities in solving complex scientific problems. <div>
arXiv:2507.18671v1 Announce Type: new 
Abstract: A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Market Making Strategies with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.18680</link>
<guid>https://arxiv.org/abs/2507.18680</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Market Making, Financial Markets, Deep Reinforcement Learning, Multi-Objective Reinforcement Learning

Summary: 
This thesis explores the application of Reinforcement Learning (RL), particularly Deep Reinforcement Learning (DRL), to market making in financial markets. It formulates the market making task as a RL problem and designs agents for single-agent and multi-agent settings in simulated financial environments. The study addresses inventory management using reward engineering and Multi-Objective Reinforcement Learning (MORL) to balance competing objectives. To tackle non-stationarity, a novel policy weighting algorithm called POW-dTS is introduced based on Discounted Thompson Sampling. Experimental results show that RL-based approaches outperform traditional strategies. The research contributes new methodologies for designing effective market making agents, showcasing the potential of RL in revolutionizing algorithmic trading in complex financial systems.<br /><br />Summary: <div>
arXiv:2507.18680v1 Announce Type: new 
Abstract: This thesis presents the results of a comprehensive research project focused on applying Reinforcement Learning (RL) to the problem of market making in financial markets. Market makers (MMs) play a fundamental role in providing liquidity, yet face significant challenges arising from inventory risk, competition, and non-stationary market dynamics. This research explores how RL, particularly Deep Reinforcement Learning (DRL), can be employed to develop autonomous, adaptive, and profitable market making strategies.
  The study begins by formulating the MM task as a reinforcement learning problem, designing agents capable of operating in both single-agent and multi-agent settings within a simulated financial environment. It then addresses the complex issue of inventory management using two complementary approaches: reward engineering and Multi-Objective Reinforcement Learning (MORL). While the former uses dynamic reward shaping to guide behavior, the latter leverages Pareto front optimization to explicitly balance competing objectives.
  To address the problem of non-stationarity, the research introduces POW-dTS, a novel policy weighting algorithm based on Discounted Thompson Sampling. This method allows agents to dynamically select and combine pretrained policies, enabling continual adaptation to shifting market conditions.
  The experimental results demonstrate that the proposed RL-based approaches significantly outperform traditional and baseline algorithmic strategies across various performance metrics. Overall, this research thesis contributes new methodologies and insights for the design of robust, efficient, and adaptive market making agents, reinforcing the potential of RL to transform algorithmic trading in complex financial systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Probing: Where to Find Human-Defined Concepts (Extended Version)</title>
<link>https://arxiv.org/abs/2507.18681</link>
<guid>https://arxiv.org/abs/2507.18681</guid>
<content:encoded><![CDATA[
<div> Concept Probing, Artificial Neural Networks, Internal Representations, Human-Defined Concepts, Layer Selection <br />
Summary: Concept probing is a popular method to understand the information encoded in neural network models. This paper introduces a novel approach to automatically determine which layer's representations should be examined when probing for specific human-defined concepts. The success of concept probes relies heavily on the quality and relevance of the internal representations they analyze. By assessing the informativeness and regularity of different layers with respect to a given concept, the proposed method aids in identifying the most suitable layer for probing. Through extensive experimentation on various neural network models and datasets, the effectiveness of the approach is validated. This automated layer selection process streamlines the concept probing process and enhances the interpretability of neural network models. <br /> <div>
arXiv:2507.18681v1 Announce Type: new 
Abstract: Concept probing has recently gained popularity as a way for humans to peek into what is encoded within artificial neural networks. In concept probing, additional classifiers are trained to map the internal representations of a model into human-defined concepts of interest. However, the performance of these probes is highly dependent on the internal representations they probe from, making identifying the appropriate layer to probe an essential task. In this paper, we propose a method to automatically identify which layer's representations in a neural network model should be considered when probing for a given human-defined concept of interest, based on how informative and regular the representations are with respect to the concept. We validate our findings through an exhaustive empirical analysis over different neural network models and datasets.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models</title>
<link>https://arxiv.org/abs/2507.18725</link>
<guid>https://arxiv.org/abs/2507.18725</guid>
<content:encoded><![CDATA[
<div> un-pruning, sparse models, machine unlearning, right to be forgotten, model pruning

Summary: 
The paper introduces a new concept called "un-pruning" in the context of machine unlearning for sparse models. It addresses the issue of deleted data impacting model pruning by proposing an un-pruning algorithm that approximates the pruned topology based on retained data. The proposed algorithm can be integrated with existing unlearning algorithms and ensures an upper-bound on the error of un-pruning. The study also highlights the unreliability of Membership Inference Attack accuracy in assessing the forgetting of deleted data in sparse models. New performance metrics are introduced to evaluate the success of un-pruning in sparse models. Extensive experiments are conducted to validate the effectiveness of the un-pruning approach with different pruning methods and unlearning algorithms. <div>
arXiv:2507.18725v1 Announce Type: new 
Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted data from trained models and address the right to be forgotten. Despite the success of existing unlearning algorithms, unlearning in sparse models has not yet been well studied. In this paper, we empirically find that the deleted data has an impact on the pruned topology in a sparse model. Motivated by the observation and the right to be forgotten, we define a new terminology ``un-pruning" to eliminate the impact of deleted data on model pruning. Then we propose an un-pruning algorithm to approximate the pruned topology driven by retained data. We remark that any existing unlearning algorithm can be integrated with the proposed un-pruning workflow and the error of un-pruning is upper-bounded in theory. Also, our un-pruning algorithm can be applied to both structured sparse models and unstructured sparse models. In the experiment, we further find that Membership Inference Attack (MIA) accuracy is unreliable for assessing whether a model has forgotten deleted data, as a small change in the amount of deleted data can produce arbitrary MIA results. Accordingly, we devise new performance metrics for sparse models to evaluate the success of un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of un-pruning with various pruning methods and unlearning algorithms. Our code is released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation</title>
<link>https://arxiv.org/abs/2507.18756</link>
<guid>https://arxiv.org/abs/2507.18756</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Armed Bandit, Exploration-Exploitation Trade-off, Contextual Linear Bandits, Offline Evaluation, Recommender Systems 

Summary: 
This study compares various linear Multi-Armed Bandit algorithms in offline evaluation for recommender systems. The exploration-exploitation trade-off is crucial in contextual linear bandits, but a greedy linear model without exploration consistently performs well across datasets, often outperforming exploratory models. Hyperparameter optimization also favors configurations with minimal exploration. These results highlight limitations in current offline evaluation protocols for bandits, as they may not accurately reflect the effectiveness of exploration strategies. There is a need for more robust assessment methodologies to guide future research on recommender systems and interactive learning. 

<br /><br />Summary: <div>
arXiv:2507.18756v1 Announce Type: new 
Abstract: Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems that require continuous, incremental learning. A core aspect of MABs is the exploration-exploitation trade-off: choosing between exploiting items likely to be enjoyed and exploring new ones to gather information. In contextual linear bandits, this trade-off is particularly central, as many variants share the same linear regression backbone and differ primarily in their exploration strategies. Despite its prevalent use, offline evaluation of MABs is increasingly recognized for its limitations in reliably assessing exploration behavior. This study conducts an extensive offline empirical comparison of several linear MABs. Strikingly, across over 90% of various datasets, a greedy linear model, with no type of exploration, consistently achieves top-tier performance, often outperforming or matching its exploratory counterparts. This observation is further corroborated by hyperparameter optimization, which consistently favors configurations that minimize exploration, suggesting that pure exploitation is the dominant strategy within these evaluation settings. Our results expose significant inadequacies in offline evaluation protocols for bandits, particularly concerning their capacity to reflect true exploratory efficacy. Consequently, this research underscores the urgent necessity for developing more robust assessment methodologies, guiding future investigations into alternative evaluation frameworks for interactive learning in recommender systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization</title>
<link>https://arxiv.org/abs/2507.18794</link>
<guid>https://arxiv.org/abs/2507.18794</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrastive Learning, Anti-contrastive Regularization, Healthcare Applications, Task-relevant Features, Task-irrelevant Features

Summary:
CLEAR introduces a framework for learning representations that separate essential task-relevant characteristics from superficial task-irrelevant characteristics, ensuring better prediction performance when superficial features change at test time. The Pair-Switching penalty minimizes the Mutual Information between style attributes and content labels, effectively removing associations that may degrade performance. Implemented in a Variational Auto-Encoder (VAE) framework, CLEAR-VAE allows for swapping and interpolating content and style between samples and improves downstream classification performance with unseen combinations of content and style. Experimental results on image datasets demonstrate the efficacy of CLEAR-VAE in achieving equitable and generalizable predictions across different demographics. The code for CLEAR will be publicly available. 

<br /><br />Summary: 
- CLEAR framework separates task-relevant and task-irrelevant features
- Pair-Switching penalty minimizes Mutual Information between style and content 
- Implemented in VAE for swapping and interpolating content and style
- Improves downstream classification with unseen content-style combinations
- Results show enhanced prediction performance across various demographics. <div>
arXiv:2507.18794v1 Announce Type: new 
Abstract: Learning representations unaffected by superficial characteristics is important to ensure that shifts in these characteristics at test time do not compromise downstream prediction performance. For instance, in healthcare applications, we might like to learn features that contain information about pathology yet are unaffected by race, sex, and other sources of physiologic variability, thereby ensuring predictions are equitable and generalizable across all demographics. Here we propose Contrastive LEarning with Anti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement framework that effectively separates essential (i.e., task-relevant) characteristics from superficial (i.e., task-irrelevant) characteristics during training, leading to better performance when superficial characteristics shift at test time. We begin by supposing that data representations can be semantically separated into task-relevant content features, which contain information relevant to downstream tasks, and task-irrelevant style features, which encompass superficial attributes that are irrelevant to these tasks, yet may degrade performance due to associations with content present in training data that do not generalize. We then prove that our anti-contrastive penalty, which we call Pair-Switching (PS), minimizes the Mutual Information between the style attributes and content labels. Finally, we instantiate CLEAR in the latent space of a Variational Auto-Encoder (VAE), then perform experiments to quantitatively and qualitatively evaluate the resulting CLEAR-VAE over several image datasets. Our results show that CLEAR-VAE allows us to: (a) swap and interpolate content and style between any pair of samples, and (b) improve downstream classification performance in the presence of previously unseen combinations of content and style. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors</title>
<link>https://arxiv.org/abs/2507.18804</link>
<guid>https://arxiv.org/abs/2507.18804</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, robustness, bit-flip errors, Ralts, system-level optimization

Summary:
Graph neural networks (GNNs) are widely used in safety-critical applications but are vulnerable to hardware-induced bit-flip errors. This study analyzes GNN robustness against bit-flip errors and proposes Ralts, a lightweight solution to enhance GNN resilience. Ralts utilizes graph similarity metrics to filter outliers and recover compromised graph topology, integrating these techniques into aggregation functions to support any message-passing GNNs. Evaluation results show Ralts effectively improves GNN robustness across various models, datasets, error patterns, and architectures. Under a bit-error rate (BER) of $3\times10^{-5}$, Ralts enhances prediction accuracy by at least 20% for errors in model weights or node embeddings, and by at least 10% for errors in adjacency matrices. Moreover, Ralts maintains comparable execution efficiency to built-in aggregation functions in PyTorch Geometric. This work highlights the importance of addressing hardware-induced faults in GNN systems for reliable and efficient operation.

<br /><br />Summary: <div>
arXiv:2507.18804v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have been widely applied in safety-critical applications, such as financial and medical networks, in which compromised predictions may cause catastrophic consequences. While existing research on GNN robustness has primarily focused on software-level threats, hardware-induced faults and errors remain largely underexplored. As hardware systems progress toward advanced technology nodes to meet high-performance and energy efficiency demands, they become increasingly susceptible to transient faults, which can cause bit flips and silent data corruption, a prominent issue observed by major technology companies (e.g., Meta and Google). In response, we first present a comprehensive analysis of GNN robustness against bit-flip errors, aiming to reveal system-level optimization opportunities for future reliable and efficient GNN systems. Second, we propose Ralts, a generalizable and lightweight solution to bolster GNN resilience to bit-flip errors. Specifically, Ralts exploits various graph similarity metrics to filter out outliers and recover compromised graph topology, and incorporates these protective techniques directly into aggregation functions to support any message-passing GNNs. Evaluation results demonstrate that Ralts effectively enhances GNN robustness across a range of GNN models, graph datasets, error patterns, and both dense and sparse architectures. On average, under a BER of $3\times10^{-5}$, these robust aggregation functions improve prediction accuracy by at least 20\% when errors present in model weights or node embeddings, and by at least 10\% when errors occur in adjacency matrices. Ralts is also optimized to deliver execution efficiency comparable to built-in aggregation functions in PyTorch Geometric.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator</title>
<link>https://arxiv.org/abs/2507.18807</link>
<guid>https://arxiv.org/abs/2507.18807</guid>
<content:encoded><![CDATA[
<div> Fisher Information Matrix, parameter sensitivity, gradient estimation, adaptive gradient methods, Squisher.  
Summary:  
The paper investigates the use of the Squisher, an approximation of the Fisher diagonal, calculated from the squared gradient accumulator in the Adam optimizer. The study finds that the Squisher method performs similarly to the Fisher diagonal across various applications of parameter sensitivity estimation. Through extensive experiments, the research demonstrates the efficacy of the Squisher in providing a cost-effective alternative to estimating the Fisher diagonal, with improved performance compared to baseline methods. The study highlights the similarities and differences between the Squisher and the Fisher diagonal, offering empirical insights into their respective impacts on parameter sensitivity measurement. Overall, the Squisher offers a promising approach to efficiently approximate the Fisher diagonal without incurring additional computational costs.  
<br /><br />Summary: <div>
arXiv:2507.18807v1 Announce Type: new 
Abstract: The diagonal of a model's Fisher Information Matrix (the "Fisher diagonal") has frequently been used as a way to measure parameter sensitivity. Typically, the Fisher diagonal is estimated via squared sampled gradients of the model's likelihood with respect to its parameters, averaged over a few hundred or thousand examples -- a process which incurs nontrivial computational costs. At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training. This paper therefore explores whether an approximation of the Fisher diagonal can be obtained "for free" by recycling the squared gradient accumulator that has already been computed over the course of training. Through a comprehensive set of experiments covering five applications of the Fisher diagonal, we demonstrate that the "Squisher" (SQUared gradient accumulator as an approximation of the FISHER) consistently performs similarly to the Fisher diagonal while outperforming baseline methods. Additionally, we clarify the exact differences between the Squisher and the Fisher diagonal and provide empirical quantification of their respective impact.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Offline Reinforcement Learning on Goal-related Experience</title>
<link>https://arxiv.org/abs/2507.18809</link>
<guid>https://arxiv.org/abs/2507.18809</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, offline reinforcement learning, test-time training, self-supervised data selection, loco-navigation tasks

Summary: 
Foundation models and offline reinforcement learning algorithms share similarities in training a universal value function on multiple goals. Test-time training on related experience can significantly enhance policy performance without high computational costs. A novel self-supervised data selection criterion is proposed, selecting relevant transitions for test-time training. Fine-tuning policies on selected data for a few gradient steps improves performance over standard pre-training in high-dimensional tasks. The Goal-Conditioned Test-Time Training (GC-TTT) algorithm adapts policies during evaluation, leading to performance gains. Compute allocation at inference shows GC-TTT's effectiveness compared to simply scaling model size. This study highlights the potential benefits of utilizing test-time training in enhancing policy performance for a variety of tasks. 

<br /><br />Summary: <div>
arXiv:2507.18809v1 Announce Type: new 
Abstract: Foundation models compress a large amount of information in a single, large neural network, which can then be queried for individual tasks. There are strong parallels between this widespread framework and offline goal-conditioned reinforcement learning algorithms: a universal value function is trained on a large number of goals, and the policy is evaluated on a single goal in each test episode. Extensive research in foundation models has shown that performance can be substantially improved through test-time training, specializing the model to the current goal. We find similarly that test-time offline reinforcement learning on experience related to the test goal can lead to substantially better policies at minimal compute costs. We propose a novel self-supervised data selection criterion, which selects transitions from an offline dataset according to their relevance to the current state and quality with respect to the evaluation goal. We demonstrate across a wide range of high-dimensional loco-navigation and manipulation tasks that fine-tuning a policy on the selected data for a few gradient steps leads to significant performance gains over standard offline pre-training. Our goal-conditioned test-time training (GC-TTT) algorithm applies this routine in a receding-horizon fashion during evaluation, adapting the policy to the current trajectory as it is being rolled out. Finally, we study compute allocation at inference, demonstrating that, at comparable costs, GC-TTT induces performance gains that are not achievable by scaling model size.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses</title>
<link>https://arxiv.org/abs/2507.18811</link>
<guid>https://arxiv.org/abs/2507.18811</guid>
<content:encoded><![CDATA[
<div> flow matching, generative neural networks, high-energy physics, simulation, zero degree calorimeters

Summary:
Flow matching (FM) generative neural networks have been developed for fast simulations of zero degree calorimeters in the ALICE experiment. A novel training strategy allows for the creation of efficient generative models with minimal parameters. The FM model achieves state-of-the-art simulation fidelity for both neutron (ZN) and proton (ZP) detectors, significantly reducing computational costs. For ZN simulation, the FM model achieves a Wasserstein distance of 1.27 with an inference time of 0.46 ms per sample, outperforming existing methods. The latent FM model further enhances inference speed to 0.026 ms per sample with minimal loss in accuracy. In the ZP simulation, the FM model achieves a Wasserstein distance of 1.30, surpassing the current best of 2.08. This work demonstrates the potential of FM generative models for accelerating simulations in high-energy physics, offering efficient solutions to meet the growing computational demands in research institutions. The source code is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.18811v1 Announce Type: new 
Abstract: Recent advances in generative neural networks, particularly flow matching (FM), have enabled the generation of high-fidelity samples while significantly reducing computational costs. A promising application of these models is accelerating simulations in high-energy physics (HEP), helping research institutions meet their increasing computational demands. In this work, we leverage FM to develop surrogate models for fast simulations of zero degree calorimeters in the ALICE experiment. We present an effective training strategy that enables the training of fast generative models with an exceptionally low number of parameters. This approach achieves state-of-the-art simulation fidelity for both neutron (ZN) and proton (ZP) detectors, while offering substantial reductions in computational costs compared to existing methods. Our FM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an inference time of 0.46 ms per sample, compared to the current best of 1.20 with an inference time of approximately 109 ms. The latent FM model further improves the inference speed, reducing the sampling time to 0.026 ms per sample, with a minimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein distance of 1.30 for the ZP simulation, outperforming the current best of 2.08. The source code is available at https://github.com/m-wojnar/faster_zdc.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-Consistent Learning for Partial Differential Equations</title>
<link>https://arxiv.org/abs/2507.18813</link>
<guid>https://arxiv.org/abs/2507.18813</guid>
<content:encoded><![CDATA[
<div> scale-consistency, neural operator, PDEs, data augmentation, machine learning

Summary: 
The article introduces a data augmentation scheme for solving partial differential equations (PDEs) using machine learning (ML) models. Traditional ML models struggle to generalize beyond training data, limiting their applicability. To address this, a scale-consistency loss is proposed based on the properties of PDEs, allowing a neural operator to model a wide range of scales. By leveraging the rescaling of PDE domains and the consistency of solution operators on sub-domains, the model can be trained to generalize across different scales. Experimental results on various PDEs, including Burgers' equation and Navier-Stokes equations, show that the model trained with scale-consistency can generalize to different Reynolds numbers and reduce error by 34% on average compared to baselines. This approach demonstrates the effectiveness of incorporating scale information into neural PDE solvers for improved generalization. 

<br /><br />Summary: <div>
arXiv:2507.18813v1 Announce Type: new 
Abstract: Machine learning (ML) models have emerged as a promising approach for solving partial differential equations (PDEs) in science and engineering. Previous ML models typically cannot generalize outside the training data; for example, a trained ML model for the Navier-Stokes equations only works for a fixed Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations, we propose a data augmentation scheme based on scale-consistency properties of PDEs and design a scale-informed neural operator that can model a wide range of scales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more concretely, a given domain can be re-scaled to unit size, and the parameters and the boundary conditions of the PDE can be appropriately adjusted to represent the original solution, and (ii) the solution operators on a given domain are consistent on the sub-domains. We leverage these facts to create a scale-consistency loss that encourages matching the solutions evaluated on a given domain and the solution obtained on its sub-domain from the rescaled PDE. Since neural operators can fit to multiple scales and resolutions, they are the natural choice for incorporating scale-consistency loss during training of neural PDE solvers. We experiment with scale-consistency loss and the scale-informed neural operator model on the Burgers' equation, Darcy Flow, Helmholtz equation, and Navier-Stokes equations. With scale-consistency, the model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000, and reduces the error by 34% on average of all datasets compared to baselines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models</title>
<link>https://arxiv.org/abs/2507.18858</link>
<guid>https://arxiv.org/abs/2507.18858</guid>
<content:encoded><![CDATA[
<div> Keywords: Weak-to-Strong generalization, decision-making environments, trajectory trees, Monte Carlo Tree Search, empirical evaluations

Summary: 
Weak-to-Strong generalization (W2SG) is a new approach that leverages the supervision of a weak model to improve the performance of a strong model in complex decision-making environments. The study extends existing W2SG research by fine-tuning the strong model with trajectories of intermediate actions from the weak model. The proposed method generalizes both successful knowledge and failure experiences, simulating the human learning process. Trajectory trees are introduced as a hierarchical representation of action trajectories generated by weak models, optimized using Monte Carlo Tree Search (MCTS) to enhance the strong model. Theoretical analysis establishes the effectiveness of the method in improving W2SG performance. Empirical evaluations across diverse task domains validate the scalability and robustness of the framework. The code for the proposed approach is publicly available for further research and implementation.

<br /><br />Summary: <div>
arXiv:2507.18858v1 Announce Type: new 
Abstract: Weak-to-Strong generalization (W2SG) is a new trend to elicit the full capabilities of a strong model with supervision from a weak model. While existing W2SG studies focus on simple tasks like binary classification, we extend this paradigm to complex interactive decision-making environments. Specifically, we fine-tune a strong model with trajectories of intermediate actions generated by a weak model. Motivated by the human learning process, we propose to generalize not only success knowledge but also failure experience so that the strong model can learn from failed trajectories accumulated by weak models. To effectively and efficiently elicit the potential of strong agents, we further construct ``trajectory trees," a hierarchical representation that organizes weak model-generated action trajectories, coupled with Monte Carlo Tree Search (MCTS) to optimize the strong model. Through theoretical analysis, we provide formal guarantees for the effectiveness of our method in improving W2SG performance. Our empirical evaluations demonstrate substantial improvements in reasoning and decision-making capabilities across diverse task domains, validating the scalability and robustness of our proposed framework. Our code is available at: https://github.com/yeruimeng/TraTree
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Mortality Prediction in ICU Patients with Hypertensive Kidney Disease Using Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2507.18866</link>
<guid>https://arxiv.org/abs/2507.18866</guid>
<content:encoded><![CDATA[
<div> machine learning, hypertensive kidney disease, ICU patients, mortality prediction, clinical decision-making

Summary:<br />
- A machine learning framework was developed to predict 30-day in-hospital mortality among ICU patients with hypertensive kidney disease using early clinical data.
- The model achieved an AUROC of 0.88 on the independent test set with high sensitivity and specificity.
- Important predictors identified by the model included altered consciousness, vasopressor use, and coagulation status.
- The DREAM algorithm was integrated to estimate patient-specific posterior risk distributions, allowing for assessment of predicted mortality and uncertainty.
- This interpretable machine learning pipeline supports individualized triage and transparent clinical decisions, showing promise for clinical deployment and potential validation in broader critical care populations.

Summary: <div>
arXiv:2507.18866v1 Announce Type: new 
Abstract: Background: Hypertensive kidney disease (HKD) patients in intensive care units (ICUs) face high short-term mortality, but tailored risk prediction tools are lacking. Early identification of high-risk individuals is crucial for clinical decision-making. Methods: We developed a machine learning framework to predict 30-day in-hospital mortality among ICU patients with HKD using early clinical data from the MIMIC-IV v2.2 database. A cohort of 1,366 adults was curated with strict criteria, excluding malignancy cases. Eighteen clinical features-including vital signs, labs, comorbidities, and therapies-were selected via random forest importance and mutual information filtering. Several models were trained and compared with stratified five-fold cross-validation; CatBoost demonstrated the best performance. Results: CatBoost achieved an AUROC of 0.88 on the independent test set, with sensitivity of 0.811 and specificity of 0.798. SHAP values and Accumulated Local Effects (ALE) plots showed the model relied on meaningful predictors such as altered consciousness, vasopressor use, and coagulation status. Additionally, the DREAM algorithm was integrated to estimate patient-specific posterior risk distributions, allowing clinicians to assess both predicted mortality and its uncertainty. Conclusions: We present an interpretable machine learning pipeline for early, real-time risk assessment in ICU patients with HKD. By combining high predictive performance with uncertainty quantification, our model supports individualized triage and transparent clinical decisions. This approach shows promise for clinical deployment and merits external validation in broader critical care populations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise</title>
<link>https://arxiv.org/abs/2507.18867</link>
<guid>https://arxiv.org/abs/2507.18867</guid>
<content:encoded><![CDATA[
<div> Exploration, Multi-agent reinforcement learning, Sparse rewards, Individual rewards, Human expertise <br />
Summary: The article introduces a novel framework called LIGHT, which addresses the challenge of efficient exploration in multi-agent reinforcement learning with sparse rewards. By integrating human knowledge into the learning process, LIGHT guides agents to make informed decisions by considering individual action distribution and human expertise preferences. It generates individual intrinsic rewards for each agent based on Q-learning relevant transformations, aligning their actions with human expertise while maximizing joint action value. Experimental results show that LIGHT outperforms baselines in terms of performance and knowledge reusability across various sparse-reward tasks. <div>
arXiv:2507.18867v1 Announce Type: new 
Abstract: Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction</title>
<link>https://arxiv.org/abs/2507.18926</link>
<guid>https://arxiv.org/abs/2507.18926</guid>
<content:encoded><![CDATA[
<div> prediction, blood-brain barrier permeability, graph neural networks, geometric features, drug development 
Summary:
The paper introduces the GMC-MPNN, a novel graph neural network framework that incorporates atomic-level geometric features and long-range interactions to predict blood-brain barrier permeability (BBBP). The model constructs weighted colored subgraphs based on atom types to capture spatial relationships and chemical context governing BBB permeability. Evaluation on benchmark datasets shows GMC-MPNN outperforms existing models, achieving high accuracy in classifying compounds and regressing permeability values. An ablation study highlights the model's ability to learn from common and rare functional motifs. By integrating spatial geometry, GMC-MPNN offers a more accurate and generalizable tool for drug discovery pipelines. 
<br /><br />Summary: <div>
arXiv:2507.18926v1 Announce Type: new 
Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential for central nervous system (CNS) drug development. While graph neural networks (GNNs) have advanced molecular property prediction, they often rely on molecular topology and neglect the three-dimensional geometric information crucial for modeling transport mechanisms. This paper introduces the geometric multi-color message-passing graph neural network (GMC-MPNN), a novel framework that enhances standard message-passing architectures by explicitly incorporating atomic-level geometric features and long-range interactions. Our model constructs weighted colored subgraphs based on atom types to capture the spatial relationships and chemical context that govern BBB permeability. We evaluated GMC-MPNN on three benchmark datasets for both classification and regression tasks, using rigorous scaffold-based splitting to ensure a robust assessment of generalization. The results demonstrate that GMC-MPNN consistently outperforms existing state-of-the-art models, achieving superior performance in both classifying compounds as permeable/non-permeable (AUC-ROC of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of 0.4609, Pearson correlation of 0.7759). An ablation study further quantified the impact of specific atom-pair interactions, revealing that the model's predictive power derives from its ability to learn from both common and rare, but chemically significant, functional motifs. By integrating spatial geometry into the graph representation, GMC-MPNN sets a new performance benchmark and offers a more accurate and generalizable tool for drug discovery pipelines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Best Arm Identification in the Presence of a Copycat</title>
<link>https://arxiv.org/abs/2507.18975</link>
<guid>https://arxiv.org/abs/2507.18975</guid>
<content:encoded><![CDATA[
<div> algorithm, best arm identification, security constraint, stochastic linear bandits, coded arms

Summary:
The article addresses the problem of best arm identification in stochastic linear bandits with a security constraint. It introduces a scenario where a player aims to find the best arm without revealing this information to an observer. Existing optimal algorithms easily disclose the best arm due to the increased frequency of playing it, while a naive secure approach compromises identification speed. The proposed algorithm utilizes coded arms without cryptographic elements, achieving a superior error exponent while ensuring minimal disclosure of the best arm. By playing with coded arms, the algorithm significantly enhances security and maintains a competitive error exponent, standing out as a promising approach in the realm of best arm identification in stochastic linear bandits. <div>
arXiv:2507.18975v1 Announce Type: new 
Abstract: Consider the problem of best arm identification with a security constraint. Specifically, assume a setup of stochastic linear bandits with $K$ arms of dimension $d$. In each arm pull, the player receives a reward that is the sum of the dot product of the arm with an unknown parameter vector and independent noise. The player's goal is to identify the best arm after $T$ arm pulls. Moreover, assume a copycat Chloe is observing the arm pulls. The player wishes to keep Chloe ignorant of the best arm.
  While a minimax--optimal algorithm identifies the best arm with an $\Omega\left(\frac{T}{\log(d)}\right)$ error exponent, it easily reveals its best-arm estimate to an outside observer, as the best arms are played more frequently. A naive secure algorithm that plays all arms equally results in an $\Omega\left(\frac{T}{d}\right)$ exponent. In this paper, we propose a secure algorithm that plays with \emph{coded arms}. The algorithm does not require any key or cryptographic primitives, yet achieves an $\Omega\left(\frac{T}{\log^2(d)}\right)$ exponent while revealing almost no information on the best arm.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable Regimes</title>
<link>https://arxiv.org/abs/2507.18983</link>
<guid>https://arxiv.org/abs/2507.18983</guid>
<content:encoded><![CDATA[
<div> Keyword: Kolmogorov-Arnold networks, regime detection, sparse splines, symbolic rule extraction, financial forecasting 

Summary:
Kolmogorov-Arnold networks for stock prediction and explainable regimes (KASPER) is a new framework introduced for forecasting in financial markets. It integrates regime detection, sparse spline-based function modeling, and symbolic rule extraction to address the challenges posed by nonlinear and regime-dependent dynamics. The framework uses a Gumbel-Softmax-based mechanism to identify hidden market conditions and applies Kolmogorov-Arnold networks with sparse spline activations for regime-specific forecasting. Interpretability is achieved through symbolic learning based on Monte Carlo Shapley values, extracting human-readable rules for each regime. When applied to real-world financial time series from Yahoo Finance, the model outperforms existing methods with an $R^2$ score of 0.89, a Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001. This research paves the way for regime-aware, transparent, and robust forecasting in financial markets.<br /><br />Summary: <div>
arXiv:2507.18983v1 Announce Type: new 
Abstract: Forecasting in financial markets remains a significant challenge due to their nonlinear and regime-dependent dynamics. Traditional deep learning models, such as long short-term memory networks and multilayer perceptrons, often struggle to generalize across shifting market conditions, highlighting the need for a more adaptive and interpretable approach. To address this, we introduce Kolmogorov-Arnold networks for stock prediction and explainable regimes (KASPER), a novel framework that integrates regime detection, sparse spline-based function modeling, and symbolic rule extraction. The framework identifies hidden market conditions using a Gumbel-Softmax-based mechanism, enabling regime-specific forecasting. For each regime, it employs Kolmogorov-Arnold networks with sparse spline activations to capture intricate price behaviors while maintaining robustness. Interpretability is achieved through symbolic learning based on Monte Carlo Shapley values, which extracts human-readable rules tailored to each regime. Applied to real-world financial time series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming existing methods. This research establishes a new direction for regime-aware, transparent, and robust forecasting in financial markets.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model</title>
<link>https://arxiv.org/abs/2507.18987</link>
<guid>https://arxiv.org/abs/2507.18987</guid>
<content:encoded><![CDATA[
<div> Machine Learning Models, DTC Recurrence, Feature Selection, Bayesian Neural Networks, Uncertainty Quantification

Summary:
- Study introduces a framework for classifying DTC recurrence using 383 patients' data and 16 variables.
- Support Vector Machines model achieved 94.81% accuracy initially.
- Feature selection with Boruta algorithm reduced complexity, and Logistic Regression model reached 96.11% accuracy.
- Bayesian Neural Networks (BNN) with various prior distributions were applied for uncertainty quantification.
- BNN with Normal 0,10 prior distribution demonstrated accuracies of 97.40% and 98.70% before and after feature selection, respectively.

<br /><br />Summary: <div>
arXiv:2507.18987v1 Announce Type: new 
Abstract: Differentiated thyroid cancer DTC recurrence is a major public health concern, requiring classification and predictive models that are not only accurate but also interpretable and uncertainty aware. This study introduces a comprehensive framework for DTC recurrence classification using a dataset containing 383 patients and 16 clinical and pathological variables. Initially, 11 machine learning ML models were employed using the complete dataset, where the Support Vector Machines SVM model achieved the highest accuracy of 0.9481. To reduce complexity and redundancy, feature selection was carried out using the Boruta algorithm, and the same ML models were applied to the reduced dataset, where it was observed that the Logistic Regression LR model obtained the maximum accuracy of 0.9611. However, these ML models often lack uncertainty quantification, which is critical in clinical decision making. Therefore, to address this limitation, the Bayesian Neural Networks BNN with six varying prior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy 0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and reduced datasets. The BNN model with Normal 0,10 prior distribution exhibited maximum accuracies of 0.9740 and 0.9870 before and after feature selection, respectively.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</title>
<link>https://arxiv.org/abs/2507.18989</link>
<guid>https://arxiv.org/abs/2507.18989</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, machine learning, arithmetic units, optimization, digital systems

Summary:
- GENIAL is a machine learning-based framework that automatically generates and optimizes arithmetic units, specifically multipliers, by training a Transformer-based surrogate model.
- The surrogate model forecasts hardware metrics such as power and area, enabling efficient search for operand encodings that minimize power consumption in arithmetic units for specific input data distributions.
- GENIAL is more sample efficient and converges faster towards optimized designs compared to other methods.
- It automatically discovers encodings leading to up to 18% switching activity savings within multipliers for AI workloads.
- The approach is versatile and shows improvements in Finite State Machines, extending its applicability to a wide range of logic functions.

<br /><br />Summary: <div>
arXiv:2507.18989v1 Announce Type: new 
Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important to reduce the footprint of digital systems. Conventional design flows, which often rely on manual or heuristics-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, more specifically multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables to deploy a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two's complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL's applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning via Conservative Agent for Environments with Random Delays</title>
<link>https://arxiv.org/abs/2507.18992</link>
<guid>https://arxiv.org/abs/2507.18992</guid>
<content:encoded><![CDATA[
<div> delay-compensating methods, reinforcement learning, random delays, conservative agent, continuous control tasks

Summary: 
This article introduces a new approach for addressing the challenges posed by random delays in reinforcement learning applications. While existing methods are designed for environments with constant delays, the conservative agent proposed here can effectively handle random delays by transforming them into their constant-delay equivalents. This allows for the seamless extension of state-of-the-art constant-delay methods to random-delay environments without the need for algorithmic modifications. Evaluation on continuous control tasks shows that the conservative agent-based algorithm outperforms baseline algorithms in terms of both asymptotic performance and sample efficiency. This novel approach offers a promising solution to the issue of delayed feedback in real-world reinforcement learning applications. <div>
arXiv:2507.18992v1 Announce Type: new 
Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting to Fragmented and Evolving Data: A Fisher Information Perspective</title>
<link>https://arxiv.org/abs/2507.18996</link>
<guid>https://arxiv.org/abs/2507.18996</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, sequential covariate shift, robust learning, adaptation, Fisher information geometry

Summary:
FADE (Fisher-based Adaptation to Dynamic Environments) is a framework designed for modern machine learning systems that operate in dynamic environments facing sequential covariate shift (SCS). It addresses this challenge by incorporating a shift-aware regularization mechanism based on Fisher information geometry. By modulating parameter updates according to sensitivity and stability, FADE can adapt to significant distribution changes without requiring task boundaries, target supervision, or experience replay. The framework utilizes a Cramer-Rao-informed shift signal to detect distribution shifts and operates online with fixed memory and no access to target labels. In evaluations across various benchmarks, FADE outperformed existing methods like TENT and DIW, achieving up to 19% higher accuracy under severe shifts. The framework also extends naturally to federated learning, offering scalable and stable adaptation in decentralized settings. Theoretical analysis guarantees bounded regret and parameter consistency, while empirical results demonstrate FADE's robustness across different modalities and shift intensities. <br /><br />Summary: <div>
arXiv:2507.18996v1 Announce Type: new 
Abstract: Modern machine learning systems operating in dynamic environments often face \textit{sequential covariate shift} (SCS), where input distributions evolve over time while the conditional distribution remains stable. We introduce FADE (Fisher-based Adaptation to Dynamic Environments), a lightweight and theoretically grounded framework for robust learning under SCS. FADE employs a shift-aware regularization mechanism anchored in Fisher information geometry, guiding adaptation by modulating parameter updates based on sensitivity and stability. To detect significant distribution changes, we propose a Cramer-Rao-informed shift signal that integrates KL divergence with temporal Fisher dynamics. Unlike prior methods requiring task boundaries, target supervision, or experience replay, FADE operates online with fixed memory and no access to target labels. Evaluated on seven benchmarks spanning vision, language, and tabular data, FADE achieves up to 19\% higher accuracy under severe shifts, outperforming methods such as TENT and DIW. FADE also generalizes naturally to federated learning by treating heterogeneous clients as temporally fragmented environments, enabling scalable and stable adaptation in decentralized settings. Theoretical analysis guarantees bounded regret and parameter consistency, while empirical results demonstrate FADE's robustness across modalities and shift intensities.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A diffusion-based generative model for financial time series via geometric Brownian motion</title>
<link>https://arxiv.org/abs/2507.19003</link>
<guid>https://arxiv.org/abs/2507.19003</guid>
<content:encoded><![CDATA[
<div> diffusion-based generative framework, financial time series, geometric Brownian motion, heteroskedasticity, denoising score matching<br />
Summary:<br />
The article introduces a novel diffusion-based generative framework for financial time series, incorporating geometric Brownian motion into the forward noising process. This method injects noise proportional to asset prices at each time step, reflecting the heteroskedasticity seen in financial data. By balancing drift and diffusion terms, the log-price process aligns with score-based generative models. The reverse-time generative process is trained using denoising score matching with a Transformer-based architecture adapted from the CSDI framework. Evaluations on historical stock data show the model replicates key stylized facts such as heavy-tailed return distributions, volatility clustering, and the leverage effect more realistically than traditional diffusion models.<br /> <div>
arXiv:2507.19003v1 Announce Type: new 
Abstract: We propose a novel diffusion-based generative framework for financial time series that incorporates geometric Brownian motion (GBM), the foundation of the Black--Scholes theory, into the forward noising process. Unlike standard score-based models that treat price trajectories as generic numerical sequences, our method injects noise proportionally to asset prices at each time step, reflecting the heteroskedasticity observed in financial time series. By accurately balancing the drift and diffusion terms, we show that the resulting log-price process reduces to a variance-exploding stochastic differential equation, aligning with the formulation in score-based generative models. The reverse-time generative process is trained via denoising score matching using a Transformer-based architecture adapted from the Conditional Score-based Diffusion Imputation (CSDI) framework. Empirical evaluations on historical stock data demonstrate that our model reproduces key stylized facts heavy-tailed return distributions, volatility clustering, and the leverage effect more realistically than conventional diffusion models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster</title>
<link>https://arxiv.org/abs/2507.19017</link>
<guid>https://arxiv.org/abs/2507.19017</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, large language models, distributed training, scalability, optimization

Summary: 
MindSpeed RL is introduced as an efficient system for large-scale reinforcement learning (RL) training, focusing on organizing data dependencies and optimizing performance. By utilizing a distributed transfer dock strategy and an allgather-swap strategy, MindSpeed RL aims to improve cluster scalability and memory utilization in RL training. The system integrates various parallelization strategies and acceleration techniques for systematic optimization. Experimental results on popular RL models demonstrate a throughput increase of 1.42 to 3.97 times compared to existing systems. MindSpeed RL is open-sourced and tested on a super pod of Ascend with 384 neural processing units (NPUs), showcasing its powerful performance and reliability. <br /><br />Summary: <div>
arXiv:2507.19017v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization. In this article, we introduce MindSpeed RL, an effective and efficient system for large-scale RL training. Unlike existing centralized methods, MindSpeed RL organizes the essential data dependencies in RL training, i.e., sample flow and resharding flow, from a distributed view. On the one hand, a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. In addition, MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization. Compared with existing state-of-the-art systems, comprehensive experiments on the RL training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. Finally, we open--source MindSpeed RL and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs</title>
<link>https://arxiv.org/abs/2507.19031</link>
<guid>https://arxiv.org/abs/2507.19031</guid>
<content:encoded><![CDATA[
<div> GNN-to-MLP, knowledge distillation, Progressive framework, inference cost, accuracy <br />
<br />
Summary: 
The article introduces a new Progressive framework, ProGMLP, for accelerating Graph Neural Networks (GNNs) by distilling their knowledge into Multi-Layer Perceptrons (MLPs) in a flexible and dynamic manner. ProGMLP utilizes a Progressive Training Structure (PTS) to sequentially train multiple MLP students, incorporating Progressive Knowledge Distillation (PKD) and Progressive Mixup Augmentation (PMA) for improved distillation and generalization. Through experiments on eight graph datasets, ProGMLP shows high accuracy while dynamically adjusting to different runtime scenarios, making it suitable for diverse application environments.Overall, the ProGMLP framework provides a promising approach for efficiently deploying GNNs in resource-constrained settings. <br /> <div>
arXiv:2507.19031v1 Announce Type: new 
Abstract: GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate Graph Neural Networks (GNNs) by distilling their knowledge into simpler Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the expressive power of GNNs and the computational efficiency of MLPs, making them well-suited for resource-constrained environments. However, existing G2M methods are limited by their inability to flexibly adjust inference cost and accuracy dynamically, a critical requirement for real-world applications where computational resources and time constraints can vary significantly. To address this, we introduce a Progressive framework designed to offer flexible and on-demand trade-offs between inference cost and accuracy for GNN-to-MLP knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training Structure (PTS), where multiple MLP students are trained in sequence, each building on the previous one. Furthermore, ProGMLP incorporates Progressive Knowledge Distillation (PKD) to iteratively refine the distillation process from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance generalization by progressively generating harder mixed samples. Our approach is validated through comprehensive experiments on eight real-world graph datasets, demonstrating that ProGMLP maintains high accuracy while dynamically adapting to varying runtime scenarios, making it highly effective for deployment in diverse application settings.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Ordinary Differential Equations for Learning and Extrapolating System Dynamics Across Bifurcations</title>
<link>https://arxiv.org/abs/2507.19036</link>
<guid>https://arxiv.org/abs/2507.19036</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, bifurcations, Neural Ordinary Differential Equations, system dynamics, predator-prey system

Summary: 
Neural Ordinary Differential Equations are utilized to forecast system behavior near and across bifurcations in dynamical systems. Unlike traditional methods, this approach offers a continuous and data-driven framework that can learn parameter-dependent vector fields directly from timeseries data. The study focuses on a predator-prey system containing both local and global bifurcations, serving as a challenging test case. Results indicate that Neural Ordinary Differential Equations can accurately recover bifurcation structures and forecast potential shifts even outside the parameter regions represented in the training data. The model's performance remains effective under limited and noisy data conditions, with accuracy primarily dependent on the quality of information derived from the training data rather than the quantity available.<br /><br />Summary: <div>
arXiv:2507.19036v1 Announce Type: new 
Abstract: Forecasting system behaviour near and across bifurcations is crucial for identifying potential shifts in dynamical systems. While machine learning has recently been used to learn critical transitions and bifurcation structures from data, most studies remain limited as they exclusively focus on discrete-time methods and local bifurcations. To address these limitations, we use Neural Ordinary Differential Equations which provide a continuous, data-driven framework for learning system dynamics. We apply our approach to a predator-prey system that features both local and global bifurcations, presenting a challenging test case. Our results show that Neural Ordinary Differential Equations can recover underlying bifurcation structures directly from timeseries data by learning parameter-dependent vector fields. Notably, we demonstrate that Neural Ordinary Differential Equations can forecast bifurcations even beyond the parameter regions represented in the training data. We also assess the method's performance under limited and noisy data conditions, finding that model accuracy depends more on the quality of information that can be inferred from the training data, than on the amount of data available.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics-Informed Reservoir Computing with Visibility Graphs</title>
<link>https://arxiv.org/abs/2507.19046</link>
<guid>https://arxiv.org/abs/2507.19046</guid>
<content:encoded><![CDATA[
<div> Reservoir computing; time series prediction; dynamics-informed; visibility graph; nonlinear dynamics <br />
<br />Summary: <br />Accurate prediction of complex and nonlinear time series poses challenges in various fields. Reservoir computing (RC) offers an efficient solution by training only the read-out layer on a randomly structured reservoir network. However, this randomness often leads to suboptimal networks with unclear dynamics. To address this, a Dynamics-Informed Reservoir Computing (DyRC) framework is proposed, where the reservoir structure is inferred from the input sequence using the visibility graph (VG) technique. By directly adopting the VG network from the training data, the DyRC-VG method constructs a reservoir tailored to the prediction task dynamics. Evaluation on a Duffing oscillator shows DyRC-VG outperforms an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and density in prediction accuracy and consistency, showcasing the effectiveness of informed reservoir structure in improving prediction quality. <div>
arXiv:2507.19046v1 Announce Type: new 
Abstract: Accurate prediction of complex and nonlinear time series remains a challenging problem across engineering and scientific disciplines. Reservoir computing (RC) offers a computationally efficient alternative to traditional deep learning by training only the read-out layer while employing a randomly structured and fixed reservoir network. Despite its advantages, the largely random reservoir graph architecture often results in suboptimal and oversized networks with poorly understood dynamics. Addressing this issue, we propose a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence. This work proposes to employ the visibility graph (VG) technique, which converts time series data into networks by representing measurement points as nodes linked by mutual visibility. The reservoir network is constructed by directly adopting the VG network from a training data sequence, leveraging the parameter-free visibility graph approach to avoid expensive hyperparameter tuning. This process results in a reservoir that is directly informed by the specific dynamics of the prediction task under study. We assess the DyRC-VG method through prediction tasks involving the canonical nonlinear Duffing oscillator, evaluating prediction accuracy and consistency. Compared to an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and comparable density, we observe higher prediction quality and more consistent performance over repeated implementations in the DyRC-VG.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring molecular assembly as a biosignature using mass spectrometry and machine learning</title>
<link>https://arxiv.org/abs/2507.19057</link>
<guid>https://arxiv.org/abs/2507.19057</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular assembly, mass spectrometry, agnostic biosignature, machine learning, astrobiology

Summary: 
Molecular assembly, a novel approach for detecting life beyond Earth, offers a way to interpret and measure evolutionary products without requiring knowledge of their structures. This method, which focuses on the assembly of molecules and their bonds, can be detected using mass spectrometry, making it ideal for unbiased life detection. To address limitations in data interpretation due to mission constraints, a machine learning model was developed that accurately predicts molecular assembly from mass spectrometry data. Results indicated a significant reduction in error compared to baseline models, highlighting the importance of standardization in data analysis. Despite potential instrumental inconsistencies impacting model performance, the use of standardized mass spectrometry databases could facilitate accurate molecular assembly prediction without structural elucidation, presenting a viable approach for future astrobiology missions. 

<br /><br />Summary: <div>
arXiv:2507.19057v1 Announce Type: new 
Abstract: Molecular assembly offers a promising path to detect life beyond Earth, while minimizing assumptions based on terrestrial life. As mass spectrometers will be central to upcoming Solar System missions, predicting molecular assembly from their data without needing to elucidate unknown structures will be essential for unbiased life detection. An ideal agnostic biosignature must be interpretable and experimentally measurable. Here, we show that molecular assembly, a recently developed approach to measure objects that have been produced by evolution, satisfies both criteria. First, it is interpretable for life detection, as it reflects the assembly of molecules with their bonds as building blocks, in contrast to approaches that discount construction history. Second, it can be determined without structural elucidation, as it can be physically measured by mass spectrometry, a property that distinguishes it from other approaches that use structure-based information measures for molecular complexity. Whilst molecular assembly is directly measurable using mass spectrometry data, there are limits imposed by mission constraints. To address this, we developed a machine learning model that predicts molecular assembly with high accuracy, reducing error by three-fold compared to baseline models. Simulated data shows that even small instrumental inconsistencies can double model error, emphasizing the need for standardization. These results suggest that standardized mass spectrometry databases could enable accurate molecular assembly prediction, without structural elucidation, providing a proof-of-concept for future astrobiology missions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering-Oriented Generative Attribute Graph Imputation</title>
<link>https://arxiv.org/abs/2507.19085</link>
<guid>https://arxiv.org/abs/2507.19085</guid>
<content:encoded><![CDATA[
<div> Attribute-missing graph clustering, unsupervised task, imputation, refinement, Clustering-oriented Generative Imputation with reliable Refinement (CGIR) model<br />
<br />Summary: 
The article introduces the Clustering-oriented Generative Imputation with reliable Refinement (CGIR) model for attribute-missing graph clustering. This model addresses the issue of sub-optimal imputation for clustering by estimating subcluster distributions to capture class-specific characteristics and guiding node imputation to align with correct clusters. Furthermore, CGIR merges multiple subclusters to guide the edge attention network, which identifies edge-wise attributes for each class to enhance embedding refinement. The model splits the clustering task into searching for subclusters and merging them, providing a unified framework for node imputation and refinement. Extensive experiments demonstrate the superiority of CGIR over existing approaches in attribute-missing graph clustering. <div>
arXiv:2507.19085v1 Announce Type: new 
Abstract: Attribute-missing graph clustering has emerged as a significant unsupervised task, where only attribute vectors of partial nodes are available and the graph structure is intact. The related models generally follow the two-step paradigm of imputation and refinement. However, most imputation approaches fail to capture class-relevant semantic information, leading to sub-optimal imputation for clustering. Moreover, existing refinement strategies optimize the learned embedding through graph reconstruction, while neglecting the fact that some attributes are uncorrelated with the graph. To remedy the problems, we establish the Clustering-oriented Generative Imputation with reliable Refinement (CGIR) model. Concretely, the subcluster distributions are estimated to reveal the class-specific characteristics precisely, and constrain the sampling space of the generative adversarial module, such that the imputation nodes are impelled to align with the correct clusters. Afterwards, multiple subclusters are merged to guide the proposed edge attention network, which identifies the edge-wise attributes for each class, so as to avoid the redundant attributes in graph reconstruction from disturbing the refinement of overall embedding. To sum up, CGIR splits attribute-missing graph clustering into the search and mergence of subclusters, which guides to implement node imputation and refinement within a unified framework. Extensive experiments prove the advantages of CGIR over state-of-the-art competitors.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network</title>
<link>https://arxiv.org/abs/2507.19095</link>
<guid>https://arxiv.org/abs/2507.19095</guid>
<content:encoded><![CDATA[
<div> Graph Clustering, Deep Learning, Graph Convolution, Attributed Graph, Node Representation  
Summary:  
- The paper introduces a novel deep graph clustering model, GCL-GCN, to overcome challenges in leveraging graph information for clustering sparse and heterogeneous graph data.  
- GCL-GCN utilizes a Graphormer module to capture global and local information between nodes effectively.  
- A contrastive learning module is proposed to enhance feature representations' discriminative power in the pre-training phase.  
- Experimental results on six datasets show that GCL-GCN outperforms 14 advanced methods in terms of clustering quality and robustness.  
- On the Cora dataset, GCL-GCN improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%, respectively, compared to the primary comparison method MBN.  

<br /><br />Summary: <div>
arXiv:2507.19095v1 Announce Type: new 
Abstract: Attributed graph clustering holds significant importance in modern data analysis. However, due to the complexity of graph data and the heterogeneity of node attributes, leveraging graph information for clustering remains challenging. To address this, we propose a novel deep graph clustering model, GCL-GCN, specifically designed to address the limitations of existing models in capturing local dependencies and complex structures when dealing with sparse and heterogeneous graph data. GCL-GCN introduces an innovative Graphormer module that combines centrality encoding and spatial relationships, effectively capturing both global and local information between nodes, thereby enhancing the quality of node representations. Additionally, we propose a novel contrastive learning module that significantly enhances the discriminative power of feature representations. In the pre-training phase, this module increases feature distinction through contrastive learning on the original feature matrix, ensuring more identifiable initial representations for subsequent graph convolution and clustering tasks. Extensive experimental results on six datasets demonstrate that GCL-GCN outperforms 14 advanced methods in terms of clustering quality and robustness. Specifically, on the Cora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%, respectively, compared to the primary comparison method MBN.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Structure Learning with Privacy Guarantees for Open Graph Data</title>
<link>https://arxiv.org/abs/2507.19116</link>
<guid>https://arxiv.org/abs/2507.19116</guid>
<content:encoded><![CDATA[
<div> privacy, differential privacy, data publishing, graph recovery, Gaussian differential privacy

Summary:
This article introduces a novel approach to privacy-preserving data publishing for open graph data, specifically addressing the graph recovery problem. By leveraging Gaussian differential privacy (GDP) with a structured noise-injection mechanism, the proposed framework ensures unbiased graph structure recovery while enforcing privacy guarantees at the data publishing stage. The method goes beyond traditional approaches by focusing on preserving privacy in the presence of distinct data publishers and users, offering theoretical guarantees on estimation accuracy. Additionally, the extension of the method to discrete-variable graphs enhances its versatility and applicability in diverse data settings. Experimental results demonstrate robust performance in graph learning tasks, providing a promising solution for privacy-conscious graph analysis. <div>
arXiv:2507.19116v1 Announce Type: new 
Abstract: Ensuring privacy in large-scale open datasets is increasingly challenging under regulations such as the General Data Protection Regulation (GDPR). While differential privacy (DP) provides strong theoretical guarantees, it primarily focuses on noise injection during model training, neglecting privacy preservation at the data publishing stage. Existing privacy-preserving data publishing (PPDP) approaches struggle to balance privacy and utility, particularly when data publishers and users are distinct entities. To address this gap, we focus on the graph recovery problem and propose a novel privacy-preserving estimation framework for open graph data, leveraging Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike traditional methods that perturb gradients or model updates, our approach ensures unbiased graph structure recovery while enforcing DP at the data publishing stage. Moreover, we provide theoretical guarantees on estimation accuracy and extend our method to discrete-variable graphs, a setting often overlooked in DP research. Experimental results in graph learning demonstrate robust performance, offering a viable solution for privacy-conscious graph analysis.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Photovoltaic Assessment with Large Language Model</title>
<link>https://arxiv.org/abs/2507.19144</link>
<guid>https://arxiv.org/abs/2507.19144</guid>
<content:encoded><![CDATA[
<div> solar photovoltaic panels, satellite imagery, renewable energy, large language models, grid optimization <br />
Summary:<br />
The paper explores the use of large language models (LLMs) to improve the detection and localization of solar photovoltaic (PV) panels in satellite imagery for optimizing microgrids and active distribution networks. Current methods lack transparency and struggle with generalization to new regions. The proposed PV Assessment with LLMs (PVAL) framework addresses these challenges by incorporating task decomposition, output standardization, few-shot prompting, and fine-tuning using curated PV datasets. PVAL ensures transparency, scalability, and adaptability while minimizing computational overhead. By leveraging open-source accessibility and robust methodologies, PVAL establishes an automated and reproducible pipeline for solar panel detection, facilitating large-scale renewable energy integration and optimized grid management. <div>
arXiv:2507.19144v1 Announce Type: new 
Abstract: Accurate detection and localization of solar photovoltaic (PV) panels in satellite imagery is essential for optimizing microgrids and active distribution networks (ADNs), which are critical components of renewable energy systems. Existing methods lack transparency regarding their underlying algorithms or training datasets, rely on large, high-quality PV training data, and struggle to generalize to new geographic regions or varied environmental conditions without extensive re-training. These limitations lead to inconsistent detection outcomes, hindering large-scale deployment and data-driven grid optimization. In this paper, we investigate how large language models (LLMs) can be leveraged to overcome these challenges. Despite their promise, LLMs face several challenges in solar panel detection, including difficulties with multi-step logical processes, inconsistent output formatting, frequent misclassification of visually similar objects (e.g., shadows, parking lots), and low accuracy in complex tasks such as spatial localization and quantification. To overcome these issues, we propose the PV Assessment with LLMs (PVAL) framework, which incorporates task decomposition for more efficient workflows, output standardization for consistent and scalable formatting, few-shot prompting to enhance classification accuracy, and fine-tuning using curated PV datasets with detailed annotations. PVAL ensures transparency, scalability, and adaptability across heterogeneous datasets while minimizing computational overhead. By combining open-source accessibility with robust methodologies, PVAL establishes an automated and reproducible pipeline for solar panel detection, paving the way for large-scale renewable energy integration and optimized grid management.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers</title>
<link>https://arxiv.org/abs/2507.19168</link>
<guid>https://arxiv.org/abs/2507.19168</guid>
<content:encoded><![CDATA[
<div> vibration, acoustic signals, fault detection, unsupervised, explainable artificial intelligence

Summary:
(1) The article introduces a novel unsupervised fault detection and segmentation framework for high-voltage circuit breakers (CBs) based on vibration and acoustic signals.
(2) The proposed framework can detect deviations from the healthy state without the need for ground-truth fault labels.
(3) It utilizes explainable artificial intelligence (XAI) for fault diagnostics, providing potential indications of aged or faulty components to domain experts.
(4) The framework is capable of detecting faults and clustering different fault types using only healthy data during training.
(5) Experimental validation on a high-voltage CB dataset with healthy and artificially induced fault conditions demonstrates the reliability of the system for CB operation.
<br /><br />Summary: <div>
arXiv:2507.19168v1 Announce Type: new 
Abstract: Commercial high-voltage circuit breaker (CB) condition monitoring systems rely on directly observable physical parameters such as gas filling pressure with pre-defined thresholds. While these parameters are crucial, they only cover a small subset of malfunctioning mechanisms and usually can be monitored only if the CB is disconnected from the grid. To facilitate online condition monitoring while CBs remain connected, non-intrusive measurement techniques such as vibration or acoustic signals are necessary. Currently, CB condition monitoring studies using these signals typically utilize supervised methods for fault diagnostics, where ground-truth fault types are known due to artificially introduced faults in laboratory settings. This supervised approach is however not feasible in real-world applications, where fault labels are unavailable. In this work, we propose a novel unsupervised fault detection and segmentation framework for CBs based on vibration and acoustic signals. This framework can detect deviations from the healthy state. The explainable artificial intelligence (XAI) approach is applied to the detected faults for fault diagnostics. The specific contributions are: (1) we propose an integrated unsupervised fault detection and segmentation framework that is capable of detecting faults and clustering different faults with only healthy data required during training (2) we provide an unsupervised explainability-guided fault diagnostics approach using XAI to offer domain experts potential indications of the aged or faulty components, achieving fault diagnostics without the prerequisite of ground-truth fault labels. These contributions are validated using an experimental dataset from a high-voltage CB under healthy and artificially introduced fault conditions, contributing to more reliable CB system operation.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection</title>
<link>https://arxiv.org/abs/2507.19174</link>
<guid>https://arxiv.org/abs/2507.19174</guid>
<content:encoded><![CDATA[
<div> Cough Analysis, Non-Small Cell Lung Cancer, Machine Learning, Deep Learning, Fairness Analysis
<br />
Summary: 
In this study, the potential of automatic cough analysis as a pre-screening tool for distinguishing between non-small cell lung cancer (NSCLC) patients and healthy controls was explored. Machine learning techniques, including support vector machine (SVM) and XGBoost, as well as deep learning approaches like convolutional neural networks (CNN) and transfer learning with VGG16, were utilized to analyze cough audio recordings from 227 subjects. The CNN model demonstrated the highest accuracy of 0.83 on the test set, while SVM performed well with an accuracy of 0.76 in validation and 0.78 in the test set, making it suitable for low computational power contexts. The use of Shapley Additive Explanations (SHAP) improved model interpretability, especially for SVM. Fairness analysis revealed slightly higher disparities across age groups (0.15) compared to gender (0.09) on the test set. However, to enhance the reliability of the findings, a larger, more diverse, and unbiased dataset including at-risk individuals and those in early disease stages is essential. 
<br /><br />Summary: <div>
arXiv:2507.19174v1 Announce Type: new 
Abstract: Early detection of non-small cell lung cancer (NSCLC) is critical for improving patient outcomes, and novel approaches are needed to facilitate early diagnosis. In this study, we explore the use of automatic cough analysis as a pre-screening tool for distinguishing between NSCLC patients and healthy controls. Cough audio recordings were prospectively acquired from a total of 227 subjects, divided into NSCLC patients and healthy controls. The recordings were analyzed using machine learning techniques, such as support vector machine (SVM) and XGBoost, as well as deep learning approaches, specifically convolutional neural networks (CNN) and transfer learning with VGG16. To enhance the interpretability of the machine learning model, we utilized Shapley Additive Explanations (SHAP). The fairness of the models across demographic groups was assessed by comparing the performance of the best model across different age groups (less than or equal to 58y and higher than 58y) and gender using the equalized odds difference on the test set. The results demonstrate that CNN achieves the best performance, with an accuracy of 0.83 on the test set. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76 in validation and 0.78 in the test set), making it suitable in contexts with low computational power. The use of SHAP for SVM interpretation further enhances model transparency, making it more trustworthy for clinical applications. Fairness analysis shows slightly higher disparity across age (0.15) than gender (0.09) on the test set. Therefore, to strengthen our findings' reliability, a larger, more diverse, and unbiased dataset is needed -- particularly including individuals at risk of NSCLC and those in early disease stages.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design</title>
<link>https://arxiv.org/abs/2507.19197</link>
<guid>https://arxiv.org/abs/2507.19197</guid>
<content:encoded><![CDATA[
<div> pixel-wise regression, power integrity, VLSI design, Weakness-Aware Channel Attention, ConvNeXtV2<br />
Summary:
Accurate prediction of power integrity issues like IR drop in VLSI design is crucial but challenging due to the computational cost and scalability limitations of traditional simulation-based solvers. This study introduces a novel approach that reformulates IR drop estimation as a pixel-wise regression task using multi-channel physical maps derived from circuit layouts. By incorporating a Weakness-Aware Channel Attention mechanism into a ConvNeXtV2-based attention U-Net, the method effectively balances the importance of different input layers for improved prediction accuracy. On the ICCAD-2023 benchmark, the proposed approach outperforms the contest winner, achieving a significant reduction in mean absolute error and improved F1-score. These results highlight the significance of considering channel-wise heterogeneity as a crucial factor in physical layout analysis for VLSI design. <br /><br /> <div>
arXiv:2507.19197v1 Announce Type: new 
Abstract: Accurate spatial prediction of power integrity issues, such as IR drop, is critical for reliable VLSI design. However, traditional simulation-based solvers are computationally expensive and difficult to scale. We address this challenge by reformulating IR drop estimation as a pixel-wise regression task on heterogeneous multi-channel physical maps derived from circuit layouts. Prior learning-based methods treat all input layers (e.g., metal, via, and current maps) equally, ignoring their varying importance to prediction accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention (WACA) mechanism, which recursively enhances weak feature channels while suppressing over-dominant ones through a two-stage gating strategy. Integrated into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and balanced feature representation. On the public ICCAD-2023 benchmark, our method outperforms the ICCAD-2023 contest winner by reducing mean absolute error by 61.1% and improving F1-score by 71.0%. These results demonstrate that channel-wise heterogeneity is a key inductive bias in physical layout analysis for VLSI.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems</title>
<link>https://arxiv.org/abs/2507.19205</link>
<guid>https://arxiv.org/abs/2507.19205</guid>
<content:encoded><![CDATA[
<div> machine learning, high-energy physics, graph neural networks, transverse momentum, physics-informed approach

Summary:
The article introduces a physics-informed Graph Neural Network (GNN) framework for real-time transverse momentum ($p_T$) estimation in high-energy physics. The framework incorporates four distinct graph construction strategies, including station-as-node and feature-as-node representations. A novel Message Passing Layer (MPL) with intra-message attention and gated updates enhances the model's accuracy. The framework also includes domain-specific loss functions that consider $p_T$-distribution priors. Experimental results on the CMS Trigger Dataset show that the station-informed EdgeConv model achieves a state-of-the-art Mean Absolute Error (MAE) with significantly fewer parameters than deep learning baselines. The $\eta$-centric MPL configuration also demonstrates improved accuracy with comparable efficiency, showcasing the potential of physics-guided GNNs for deployment in resource-constrained trigger systems. 

Summary: <br /><br /> <div>
arXiv:2507.19205v1 Announce Type: new 
Abstract: Real-time particle transverse momentum ($p_T$) estimation in high-energy physics demands algorithms that are both efficient and accurate under strict hardware constraints. Static machine learning models degrade under high pileup and lack physics-aware optimization, while generic graph neural networks (GNNs) often neglect domain structure critical for robust $p_T$ regression. We propose a physics-informed GNN framework that systematically encodes detector geometry and physical observables through four distinct graph construction strategies that systematically encode detector geometry and physical observables: station-as-node, feature-as-node, bending angle-centric, and pseudorapidity ($\eta$)-centric representations. This framework integrates these tailored graph structures with a novel Message Passing Layer (MPL), featuring intra-message attention and gated updates, and domain-specific loss functions incorporating $p_{T}$-distribution priors. Our co-design methodology yields superior accuracy-efficiency trade-offs compared to existing baselines. Extensive experiments on the CMS Trigger Dataset validate the approach: a station-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with $\ge55\%$ fewer parameters than deep learning baselines, especially TabNet, while an $\eta$-centric MPL configuration also demonstrates improved accuracy with comparable efficiency. These results establish the promise of physics-guided GNNs for deployment in resource-constrained trigger systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dependency-aware synthetic tabular data generation</title>
<link>https://arxiv.org/abs/2507.19211</link>
<guid>https://arxiv.org/abs/2507.19211</guid>
<content:encoded><![CDATA[
<div> Generative models, privacy-sensitive domains, synthetic tabular data, Hierarchical Feature Generation Framework, functional dependencies, logical dependencies<br />
<br />
Summary: 
The article introduces a new framework, Hierarchical Feature Generation Framework (HFGF), designed to improve the preservation of functional dependencies (FDs) and logical dependencies (LDs) in synthetic tabular data. Existing generative models often struggle to maintain inter-attribute relationships effectively, especially when it comes to deterministic and rule-based associations between features. HFGF first generates independent features using a standard generative model and then reconstructs dependent features based on predefined FD and LD rules. Experiments on benchmark datasets of varying sizes and complexities showed that HFGF enhances the structural fidelity of synthetic tabular data and is compatible with popular generative models like CTGAN, TVAE, and GReaT. This research fills a gap in the field by providing a framework that can significantly improve the quality and utility of synthetic tabular data, particularly in privacy-sensitive domains like health care. <div>
arXiv:2507.19211v1 Announce Type: new 
Abstract: Synthetic tabular data is increasingly used in privacy-sensitive domains such as health care, but existing generative models often fail to preserve inter-attribute relationships. In particular, functional dependencies (FDs) and logical dependencies (LDs), which capture deterministic and rule-based associations between features, are rarely or often poorly retained in synthetic datasets. To address this research gap, we propose the Hierarchical Feature Generation Framework (HFGF) for synthetic tabular data generation. We created benchmark datasets with known dependencies to evaluate our proposed HFGF. The framework first generates independent features using any standard generative model, and then reconstructs dependent features based on predefined FD and LD rules. Our experiments on four benchmark datasets with varying sizes, feature imbalance, and dependency complexity demonstrate that HFGF improves the preservation of FDs and LDs across six generative models, including CTGAN, TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance the structural fidelity and downstream utility of synthetic tabular data.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Component-Based Machine Learning for Indoor Flow and Temperature Fields Prediction Latent Feature Aggregation and Flow Interaction</title>
<link>https://arxiv.org/abs/2507.19233</link>
<guid>https://arxiv.org/abs/2507.19233</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor airflow, temperature distribution, machine learning, surrogate modeling, CFD simulations

Summary: 
This study introduces a component-based machine learning (CBML) approach to efficiently predict indoor airflow and temperature distributions. Traditional computational fluid dynamics (CFD) simulations are time-consuming, hindering real-time applications. The CBML model consists of three neural networks: a convolutional autoencoder with residual connections (CAER) for feature extraction, a multilayer perceptron (MLP) for mapping inlet velocities, and a convolutional neural network (CNN) for aggregating single-inlet features. Using a two-dimensional room as a benchmark case, the model accurately predicts velocity and temperature fields across training and testing datasets. The CBML model provides a fast and accurate alternative to traditional CFD simulations for indoor airflow and temperature prediction. <br /><br />Summary: <div>
arXiv:2507.19233v1 Announce Type: new 
Abstract: Accurate and efficient prediction of indoor airflow and temperature distributions is essential for building energy optimization and occupant comfort control. However, traditional CFD simulations are computationally intensive, limiting their integration into real-time or design-iterative workflows. This study proposes a component-based machine learning (CBML) surrogate modeling approach to replace conventional CFD simulation for fast prediction of indoor velocity and temperature fields. The model consists of three neural networks: a convolutional autoencoder with residual connections (CAER) to extract and compress flow features, a multilayer perceptron (MLP) to map inlet velocities to latent representations, and a convolutional neural network (CNN) as an aggregator to combine single-inlet features into dual-inlet scenarios. A two-dimensional room with varying left and right air inlet velocities is used as a benchmark case, with CFD simulations providing training and testing data. Results show that the CBML model accurately and fast predicts two-component aggregated velocity and temperature fields across both training and testing datasets.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Categorical Framework for Language Modeling</title>
<link>https://arxiv.org/abs/2507.19247</link>
<guid>https://arxiv.org/abs/2507.19247</guid>
<content:encoded><![CDATA[
<div> Markov Categories, Auto-regressive language models, NLL objective, Information flow, Spectral contrastive learning <br />
Summary:<br />
This work introduces a new analytical framework using Markov Categories to understand auto-regressive language models (LMs). The framework deconstructs the generation process and the negative log-likelihood (NLL) objective of LMs. The study provides an information-theoretic rationale for the success of speculative decoding methods, quantifying the information surplus in hidden states. It also formalizes how NLL minimization influences the model to learn the intrinsic conditional stochasticity of the data. The research proves that NLL training acts as an implicit form of spectral contrastive learning, aligning the learned representation space with a predictive similarity operator's eigenspectrum. This compositional and information-geometric perspective reveals the underlying principles that make modern LMs effective in learning versatile representations.  <div>
arXiv:2507.19247v1 Announce Type: new 
Abstract: Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the model's prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: https://github.com/asiresearch/lm-theory
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs</title>
<link>https://arxiv.org/abs/2507.19334</link>
<guid>https://arxiv.org/abs/2507.19334</guid>
<content:encoded><![CDATA[
<div> SPADA, Tabular data, Dependency modeling, Lightweight generative framework, Sparse dependencies<br />
Summary: <br />
Tabular data is crucial in various fields but quality datasets are limited due to privacy and cost constraints. Current approaches use large language models for augmentation but face limitations in dense dependency modeling and high computational overhead. To overcome these challenges, SPADA is introduced as a lightweight generative framework that captures sparse dependencies using an LLM-induced graph. It treats each feature as a node and generates values by considering only parent nodes. Two synthesis strategies, non-parametric using Gaussian kernel density estimation and a conditional normalizing flow model, are examined. Experiments show that SPADA reduces bias compared to other methods and speeds up generation significantly. <div>
arXiv:2507.19334v1 Announce Type: new 
Abstract: Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over LLM-based baselines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-Form Video Recommendations with Multimodal Embeddings: Addressing Cold-Start and Bias Challenges</title>
<link>https://arxiv.org/abs/2507.19346</link>
<guid>https://arxiv.org/abs/2507.19346</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, short-form video, e-commerce, recommender systems, multimodal vision-language model

Summary: 
Short-form video platforms have become popular, leading other domains like e-commerce to incorporate them for user engagement. The unique UI innovation of recommending one video at a time poses challenges for recommender systems. Short videos and position bias impact model effectiveness, making it challenging to build solutions. Leveraging a fine-tuned multimodal vision-language model over supervised learning methods proved more effective in online experiments on an e-commerce platform. This approach addressed challenges in launching a new short-form video experience and optimized content recommendations for increased user engagement. The study highlights the importance of adapting recommender systems to immersive feed experiences and the benefits of utilizing advanced models for video retrieval. <div>
arXiv:2507.19346v1 Announce Type: new 
Abstract: In recent years, social media users have spent significant amounts of time on short-form video platforms. As a result, established platforms in other domains, such as e-commerce, have begun introducing short-form video content to engage users and increase their time spent on the platform. The success of these experiences is due not only to the content itself but also to a unique UI innovation: instead of offering users a list of choices to click, platforms actively recommend content for users to watch one at a time. This creates new challenges for recommender systems, especially when launching a new video experience. Beyond the limited interaction data, immersive feed experiences introduce stronger position bias due to the UI and duration bias when optimizing for watch-time, as models tend to favor shorter videos. These issues, together with the feedback loop inherent in recommender systems, make it difficult to build effective solutions. In this paper, we highlight the challenges faced when introducing a new short-form video experience and present our experience showing that, even with sufficient video interaction data, it can be more beneficial to leverage a video retrieval system using a fine-tuned multimodal vision-language model to overcome these challenges. This approach demonstrated greater effectiveness compared to conventional supervised learning methods in online experiments conducted on our e-commerce platform.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators</title>
<link>https://arxiv.org/abs/2507.19349</link>
<guid>https://arxiv.org/abs/2507.19349</guid>
<content:encoded><![CDATA[
<div> reconstruction, spatial signals, signal-to-interference-noise ratio, Group Equivariant Non-Expansive Operators (GENEOs), urban wireless communication networks<br />
<br />
Summary: <br />
The article introduces a new approach for reconstructing spatially-varying quantities like signal-to-interference-noise ratio (SINR) maps in 6G wireless networks using Group Equivariant Non-Expansive Operators (GENEOs). GENEOs, originating in topological data analysis, offer a low-complexity alternative to traditional neural networks by incorporating application-specific invariances to reduce parameters and enforce known constraints. The study focuses on SINR map reconstruction in urban wireless communication networks from extremely sparse measurements. Results show that the GENEO-based approach achieves competitive performance compared to established methods, accurately reconstructing spatial signals under severe data limitations. The evaluation, using statistical and TDA metrics, demonstrates the advantages of this mathematical framework in efficiently recovering SINR maps with limited samples. <div>
arXiv:2507.19349v1 Announce Type: new 
Abstract: In emerging communication systems such as sixth generation (6G) wireless networks, efficient resource management and service delivery rely on accurate knowledge of spatially-varying quantities like signal-to-interference-noise ratio (SINR) maps, which are costly to acquire at high resolution. This work explores the reconstruction of such spatial signals from sparse measurements using Group Equivariant Non-Expansive Operators (GENEOs), offering a low-complexity alternative to traditional neural networks. The concept of GENEO, which originated in topological data analysis (TDA), is a mathematical tool used in machine learning to represent agents modelled as functional operators acting on data while incorporating application-specific invariances. Leveraging these invariances reduces the number of parameters with respect to traditional neural networks and mitigates data scarcity by enforcing known algebraic and geometric constraints that reflect symmetries in the agents' actions. In this paper, we introduce a novel GENEO-based approach for SINR map reconstruction in urban wireless communication networks using extremely sparse sampling. We demonstrate that this mathematical framework achieves competitive performance compared to established methods. Our evaluation, conducted using both statistical and TDA metrics, highlights the advantages of our approach in accurately reconstructing spatial signals under severe data limitations on the number of samples.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Driven Approach to Estimate LEO Orbit Capacity Models</title>
<link>https://arxiv.org/abs/2507.19365</link>
<guid>https://arxiv.org/abs/2507.19365</guid>
<content:encoded><![CDATA[
<div> Keywords: SINDy, LSTM, resident space objects, LEO, satellite debris

Summary:<br />
Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy) and Long Short-Term Memory Recurrent Neural Networks (LSTM), the researchers developed a model to accurately predict the propagation of resident space objects in Low Earth Orbit (LEO). The population of objects was categorized into Active, Derelict, and Debris. The approach combines data from the MOCAT-MC high-fidelity model with a low-fidelity counterpart to provide accurate forecasting in a shorter time frame. The model aims to address the computational complexity of predicting satellite and debris paths by offering a more efficient solution. By leveraging advanced algorithms like SINDy and LSTM, the researchers were able to achieve accurate modeling and prediction of future object movements in space. This method could potentially aid in better understanding and managing space debris, leading to improved satellite operations and risk mitigation strategies.<br /><br />Summary: <div>
arXiv:2507.19365v1 Announce Type: new 
Abstract: Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy) and Long Short-Term Memory Recurrent Neural Networks (LSTM), the population of resident space objects, divided into Active, Derelict, and Debris, in LEO can be accurately modeled to predict future satellite and debris propagation. This proposed approach makes use of a data set coming from a computational expensive high-fidelity model, the MOCAT-MC, to provide a light, low-fidelity counterpart that provides accurate forecasting in a shorter time frame.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation</title>
<link>https://arxiv.org/abs/2507.19368</link>
<guid>https://arxiv.org/abs/2507.19368</guid>
<content:encoded><![CDATA[
<div> counterfactual explanations, deep learning, medical image analysis, generative models, probabilistic models<br />
Summary:<br />
This paper addresses the challenge of generating plausible counterfactual explanations in medical image analysis using model-specific optimization approaches. By combining variational autoencoders (VAEs) with sum-product networks (SPNs), the authors aim to create counterfactuals that adhere to similarity constraints and provide human-interpretable explanations. The integration of SPNs allows for the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. Experimental evaluation on the cheXpert dataset compares the SPN-guided approach with a neural network baseline and analyzes the trade-off between latent variable regularization and counterfactual quality. This research showcases the potential of leveraging generative and probabilistic models to enhance the interpretability of deep learning models in medical image analysis. <br /> <div>
arXiv:2507.19368v1 Announce Type: new 
Abstract: Artificial intelligence is increasingly leveraged across various domains to automate decision-making processes that significantly impact human lives. In medical image analysis, deep learning models have demonstrated remarkable performance. However, their inherent complexity makes them black box systems, raising concerns about reliability and interpretability. Counterfactual explanations provide comprehensible insights into decision processes by presenting hypothetical "what-if" scenarios that alter model classifications. By examining input alterations, counterfactual explanations provide patterns that influence the decision-making process. Despite their potential, generating plausible counterfactuals that adhere to similarity constraints providing human-interpretable explanations remains a challenge. In this paper, we investigate this challenge by a model-specific optimization approach. While deep generative models such as variational autoencoders (VAEs) exhibit significant generative power, probabilistic models like sum-product networks (SPNs) efficiently represent complex joint probability distributions. By modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we leverage its dual role as both a latent space descriptor and a classifier for a given discrimination task. This formulation enables the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. We conduct experimental evaluation on the cheXpert dataset. To evaluate the effectiveness of the integration of SPNs, our SPN-guided latent space manipulation is compared against a neural network baseline. Additionally, the trade-off between latent variable regularization and counterfactual quality is analyzed.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>