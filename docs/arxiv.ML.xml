<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title>
<link>https://arxiv.org/abs/2508.09162</link>
<guid>https://arxiv.org/abs/2508.09162</guid>
<content:encoded><![CDATA[
<div> Keywords: advanced nuclear reactors, replay attacks, explainable AI, autoencoder, windowSHAP<br><br>Summary:<br><br>1. Next generation advanced nuclear reactors are smaller and utilize fully digital instrumentation and control systems, producing complex multivariate time series data representing various nonlinear cyber-physical and operational states. <br><br>2. Ensuring data integrity against deception attacks, particularly replay attacks, is critical for safe and reliable reactor operation amid networked communication environments. <br><br>3. Existing approaches to replay attack detection primarily rely on watermarking or supervised anomaly detection, often using synthetic data with simplifying assumptions like uncorrelated Gaussian noise, full state feedback, or linear systems, which limit effectiveness and fail to capture unmodeled dynamics.<br><br>4. There is a need for unsupervised, explainable AI methods using real data to characterize replay attacks, including detection, timing, source identification, and attack type, especially in regulated nuclear cyber-physical systems.<br><br>5. This study proposes an unsupervised explainable AI framework combining an autoencoder with a customized windowSHAP algorithm to identify and characterize replay attacks in real time during dynamic reactor processes. The framework was tested on real datasets from Purdue’s PUR-1 reactor with up to six concurrently replayed signals, achieving at least 95% accuracy in detecting the presence, source, number, and duration of replayed falsified signals. <div>
arXiv:2508.09162v4 Announce Type: replace 
Abstract: Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory</title>
<link>https://arxiv.org/abs/2511.23083</link>
<guid>https://arxiv.org/abs/2511.23083</guid>
<content:encoded><![CDATA[
<div> Keywords: Kernel Hopfield Networks, Ridge of Optimization, Edge of Stability, Fisher Information Matrix, Minimum Description Length principle<br><br>Summary:<br><br>This article investigates the dynamics of high-capacity kernel Hopfield networks, focusing on a phenomenon known as the Ridge of Optimization, which is characterized by extreme stability in network performance. Previously, this Ridge was linked to Spectral Concentration, though its fundamental origin had not been fully understood. By analyzing the network dynamics on a statistical manifold, the authors reveal that the Ridge corresponds to the Edge of Stability—a critical boundary at which the Fisher Information Matrix becomes singular, indicating a phase transition in system behavior. Furthermore, the study clarifies that the observed antagonism of Euclidean forces in the network dynamics is actually due to a Dual Equilibrium condition when the system is viewed through the lens of Riemannian geometry. This insight provides a unified framework connecting learning dynamics and network capacity through the Minimum Description Length principle, which quantifies model complexity and generalization. Ultimately, the work offers a geometric theory for self-organized criticality in neural networks, highlighting the intrinsic balance between stability and adaptability that governs learning processes and memory capacity in kernel Hopfield models. <div>
arXiv:2511.23083v5 Announce Type: replace 
Abstract: High-capacity kernel Hopfield networks exhibit a \textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contrastive Learning via Spectral Graph Alignment</title>
<link>https://arxiv.org/abs/2512.07878</link>
<guid>https://arxiv.org/abs/2512.07878</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, graph embeddings, normalized Laplacian, SpecMatch-CL, graph-of-graphs<br><br>Summary: This paper introduces SpecMatch-CL, a novel loss function designed to improve contrastive learning on graph data by focusing on aligning the global structure of view-specific graph-of-graphs representations. Traditional contrastive methods like InfoNCE optimize pairwise alignment of graph embeddings from augmented views but do not explicitly control the global structural relationship among these embeddings. SpecMatch-CL addresses this gap by minimizing the difference between the normalized Laplacians of the graph-of-graphs from each view, providing a theoretically grounded mechanism to better align views at a global scale. The authors prove that under certain assumptions, the difference between normalized Laplacians bounds the gap between the current contrastive loss and an ideal Perfect Alignment loss, and also bounds the Uniformly loss, thereby connecting spectral properties with contrastive learning objectives. Empirically, SpecMatch-CL achieves new state-of-the-art results on eight TU benchmarks for both unsupervised and semi-supervised learning at low label rates. Furthermore, it demonstrates consistent performance improvements in transfer learning tasks on large-scale PPI-306K and ZINC 2M graph datasets, indicating its effectiveness and generalization across various graph learning settings. This work contributes both theoretical insights and practical advances in unsupervised graph representation learning. <div>
arXiv:2512.07878v3 Announce Type: replace 
Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural CDEs as Correctors for Learned Time Series Models</title>
<link>https://arxiv.org/abs/2512.12116</link>
<guid>https://arxiv.org/abs/2512.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series forecasting, Predictor-Corrector mechanism, neural controlled differential equation, multi-step prediction, regularization strategies<br><br>Summary:<br><br>This article addresses the challenge of error accumulation in multi-step time-series forecasting using learned models. It proposes a Predictor-Corrector mechanism where the Predictor is any learned time-series model that generates forecasts and the Corrector is a neural controlled differential equation designed to predict and correct the forecasting errors. The combination improves overall forecasting accuracy by adding predicted errors back to the original forecasts. This Corrector is adaptable, working effectively with irregularly sampled time series and both continuous- and discrete-time models. Additionally, the study introduces two novel regularization strategies aimed at enhancing the Corrector’s extrapolation capabilities and speeding up training. The framework is evaluated with a variety of Predictor models including neural ordinary differential equations, Contiformer, and DLinear. Experiments are conducted on synthetic data, physics simulations, and diverse real-world forecasting datasets. The results consistently demonstrate that the Predictor-Corrector approach outperforms using the Predictor model alone, highlighting its robustness and broad applicability for improving multi-step forecasting performance across different settings and data types. <div>
arXiv:2512.12116v2 Announce Type: replace 
Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</title>
<link>https://arxiv.org/abs/2512.17934</link>
<guid>https://arxiv.org/abs/2512.17934</guid>
<content:encoded><![CDATA[
<div> Lung cancer, mortality prediction, machine learning, SHAP analysis, geographic disparities

<br /><br />Summary:  
Lung cancer (LC) remains a leading cause of cancer-related deaths in the U.S., necessitating accurate mortality rate predictions to inform targeted interventions and reduce health disparities. This study compared three predictive models—random forest (RF), gradient boosting regression (GBR), and linear regression (LR)—to estimate county-level LC mortality rates across the country. Model evaluation using R-squared and root mean squared error (RMSE) revealed the RF model outperformed the others, achieving an R2 of 41.9% and RMSE of 12.8. Using Shapley Additive Explanations (SHAP), smoking rate emerged as the most influential predictor, followed by median home value and the percentage of the Hispanic population, highlighting key socioeconomic and behavioral factors shaping LC mortality. Furthermore, spatial analysis via Getis-Ord (Gi*) hotspot detection identified significant clusters of elevated LC mortality primarily in mid-eastern U.S. counties, underscoring geographic disparities. The findings demonstrate that explainable machine learning methods, particularly RF combined with SHAP, provide improved accuracy and interpretability for LC mortality prediction. These insights support the development of focused public health strategies, such as smoking cessation programs and enhanced screening efforts, especially in high-risk regions, to effectively address lung cancer outcomes and related health equity issues across the United States. <div>
arXiv:2512.17934v1 Announce Type: new 
Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD</title>
<link>https://arxiv.org/abs/2512.17945</link>
<guid>https://arxiv.org/abs/2512.17945</guid>
<content:encoded><![CDATA[
<div> Monotonicity constraints, Credit risk, Gradient boosting, Price of Monotonicity, Predictive accuracy<br /><br />Summary:<br /><br />1. This paper addresses the challenge financial institutions face when deploying machine learning models for credit risk, specifically balancing predictive accuracy and model interpretability. 2. It focuses on monotonicity constraints, which enforce model behavior consistent with domain knowledge but may potentially reduce accuracy. 3. The authors introduce the "Price of Monotonicity" (PoM), defined as the relative performance drop when moving from unconstrained to monotone-constrained gradient boosting models, measured using paired comparisons and bootstrap uncertainty. 4. The empirical benchmark includes five public credit datasets and three different gradient boosting libraries, comparing monotone-constrained and unconstrained models for the probability of default prediction. 5. Results show PoM in AUC ranges from nearly zero to about 2.9%, with the penalty being minimal (often below 0.2%) on large datasets and more significant (2-3%) on smaller datasets with heavy monotonic constraints. Overall, the study finds that well-specified monotonicity constraints can provide valuable interpretability with only minor trade-offs in accuracy, especially beneficial for large-scale credit portfolios. <div>
arXiv:2512.17945v1 Announce Type: new 
Abstract: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional-neural-operator-based transfer learning for solving PDEs</title>
<link>https://arxiv.org/abs/2512.17969</link>
<guid>https://arxiv.org/abs/2512.17969</guid>
<content:encoded><![CDATA[
<div> Convolutional neural operator, few-shot learning, PDEs, neuron linear transformation, surrogate accuracy<br /><br />Summary:<br /><br />1. The convolutional neural operator (CNO) is a CNN-based architecture designed to preserve the continuous-discrete equivalence in learning solution operators for partial differential equations (PDEs), enabling alias-free and structure-preserving modeling.<br />2. Previous studies demonstrated that CNO outperforms baseline methods such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy for certain PDE problems.<br />3. However, CNO has not been thoroughly validated in few-shot learning scenarios where limited training data is available for the target task.<br />4. This work extends CNO to few-shot learning by first pre-training it on a source dataset and subsequently adapting its parameters using only a small target dataset.<br />5. Three parameter adjustment strategies were evaluated: fine-tuning, low-rank adaptation, and neuron linear transformation.<br />6. Among these strategies, neuron linear transformation achieved the highest surrogate accuracy across several challenging PDEs, including the Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.<br />7. The results highlight the potential of neuron linear transformation in enabling efficient few-shot learning for neural operators in PDE modeling tasks. <div>
arXiv:2512.17969v1 Announce Type: new 
Abstract: Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs</title>
<link>https://arxiv.org/abs/2512.17970</link>
<guid>https://arxiv.org/abs/2512.17970</guid>
<content:encoded><![CDATA[
<div> Weight-only quantization, codebook-based quantization, GEMM kernel, latency optimization, Llama-3 models<br /><br />Summary: Weight-only quantization helps reduce memory bottlenecks during large language model (LLM) inference. Codebook-based quantization achieves strong accuracy at extremely low bit widths, such as 2-bit, but existing implementations heavily rely on dequantization, which causes latency and cache inefficiencies by repeatedly fetching centroids and reconstructing weights. The paper introduces CodeGEMM, a novel GEMM kernel that centers around the codebook and avoids dequantization by precomputing inner products between centroids and activations, stored in a lightweight structure called the Psumbook. During inference, code indices directly gather partial sums from Psumbook, eliminating costly per-element lookups and significantly reducing on-chip memory usage. CodeGEMM also systematically explores the trade-offs between latency, memory usage, and accuracy within a single framework. Evaluations on Llama-3 models demonstrate that CodeGEMM achieves substantial speedups—1.83x for the 8B model and 8.93x for the 70B model—compared to the previous state-of-the-art codebook-based quantization approaches, all while maintaining comparable accuracy. This kernel consequently improves overall computational efficiency and memory subsystem utilization during LLM inference. <div>
arXiv:2512.17970v1 Announce Type: new 
Abstract: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</title>
<link>https://arxiv.org/abs/2512.17983</link>
<guid>https://arxiv.org/abs/2512.17983</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Quantized LoRA, Human Activity Recognition, Masked Autoencoder, Parameter-Efficient Fine-Tuning<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting large pretrained Human Activity Recognition (HAR) models to new domains under limited computational resources on target devices. It explores parameter-efficient fine-tuning methods, particularly Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA), as alternatives to full model fine-tuning. The proposed framework uses a Masked Autoencoder backbone for feature extraction. The study evaluates performance using a Leave-One-Dataset-Out validation protocol on five publicly available HAR datasets. Experimental results show that both LoRA and QLoRA achieve recognition performance comparable to full fine-tuning but significantly reduce trainable parameters, memory consumption, and training time. Additional analysis demonstrates that LoRA remains effective even with limited supervision and that the adapter rank can be adjusted to balance accuracy against computational efficiency. Furthermore, QLoRA enhances these benefits by quantizing frozen model weights, thereby decreasing memory footprint without substantially degrading classification accuracy. This work highlights scalable, resource-friendly adaptation strategies for deploying large HAR models in practical, resource-constrained environments. <div>
arXiv:2512.17983v1 Announce Type: new 
Abstract: Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</title>
<link>https://arxiv.org/abs/2512.17984</link>
<guid>https://arxiv.org/abs/2512.17984</guid>
<content:encoded><![CDATA[
<div> Traffic flow imputation, Inductive learning, Transductive learning, Graph Neural Networks, Traffic simulation<br /><br />Summary:<br /><br />1. The article addresses the challenge of accurately imputing traffic flow at unsensed locations, noting that data sources like loop detectors are sparse, and probe vehicle speeds only weakly correlate with flow. Additionally, heterophily in neighboring traffic segments disrupts common GNN assumptions.<br /><br />2. The authors propose HINT (Hybrid INductive-Transductive Network), which combines an inductive spatial transformer for learning long-range similarity-driven interactions with a FiLM-conditioned diffusion GCN that incorporates rich static contextual information from OSM and traffic simulation.<br /><br />3. A node-wise calibration layer in HINT corrects scale biases unique to each traffic segment, improving accuracy.<br /><br />4. The INDU-TRANSDUCTIVE training strategy treats speed as a transductive network-wide signal while learning flow inductively; the method uses masked reconstruction, epoch-wise node sampling, hard-node mining, and noise injection to prevent overfitting.<br /><br />5. Experiments on three real-world datasets (Antwerp, Torino, Essen) show that HINT consistently outperforms state-of-the-art inductive methods, reducing MAE significantly, especially when including traffic simulation data.<br /><br />6. The results demonstrate the benefit of combining inductive flow imputation with transductive speed signals, simulations, and external geospatial data to improve traffic flow prediction accuracy at unsensed locations. <div>
arXiv:2512.17984v1 Announce Type: new 
Abstract: Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.
  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar &amp; Unfamiliar Movements</title>
<link>https://arxiv.org/abs/2512.17985</link>
<guid>https://arxiv.org/abs/2512.17985</guid>
<content:encoded><![CDATA[
<div> Keywords: next point of interest prediction, human mobility, Mixture-of-Experts, Transformer model, personalized recommendation<br /><br />Summary:  
The paper addresses the challenge of accurately predicting the next point of interest (POI) within human mobility trajectories, which is pivotal for location-based services and personalized recommendations. It highlights that users show different POI preferences when moving in familiar versus unfamiliar areas, a distinction often overlooked by existing methods. To tackle this, the authors propose MoE-TransMov, a Transformer-based model augmented with a Mixture-of-Experts (MoE) architecture. This model captures distinct mobility patterns by dynamically selecting expert networks specialized for familiar and unfamiliar movement contexts through adaptive gating mechanisms. The framework does not require separate training of models for different contexts, enabling efficient learning within a unified system. Experiments were conducted on two real-world datasets: the small-scale open-source Foursquare NYC dataset and the large-scale Kyoto dataset by LY Corporation. Results demonstrate that MoE-TransMov consistently outperforms state-of-the-art baselines in multiple evaluation metrics, including Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). The improved prediction accuracy emphasizes the value of incorporating user familiarity into mobility modeling. Ultimately, the approach advances personalized recommendation systems and supports enhanced urban applications by better understanding users’ context-dependent movement behaviors. <div>
arXiv:2512.17985v1 Announce Type: new 
Abstract: Accurate prediction of the next point of interest (POI) within human mobility trajectories is essential for location-based services, as it enables more timely and personalized recommendations. In particular, with the rise of these approaches, studies have shown that users exhibit different POI choices in their familiar and unfamiliar areas, highlighting the importance of incorporating user familiarity into predictive models. However, existing methods often fail to distinguish between the movements of users in familiar and unfamiliar regions. To address this, we propose MoE-TransMov, a Transformer-based model with a Transformer model with a Mixture-of-Experts (MoE) architecture designed to use one framework to capture distinct mobility patterns across different moving contexts without requiring separate training for certain data. Using user-check-in data, we classify movements into familiar and unfamiliar categories and develop a specialized expert network to improve prediction accuracy. Our approach integrates self-attention mechanisms and adaptive gating networks to dynamically select the most relevant expert models for different mobility contexts. Experiments on two real-world datasets, including the widely used but small open-source Foursquare NYC dataset and the large-scale Kyoto dataset collected with LY Corporation (Yahoo Japan Corporation), show that MoE-TransMov outperforms state-of-the-art baselines with notable improvements in Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). Given the results, we find that by using this approach, we can efficiently improve mobility predictions under different moving contexts, thereby enhancing the personalization of recommendation systems and advancing various urban applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability</title>
<link>https://arxiv.org/abs/2512.17986</link>
<guid>https://arxiv.org/abs/2512.17986</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Client-Drift, Autoencoder Denoiser, Non-IID Data, Partial Client Participation<br /><br />Summary:<br /><br />1. The paper addresses challenges in Federated Learning (FL) related to client-drift and increased variance caused by heterogeneous data and limited client participation.  
2. It highlights the impact of data privacy regulations such as GDPR and HIPAA on the deployment of traditional machine learning models, motivating the use of FL where raw data remains on local devices.  
3. The authors propose FedOAED, a novel FL algorithm that integrates an on-device autoencoder denoiser to reduce client-drift caused by multiple local training updates and to mitigate variance from partial client availability.  
4. FedOAED is designed specifically to handle Non-IID data distributions, where data across clients is not identically and independently distributed—a common real-world scenario in FL.  
5. Experimental results on multiple vision datasets demonstrate that FedOAED consistently outperforms current state-of-the-art federated learning approaches, showing improved model accuracy and robustness in heterogeneous and constrained client participation environments. <div>
arXiv:2512.17986v1 Announce Type: new 
Abstract: Over the last few decades, machine learning (ML) and deep learning (DL) solutions have demonstrated their potential across many applications by leveraging large amounts of high-quality data. However, strict data-sharing regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) have prevented many data-driven applications from being realised. Federated Learning (FL), in which raw data never leaves local devices, has shown promise in overcoming these limitations. Although FL has grown rapidly in recent years, it still struggles with heterogeneity, which produces gradient noise, client-drift, and increased variance from partial client participation. In this paper, we propose FedOAED, a novel federated learning algorithm designed to mitigate client-drift arising from multiple local training updates and the variance induced by partial client participation. FedOAED incorporates an on-device autoencoder denoiser on the client side to mitigate client-drift and variance resulting from heterogeneous data under limited client availability. Experiments on multiple vision datasets under Non-IID settings demonstrate that FedOAED consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</title>
<link>https://arxiv.org/abs/2512.18031</link>
<guid>https://arxiv.org/abs/2512.18031</guid>
<content:encoded><![CDATA[
<div> Atrial fibrillation, ICU, machine learning, ECG foundation models, transfer learning  

<br /><br />Summary:  
1. The study focuses on detecting atrial fibrillation (AF), the most common cardiac arrhythmia in intensive care unit (ICU) patients, which can lead to significant adverse health outcomes.  
2. A newly labeled ICU dataset is published alongside benchmark results to facilitate research in AF detection.  
3. The research compares three AI-based machine learning approaches for AF detection: traditional feature-based classifiers, deep learning (DL) models, and ECG foundation models (FMs).  
4. Experiments utilize ECG data from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge, exploring different training regimes including zero-shot inference and transfer learning.  
5. Results show that ECG foundation models outperform deep learning and feature-based methods, with the best performance (F1 score of 0.89) achieved by an ECG-FM using transfer learning on the ICU test set.  
6. The study demonstrates AI's promising potential for enhancing automatic patient monitoring systems in critical care.  
7. By releasing their labeled ICU dataset and benchmarking results, the authors invite the research community to improve and advance AF detection technologies in ICU settings. <div>
arXiv:2512.18031v1 Announce Type: new 
Abstract: Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models</title>
<link>https://arxiv.org/abs/2512.18035</link>
<guid>https://arxiv.org/abs/2512.18035</guid>
<content:encoded><![CDATA[
<div> Keywords: selective forgetting, machine unlearning, privacy leakage, data protection, privacy attacks  

<br /><br />Summary:  
1. This article addresses the emerging concept of selective forgetting, also known as machine unlearning, which enables AI models to erase the influence of specific previously seen data.  
2. Selective forgetting has become increasingly important for ensuring privacy, regulatory compliance, and alignment with human values, especially as AI systems are deployed in sensitive domains.  
3. Despite its potential benefits, selective forgetting raises significant privacy concerns due to new privacy attacks tailored to exploit vulnerabilities introduced during the unlearning process.  
4. Existing privacy attacks vary widely in their evaluation setups, often favoring certain methods without fair or consistent comparisons, leading to overly optimistic assessments of vulnerabilities.  
5. The work presents the first comprehensive benchmark that systematically evaluates privacy leakage risks associated with different unlearning techniques, victim data types, privacy attacks, and model architectures.  
6. The study identifies critical factors influencing privacy leakage induced by unlearning and provides novel insights into the risks and trade-offs inherent in deploying machine unlearning technologies.  
7. Ultimately, this benchmark serves as a standardized tool, enabling practitioners to assess privacy risks faithfully and customize unlearning applications according to their specific privacy needs. <div>
arXiv:2512.18035v1 Announce Type: new 
Abstract: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics</title>
<link>https://arxiv.org/abs/2512.18056</link>
<guid>https://arxiv.org/abs/2512.18056</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic digital twin, user identity, variational autoencoder, latent representation, uncertainty quantification<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding user identity and behavior, critical for applications like personalization and recommendation systems.<br />2. It critiques existing methods for relying on deterministic embeddings or opaque predictive models that lack uncertainty quantification and interpretability.<br />3. To overcome these limitations, the authors propose a probabilistic digital twin framework modeling each user as a latent stochastic state that generates observed behavioral data.<br />4. The framework employs amortized variational inference for scalable posterior estimation, maintaining a full probabilistic perspective.<br />5. The digital twin is instantiated via a variational autoencoder (VAE) applied to a specially designed user-response dataset that captures stable aspects of user identity.<br />6. Beyond traditional reconstruction metrics, the study introduces an interpretation pipeline that statistically links latent dimensions to observable behavioral traits.<br />7. By examining users at the extremes of latent dimensions and using nonparametric hypothesis testing plus effect size measures, specific latent dimensions are shown to correspond to meaningful traits like opinion strength and decisiveness.<br />8. The empirical analysis reveals that user structures are mostly continuous rather than discretely clustered, with weak but interpretable structures emerging along a limited number of dominant latent axes.<br />9. The results highlight that probabilistic digital twins can produce interpretable, uncertainty-aware user representations that surpass deterministic embedding approaches in insight and flexibility. <div>
arXiv:2512.18056v1 Announce Type: new 
Abstract: Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins</title>
<link>https://arxiv.org/abs/2512.18104</link>
<guid>https://arxiv.org/abs/2512.18104</guid>
<content:encoded><![CDATA[
<div> Aleatoric uncertainty, Variational Deep Material Network, digital twins, microstructure variability, inverse calibration<br /><br />Summary:<br /><br />1. The paper addresses the challenge of aleatoric uncertainties, which are inherent and irremovable variabilities in material microstructure, constituent behavior, and processing conditions, that complicate the development of uncertainty-robust digital twins.<br />2. It introduces the Variational Deep Material Network (VDMN), a physics-informed surrogate model designed to perform efficient and probabilistic forward and inverse predictions of material behavior while capturing microstructure-induced variability.<br />3. The VDMN architecture embeds variational distributions within a hierarchical, mechanistic framework and employs an analytic uncertainty propagation method based on Taylor-series expansion combined with automatic differentiation to efficiently handle uncertainty during both training and prediction phases.<br />4. The model's effectiveness is demonstrated through two applications: first, as an uncertainty-aware materials digital twin predicting and experimentally validating nonlinear mechanical variability in additively manufactured polymer composites; second, as an inverse calibration tool that disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties.<br />5. The study establishes the VDMN as a foundational approach for creating materials digital twins that are robust to uncertainty, enabling enhanced predictive accuracy and improved understanding of material variability in digital manufacturing processes. <div>
arXiv:2512.18104v1 Announce Type: new 
Abstract: Aleatoric uncertainties - irremovable variability in microstructure morphology, constituent behavior, and processing conditions - pose a major challenge to developing uncertainty-robust digital twins. We introduce the Variational Deep Material Network (VDMN), a physics-informed surrogate model that enables efficient and probabilistic forward and inverse predictions of material behavior. The VDMN captures microstructure-induced variability by embedding variational distributions within its hierarchical, mechanistic architecture. Using an analytic propagation scheme based on Taylor-series expansion and automatic differentiation, the VDMN efficiently propagates uncertainty through the network during training and prediction. We demonstrate its capabilities in two digital-twin-driven applications: (1) as an uncertainty-aware materials digital twin, it predicts and experimentally validates the nonlinear mechanical variability in additively manufactured polymer composites; and (2) as an inverse calibration engine, it disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties. Together, these results establish the VDMN as a foundation for uncertainty-robust materials digital twins.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalizable Neural Operators for Inverse Problems</title>
<link>https://arxiv.org/abs/2512.18120</link>
<guid>https://arxiv.org/abs/2512.18120</guid>
<content:encoded><![CDATA[
<div> Keywords: inverse problems, neural operators, basis-to-basis, ill-posedness, PDE benchmarks<br /><br />Summary: Inverse problems in neural operator learning present challenges due to violations of continuity, uniqueness, and stability, which are critical assumptions for existing architectures. The article introduces B2B${}^{-1}$, a novel inverse basis-to-basis neural operator framework that addresses these challenges by decoupling the function representation from the inverse mapping process. The core innovation is learning neural basis functions separately for input and output function spaces, then performing inversion on their coefficient spaces, which simplifies and stabilizes the inverse problem. This architecture supports training deterministic, invertible, and probabilistic inverse models within a unified framework, allowing model selection based on the degree of ill-posedness in the problem. Evaluation is conducted on six inverse partial differential equation (PDE) benchmarks, including two newly created datasets, demonstrating competitive performance relative to existing invertible neural operator baselines. The probabilistic models developed effectively capture uncertainty and input variability and exhibit robustness to measurement noise through implicit denoising during coefficient computation. Results highlight consistent re-simulation accuracy across different levels of ill-posedness, underscoring the framework's ability to generate scalable surrogate models that generalize well across various instances, spatial domains, and problem difficulties. <div>
arXiv:2512.18120v1 Announce Type: new 
Abstract: Inverse problems challenge existing neural operator architectures because ill-posed inverse maps violate continuity, uniqueness, and stability assumptions. We introduce B2B${}^{-1}$, an inverse basis-to-basis neural operator framework that addresses this limitation. Our key innovation is to decouple function representation from the inverse map. We learn neural basis functions for the input and output spaces, then train inverse models that operate on the resulting coefficient space. This structure allows us to learn deterministic, invertible, and probabilistic models within a single framework, and to choose models based on the degree of ill-posedness. We evaluate our approach on six inverse PDE benchmarks, including two novel datasets, and compare against existing invertible neural operator baselines. We learn probabilistic models that capture uncertainty and input variability, and remain robust to measurement noise due to implicit denoising in the coefficient calculation. Our results show consistent re-simulation performance across varying levels of ill-posedness. By separating representation from inversion, our framework enables scalable surrogate models for inverse problems that generalize across instances, domains, and degrees of ill-posedness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates</title>
<link>https://arxiv.org/abs/2512.18129</link>
<guid>https://arxiv.org/abs/2512.18129</guid>
<content:encoded><![CDATA[
<div> Keywords: survival analysis, longitudinal covariates, transformer, hazard function, model calibration<br /><br />Summary:<br /><br />1. This paper introduces TraCeR, a novel transformer-based framework for survival analysis that effectively incorporates longitudinal covariates, addressing a limitation of many previous models that mainly use cross-sectional features.<br /><br />2. TraCeR utilizes a factorized self-attention architecture to model the hazard function from sequences of time-varying measurements, enabling it to capture complex temporal interactions among covariates without relying on strict assumptions about the data-generating process.<br /><br />3. The design of TraCeR inherently supports censored data and competing risks, making it applicable to realistic survival analysis scenarios where some event times are unknown or multiple event types may occur.<br /><br />4. Extensive experiments on various real-world datasets demonstrate that TraCeR significantly outperforms existing state-of-the-art survival analysis methods, showing notable improvements in predictive accuracy.<br /><br />5. Beyond traditional evaluation metrics focused on discrimination, the study emphasizes assessing model calibration as well, filling a critical gap in survival model evaluation and ensuring more reliable risk predictions in practice. <div>
arXiv:2512.18129v1 Announce Type: new 
Abstract: Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</title>
<link>https://arxiv.org/abs/2512.18133</link>
<guid>https://arxiv.org/abs/2512.18133</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Fraud Detection, Adaptive Camouflage, relation diffusion, graph contrastive learning, financial security<br /><br />Summary: Graph Fraud Detection (GFD) in financial scenarios is crucial for ensuring online payment security. Fraudsters increasingly use sophisticated camouflage strategies called Adaptive Camouflage, where they mimic benign user behaviors to hide their fraudulent activities, making it hard for current GFD models to detect them. To tackle this challenge, the authors propose Grad, a relation diffusion-based graph augmentation model. Grad integrates a supervised graph contrastive learning module which amplifies the differences between fraudulent and benign users. Additionally, it features a guided relation diffusion generator that creates auxiliary homophilic relations from scratch to strengthen weak fraudulent signals. By enhancing these signals during the aggregation process, fraudulent activities become more distinguishable and detectable. The model’s effectiveness is validated through extensive experiments on two real-world datasets from WeChat Pay—one of the largest online payment platforms—and three public datasets. Grad consistently outperforms state-of-the-art methods across various scenarios, achieving improvements of up to 11.10% in AUC and 43.95% in Average Precision (AP). The authors have released the source code publicly to support further research and development in fraud detection. <div>
arXiv:2512.18133v1 Announce Type: new 
Abstract: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2512.18174</link>
<guid>https://arxiv.org/abs/2512.18174</guid>
<content:encoded><![CDATA[
<div> Keywords: data portability, user autonomy, chain-of-thought reasoning, personal data, model distillation  

<br /><br />Summary:  
This paper addresses the emerging challenges surrounding data portability and user autonomy within the context of large language models (LLMs) that employ chain-of-thought (CoT) reasoning. First, it argues from a legal perspective that the intermediate reasoning steps generated by LLMs—referred to as CoT traces—should be classified as users' personal data under current privacy and portability laws. Building on this, the paper introduces and extends the concept of Conscious Data Contribution, proposing a framework whereby communities dissatisfied with existing LLM utility can collaboratively aggregate and distill their shared domain knowledge into a new, community-aligned model. The authors empirically validate this methodology, demonstrating that it enables the creation of alternate models better tailored to the specific goals and values of distinct user groups. Additionally, the study examines how factors such as community diversity, the granularity of reasoning steps, and the size of the contributing community influence the effectiveness and performance of the model distillation process. Overall, this work highlights a novel intersection of legal considerations, ethical data usage, and technical approaches to empower user communities in asserting greater control over AI model behavior through targeted knowledge distillation. <div>
arXiv:2512.18174v1 Announce Type: new 
Abstract: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that "reason" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairExpand: Individual Fairness on Graphs with Partial Similarity Information</title>
<link>https://arxiv.org/abs/2512.18180</link>
<guid>https://arxiv.org/abs/2512.18180</guid>
<content:encoded><![CDATA[
<div> Individual fairness, graph representation learning, similarity propagation, partial similarity information, FairExpand<br /><br />Summary:<br /><br />1. The paper addresses individual fairness in graph representation learning, emphasizing that similar individuals should be treated similarly by algorithmic systems.  
2. Existing methods rely on predefined similarity information across all node pairs, which is impractical in many real-world scenarios due to incomplete knowledge.  
3. The authors propose FairExpand, a novel framework designed to enforce individual fairness when similarity data is only available for a limited subset of node pairs.  
4. FairExpand operates via a two-step iterative process: refining node representations using a backbone model like a graph neural network, and gradually propagating the limited similarity information throughout the graph.  
5. Experimental results demonstrate that FairExpand significantly improves individual fairness metrics while maintaining the overall model performance, proving its practical utility for real-world applications where similarity information is partial or incomplete. <div>
arXiv:2512.18180v1 Announce Type: new 
Abstract: Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics</title>
<link>https://arxiv.org/abs/2512.18209</link>
<guid>https://arxiv.org/abs/2512.18209</guid>
<content:encoded><![CDATA[
<div> Keywords: power-law scaling, Generalized Resolution-Shell Dynamics, renormalizability, gradient flow, log-shift invariance

<br /><br />Summary:  
This article investigates the theoretical foundation of empirical power-law scaling observed in deep learning systems, focusing on the Generalized Resolution-Shell Dynamics (GRSD) framework, which models learning as spectral energy transport across logarithmic resolution shells. It highlights that power-law scaling corresponds to a renormalized shell dynamics within GRSD but is not guaranteed without specific structural properties of the learning process. The study identifies sufficient conditions for renormalizable coarse-grained shell dynamics, including bounded gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution during training, and log-shift invariance of renormalized shell couplings. Importantly, the paper shows that renormalizability alone does not produce power-law scaling; instead, this behavior emerges as a rigidity consequence when log-shift invariance is combined with the time-rescaling covariance intrinsic to gradient flow. Under these combined conditions, the renormalized GRSD velocity field necessarily adopts a power-law form, providing a deeper understanding of when and why power-law scaling arises in deep learning dynamics. This work bridges empirical observations with theoretical insights on scaling laws, offering a structured framework to explain learning behavior from a coarse-grained dynamical systems perspective. <div>
arXiv:2512.18209v1 Announce Type: new 
Abstract: Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.
  In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable and Efficient Single-Rollout RL for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.18215</link>
<guid>https://arxiv.org/abs/2512.18215</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Multimodal Large Language Models, Single-Rollout, Stability, Advantage-Shaping<br /><br />Summary:<br /><br />This paper addresses the challenge of optimizing Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs), focusing on the trade-off between training stability and efficiency. Existing group-based algorithms like GRPO rely on multiple rollouts per prompt but are computationally expensive. Conversely, single-rollout methods explored in text-only contexts suffer from instability and training collapse when applied to multimodal settings. To overcome these issues, the authors propose MSSR (Multimodal Stabilized Single-Rollout), a novel RLVR framework that eliminates group-based rollouts while ensuring stable and effective training. MSSR incorporates an entropy-based advantage-shaping mechanism that adaptively regulates advantage magnitudes, preventing collapse and preserving stability during training. This approach is shown to be essential for multimodal single-rollout environments, not just beneficial as in group-based settings. Experimental results demonstrate that MSSR achieves comparable validation accuracy to group-based methods using half the training steps, indicating superior compute efficiency. When trained equivalently, MSSR outperforms group-based baselines and generalizes better across five benchmarks focused on complex multimodal reasoning. Overall, MSSR enables stable, efficient, and high-performing reinforcement learning tailored to the demanding context of multimodal reasoning tasks. <div>
arXiv:2512.18215v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Behavioral Data Selection</title>
<link>https://arxiv.org/abs/2512.18246</link>
<guid>https://arxiv.org/abs/2512.18246</guid>
<content:encoded><![CDATA[
<div> Keywords: Behavioral cloning, Offline policy learning, Data selection, Stepwise Dual Ranking, D4RL benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of computationally intensive training caused by the large scale of offline behavioral datasets used in behavioral cloning for offline policy learning. 2. It reveals a phenomenon called data saturation, where policy performance quickly plateaus even when trained on a small fraction of the dataset, indicating inefficiencies in using the entire dataset. 3. This saturation is linked to a weak correlation between policy performance and test loss, suggesting that not all data contribute equally to improving performance. 4. To improve data efficiency, the authors propose Stepwise Dual Ranking (SDR), a method that extracts a compact, informative subset from large offline behavioral datasets by prioritizing early-stage data (stepwise clip) and selecting samples based on a combined ranking of high action-value and low state-density. 5. Extensive experiments and ablation studies on the D4RL benchmark suite demonstrate that SDR significantly enhances the quality of data selection, improving offline behavioral cloning performance while reducing computational demands. <div>
arXiv:2512.18246v1 Announce Type: new 
Abstract: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence Rate of LoRA Gradient Descent</title>
<link>https://arxiv.org/abs/2512.18248</link>
<guid>https://arxiv.org/abs/2512.18248</guid>
<content:encoded><![CDATA[
<div> LoRA, convergence analysis, gradient descent, low-rank adaptation, non-asymptotic  

<br /><br />Summary:  
The paper analyzes the low-rank adaptation (LoRA) algorithm, which is widely used to fine-tune large models efficiently by training two low-rank adapter matrices, thereby significantly reducing the number of parameters updated during training. Despite its empirical success, the convergence behavior of LoRA remains poorly understood because the problem lacks Lipschitz smoothness, a common assumption in classical convergence proofs. Existing theoretical work either focuses on asymptotic results or imposes strong boundedness assumptions that artificially ensure Lipschitz smoothness, limiting practical relevance. This study provides the first non-asymptotic convergence analysis of the original LoRA gradient descent algorithm under realistic settings without such assumptions. The authors achieve this by reformulating the problem in terms of the outer product of the concatenated adapter matrices, introducing a modified descent lemma tailored to a "Lipschitz-like" property of the reparametrized function, and controlling the step size carefully. Their analysis proves that LoRA gradient descent converges to a stationary point at a rate of \(O(\frac{1}{\log T})\), where \(T\) is the number of iterations, offering new theoretical insights into the optimization dynamics and guarantees for LoRA’s practical use. <div>
arXiv:2512.18248v1 Announce Type: new 
Abstract: The low-rank adaptation (LoRA) algorithm for fine-tuning large models has grown popular in recent years due to its remarkable performance and low computational requirements. LoRA trains two ``adapter" matrices that form a low-rank representation of the model parameters, thereby massively reducing the number of parameters that need to be updated at every step. Although LoRA is simple, its convergence is poorly understood due to the lack of Lipschitz smoothness, a key condition for classic convergence analyses. As a result, current theoretical results only consider asymptotic behavior or assume strong boundedness conditions which artificially enforce Lipschitz smoothness. In this work, we provide for the first time a non-asymptotic convergence analysis of the \textit{original LoRA gradient descent} algorithm, which reflects widespread practice, without such assumptions. Our work relies on three key steps: i) reformulating the problem in terms of the outer product of the stacked adapter matrices, ii) a modified descent lemma for the ``Lipschitz-like" reparametrized function, and iii) controlling the step size. With this approach, we prove that LoRA gradient descent converges to a stationary point at rate $O(\frac{1}{\log T})$, where $T$ is the number of iterations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform</title>
<link>https://arxiv.org/abs/2512.18266</link>
<guid>https://arxiv.org/abs/2512.18266</guid>
<content:encoded><![CDATA[
<div> Keywords: Databricks, job scheduling, cost optimization, machine learning, resource allocation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of managing rising operational costs in job execution on the Databricks platform, a widely used environment for big data processing.<br /><br />2. Traditional cost management approaches are limited by static configurations or reactive mechanisms, which are insufficient for handling the dynamic and variable nature of workloads.<br /><br />3. The authors propose LeJOT, an intelligent orchestration framework that integrates machine learning-based execution time predictions with a solver-driven optimization model for real-time resource allocation.<br /><br />4. LeJOT proactively forecasts workload demands, enabling dynamic allocation of computing resources that minimizes cloud computing expenses without compromising performance targets.<br /><br />5. Experimental evaluation on real Databricks workloads demonstrates that LeJOT can reduce cloud computing costs by an average of 20% within scheduling intervals of about one minute, outperforming conventional static allocation strategies.<br /><br />6. The framework offers a scalable and adaptive solution particularly suited for cost-efficient job scheduling in Data Lakehouse environments, enhancing both operational efficiency and cost-effectiveness. <div>
arXiv:2512.18266v1 Announce Type: new 
Abstract: With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a critical challenge. Existing solutions rely on static configurations or reactive adjustments, which fail to adapt to the dynamic nature of workloads. To address this, we introduce LeJOT, an intelligent job cost orchestration framework that leverages machine learning for execution time prediction and a solver-based optimization model for real-time resource allocation. Unlike conventional scheduling techniques, LeJOT proactively predicts workload demands, dynamically allocates computing resources, and minimizes costs while ensuring performance requirements are met. Experimental results on real-world Databricks workloads demonstrate that LeJOT achieves an average 20% reduction in cloud computing costs within a minute-level scheduling timeframe, outperforming traditional static allocation strategies. Our approach provides a scalable and adaptive solution for cost-efficient job scheduling in Data Lakehouse environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation</title>
<link>https://arxiv.org/abs/2512.18275</link>
<guid>https://arxiv.org/abs/2512.18275</guid>
<content:encoded><![CDATA[
<div> Federated Learning, client participation, FedSUM, delay metrics, convergence guarantees<br /><br />Summary:<br /><br />1. Federated Learning (FL) methods traditionally assume specific client participation patterns, which restrict their effectiveness in real-world applications where client availability varies.  
2. The paper introduces the FedSUM family of algorithms designed to support arbitrary client participation without relying on assumptions about the heterogeneity of client data.  
3. FedSUM models client participation variability through two delay metrics: maximum delay ($\tau_{\max}$) and average delay ($\tau_{\text{avg}}$), capturing different aspects of participation timing.  
4. The FedSUM family consists of three variants tailored for different needs: FedSUM-B (basic), FedSUM (standard), and FedSUM-CR (which reduces communication overhead).  
5. The authors provide unified convergence guarantees for all FedSUM variants, showing their effectiveness across a wide range of client participation scenarios, thereby enhancing the robustness and applicability of FL in diverse practical deployments. <div>
arXiv:2512.18275v1 Announce Type: new 
Abstract: Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $\tau_{\max}$ and the average delay $\tau_{\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</title>
<link>https://arxiv.org/abs/2512.18295</link>
<guid>https://arxiv.org/abs/2512.18295</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Graph Learning, Analytic Learning Theory, Recursive Least Squares, Catastrophic Forgetting, Privacy Preservation  

<br /><br />Summary:  
Continual Graph Learning (CGL) is focused on enabling graph neural networks to progressively learn from streaming graph data without losing previously acquired knowledge. Existing experience replay methods mitigate forgetting by storing and revisiting past graph data but suffer from privacy issues and inefficiency. The proposed AL GNN framework offers a novel solution by eliminating the need for backpropagation and replay buffers. Instead, AL GNN utilizes analytic learning theory to express learning as a recursive least squares optimization, allowing it to update model knowledge through closed-form classifier updates and a regularized feature autocorrelation matrix. This approach enables efficient one-pass training for each task while inherently preserving data privacy since no historical data samples are stored. Extensive experiments on dynamic graph classification benchmarks show that AL GNN achieves competitive or better results compared to existing methods. Empirically, it improves average task performance by 10% on the CoraFull dataset, reduces catastrophic forgetting by over 30% on Reddit, and cuts training time by nearly 50% due to its backpropagation-free nature. This work advances continual learning for graphs by combining efficiency, privacy, and strong performance. <div>
arXiv:2512.18295v1 Announce Type: new 
Abstract: Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</title>
<link>https://arxiv.org/abs/2512.18309</link>
<guid>https://arxiv.org/abs/2512.18309</guid>
<content:encoded><![CDATA[
<div> Embedded Safety-Aligned Intelligence, Multi-agent Reinforcement Learning, Internal Alignment Embeddings, Counterfactual Reasoning, Graph Diffusion<br /><br />Summary:<br /><br />This paper introduces Embedded Safety-Aligned Intelligence (ESAI), a novel theoretical framework for multi-agent reinforcement learning that integrates alignment constraints directly into the agents' internal representations. Instead of relying on external reward shaping or post-hoc safety constraints, ESAI uses differentiable internal alignment embeddings—latent variables learned to predict potential externalized harm through counterfactual reasoning. These embeddings influence policy updates by leveraging attention mechanisms and graph-based propagation to steer agents toward harm reduction. The ESAI framework combines four core mechanisms: differentiable counterfactual alignment penalties derived from soft reference distributions, alignment-weighted perceptual attention to prioritize important information, Hebbian associative memory facilitating temporal credit assignment, and similarity-weighted graph diffusion enhanced with bias mitigation controls. The paper further analyzes conditions for the stability of these internal embeddings, emphasizing Lipschitz continuity and spectral constraints, and studies theoretical properties such as contraction behavior and the tradeoffs between fairness and performance. While ESAI represents an important conceptual advancement in differentiable alignment for multi-agent systems, several open theoretical questions remain regarding convergence guarantees, optimal embedding dimensionality, and scalability to high-dimensional environments. Empirical validation of the framework is left for future work. <div>
arXiv:2512.18309v1 Announce Type: new 
Abstract: We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.
  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.
  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</title>
<link>https://arxiv.org/abs/2512.18317</link>
<guid>https://arxiv.org/abs/2512.18317</guid>
<content:encoded><![CDATA[
<div> compressed air systems, reinforcement learning, energy efficiency, explainability, industrial control<br /><br />Summary:<br /><br />This paper introduces a trustworthy reinforcement learning (RL) framework tailored for controlling industrial compressed air systems with the goal of achieving safe and energy-efficient operation under realistic boundary conditions. The authors develop a multi-level explainability pipeline that integrates input perturbation tests, gradient-based sensitivity analysis, and SHAP feature attribution to ensure transparency and interpretability of the learned RL policy. Empirical evaluations conducted on multiple compressor configurations demonstrate that the learned policy is physically plausible, effectively anticipates future demand, and consistently respects system boundaries, thereby validating its practical applicability. Compared to the traditional industrial controller currently installed, the proposed RL approach reduces unnecessary overpressure and leads to energy savings of approximately 4% without depending on explicit physics-based models. The analysis further reveals that system pressure and forecast information predominantly influence policy decisions, while inputs specific to individual compressors have a lesser impact. Overall, the combination of measurable energy efficiency improvements, predictive control behavior, and a robust explainability methodology supports the reliable and transparent deployment of reinforcement learning techniques in industrial energy management systems. <div>
arXiv:2512.18317v1 Announce Type: new 
Abstract: This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\,\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</title>
<link>https://arxiv.org/abs/2512.18373</link>
<guid>https://arxiv.org/abs/2512.18373</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network optimization, stochastic gradient descent, second-order methods, adaptive learning rates, deep learning training  

<br /><br />Summary:  
This thesis addresses the critical and complex challenge of optimizing neural networks, a foundational aspect of modern AI research. It begins by analyzing classical first-order methods such as stochastic gradient descent (SGD) and adaptive gradient algorithms, highlighting their empirical success but also their theoretical limitations, especially in handling data anisotropy typical of real-world scenarios. Motivated by these limitations, the work explores more advanced techniques that incorporate curvature information, including second-order approximations, layer-wise preconditioning, and adaptive learning rates, which provide sophisticated means to improve training efficiency and model performance. Additionally, the thesis emphasizes the importance of integrating these optimization approaches with broader training strategies like maximal update parametrization, learning rate schedules, and exponential moving averages, which collectively contribute to empirical improvements. By bridging theoretical insights with practical implementations, the paper offers actionable guidelines and strategies for effectively incorporating these advanced optimization techniques into contemporary deep learning workflows, aiming to enhance feature learning, reduce training times, and improve interpretability in foundation models. Ultimately, this research demystifies neural network training by tracing optimization evolution and proposing principled algorithmic solutions. <div>
arXiv:2512.18373v1 Announce Type: new 
Abstract: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?</title>
<link>https://arxiv.org/abs/2512.18390</link>
<guid>https://arxiv.org/abs/2512.18390</guid>
<content:encoded><![CDATA[
<div> Keywords: model replacement, learning curves, cost optimization, sequential algorithms, credit scoring<br /><br />Summary:<br /><br />This paper addresses the decision problem of when an organization should replace an incumbent predictive model with a new challenger that incorporates recently available features. It proposes a unified framework that integrates learning curve dynamics, costs related to data acquisition and model retraining, and discounting of future benefits to determine optimal switching timing. The authors first analyze idealized scenarios and derive closed-form expressions that clarify how factors such as the length of the decision horizon, the shape of the learning curve, and differences in cost influence the optimal replacement strategy. They then introduce three practical algorithms for deciding switching timings: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit scoring dataset with gradually introduced alternative data, empirical results show that optimal switching time depends systematically on costs and learning behavior. Moreover, the look-ahead sequential algorithm outperforms the other methods, nearing the performance of an oracle that has complete foresight. Finally, finite-sample theoretical guarantees are provided, demonstrating conditions under which the sequential look-ahead method achieves sublinear regret compared to the oracle. The findings offer a practical guide for economically justified transitions between predictive models as new data sources emerge. <div>
arXiv:2512.18390v1 Announce Type: new 
Abstract: We study the problem of deciding whether, and when an organization should replace a trained incumbent model with a challenger relying on newly available features. We develop a unified economic and statistical framework that links learning-curve dynamics, data-acquisition and retraining costs, and discounting of future gains. First, we characterize the optimal switching time in stylized settings and derive closed-form expressions that quantify how horizon length, learning-curve curvature, and cost differentials shape the optimal decision. Second, we propose three practical algorithms: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit-scoring dataset with gradually arriving alternative data, we show that (i) optimal switching times vary systematically with cost parameters and learning-curve behavior, and (ii) the look-ahead sequential method outperforms other methods and is able to approach in value an oracle with full foresight. Finally, we establish finite-sample guarantees, including conditions under which the sequential look-ahead method achieve sublinear regret relative to that oracle. Our results provide an operational blueprint for economically sound model transitions as new data sources become available.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem</title>
<link>https://arxiv.org/abs/2512.18409</link>
<guid>https://arxiv.org/abs/2512.18409</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic bandits, optimism-based algorithms, logarithmic regret, concentration condition, unified analysis<br /><br />Summary:  
This article addresses several optimism-based stochastic bandit algorithms, including UCB, UCB-V, linear UCB, and finite-arm GP-UCB, highlighting that despite their apparent differences, their regret analyses share a fundamental common structure. The authors isolate the core components underlying these analyses into a minimal set of conditions, primarily a single high-probability concentration condition on the estimators. From this condition, they derive logarithmic regret guarantees using two deterministic lemmas that describe radius collapse—the narrowing confidence bounds—and optimism-forced deviations—choices favored by the optimism principle that guide exploration. This distilled framework provides a unified and near-minimal proof methodology for these classical bandit algorithms, offering clarity and simplicity over previous fragmented proofs. Furthermore, the approach naturally extends to many modern bandit variants, suggesting broad applicability and potential for simplifying regret analyses across the bandit learning literature. This contribution serves both as a unifying theoretical insight and a practical toolset for researchers working on advanced bandit algorithm analyses, promoting greater understanding and ease of verification. <div>
arXiv:2512.18409v1 Announce Type: new 
Abstract: Several optimism-based stochastic bandit algorithms -- including UCB, UCB-V, linear UCB, and finite-arm GP-UCB -- achieve logarithmic regret using proofs that, despite superficial differences, follow essentially the same structure. This note isolates the minimal ingredients behind these analyses: a single high-probability concentration condition on the estimators, after which logarithmic regret follows from two short deterministic lemmas describing radius collapse and optimism-forced deviations. The framework yields unified, near-minimal proofs for these classical algorithms and extends naturally to many contemporary bandit variants.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE Pathfinder: Trajectory-driven Expert Pruning</title>
<link>https://arxiv.org/abs/2512.18425</link>
<guid>https://arxiv.org/abs/2512.18425</guid>
<content:encoded><![CDATA[
<div> Mixture-of-experts, expert pruning, large language models, trajectory-based selection, global optimal path planning<br /><br />Summary:  
The paper addresses practical challenges in deploying mixture-of-experts (MoE) architectures within large language models, specifically focusing on reducing computational overhead through expert pruning. Unlike existing pruning methods that use local importance metrics and uniformly prune experts across layers, this work proposes a novel approach that considers the activation trajectory of experts throughout the layers. By treating the MoE model as a weighted computation graph, the expert selection problem is formulated as a global optimal path planning task, enabling a comprehensive, trajectory-level evaluation. The method integrates multiple complementary importance signals, including reconstruction error, routing probabilities, and activation strength, to capture the diverse contributions of experts more effectively. This framework naturally allows for non-uniform expert pruning, preserving experts based on their global importance rather than local criteria. Experimental results demonstrate that this trajectory-based expert pruning outperforms most existing methods across a wide range of tasks, achieving better pruning efficiency and maintaining performance. The approach simplifies MoE deployment by optimizing expert retention patterns and reducing unnecessary computation without significant loss in model capability. Overall, this research offers a more effective and principled strategy for expert pruning in MoE-based large language models. <div>
arXiv:2512.18425v1 Announce Type: new 
Abstract: Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Universality of Transformer Architectures; How Much Attention Is Enough?</title>
<link>https://arxiv.org/abs/2512.18445</link>
<guid>https://arxiv.org/abs/2512.18445</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, universality, expressiveness, architectural refinements, theoretical research  

<br /><br />Summary:  
Transformers have become fundamental across various AI domains, including large language models, computer vision, and reinforcement learning, due to their believed universality and scalability compared to other architectures. This article addresses the critical question of the universality of Transformers, investigating whether and how they can approximate a wide class of functions or computations. It reviews recent advancements such as improvements in structural minimality, which focus on simplifying the Transformer architecture without losing expressive power, and examines results related to approximation rates that quantify how efficiently Transformers can represent target functions. The work also surveys state-of-the-art theoretical and empirical progress to distinguish between guarantees about Transformer expressiveness that are well-founded and those that may be less robust or more fragile. By clarifying the current landscape of knowledge, the paper aims to provide a clear understanding of the expressive capabilities of Transformers. Furthermore, it identifies important open questions and promising directions for future theoretical research to better comprehend and potentially improve the universality and practical utility of Transformer models. <div>
arXiv:2512.18445v1 Announce Type: new 
Abstract: Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture's perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secret mixtures of experts inside your LLM</title>
<link>https://arxiv.org/abs/2512.18452</link>
<guid>https://arxiv.org/abs/2512.18452</guid>
<content:encoded><![CDATA[
<div> MLP, Mixture of Experts, Sparse Autoencoder, Large Language Models, Activation Distribution<br /><br />Summary:<br /><br />1. The paper investigates the role and function of Multilayer Perceptron (MLP) layers within transformer architectures, particularly in large language models (LLMs), noting that these layers are dense and lack straightforward interpretability. <br /><br />2. It proposes the novel hypothesis that MLP layers approximate sparse computations, effectively functioning like sparsely-activating Mixture of Experts (MoE) layers, despite their dense appearance. <br /><br />3. This hypothesis is supported by a newly uncovered theoretical link between MoE models and the structure of Sparse Autoencoders (SAE) in the activation space of neural networks. <br /><br />4. Empirical validation on pretrained LLMs confirms that the sparse approximation hypothesis holds true, with a key insight that the activation distributions’ structure is essential for this effect; random Gaussian data do not replicate these results. <br /><br />5. The findings elucidate why MoE-based transformers are effective and inspire future research towards more efficient MoE architectures, proposing designs incorporating low-rank routing mechanisms to better exploit sparsity. <div>
arXiv:2512.18452v1 Announce Type: new 
Abstract: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.
  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.
  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</title>
<link>https://arxiv.org/abs/2512.18453</link>
<guid>https://arxiv.org/abs/2512.18453</guid>
<content:encoded><![CDATA[
<div> Winograd convolution, numerical instability, optimization, FP16 inference, fractional transforms<br /><br />Summary:<br /><br />1. Winograd convolution is widely used to accelerate inference by reducing arithmetic complexity, notably by 2.25x for 3x3 kernels.<br />2. However, scaling Winograd tiles to larger sizes (e.g., F(6,3), F(8,3)) faces severe numerical instability due to exploding condition numbers, especially in low precision formats like FP16 and Int8.<br />3. NOVA (Numerical Optimization of Vandermonde Arithmetic) is introduced as a new framework that treats selecting Winograd points as a continuous optimization problem rather than relying on traditional integer-based interpolation.<br />4. Using Evolution Strategies, NOVA explores the real-valued space, snaps solutions to simple rationals, and verifies correctness symbolically.<br />5. NOVA discovers stable, fractional point configurations that significantly improve conditioning—for F(8,3), a 415x improvement in 1D conditioning is achieved, which equates to over 170,000x improvement for 2D convolutions.<br />6. In practical FP16 ImageNet inference, NOVA’s transforms avoid the collapse to random chance seen in standard transforms, restoring accuracy from around 4.7% to between 75% and 78% on VGG16 without any retraining or calibration.<br />7. These transforms can be used as drop-in replacements, enabling efficient and stable large-tile Winograd convolution on modern low-precision hardware. <div>
arXiv:2512.18453v1 Announce Type: new 
Abstract: Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the condition numbers of standard integer based transforms explode, reaching kappa = 2 x 10^5 for F(8,3), rendering them unusable in FP16 or Int8. We introduce NOVA (Numerical Optimization of Vandermonde Arithmetic), a discovery framework that breaks the decades old convention of integer interpolation. Treating Winograd point selection as a continuous optimization problem, NOVA searches the manifold R^n-1 via Evolution Strategy, snaps candidates to simple rationals, and guarantees correctness via symbolic verification. This process uncovers a hidden landscape of stable, fractional configurations such as {+-5/6, +-7/6, +-3/5} that defy traditional vocabulary constraints. The impact is transformative: NOVA improves the conditioning of F(8,3) by 415x in 1D, which squares to a 172,484x improvement for 2D convolution. In real world FP16 ImageNet inference, where standard transforms collapse to random chance (e.g., 4.7 percent accuracy on VGG16), NOVA's points restore full accuracy (75 to 78 percent), recovering over 70 percentage points without retraining, calibration, or learned parameters. These discovered transforms act as drop in replacements, effectively unlocking the efficiency of large tile Winograd convolution for next generation hardware.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs</title>
<link>https://arxiv.org/abs/2512.18454</link>
<guid>https://arxiv.org/abs/2512.18454</guid>
<content:encoded><![CDATA[
<div> Keywords: OOD detection, 3D graphs, diffusion model, protein-ligand complexes, geometric deep learning<br /><br />Summary:<br />1. The paper addresses the challenge of out-of-distribution (OOD) detection for irregular 3D graph data, which contain both continuous geometric data and discrete categorical features and lack an inherent ordering.  
2. The authors propose a probabilistic OOD detection framework based on a diffusion model that learns the density of the training data in a fully unsupervised way, enabling reliable identification of OOD samples.  
3. A novel contribution is the introduction of a unified continuous diffusion process acting on both the 3D coordinates and discrete categorical identities, where categorical features are embedded into a continuous space and trained with cross-entropy loss; the diffusion score for discrete features is calculated analytically via posterior-mean interpolation from predicted class probabilities.  
4. This approach yields a single probability-flow ODE (PF-ODE) that produces per-sample log-likelihood estimates, which serve as principled typicality scores to detect distribution shifts.  
5. The method is validated on protein-ligand complex datasets, where entire protein families are withheld during training to simulate strict OOD scenarios; PF-ODE likelihoods successfully identify held-out families as OOD and correlate with errors in an independent binding-affinity model, demonstrating utility for a priori reliability estimation.  
6. Beyond scalar likelihoods, additional multi-scale trajectory statistics derived from PF-ODE flows—such as path tortuosity, flow stiffness, and vector-field instability—offer complementary OOD indicators.  
7. Combining these trajectory features into a joint model produces a highly sensitive OOD detector that outperforms likelihood-only baselines, providing a practical, label-free workflow for OOD quantification in geometric deep learning settings. <div>
arXiv:2512.18454v1 Announce Type: new 
Abstract: Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review</title>
<link>https://arxiv.org/abs/2512.18466</link>
<guid>https://arxiv.org/abs/2512.18466</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-Organizing Map, water quality assessment, ecological monitoring, multidimensional data, sustainable management<br /><br />Summary:<br /><br />This review article explores the application of the Self-Organizing Map (SOM), an unsupervised artificial intelligence technique, in assessing and managing water quality in lakes and reservoirs. It addresses the challenges posed by data sparsity, heterogeneity, and complex nonlinear relationships among water quality parameters. The study synthesizes existing research focused on parameter selection, spatial and temporal sampling strategies, and various clustering approaches integrated with SOM. Emphasizing SOM's capability to process and visualize multidimensional data, the review highlights how it uncovers hidden ecological patterns and critical correlations that support effective water resource management. The increasing availability of environmental data from sensors, remote sensing, IoT devices, and historical datasets is noted as a major enabler of enhanced monitoring and analysis. The review also underscores SOM’s versatility across different ecological applications, including trophic state classification, algal bloom monitoring, and evaluating catchment area impacts. By providing comprehensive insights into SOM methodologies and use cases, the article aims to guide future research and practical efforts toward improving sustainable water quality monitoring and management of lake and reservoir ecosystems. <div>
arXiv:2512.18466v1 Announce Type: new 
Abstract: Sustainable water quality underpins ecological balance and water security. Assessing and managing lakes and reservoirs is difficult due to data sparsity, heterogeneity, and nonlinear relationships among parameters. This review examines how Self-Organizing Map (SOM), an unsupervised AI technique, is applied to water quality assessment. It synthesizes research on parameter selection, spatial and temporal sampling strategies, and clustering approaches. Emphasis is placed on how SOM handles multidimensional data and uncovers hidden patterns to support effective water management. The growing availability of environmental data from in-situ sensors, remote sensing imagery, IoT technologies, and historical records has significantly expanded analytical opportunities in environmental monitoring. SOM has proven effective in analysing complex datasets, particularly when labelled data are limited or unavailable. It enables high-dimensional data visualization, facilitates the detection of hidden ecological patterns, and identifies critical correlations among diverse water quality indicators. This review highlights SOMs versatility in ecological assessments, trophic state classification, algal bloom monitoring, and catchment area impact evaluations. The findings offer comprehensive insights into existing methodologies, supporting future research and practical applications aimed at improving the monitoring and sustainable management of lake and reservoir ecosystems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Abstraction: Continual Learning via Recursive Quotienting</title>
<link>https://arxiv.org/abs/2512.18471</link>
<guid>https://arxiv.org/abs/2512.18471</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, geometric barrier, recursive metric contraction, topological deformation, catastrophic forgetting<br /><br />Summary:<br /><br />This article addresses a fundamental geometric issue in continual learning systems known as the flat manifold problem, where the linear growth of geodesic distance in fixed-dimensional Euclidean spaces leads to inevitable trajectory overlap and catastrophic interference. To overcome this, the authors propose Recursive Metric Contraction, a geometric solution that formalizes abstraction as a topological deformation via quotient maps collapsing the metric tensor within temporal neighborhoods, effectively reducing local manifold diameters to zero. Four key theoretical results support this framework: (1) The Bounded Capacity Theorem shows that using recursive quotient maps enables embedding arbitrarily long trajectories into bounded volumes, trading linear metric growth for logarithmic topological depth. (2) The Topological Collapse Separability Theorem demonstrates that recursive quotienting transforms non-linearly separable temporal sequences into linearly separable ones without requiring infinite-dimensional kernel projections. (3) The Parity-Partitioned Stability Theorem proves that by partitioning the state space into orthogonal flow and scaffold manifolds, metric deformations during active learning preserve memory stability, resolving catastrophic forgetting. (4) The analysis suggests that neural tokens correspond physically to singularities or wormholes—regions of extreme positive curvature that connect distant points on the temporal manifold—offering a novel insight into neural architecture representations. <div>
arXiv:2512.18471v1 Announce Type: new 
Abstract: Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</title>
<link>https://arxiv.org/abs/2512.18473</link>
<guid>https://arxiv.org/abs/2512.18473</guid>
<content:encoded><![CDATA[
<div> Keywords: APC-GNN++, diabetes classification, graph neural network, patient-centric, explainable predictions<br /><br />Summary:  
1. This paper introduces APC-GNN++, an adaptive patient-centric Graph Neural Network designed specifically for diabetes classification.  
2. The model incorporates context-aware edge attention, confidence-guided blending of node features with graph representations, and neighborhood consistency regularization to more effectively capture clinically significant relationships between patients.  
3. To address the challenge of classifying unseen patients, a mini-graph approach is proposed that leverages nearest neighbors of the new patient, allowing for real-time, explainable predictions without needing to retrain the global model.  
4. APC-GNN++ is evaluated on a real-world diabetes dataset from a regional hospital in Algeria, where it demonstrates superior performance over traditional machine learning algorithms such as MLP, Random Forest, XGBoost, and a basic Graph Convolutional Network (GCN), achieving higher test accuracy and macro F1-score.  
5. The study also provides interpretability by analyzing node-level confidence scores, showing how the model balances information derived from each patient and their graph context to deliver patient-centric insights. Additionally, a Tkinter-based graphical user interface (GUI) is developed for interactive use by healthcare professionals, facilitating practical deployment. <div>
arXiv:2512.18473v1 Announce Type: new 
Abstract: We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts</title>
<link>https://arxiv.org/abs/2512.18522</link>
<guid>https://arxiv.org/abs/2512.18522</guid>
<content:encoded><![CDATA[
<div> Drought, Machine Learning, Impact Forecasting, XGBoost, New Mexico<br /><br />Summary:<br /><br />1. The study addresses drought as a complex natural hazard affecting both ecological and human systems, with increasing severity and frequency necessitating improved monitoring and mitigation.<br /><br />2. It emphasizes predicting drought impacts rather than merely drought conditions to enhance early warning systems and proactive decision-making.<br /><br />3. Machine learning techniques, specifically eXtreme Gradient Boosting (XGBoost), were applied to link drought indices—the Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI)—with historical drought impact data from 2005 to 2024.<br /><br />4. The study focused on modeling and forecasting weekly drought impacts, successfully predicting Fire and Relief impacts with the highest accuracy, followed by Agriculture and Water, while impacts on Plants and Society were more variable.<br /><br />5. The model generated forecasts for New Mexico at county and state levels up to eight weeks in advance, using the preceding eight weeks of data.<br /><br />6. This approach supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and shows potential for wider application in other drought-prone regions.<br /><br />7. The findings aim to assist stakeholders, land managers, and decision-makers in implementing more effective drought mitigation and adaptation strategies. <div>
arXiv:2512.18522v1 Announce Type: new 
Abstract: Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2512.18524</link>
<guid>https://arxiv.org/abs/2512.18524</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Graph classification, Synthetic graph models, Feature selection, Generative graph families<br /><br />Summary:<br /><br />This paper addresses the problem of discriminating between different generative graph models, which is important for understanding complex structural patterns in both synthetic and real-world graphs. The authors propose a hybrid approach combining Graph Neural Networks (GNNs) with engineered graph-theoretic features to improve classification performance. They create a large synthetic dataset consisting of graphs from five representative generative families: Erdos-Renyi, Watts-Strogatz, Barabási-Albert, Holme-Kim, and the Stochastic Block Model, with graphs containing up to 10,000 nodes and 110,000 edges. A wide range of node- and graph-level features is extracted and then pruned using a Random Forest-based feature selection method to retain the most relevant features. These selected features are integrated into six different GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE, and GTN. Hyperparameters for each model are optimized using the Optuna framework. The evaluation shows that GraphSAGE and GTN achieve the best results, with classification accuracy reaching 98.5%, and clear class separation is visualized through t-SNE and UMAP plots. Other models like GCN and GIN also perform well, while GAT-based models perform worse due to their limited capacity to capture global graph structures. A Support Vector Machine baseline trained solely on handcrafted features confirms the added value of message passing in GNNs for enhanced performance and meaningful class separation. <div>
arXiv:2512.18524v1 Announce Type: new 
Abstract: The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment</title>
<link>https://arxiv.org/abs/2512.18566</link>
<guid>https://arxiv.org/abs/2512.18566</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamical systems, recurrent neural networks, coordinate transformation, topological equivalence, invariant manifolds<br /><br />Summary: This paper addresses challenges in analyzing dynamical systems models like recurrent neural networks (RNNs) used in theoretical neuroscience. First, it highlights the difficulty of comparing learned dynamics across models due to the lack of coordinate system equivalence. Second, it points out the intractability of identifying important low-dimensional dynamical motifs in high-dimensional nonlinear systems. To overcome these issues, the authors introduce DFORM (Diffeomorphic vector field alignment FOR learned Models), a framework that learns nonlinear coordinate transformations to align trajectories between two dynamical systems maximally one-to-one. By doing so, DFORM enables assessment of topological equivalence, revealing similar mechanisms despite different coordinate systems. Additionally, DFORM aids in locating key dynamical motifs such as invariant manifolds and saddle limit sets embedded within high-dimensional spaces. The method was validated on canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows, demonstrating its ability to identify both linear and nonlinear coordinate transformations. DFORM can also quantify the similarity between topologically distinct systems. Finally, the framework was applied to RNNs trained on human fMRI data, successfully identifying limit cycles consistent with prior numerical analyses, showcasing its utility for data-driven neuroscientific models. <div>
arXiv:2512.18566v1 Announce Type: new 
Abstract: Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</title>
<link>https://arxiv.org/abs/2512.18575</link>
<guid>https://arxiv.org/abs/2512.18575</guid>
<content:encoded><![CDATA[
<div> Keywords: spiking neural networks, memory mechanisms, cross-modal, energy efficiency, neuromorphic computing<br /><br />Summary:  
This paper investigates memory-augmented spiking neural networks (SNNs) to understand their generalization ability across different sensory modalities, specifically visual and auditory tasks. The authors conduct the first comprehensive cross-modal ablation study evaluating memory mechanisms including Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL). They test these methods on neuromorphic datasets: N-MNIST (visual) and SHD (auditory). Their evaluation of five architectures reveals that Hopfield networks excel in visual tasks with 97.68% accuracy but perform poorly on auditory tasks (76.15%), showing significant modality-dependent specialization. In contrast, SCL provides a more balanced performance across modalities (96.72% visual, 82.16% auditory). The study demonstrates that memory mechanisms offer task-specific advantages rather than universal benefits. Furthermore, joint multi-modal training with HGRN attains 94.41% accuracy in visual and 79.37% in auditory tasks, with an average accuracy of 88.78%, rivaling parallel specialized HGRN models but through a unified architecture. Engram analysis reveals weak similarity (0.038) between modalities, justifying the architecture choices. Notably, the proposed approaches achieve a remarkable 603-fold improvement in energy efficiency compared to traditional neural networks, highlighting their potential for neuromorphic computing. <div>
arXiv:2512.18575v1 Announce Type: new 
Abstract: Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</title>
<link>https://arxiv.org/abs/2512.18583</link>
<guid>https://arxiv.org/abs/2512.18583</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial Imitation Learning, Diffusion Models, Synthetic Demonstrations, Prioritized Expert Demonstration Replay, Policy Optimization<br /><br />Summary:<br /><br />1. The paper addresses Adversarial Imitation Learning (AIL), a key framework in imitation learning that uses expert demonstrations to infer rewards for policy optimization.<br />2. Collecting large amounts of expert demonstrations is often difficult, which motivates the use of synthetic demonstrations.<br />3. SD2AIL is proposed, a novel method that employs diffusion models within the discriminator to generate synthetic demonstrations, augmenting the expert data.<br />4. To efficiently use the large pool of synthetic and expert demonstrations, the authors introduce the Prioritized Expert Demonstration Replay (PEDR) strategy, which focuses on replaying the most valuable demonstrations.<br />5. Experimental results on simulation tasks, including the Hopper benchmark, show that SD2AIL outperforms previous state-of-the-art methods, achieving an average return of 3441, which is 89 points higher.<br />6. The approach demonstrates both effectiveness and robustness across tested environments.<br />7. The authors commit to open-sourcing their code to facilitate further research and application through their GitHub repository. <div>
arXiv:2512.18583v1 Announce Type: new 
Abstract: Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows</title>
<link>https://arxiv.org/abs/2512.18595</link>
<guid>https://arxiv.org/abs/2512.18595</guid>
<content:encoded><![CDATA[
<div> multiphysics, neural surrogates, benchmarking, reactive flows, model robustness<br /><br />Summary:<br /><br />This paper addresses the computational challenges in predicting multiphysics dynamics, which involve complex couplings of multi-scale and heterogeneous processes. It highlights the current limitations in neural surrogate evaluations that often rely on simplified, low-dimensional proxies, thus creating an "illusion of mastery" by failing to reveal model fragility in realistic settings. To overcome this, the authors introduce REALM (REalistic AI Learning for Multiphysics), a comprehensive benchmarking framework designed specifically for neural surrogates applied to realistic, application-driven reactive flow problems. REALM includes 11 high-fidelity datasets ranging from canonical multiphysics tasks to complex scenarios such as propulsion and fire safety, along with a standardized end-to-end training and evaluation protocol that employs multiphysics-aware preprocessing and a robust rollout strategy. Using REALM, the paper benchmarks over a dozen surrogate model families—including spectral operators, convolutional networks, Transformers, pointwise operators, and graph/mesh networks—and discovers three key trends: (i) a scaling barrier that depends on dimensionality, stiffness, and mesh irregularity, causing rapidly increasing rollout errors; (ii) that model performance is more influenced by architectural inductive biases than sheer parameter count; and (iii) a notable discrepancy between standard accuracy metrics and physically meaningful behavior, where models scoring high on correlation still fail to capture critical transient structures and integral quantities. Overall, REALM reveals the current limits of neural surrogates in realistic multiphysics flows and provides a rigorous platform to guide the development of more physically informed AI architectures. <div>
arXiv:2512.18595v1 Announce Type: new 
Abstract: Predicting multiphysics dynamics is computationally expensive and challenging due to the severe coupling of multi-scale, heterogeneous physical processes. While neural surrogates promise a paradigm shift, the field currently suffers from an "illusion of mastery", as repeatedly emphasized in top-tier commentaries: existing evaluations overly rely on simplified, low-dimensional proxies, which fail to expose the models' inherent fragility in realistic regimes. To bridge this critical gap, we present REALM (REalistic AI Learning for Multiphysics), a rigorous benchmarking framework designed to test neural surrogates on challenging, application-driven reactive flows. REALM features 11 high-fidelity datasets spanning from canonical multiphysics problems to complex propulsion and fire safety scenarios, alongside a standardized end-to-end training and evaluation protocol that incorporates multiphysics-aware preprocessing and a robust rollout strategy. Using this framework, we systematically benchmark over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks, and identify three robust trends: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities. Taken together, REALM exposes the limits of current neural surrogates on realistic multiphysics flows and offers a rigorous testbed to drive the development of next-generation physics-aware architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture</title>
<link>https://arxiv.org/abs/2512.18596</link>
<guid>https://arxiv.org/abs/2512.18596</guid>
<content:encoded><![CDATA[
<div> multi-UAV, trajectory planning, Markov decision process, reinforcement learning, smart agriculture<br /><br />Summary:<br /><br />This paper addresses the challenge of coordinating multiple unmanned aerial vehicles (UAVs) in smart agriculture for tasks such as data collection, image acquisition, and communication. To optimize the UAV trajectory planning, the authors model the problem as a Markov decision process (MDP), enabling a structured decision-making framework under uncertainty. They propose a novel reinforcement learning framework named Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC). This framework introduces two key innovations: first, agents learn adaptively from an elite agent, which helps to reduce the number of costly trial-and-error interactions; second, a shared ensemble critic mechanism works alongside each agent's local critic to obtain unbiased value estimation and avoid overestimation bias common in actor-critic methods. The experimental results demonstrate that EIA-SEC outperforms state-of-the-art baseline algorithms by achieving higher reward performance, better training stability, and faster convergence. Overall, the study contributes an effective and efficient multi-agent reinforcement learning approach that is well-suited for the complex, cooperative UAV operations needed in smart agriculture environments. <div>
arXiv:2512.18596v1 Announce Type: new 
Abstract: The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</title>
<link>https://arxiv.org/abs/2512.18604</link>
<guid>https://arxiv.org/abs/2512.18604</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, smart agriculture, trajectory planning, multi-agent reinforcement learning, imitation-based triple deep Q-network

<br /><br />Summary: Unmanned aerial vehicles (UAVs) have become valuable tools in smart agriculture for tasks such as weed detection, recognition, and data collection from wireless sensors. However, planning optimal UAV trajectories is challenging due to environmental uncertainties, partial information, and limited battery life. To tackle these issues, the authors model the trajectory planning problem using a Markov decision process (MDP) framework. They apply multi-agent reinforcement learning (MARL) techniques to learn effective policies. A novel imitation-based triple deep Q-network (ITDQN) algorithm is introduced, which incorporates an elite imitation mechanism to reduce exploration costs significantly. This approach also uses a mediator Q-network layered over a double deep Q-network (DDQN) to enhance training speed, stability, and overall performance. The method was evaluated in both simulated and real-world conditions, confirming its practical viability. Results demonstrate that ITDQN outperforms the standard DDQN algorithm, achieving a 4.43% improvement in weed recognition rates and a 6.94% increase in data collection efficiency. This work highlights the potential of advanced reinforcement learning methods in addressing complex trajectory planning problems for UAVs in agricultural applications. <div>
arXiv:2512.18604v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\% in weed recognition rate and 6.94\% in data collection rate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation</title>
<link>https://arxiv.org/abs/2512.18607</link>
<guid>https://arxiv.org/abs/2512.18607</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, interaction order, representation capacity, gradient variance, generalization  

<br /><br />Summary:  
This work investigates how deep neural networks (DNNs) encode cooperative interactions across varying levels of contextual complexity using a multi-order interaction framework, where each order measures the contextual information required to assess the joint utility of variable pairs. The authors conduct a comprehensive empirical and theoretical study of interaction structures in DNNs. First, they empirically identify a universal interaction bottleneck: DNNs readily learn low- and high-order interactions but systematically under-represent mid-order interactions. Second, they theoretically explain this bottleneck by demonstrating that mid-order interactions possess the highest contextual variability, leading to greater gradient variance and making them inherently harder to learn. Third, the study shows that this bottleneck can be modulated by introducing loss functions that bias the model toward emphasizing interactions of specific orders. Lastly, they link microscopic interaction patterns with macroscopic representational behaviors, finding that models emphasizing low-order interactions tend to generalize better and be more robust, while models focusing on high-order interactions excel at structural modeling and fitting. Collectively, these findings reveal an intrinsic representational bias in modern DNNs and position interaction order as a meaningful framework for interpreting and guiding deep learning representations. <div>
arXiv:2512.18607v1 Announce Type: new 
Abstract: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</title>
<link>https://arxiv.org/abs/2512.18610</link>
<guid>https://arxiv.org/abs/2512.18610</guid>
<content:encoded><![CDATA[
<div> Keywords: Expectation of Optimization Bias, time series, point-wise loss, Structural Signal-to-Noise Ratio, debiasing

<br /><br />Summary:  
This paper addresses a fundamental flaw in optimizing time series models using point-wise loss functions like MSE, which assume data points are independent and identically distributed (i.i.d.) and thereby ignore the causal temporal structure inherent in time series. It focuses on the independence assumption issue under covariance stationarity, providing a first-principles, information-theoretic analysis of the Expectation of Optimization Bias (EOB), defining it as the discrepancy between the true joint distribution of the time series and its flawed i.i.d. assumption counterpart. The study uncovers a paradox: the more deterministic and structured a time series is, the greater the bias introduced by point-wise loss functions. The authors derive the first closed-form expressions that quantify the non-deterministic EOB in both linear and nonlinear systems, proving that EOB is an intrinsic property depending only on sequence length and a newly proposed Structural Signal-to-Noise Ratio (SSNR). To counteract this bias, the paper proposes a principled debiasing approach based on reducing sequence length and applying structural orthogonalization using techniques such as the Discrete Fourier Transform (DFT) or Discrete Wavelet Transform (DWT). Additionally, a novel harmonized ℓ_p norm framework is introduced to address gradient issues caused by high-variance series. Extensive experiments validate the universality of EOB theory and demonstrate the superior effectiveness of the proposed debiasing program. <div>
arXiv:2512.18610v1 Announce Type: new 
Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a flawed point-wise independent and identically distributed (i.i.d.) assumption that disregards the causal temporal structure, an issue with growing awareness yet lacking formal theoretical grounding. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB), formalizing it information-theoretically as the discrepancy between the true joint distribution and its flawed i.i.d. counterpart. Our analysis reveals a fundamental paradigm paradox: the more deterministic and structured the time series, the more severe the bias by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and our proposed Structural Signal-to-Noise Ratio (SSNR). This theoretical diagnosis motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution that simultaneously achieves both principles via DFT or DWT. Furthermore, a novel harmonized $\ell_p$ norm framework is proposed to rectify gradient pathologies of high-variance series. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs</title>
<link>https://arxiv.org/abs/2512.18633</link>
<guid>https://arxiv.org/abs/2512.18633</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Routing Problems, cross-problem learning, attribute representation, analogical consistency, zero-shot generalization<br /><br />Summary:<br /><br />This paper addresses Vehicle Routing Problems (VRPs) characterized by diverse real-world attributes, which necessitate learning methods that generalize effectively across multiple problem variants. The authors introduce ARC (Attribute Representation via Compositional Learning), a novel framework designed to learn disentangled attribute representations. ARC decomposes attribute representations into two complementary parts: Intrinsic Attribute Embeddings (IAE), capturing invariant semantic information of each attribute, and Contextual Interaction Embeddings (CIE), which model the interaction effects arising from combinations of attributes. A key innovation is the enforcement of analogical consistency within the embedding space, ensuring that the semantic transformation induced by adding a particular attribute remains consistent across different contexts. This property allows ARC to transfer learned invariant semantics across trained variants and to construct reliable representations for previously unseen attribute combinations. The framework is evaluated extensively and demonstrates state-of-the-art performance on various settings including in-distribution tests, zero-shot generalization to unseen variants, few-shot adaptation to new problems, and real-world benchmarks. These results highlight the efficacy of ARC in enabling scalable and flexible solutions for complex, attribute-rich Vehicle Routing Problems. <div>
arXiv:2512.18633v1 Announce Type: new 
Abstract: Vehicle Routing Problems (VRPs) with diverse real-world attributes have driven recent interest in cross-problem learning approaches that efficiently generalize across problem variants. We propose ARC (Attribute Representation via Compositional Learning), a cross-problem learning framework that learns disentangled attribute representations by decomposing them into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. This disentanglement is achieved by enforcing analogical consistency in the embedding space to ensure the semantic transformation of adding an attribute (e.g., a length constraint) remains invariant across different problem contexts. This enables our model to reuse invariant semantics across trained variants and construct representations for unseen combinations. ARC achieves state-of-the-art performance across in-distribution, zero-shot generalization, few-shot adaptation, and real-world benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers</title>
<link>https://arxiv.org/abs/2512.18634</link>
<guid>https://arxiv.org/abs/2512.18634</guid>
<content:encoded><![CDATA[
<div> Keywords: transformers, pretraining data distribution, induction heads, positional shortcuts, out-of-distribution generalization<br /><br />Summary:<br /><br />This paper investigates how the distribution of pretraining data influences whether a shallow transformer develops a generalizable algorithmic behavior or relies on simple positional shortcuts. The authors focus on a minimal trigger-output prediction task where the model must copy the token immediately following a special trigger upon its second occurrence. They conduct a rigorous analysis of gradient-based training on a single-layer transformer, providing proofs in both infinite and finite sample regimes. A key finding is a transition in the learned mechanism determined by the diversity of input sequences, measured by a "max-sum" ratio of trigger-to-trigger distances: low ratios encourage the formation of induction heads that generalize well to unseen contexts, while high ratios cause the model to memorize fixed output positions and fail to generalize out-of-distribution (OOD). The study also uncovers a trade-off between the pretraining context length and OOD generalization and derives an optimal pretraining distribution minimizing computational costs per sample. Finally, synthetic experiments validate these theoretical insights, showing that broadening the distribution of context lengths robustly induces induction heads and improves generalization. The work offers valuable conceptual guidelines for steering pretrained transformers’ behaviors through data-driven approaches. <div>
arXiv:2512.18634v1 Announce Type: new 
Abstract: Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.18670</link>
<guid>https://arxiv.org/abs/2512.18670</guid>
<content:encoded><![CDATA[
<div> Keywords: continual reinforcement learning, demonstration-guided exploration, stability-plasticity dilemma, knowledge transfer, curriculum learning<br /><br />Summary:<br /><br />Reinforcement learning (RL) faces significant challenges in dynamic environments where the task distribution changes over time. To address this, continual reinforcement learning (CRL) aims to enable agents to continuously learn and adapt across multiple tasks while balancing stability (retaining old knowledge) and plasticity (acquiring new knowledge). However, existing CRL approaches generally influence learning optimization using past knowledge but do not directly integrate this knowledge into the agent's behavior, limiting knowledge reuse. This paper introduces demonstration-guided continual reinforcement learning (DGCRL), a novel framework that stores previous knowledge in an external, self-evolving demonstration repository. DGCRL assists RL agents by dynamically selecting relevant demonstrations for each new task, effectively guiding exploration and accelerating learning. It employs a curriculum-based strategy that initially emphasizes demonstration-guided exploration and gradually transitions to independent exploration, enhancing adaptation efficiency. Extensive experiments are conducted on 2D navigation and MuJoCo locomotion benchmarks, showing that DGCRL achieves better average performance, improved knowledge transfer between tasks, reduced forgetting of prior skills, and greater training efficiency compared to baselines. Additional sensitivity analyses and ablation studies confirm the robustness and effectiveness of the proposed method in handling the stability-plasticity trade-off in continual learning scenarios. <div>
arXiv:2512.18670v1 Announce Type: new 
Abstract: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs</title>
<link>https://arxiv.org/abs/2512.18673</link>
<guid>https://arxiv.org/abs/2512.18673</guid>
<content:encoded><![CDATA[
arXiv:2512.18673v1 Announce Type: new 
Abstract: This paper proposes a structure-aware driven scheduling graph modeling method to improve the accuracy and representation capability of anomaly identification in scheduling behaviors of complex systems. The method first designs a structure-guided scheduling graph construction mechanism that integrates task execution stages, resource node states, and scheduling path information to build dynamically evolving scheduling behavior graphs, enhancing the model's ability to capture global scheduling relationships. On this basis, a multi-scale graph semantic aggregation module is introduced to achieve semantic consistency modeling of scheduling features through local adjacency semantic integration and global topology alignment, thereby strengthening the model's capability to capture abnormal features in complex scenarios such as multi-task concurrency, resource competition, and stage transitions. Experiments are conducted on a real scheduling dataset with multiple scheduling disturbance paths set to simulate different types of anomalies, including structural shifts, resource changes, and task delays. The proposed model demonstrates significant performance advantages across multiple metrics, showing a sensitive response to structural disturbances and semantic shifts. Further visualization analysis reveals that, under the combined effect of structure guidance and semantic aggregation, the scheduling behavior graph exhibits stronger anomaly separability and pattern representation, validating the effectiveness and adaptability of the method in scheduling anomaly detection tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding</title>
<link>https://arxiv.org/abs/2512.18689</link>
<guid>https://arxiv.org/abs/2512.18689</guid>
<content:encoded><![CDATA[
arXiv:2512.18689v1 Announce Type: new 
Abstract: Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Risky Samples with Conformity Constraints via Diffusion Models</title>
<link>https://arxiv.org/abs/2512.18722</link>
<guid>https://arxiv.org/abs/2512.18722</guid>
<content:encoded><![CDATA[
arXiv:2512.18722v1 Announce Type: new 
Abstract: Although neural networks achieve promising performance in many tasks, they may still fail when encountering some examples and bring about risks to applications. To discover risky samples, previous literature attempts to search for patterns of risky samples within existing datasets or inject perturbation into them. Yet in this way the diversity of risky samples is limited by the coverage of existing datasets. To overcome this limitation, recent works adopt diffusion models to produce new risky samples beyond the coverage of existing datasets. However, these methods struggle in the conformity between generated samples and expected categories, which could introduce label noise and severely limit their effectiveness in applications. To address this issue, we propose RiskyDiff that incorporates the embeddings of both texts and images as implicit constraints of category conformity. We also design a conformity score to further explicitly strengthen the category conformity, as well as introduce the mechanisms of embedding screening and risky gradient guidance to boost the risk of generated samples. Extensive experiments reveal that RiskyDiff greatly outperforms existing methods in terms of the degree of risk, generation quality, and conformity with conditioned categories. We also empirically show the generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ML Inference Scheduling with Predictable Latency</title>
<link>https://arxiv.org/abs/2512.18725</link>
<guid>https://arxiv.org/abs/2512.18725</guid>
<content:encoded><![CDATA[
arXiv:2512.18725v1 Announce Type: new 
Abstract: Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. To this end, we evaluate the potential limitations of existing interference prediction approaches and outline our ongoing work toward achieving efficient ML inference scheduling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</title>
<link>https://arxiv.org/abs/2512.18730</link>
<guid>https://arxiv.org/abs/2512.18730</guid>
<content:encoded><![CDATA[
arXiv:2512.18730v1 Announce Type: new 
Abstract: Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.
  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.
  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Your Conditional Diffusion Model Actually Denoising?</title>
<link>https://arxiv.org/abs/2512.18736</link>
<guid>https://arxiv.org/abs/2512.18736</guid>
<content:encoded><![CDATA[
arXiv:2512.18736v1 Announce Type: new 
Abstract: We study the inductive biases of diffusion models with a conditioning-variable, which have seen widespread application as both text-conditioned generative image models and observation-conditioned continuous control policies. We observe that when these models are queried conditionally, their generations consistently deviate from the idealized "denoising" process upon which diffusion models are formulated, inducing disagreement between popular sampling algorithms (e.g. DDPM, DDIM). We introduce Schedule Deviation, a rigorous measure which captures the rate of deviation from a standard denoising process, and provide a methodology to compute it. Crucially, we demonstrate that the deviation from an idealized denoising process occurs irrespective of the model capacity or amount of training data. We posit that this phenomenon occurs due to the difficulty of bridging distinct denoising flows across different parts of the conditioning space and show theoretically how such a phenomenon can arise through an inductive bias towards smoothness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2512.18737</link>
<guid>https://arxiv.org/abs/2512.18737</guid>
<content:encoded><![CDATA[
arXiv:2512.18737v1 Announce Type: new 
Abstract: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.18763</link>
<guid>https://arxiv.org/abs/2512.18763</guid>
<content:encoded><![CDATA[
arXiv:2512.18763v1 Announce Type: new 
Abstract: Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Informed Outlier Detection Based on Granule Density</title>
<link>https://arxiv.org/abs/2512.18774</link>
<guid>https://arxiv.org/abs/2512.18774</guid>
<content:encoded><![CDATA[
arXiv:2512.18774v1 Announce Type: new 
Abstract: Outlier detection, crucial for identifying unusual patterns with significant implications across numerous applications, has drawn considerable research interest. Existing semi-supervised methods typically treat data as purely numerical and} in a deterministic manner, thereby neglecting the heterogeneity and uncertainty inherent in complex, real-world datasets. This paper introduces a label-informed outlier detection method for heterogeneous data based on Granular Computing and Fuzzy Sets, namely Granule Density-based Outlier Factor (GDOF). Specifically, GDOF first employs label-informed fuzzy granulation to effectively represent various data types and develops granule density for precise density estimation. Subsequently, granule densities from individual attributes are integrated for outlier scoring by assessing attribute relevance with a limited number of labeled outliers. Experimental results on various real-world datasets show that GDOF stands out in detecting outliers in heterogeneous data with a minimal number of labeled outliers. The integration of Fuzzy Sets and Granular Computing in GDOF offers a practical framework for outlier detection in complex and diverse data types. All relevant datasets and source codes are publicly available for further research. This is the author's accepted manuscript of a paper published in IEEE Transactions on Fuzzy Systems. The final version is available at https://doi.org/10.1109/TFUZZ.2024.3514853
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</title>
<link>https://arxiv.org/abs/2512.18815</link>
<guid>https://arxiv.org/abs/2512.18815</guid>
<content:encoded><![CDATA[
arXiv:2512.18815v1 Announce Type: new 
Abstract: AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.18826</link>
<guid>https://arxiv.org/abs/2512.18826</guid>
<content:encoded><![CDATA[
arXiv:2512.18826v1 Announce Type: new 
Abstract: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \textit{HGCAE}, \textit{\(\mathcal{P}\)-VAE}, and \textit{HGCN} demonstrates high performance, with \textit{\(\mathcal{P}\)-VAE} achieving an F1-score of 94\% on the \textit{Elliptic} dataset and \textit{HGCAE} scoring 80\% on \textit{Cora}. In contrast, Euclidean methods like \textit{DOMINANT} and \textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling through Spectral Analysis of Koopman Operator</title>
<link>https://arxiv.org/abs/2512.18837</link>
<guid>https://arxiv.org/abs/2512.18837</guid>
<content:encoded><![CDATA[
arXiv:2512.18837v1 Announce Type: new 
Abstract: We propose Koopman Spectral Wasserstein Gradient Descent (KSWGD), a generative modeling framework that combines operator-theoretic spectral analysis with optimal transport. The novel insight is that the spectral structure required for accelerated Wasserstein gradient descent can be directly estimated from trajectory data via Koopman operator approximation which can eliminate the need for explicit knowledge of the target potential or neural network training. We provide rigorous convergence analysis and establish connection to Feynman-Kac theory that clarifies the method's probabilistic foundation. Experiments across diverse settings, including compact manifold sampling, metastable multi-well systems, image generation, and high dimensional stochastic partial differential equation, demonstrate that KSWGD consistently achieves faster convergence than other existing methods while maintaining high sample quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merging of Kolmogorov-Arnold networks trained on disjoint datasets</title>
<link>https://arxiv.org/abs/2512.18921</link>
<guid>https://arxiv.org/abs/2512.18921</guid>
<content:encoded><![CDATA[
arXiv:2512.18921v1 Announce Type: new 
Abstract: Training on disjoint datasets can serve two primary goals: accelerating data processing and enabling federated learning. It has already been established that Kolmogorov-Arnold networks (KANs) are particularly well suited for federated learning and can be merged through simple parameter averaging. While the federated learning literature has mostly focused on achieving training convergence across distributed nodes, the present paper specifically targets acceleration of the training, which depends critically on the choice of an optimisation method and the type of the basis functions. To the best knowledge of the authors, the fastest currently-available combination is the Newton-Kaczmarz method and the piecewise-linear basis functions. Here, it is shown that training on disjoint datasets (or disjoint subsets of the training dataset) can further improve the performance. Experimental comparisons are provided, and all corresponding codes are publicly available.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ensemble Schr{\"o}dinger Bridge filter for Nonlinear Data Assimilation</title>
<link>https://arxiv.org/abs/2512.18928</link>
<guid>https://arxiv.org/abs/2512.18928</guid>
<content:encoded><![CDATA[
arXiv:2512.18928v1 Announce Type: new 
Abstract: This work puts forward a novel nonlinear optimal filter namely the Ensemble Schr{\"o}dinger Bridge nonlinear filter. The proposed filter finds marriage of the standard prediction procedure and the diffusion generative modeling for the analysis procedure to realize one filtering step. The designed approach finds no structural model error, and it is derivative free, training free and highly parallizable. Experimental results show that the designed algorithm performs well given highly nonlinear dynamics in (mildly) high dimension up to 40 or above under a chaotic environment. It also shows better performance than classical methods such as the ensemble Kalman filter and the Particle filter in numerous tests given different level of nonlinearity. Future work will focus on extending the proposed approach to practical meteorological applications and establishing a rigorous convergence analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems</title>
<link>https://arxiv.org/abs/2512.18932</link>
<guid>https://arxiv.org/abs/2512.18932</guid>
<content:encoded><![CDATA[
arXiv:2512.18932v1 Announce Type: new 
Abstract: Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.
  DPSR consists of three synergistic stages: (1) \textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.
  Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\% to 9.23\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\varepsilon=0.1$ to $\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18934</link>
<guid>https://arxiv.org/abs/2512.18934</guid>
<content:encoded><![CDATA[
arXiv:2512.18934v1 Announce Type: new 
Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</title>
<link>https://arxiv.org/abs/2512.18950</link>
<guid>https://arxiv.org/abs/2512.18950</guid>
<content:encoded><![CDATA[
arXiv:2512.18950v1 Announce Type: new 
Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Through Little Eyes: Attribute Discrimination Beyond Objects</title>
<link>https://arxiv.org/abs/2512.18951</link>
<guid>https://arxiv.org/abs/2512.18951</guid>
<content:encoded><![CDATA[
arXiv:2512.18951v1 Announce Type: new 
Abstract: Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</title>
<link>https://arxiv.org/abs/2512.18957</link>
<guid>https://arxiv.org/abs/2512.18957</guid>
<content:encoded><![CDATA[
arXiv:2512.18957v1 Announce Type: new 
Abstract: The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling</title>
<link>https://arxiv.org/abs/2512.18965</link>
<guid>https://arxiv.org/abs/2512.18965</guid>
<content:encoded><![CDATA[
arXiv:2512.18965v1 Announce Type: new 
Abstract: Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions "slide" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets</title>
<link>https://arxiv.org/abs/2512.18977</link>
<guid>https://arxiv.org/abs/2512.18977</guid>
<content:encoded><![CDATA[
arXiv:2512.18977v1 Announce Type: new 
Abstract: Outlier detection aims to find samples that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes' contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 15 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.asoc.2024.112070
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy</title>
<link>https://arxiv.org/abs/2512.18978</link>
<guid>https://arxiv.org/abs/2512.18978</guid>
<content:encoded><![CDATA[
arXiv:2512.18978v1 Announce Type: new 
Abstract: Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPBO: Order-Preserving Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.18980</link>
<guid>https://arxiv.org/abs/2512.18980</guid>
<content:encoded><![CDATA[
arXiv:2512.18980v1 Announce Type: new 
Abstract: Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression</title>
<link>https://arxiv.org/abs/2512.18986</link>
<guid>https://arxiv.org/abs/2512.18986</guid>
<content:encoded><![CDATA[
arXiv:2512.18986v1 Announce Type: new 
Abstract: Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results</title>
<link>https://arxiv.org/abs/2512.19007</link>
<guid>https://arxiv.org/abs/2512.19007</guid>
<content:encoded><![CDATA[
arXiv:2512.19007v1 Announce Type: new 
Abstract: This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizer Dynamics at the Edge of Stability with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.19019</link>
<guid>https://arxiv.org/abs/2512.19019</guid>
<content:encoded><![CDATA[
arXiv:2512.19019v1 Announce Type: new 
Abstract: Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development</title>
<link>https://arxiv.org/abs/2512.19031</link>
<guid>https://arxiv.org/abs/2512.19031</guid>
<content:encoded><![CDATA[
arXiv:2512.19031v1 Announce Type: new 
Abstract: Computational Fluid Dynamics (CFD)-driven training combines machine learning (ML) with CFD solvers to develop physically consistent closure models with improved predictive accuracy. In the original framework, each ML-generated candidate model is embedded in a CFD solver and evaluated against reference data, requiring hundreds to thousands of high-fidelity simulations and resulting in prohibitive computational cost for complex flows. To overcome this limitation, we propose an extended framework that integrates surrogate modeling into symbolic CFD-driven training in real time to reduce training cost. The surrogate model learns to approximate the errors of ML-generated models based on previous CFD evaluations and is continuously refined during training. Newly generated models are first assessed using the surrogate, and only those predicted to yield small errors or high uncertainty are subsequently evaluated with full CFD simulations. Discrete expressions generated by symbolic regression are mapped into a continuous space using averaged input-symbol values as inputs to a probabilistic surrogate model. To support multi-objective model training, particularly when fixed weighting of competing quantities is challenging, the surrogate is extended to a multi-output formulation by generalizing the kernel to a matrix form, providing one mean and variance prediction per training objective. Selection metrics based on these probabilistic outputs are used to identify an optimal training setup. The proposed surrogate-augmented CFD-driven training framework is demonstrated across a range of statistically one- and two-dimensional flows, including both single- and multi-expression model optimization. In all cases, the framework substantially reduces training cost while maintaining predictive accuracy comparable to that of the original CFD-driven approach.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building</title>
<link>https://arxiv.org/abs/2512.19038</link>
<guid>https://arxiv.org/abs/2512.19038</guid>
<content:encoded><![CDATA[
arXiv:2512.19038v1 Announce Type: new 
Abstract: With the press of global climate change, extreme weather and sudden weather changes are becoming increasingly common. To maintain a comfortable indoor environment and minimize the contribution of the building to climate change as much as possible, higher requirements are placed on the operation and control of HVAC systems, e.g., more energy-efficient and flexible to response to the rapid change of weather. This places demands on the rapid modeling and prediction of zone air temperatures of buildings. Compared to the traditional simulation-based approach such as EnergyPlus and DOE2, a hybrid approach combined physics and data-driven is more suitable. Recently, the availability of high-quality datasets and algorithmic breakthroughs have driven a considerable amount of work in this field. However, in the niche of short- and long-term predictions, there are still some gaps in existing research. This paper aims to develop a time series forecast model to predict the zone air temperature in a building located in America on a 2-week horizon. The findings could be further improved to support intelligent control and operation of HVAC systems (i.e. demand flexibility) and could also be used as hybrid building energy modeling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Personalization of Generative Models via Optimal Experimental Design</title>
<link>https://arxiv.org/abs/2512.19057</link>
<guid>https://arxiv.org/abs/2512.19057</guid>
<content:encoded><![CDATA[
arXiv:2512.19057v1 Announce Type: new 
Abstract: Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</title>
<link>https://arxiv.org/abs/2512.19061</link>
<guid>https://arxiv.org/abs/2512.19061</guid>
<content:encoded><![CDATA[
arXiv:2512.19061v1 Announce Type: new 
Abstract: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</title>
<link>https://arxiv.org/abs/2512.19097</link>
<guid>https://arxiv.org/abs/2512.19097</guid>
<content:encoded><![CDATA[
arXiv:2512.19097v1 Announce Type: new 
Abstract: Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Model Deep Learning for Alzheimer Prognostication</title>
<link>https://arxiv.org/abs/2512.19099</link>
<guid>https://arxiv.org/abs/2512.19099</guid>
<content:encoded><![CDATA[
arXiv:2512.19099v1 Announce Type: new 
Abstract: Disease modifying therapies for Alzheimer's disease demand precise timing decisions, yet current predictive models require longitudinal observations and provide no uncertainty quantification, rendering them impractical at the critical first visit when treatment decisions must be made. We developed PROGRESS (PRognostic Generalization from REsting Static Signatures), a dual-model deep learning framework that transforms a single baseline cerebrospinal fluid biomarker assessment into actionable prognostic estimates without requiring prior clinical history. The framework addresses two complementary clinical questions: a probabilistic trajectory network predicts individualized cognitive decline with calibrated uncertainty bounds achieving near-nominal coverage, enabling honest prognostic communication; and a deep survival model estimates time to conversion from mild cognitive impairment to dementia. Using data from over 3,000 participants across 43 Alzheimer's Disease Research Centers in the National Alzheimer's Coordinating Center database, PROGRESS substantially outperforms Cox proportional hazards, Random Survival Forests, and gradient boosting methods for survival prediction. Risk stratification identifies patient groups with seven-fold differences in conversion rates, enabling clinically meaningful treatment prioritization. Leave-one-center-out validation demonstrates robust generalizability, with survival discrimination remaining strong across held-out sites despite heterogeneous measurement conditions spanning four decades of assay technologies. By combining superior survival prediction with trustworthy trajectory uncertainty quantification, PROGRESS bridges the gap between biomarker measurement and personalized clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Timely Parameter Updating in Over-the-Air Federated Learning</title>
<link>https://arxiv.org/abs/2512.19103</link>
<guid>https://arxiv.org/abs/2512.19103</guid>
<content:encoded><![CDATA[
arXiv:2512.19103v1 Announce Type: new 
Abstract: Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction</title>
<link>https://arxiv.org/abs/2512.19114</link>
<guid>https://arxiv.org/abs/2512.19114</guid>
<content:encoded><![CDATA[
arXiv:2512.19114v1 Announce Type: new 
Abstract: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Composable Channel-Adaptive Architecture for Seizure Classification</title>
<link>https://arxiv.org/abs/2512.19123</link>
<guid>https://arxiv.org/abs/2512.19123</guid>
<content:encoded><![CDATA[
arXiv:2512.19123v1 Announce Type: new 
Abstract: Objective: We develop a channel-adaptive (CA) architecture that seamlessly processes multi-variate time-series with an arbitrary number of channels, and in particular intracranial electroencephalography (iEEG) recordings. Methods: Our CA architecture first processes the iEEG signal using state-of-the-art models applied to each single channel independently. The resulting features are then fused using a vector-symbolic algorithm which reconstructs the spatial relationship using a trainable scalar per channel. Finally, the fused features are accumulated in a long-term memory of up to 2 minutes to perform the classification. Each CA-model can then be pre-trained on a large corpus of iEEG recordings from multiple heterogeneous subjects. The pre-trained model is personalized to each subject via a quick fine-tuning routine, which uses equal or lower amounts of data compared to existing state-of-the-art models, but requiring only 1/5 of the time. Results: We evaluate our CA-models on a seizure detection task both on a short-term (~20 hours) and a long-term (~2500 hours) dataset. In particular, our CA-EEGWaveNet is trained on a single seizure of the tested subject, while the baseline EEGWaveNet is trained on all but one. Even in this challenging scenario, our CA-EEGWaveNet surpasses the baseline in median F1-score (0.78 vs 0.76). Similarly, CA-EEGNet based on EEGNet, also surpasses its baseline in median F1-score (0.79 vs 0.74). Conclusion and significance: Our CA-model addresses two issues: first, it is channel-adaptive and can therefore be trained across heterogeneous subjects without loss of performance; second, it increases the effective temporal context size to a clinically-relevant length. Therefore, our model is a drop-in replacement for existing models, bringing better characteristics and performance across the board.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage</title>
<link>https://arxiv.org/abs/2512.19142</link>
<guid>https://arxiv.org/abs/2512.19142</guid>
<content:encoded><![CDATA[
arXiv:2512.19142v1 Announce Type: new 
Abstract: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{\'a}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling</title>
<link>https://arxiv.org/abs/2512.19147</link>
<guid>https://arxiv.org/abs/2512.19147</guid>
<content:encoded><![CDATA[
arXiv:2512.19147v1 Announce Type: new 
Abstract: Nowadays, industrial hybrid modeling which integrates both mechanistic modeling and machine learning-based modeling techniques has attracted increasing interest from scholars due to its high accuracy, low computational cost, and satisfactory interpretability. Nevertheless, the existing industrial hybrid modeling methods still face two main limitations. First, current research has mainly focused on applying a single machine learning method to one specific task, failing to develop a comprehensive machine learning architecture suitable for modeling tasks, which limits their ability to effectively represent complex industrial scenarios. Second, industrial datasets often contain underlying associations (e.g., monotonicity or periodicity) that are not adequately exploited by current research, which can degrade model's predictive performance. To address these limitations, this paper proposes the Recurrent Perceptron-based Channel Attention Transformer Encoder (RP-CATE), with three distinctive characteristics: 1: We developed a novel architecture by replacing the self-attention mechanism with channel attention and incorporating our proposed Recurrent Perceptron (RP) Module into Transformer, achieving enhanced effectiveness for industrial modeling tasks compared to the original Transformer. 2: We proposed a new data type called Pseudo-Image Data (PID) tailored for channel attention requirements and developed a cyclic sliding window method for generating PID. 3: We introduced the concept of Pseudo-Sequential Data (PSD) and a method for converting industrial datasets into PSD, which enables the RP Module to capture the underlying associations within industrial dataset more effectively. An experiment aimed at hybrid modeling in chemical engineering was conducted by using RP-CATE and the experimental results demonstrate that RP-CATE achieves the best performance compared to other baseline models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments</title>
<link>https://arxiv.org/abs/2512.19154</link>
<guid>https://arxiv.org/abs/2512.19154</guid>
<content:encoded><![CDATA[
arXiv:2512.19154v1 Announce Type: new 
Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Quantum-Classical Feature Fusion for complex data Classification</title>
<link>https://arxiv.org/abs/2512.19180</link>
<guid>https://arxiv.org/abs/2512.19180</guid>
<content:encoded><![CDATA[
arXiv:2512.19180v1 Announce Type: new 
Abstract: Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.19184</link>
<guid>https://arxiv.org/abs/2512.19184</guid>
<content:encoded><![CDATA[
arXiv:2512.19184v1 Announce Type: new 
Abstract: This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction</title>
<link>https://arxiv.org/abs/2512.19194</link>
<guid>https://arxiv.org/abs/2512.19194</guid>
<content:encoded><![CDATA[
arXiv:2512.19194v1 Announce Type: new 
Abstract: Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</title>
<link>https://arxiv.org/abs/2512.19199</link>
<guid>https://arxiv.org/abs/2512.19199</guid>
<content:encoded><![CDATA[
arXiv:2512.19199v1 Announce Type: new 
Abstract: The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning</title>
<link>https://arxiv.org/abs/2512.19206</link>
<guid>https://arxiv.org/abs/2512.19206</guid>
<content:encoded><![CDATA[
arXiv:2512.19206v1 Announce Type: new 
Abstract: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-space entropy at acquisition reflects downstream learnability</title>
<link>https://arxiv.org/abs/2512.19223</link>
<guid>https://arxiv.org/abs/2512.19223</guid>
<content:encoded><![CDATA[
arXiv:2512.19223v1 Announce Type: new 
Abstract: Modern learning systems work with data that vary widely across domains, but they all ultimately depend on how much structure is already present in the measurements before any model is trained. This raises a basic question: is there a general, modality-agnostic way to quantify how acquisition itself preserves or destroys the information that downstream learners could use? Here we propose an acquisition-level scalar $\Delta S_{\mathcal B}$ based on instrument-resolved phase space. Unlike pixelwise distortion or purely spectral errors that often saturate under aggressive undersampling, $\Delta S_{\mathcal B}$ directly quantifies how acquisition mixes or removes joint space--frequency structure at the instrument scale. We show theoretically that \(\Delta S_{\mathcal B}\) correctly identifies the phase-space coherence of periodic sampling as the physical source of aliasing, recovering classical sampling-theorem consequences. Empirically, across masked image classification, accelerated MRI, and massive MIMO (including over-the-air measurements), $|\Delta S_{\mathcal B}|$ consistently ranks sampling geometries and predicts downstream reconstruction/recognition difficulty \emph{without training}. In particular, minimizing $|\Delta S_{\mathcal B}|$ enables zero-training selection of variable-density MRI mask parameters that matches designs tuned by conventional pre-reconstruction criteria. These results suggest that phase-space entropy at acquisition reflects downstream learnability, enabling pre-training selection of candidate sampling policies and as a shared notion of information preservation across modalities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regression generation adversarial network based on dual data evaluation strategy for industrial application</title>
<link>https://arxiv.org/abs/2512.19232</link>
<guid>https://arxiv.org/abs/2512.19232</guid>
<content:encoded><![CDATA[
arXiv:2512.19232v1 Announce Type: new 
Abstract: Soft sensing infers hard-to-measure data through a large number of easily obtainable variables. However, in complex industrial scenarios, the issue of insufficient data volume persists, which diminishes the reliability of soft sensing. Generative Adversarial Networks (GAN) are one of the effective solutions for addressing insufficient samples. Nevertheless, traditional GAN fail to account for the mapping relationship between labels and features, which limits further performance improvement. Although some studies have proposed solutions, none have considered both performance and efficiency simultaneously. To address these problems, this paper proposes the multi-task learning-based regression GAN framework that integrates regression information into both the discriminator and generator, and implements a shallow sharing mechanism between the discriminator and regressor. This approach significantly enhances the quality of generated samples while improving the algorithm's operational efficiency. Moreover, considering the importance of training samples and generated samples, a dual data evaluation strategy is designed to make GAN generate more diverse samples, thereby increasing the generalization of subsequent modeling. The superiority of method is validated through four classic industrial soft sensing cases: wastewater treatment plants, surface water, $CO_2$ absorption towers, and industrial gas turbines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</title>
<link>https://arxiv.org/abs/2512.19246</link>
<guid>https://arxiv.org/abs/2512.19246</guid>
<content:encoded><![CDATA[
arXiv:2512.19246v1 Announce Type: new 
Abstract: Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems</title>
<link>https://arxiv.org/abs/2512.19250</link>
<guid>https://arxiv.org/abs/2512.19250</guid>
<content:encoded><![CDATA[
arXiv:2512.19250v1 Announce Type: new 
Abstract: Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</title>
<link>https://arxiv.org/abs/2512.19253</link>
<guid>https://arxiv.org/abs/2512.19253</guid>
<content:encoded><![CDATA[
arXiv:2512.19253v1 Announce Type: new 
Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</title>
<link>https://arxiv.org/abs/2512.19280</link>
<guid>https://arxiv.org/abs/2512.19280</guid>
<content:encoded><![CDATA[
arXiv:2512.19280v1 Announce Type: new 
Abstract: Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring</title>
<link>https://arxiv.org/abs/2512.19309</link>
<guid>https://arxiv.org/abs/2512.19309</guid>
<content:encoded><![CDATA[
arXiv:2512.19309v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Achieving Superior Model Merging via Magnitude Calibration</title>
<link>https://arxiv.org/abs/2512.19320</link>
<guid>https://arxiv.org/abs/2512.19320</guid>
<content:encoded><![CDATA[
arXiv:2512.19320v1 Announce Type: new 
Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative positional encoding functions for neural transformers</title>
<link>https://arxiv.org/abs/2512.19323</link>
<guid>https://arxiv.org/abs/2512.19323</guid>
<content:encoded><![CDATA[
arXiv:2512.19323v1 Announce Type: new 
Abstract: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Logical View of GNN-Style Computation and the Role of Activation Functions</title>
<link>https://arxiv.org/abs/2512.19332</link>
<guid>https://arxiv.org/abs/2512.19332</guid>
<content:encoded><![CDATA[
arXiv:2512.19332v1 Announce Type: new 
Abstract: We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</title>
<link>https://arxiv.org/abs/2512.19361</link>
<guid>https://arxiv.org/abs/2512.19361</guid>
<content:encoded><![CDATA[
arXiv:2512.19361v1 Announce Type: new 
Abstract: The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples</title>
<link>https://arxiv.org/abs/2512.19363</link>
<guid>https://arxiv.org/abs/2512.19363</guid>
<content:encoded><![CDATA[
arXiv:2512.19363v1 Announce Type: new 
Abstract: How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture</title>
<link>https://arxiv.org/abs/2512.19367</link>
<guid>https://arxiv.org/abs/2512.19367</guid>
<content:encoded><![CDATA[
arXiv:2512.19367v1 Announce Type: new 
Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</title>
<link>https://arxiv.org/abs/2512.19379</link>
<guid>https://arxiv.org/abs/2512.19379</guid>
<content:encoded><![CDATA[
arXiv:2512.19379v1 Announce Type: new 
Abstract: Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Machine Learning for Embedded Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.19383</link>
<guid>https://arxiv.org/abs/2512.19383</guid>
<content:encoded><![CDATA[
arXiv:2512.19383v1 Announce Type: new 
Abstract: The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Grounded Axes for Reading and Steering LLM States</title>
<link>https://arxiv.org/abs/2512.19399</link>
<guid>https://arxiv.org/abs/2512.19399</guid>
<content:encoded><![CDATA[
arXiv:2512.19399v1 Announce Type: new 
Abstract: Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symplectic Reservoir Representation of Legendre Dynamics</title>
<link>https://arxiv.org/abs/2512.19409</link>
<guid>https://arxiv.org/abs/2512.19409</guid>
<content:encoded><![CDATA[
arXiv:2512.19409v1 Announce Type: new 
Abstract: Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?
  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.
  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form "cotangent lift of a base diffeomorphism followed by an exact fibre translation". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.
  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research Program: Theory of Learning in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.19410</link>
<guid>https://arxiv.org/abs/2512.19410</guid>
<content:encoded><![CDATA[
arXiv:2512.19410v1 Announce Type: new 
Abstract: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Is Not What You Need</title>
<link>https://arxiv.org/abs/2512.19428</link>
<guid>https://arxiv.org/abs/2512.19428</guid>
<content:encoded><![CDATA[
arXiv:2512.19428v1 Announce Type: new 
Abstract: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.
  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.
  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning</title>
<link>https://arxiv.org/abs/2512.19439</link>
<guid>https://arxiv.org/abs/2512.19439</guid>
<content:encoded><![CDATA[
arXiv:2512.19439v1 Announce Type: new 
Abstract: Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm</title>
<link>https://arxiv.org/abs/2512.19440</link>
<guid>https://arxiv.org/abs/2512.19440</guid>
<content:encoded><![CDATA[
arXiv:2512.19440v1 Announce Type: new 
Abstract: Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</title>
<link>https://arxiv.org/abs/2512.19472</link>
<guid>https://arxiv.org/abs/2512.19472</guid>
<content:encoded><![CDATA[
arXiv:2512.19472v1 Announce Type: new 
Abstract: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</title>
<link>https://arxiv.org/abs/2512.19488</link>
<guid>https://arxiv.org/abs/2512.19488</guid>
<content:encoded><![CDATA[
arXiv:2512.19488v1 Announce Type: new 
Abstract: The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico</title>
<link>https://arxiv.org/abs/2512.19491</link>
<guid>https://arxiv.org/abs/2512.19491</guid>
<content:encoded><![CDATA[
arXiv:2512.19491v1 Announce Type: new 
Abstract: Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</title>
<link>https://arxiv.org/abs/2512.19494</link>
<guid>https://arxiv.org/abs/2512.19494</guid>
<content:encoded><![CDATA[
arXiv:2512.19494v1 Announce Type: new 
Abstract: The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast</title>
<link>https://arxiv.org/abs/2512.19506</link>
<guid>https://arxiv.org/abs/2512.19506</guid>
<content:encoded><![CDATA[
arXiv:2512.19506v1 Announce Type: new 
Abstract: Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Scalable and Valid Conditional Independence Testing with Spectral Representations</title>
<link>https://arxiv.org/abs/2512.19510</link>
<guid>https://arxiv.org/abs/2512.19510</guid>
<content:encoded><![CDATA[
arXiv:2512.19510v1 Announce Type: new 
Abstract: Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.19516</link>
<guid>https://arxiv.org/abs/2512.19516</guid>
<content:encoded><![CDATA[
arXiv:2512.19516v1 Announce Type: new 
Abstract: Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Initialization of a Polyharmonic Cascade, Launch and Testing</title>
<link>https://arxiv.org/abs/2512.19524</link>
<guid>https://arxiv.org/abs/2512.19524</guid>
<content:encoded><![CDATA[
arXiv:2512.19524v1 Announce Type: new 
Abstract: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</title>
<link>https://arxiv.org/abs/2512.19527</link>
<guid>https://arxiv.org/abs/2512.19527</guid>
<content:encoded><![CDATA[
arXiv:2512.19527v1 Announce Type: new 
Abstract: Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</title>
<link>https://arxiv.org/abs/2512.19530</link>
<guid>https://arxiv.org/abs/2512.19530</guid>
<content:encoded><![CDATA[
arXiv:2512.19530v1 Announce Type: new 
Abstract: Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.
  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFORD: Directional Feedback based Online Ordinal Regression Learning</title>
<link>https://arxiv.org/abs/2512.19550</link>
<guid>https://arxiv.org/abs/2512.19550</guid>
<content:encoded><![CDATA[
arXiv:2512.19550v1 Announce Type: new 
Abstract: In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</title>
<link>https://arxiv.org/abs/2512.19554</link>
<guid>https://arxiv.org/abs/2512.19554</guid>
<content:encoded><![CDATA[
arXiv:2512.19554v1 Announce Type: new 
Abstract: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.19605</link>
<guid>https://arxiv.org/abs/2512.19605</guid>
<content:encoded><![CDATA[
arXiv:2512.19605v1 Announce Type: new 
Abstract: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference</title>
<link>https://arxiv.org/abs/2512.19643</link>
<guid>https://arxiv.org/abs/2512.19643</guid>
<content:encoded><![CDATA[
arXiv:2512.19643v1 Announce Type: new 
Abstract: Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Legendre Transform</title>
<link>https://arxiv.org/abs/2512.19649</link>
<guid>https://arxiv.org/abs/2512.19649</guid>
<content:encoded><![CDATA[
arXiv:2512.19649v1 Announce Type: new 
Abstract: We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</title>
<link>https://arxiv.org/abs/2512.19673</link>
<guid>https://arxiv.org/abs/2512.19673</guid>
<content:encoded><![CDATA[
arXiv:2512.19673v1 Announce Type: new 
Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</title>
<link>https://arxiv.org/abs/2512.17910</link>
<guid>https://arxiv.org/abs/2512.17910</guid>
<content:encoded><![CDATA[
arXiv:2512.17910v1 Announce Type: cross 
Abstract: Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.17911</link>
<guid>https://arxiv.org/abs/2512.17911</guid>
<content:encoded><![CDATA[
arXiv:2512.17911v1 Announce Type: cross 
Abstract: Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset</title>
<link>https://arxiv.org/abs/2512.17915</link>
<guid>https://arxiv.org/abs/2512.17915</guid>
<content:encoded><![CDATA[
arXiv:2512.17915v1 Announce Type: cross 
Abstract: The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</title>
<link>https://arxiv.org/abs/2512.17918</link>
<guid>https://arxiv.org/abs/2512.17918</guid>
<content:encoded><![CDATA[
arXiv:2512.17918v1 Announce Type: cross 
Abstract: Quantum computing offers new ways to explore the theory of computation via the laws of quantum mechanics. Due to the rising demand for quantum computing resources, there is growing interest in developing cloud-based quantum resource sharing platforms that enable researchers to test and execute their algorithms on real quantum hardware. These cloud-based systems face a fundamental challenge in efficiently allocating quantum hardware resources to fulfill the growing computational demand of modern Internet of Things (IoT) applications. So far, attempts have been made in order to make efficient resource allocation, ranging from heuristic-based solutions to machine learning. In this work, we employ quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem to support large IoT networks. We propose a python-based toolkit called QAISim for the simulation and modeling of Quantum Artificial Intelligence (QAI) models for designing resource management policies in quantum cloud environments. We have simulated policy gradient and Deep Q-Learning algorithms for reinforcement learning. QAISim exhibits a substantial reduction in model complexity compared to its classical counterparts with fewer trainable variables.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</title>
<link>https://arxiv.org/abs/2512.17923</link>
<guid>https://arxiv.org/abs/2512.17923</guid>
<content:encoded><![CDATA[
arXiv:2512.17923v1 Announce Type: cross 
Abstract: We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&amp;P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A curated UK rain radar data set for training and benchmarking nowcasting models</title>
<link>https://arxiv.org/abs/2512.17924</link>
<guid>https://arxiv.org/abs/2512.17924</guid>
<content:encoded><![CDATA[
arXiv:2512.17924v1 Announce Type: cross 
Abstract: This paper documents a data set of UK rain radar image sequences for use in statistical modeling and machine learning methods for nowcasting. The main dataset contains 1,000 randomly sampled sequences of length 20 steps (15-minute increments) of 2D radar intensity fields of dimension 40x40 (at 5km spatial resolution). Spatially stratified sampling ensures spatial homogeneity despite removal of clear-sky cases by threshold-based truncation. For each radar sequence, additional atmospheric and geographic features are made available, including date, location, mean elevation, mean wind direction and speed and prevailing storm type. New R functions to extract data from the binary "Nimrod" radar data format are provided. A case study is presented to train and evaluate a simple convolutional neural network for radar nowcasting, including self-contained R code.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</title>
<link>https://arxiv.org/abs/2512.17928</link>
<guid>https://arxiv.org/abs/2512.17928</guid>
<content:encoded><![CDATA[
arXiv:2512.17928v1 Announce Type: cross 
Abstract: Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</title>
<link>https://arxiv.org/abs/2512.17929</link>
<guid>https://arxiv.org/abs/2512.17929</guid>
<content:encoded><![CDATA[
arXiv:2512.17929v1 Announce Type: cross 
Abstract: We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>chatter: a Python library for applying information theory and AI/ML models to animal communication</title>
<link>https://arxiv.org/abs/2512.17935</link>
<guid>https://arxiv.org/abs/2512.17935</guid>
<content:encoded><![CDATA[
arXiv:2512.17935v1 Announce Type: cross 
Abstract: The study of animal communication often involves categorizing units into types (e.g. syllables in songbirds, or notes in humpback whales). While this approach is useful in many cases, it necessarily flattens the complexity and nuance present in real communication systems. chatter is a new Python library for analyzing animal communication in continuous latent space using information theory and modern machine learning techniques. It is taxonomically agnostic, and has been tested with the vocalizations of birds, bats, whales, and primates. By leveraging a variety of different architectures, including variational autoencoders and vision transformers, chatter represents vocal sequences as trajectories in high-dimensional latent space, bypassing the need for manual or automatic categorization of units. The library provides an end-to-end workflow -- from preprocessing and segmentation to model training and feature extraction -- that enables researchers to quantify the complexity, predictability, similarity, and novelty of vocal sequences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</title>
<link>https://arxiv.org/abs/2512.17936</link>
<guid>https://arxiv.org/abs/2512.17936</guid>
<content:encoded><![CDATA[
arXiv:2512.17936v1 Announce Type: cross 
Abstract: In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in T\"urkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</title>
<link>https://arxiv.org/abs/2512.17954</link>
<guid>https://arxiv.org/abs/2512.17954</guid>
<content:encoded><![CDATA[
arXiv:2512.17954v1 Announce Type: cross 
Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler</title>
<link>https://arxiv.org/abs/2512.17977</link>
<guid>https://arxiv.org/abs/2512.17977</guid>
<content:encoded><![CDATA[
arXiv:2512.17977v1 Announce Type: cross 
Abstract: Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEGState: Phoneme Decoding from Magnetoencephalography Signals</title>
<link>https://arxiv.org/abs/2512.17978</link>
<guid>https://arxiv.org/abs/2512.17978</guid>
<content:encoded><![CDATA[
arXiv:2512.17978v1 Announce Type: cross 
Abstract: Decoding linguistically meaningful representations from non-invasive neural recordings remains a central challenge in neural speech decoding. Among available neuroimaging modalities, magnetoencephalography (MEG) provides a safe and repeatable means of mapping speech-related cortical dynamics, yet its low signal-to-noise ratio and high temporal dimensionality continue to hinder robust decoding. In this work, we introduce MEGState, a novel architecture for phoneme decoding from MEG signals that captures fine-grained cortical responses evoked by auditory stimuli. Extensive experiments on the LibriBrain dataset demonstrate that MEGState consistently surpasses baseline model across multiple evaluation metrics. These findings highlight the potential of MEG-based phoneme decoding as a scalable pathway toward non-invasive brain-computer interfaces for speech.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</title>
<link>https://arxiv.org/abs/2512.17987</link>
<guid>https://arxiv.org/abs/2512.17987</guid>
<content:encoded><![CDATA[
arXiv:2512.17987v1 Announce Type: cross 
Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18004</link>
<guid>https://arxiv.org/abs/2512.18004</guid>
<content:encoded><![CDATA[
arXiv:2512.18004v1 Announce Type: cross 
Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</title>
<link>https://arxiv.org/abs/2512.18014</link>
<guid>https://arxiv.org/abs/2512.18014</guid>
<content:encoded><![CDATA[
arXiv:2512.18014v1 Announce Type: cross 
Abstract: This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models</title>
<link>https://arxiv.org/abs/2512.18021</link>
<guid>https://arxiv.org/abs/2512.18021</guid>
<content:encoded><![CDATA[
arXiv:2512.18021v1 Announce Type: cross 
Abstract: Trapped-ion quantum computers based on segmented traps rely on shuttling operations to establish connectivity between multiple sub-registers within a quantum processing unit. Several architectures of increasing complexity have already been realized, including linear arrays, racetrack loops, and junction-based layouts. As hardware capabilities advance, the need arises for flexible software layers within the control stack to manage qubit routing$\unicode{x2014}$the process of dynamically reconfiguring qubit positions so that all qubits involved in a gate operation are co-located within the same segment. Existing approaches typically employ architecture-specific heuristics, which become impractical as system complexity grows. To address this challenge, we propose a layout-independent compilation strategy based on large language models (LLMs). Specifically, we fine-tune pretrained LLMs to generate the required shuttling operations. We evaluate this approach on both linear and branched one-dimensional architectures, demonstrating that it provides a foundation for developing LLM-based shuttling compilers for trapped-ion quantum computers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-range electrostatics for machine learning interatomic potentials is easier than we thought</title>
<link>https://arxiv.org/abs/2512.18029</link>
<guid>https://arxiv.org/abs/2512.18029</guid>
<content:encoded><![CDATA[
arXiv:2512.18029v1 Announce Type: cross 
Abstract: The lack of long-range electrostatics is a key limitation of modern machine learning interatomic potentials (MLIPs), hindering reliable applications to interfaces, charge-transfer reactions, polar and ionic materials, and biomolecules. In this Perspective, we distill two design principles behind the Latent Ewald Summation (LES) framework, which can capture long-range interactions, charges, and electrical response just by learning from standard energy and force training data: (i) use a Coulomb functional form with environment-dependent charges to capture electrostatic interactions, and (ii) avoid explicit training on ambiguous density functional theory (DFT) partial charges. When both principles are satisfied, substantial flexibility remains: essentially any short-range MLIP can be augmented; charge equilibration schemes can be added when desired; dipoles and Born effective charges can be inferred or finetuned; and charge/spin-state embeddings or tensorial targets can be further incorporated. We also discuss current limitations and open challenges. Together, these minimal, physics-guided design rules suggest that incorporating long-range electrostatics into MLIPs is simpler and perhaps more broadly applicable than is commonly assumed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2512.18038</link>
<guid>https://arxiv.org/abs/2512.18038</guid>
<content:encoded><![CDATA[
arXiv:2512.18038v1 Announce Type: cross 
Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts</title>
<link>https://arxiv.org/abs/2512.18041</link>
<guid>https://arxiv.org/abs/2512.18041</guid>
<content:encoded><![CDATA[
arXiv:2512.18041v1 Announce Type: cross 
Abstract: Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOODER: Real-time Facial Authentication and Expression Recognition</title>
<link>https://arxiv.org/abs/2512.18057</link>
<guid>https://arxiv.org/abs/2512.18057</guid>
<content:encoded><![CDATA[
arXiv:2512.18057v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation and learning with compositional tensor trains</title>
<link>https://arxiv.org/abs/2512.18059</link>
<guid>https://arxiv.org/abs/2512.18059</guid>
<content:encoded><![CDATA[
arXiv:2512.18059v1 Announce Type: cross 
Abstract: We introduce compositional tensor trains (CTTs) for the approximation of multivariate functions, a class of models obtained by composing low-rank functions in the tensor-train format. This format can encode standard approximation tools, such as (sparse) polynomials, deep neural networks (DNNs) with fixed width, or tensor networks with arbitrary permutation of the inputs, or more general affine coordinate transformations, with similar complexities. This format can be viewed as a DNN with width exponential in the input dimension and structured weights matrices. Compared to DNNs, this format enables controlled compression at the layer level using efficient tensor algebra. On the optimization side, we derive a layerwise algorithm inspired by natural gradient descent, allowing to exploit efficient low-rank tensor algebra. This relies on low-rank estimations of Gram matrices, and tensor structured random sketching. Viewing the format as a discrete dynamical system, we also derive an optimization algorithm inspired by numerical methods in optimal control. Numerical experiments on regression tasks demonstrate the expressivity of the new format and the relevance of the proposed optimization algorithms. Overall, CTTs combine the expressivity of compositional models with the algorithmic efficiency of tensor algebra, offering a scalable alternative to standard deep neural networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-based Nearest Neighbors with Dynamic Updates via Random Walks</title>
<link>https://arxiv.org/abs/2512.18060</link>
<guid>https://arxiv.org/abs/2512.18060</guid>
<content:encoded><![CDATA[
arXiv:2512.18060v1 Announce Type: cross 
Abstract: Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty</title>
<link>https://arxiv.org/abs/2512.18083</link>
<guid>https://arxiv.org/abs/2512.18083</guid>
<content:encoded><![CDATA[
arXiv:2512.18083v1 Announce Type: cross 
Abstract: Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.
  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true "ATE Risk Function" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.18092</link>
<guid>https://arxiv.org/abs/2512.18092</guid>
<content:encoded><![CDATA[
arXiv:2512.18092v1 Announce Type: cross 
Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines</title>
<link>https://arxiv.org/abs/2512.18102</link>
<guid>https://arxiv.org/abs/2512.18102</guid>
<content:encoded><![CDATA[
arXiv:2512.18102v1 Announce Type: cross 
Abstract: Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.
  Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify minimal static (code) and dynamic (runtime) features for detecting high-risk inputs.
  Method: Guided by historical V8 bugs, iterative prompting generated 115 static and 49 dynamic features, with the latter requiring only five trace flags, minimizing instrumentation cost. After feature selection, 41 features remained to train an XGBoost model to predict high-risk inputs during fuzzing.
  Results: Combining static and dynamic features yields over 85% precision and under 1% false alarms. Only 25% of these features are needed for comparable performance, showing that most of the search space is irrelevant.
  Conclusion: This work introduces feature-guided fuzzing, an automated data-driven approach that replaces coverage with data-directed inference, guiding fuzzers toward high-risk states for faster, targeted, and reproducible vulnerability discovery. To support open science, all scripts and data are available at https://github.com/KKGanguly/DataCentricFuzzJS .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring polymer classification with a hybrid single-photon quantum approach</title>
<link>https://arxiv.org/abs/2512.18125</link>
<guid>https://arxiv.org/abs/2512.18125</guid>
<content:encoded><![CDATA[
arXiv:2512.18125v1 Announce Type: cross 
Abstract: Polymers exhibit complex architectures and diverse properties that place them at the center of contemporary research in chemistry and materials science. As conventional computational techniques, even multi-scale ones, struggle to capture this complexity, quantum computing offers a promising alternative framework for extracting structure-property relationships. Noisy Intermediate-Scale Quantum (NISQ) devices are commonly used to explore the implementation of algorithms, including quantum neural networks for classification tasks, despite ongoing debate regarding their practical impact.
  We present a hybrid classical-quantum formalism that couples a classical deep neural network for polymer featurization with a single-photon-based quantum classifier native to photonic quantum computing. This pipeline successfully classifies polymer species by their optical gap, with performance in line between CPU-based noisy simulations and a proof-of-principle run on Quandela's Ascella quantum processor. These findings demonstrate the effectiveness of the proposed computational workflow and indicate that chemistryfrelated classification tasks can already be tackled under the constraints of today's NISQ devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</title>
<link>https://arxiv.org/abs/2512.18134</link>
<guid>https://arxiv.org/abs/2512.18134</guid>
<content:encoded><![CDATA[
arXiv:2512.18134v1 Announce Type: cross 
Abstract: GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Solvation Free Energies with Boltzmann Generators</title>
<link>https://arxiv.org/abs/2512.18147</link>
<guid>https://arxiv.org/abs/2512.18147</guid>
<content:encoded><![CDATA[
arXiv:2512.18147v1 Announce Type: cross 
Abstract: Accurate calculations of solvation free energies remain a central challenge in molecular simulations, often requiring extensive sampling and numerous alchemical intermediates to ensure sufficient overlap between phase-space distributions of a solute in the gas phase and in solution. Here, we introduce a computational framework based on normalizing flows that directly maps solvent configurations between solutes of different sizes, and compare the accuracy and efficiency to conventional free energy estimates. For a Lennard-Jones solvent, we demonstrate that this approach yields acceptable accuracy in estimating free energy differences for challenging transformations, such as solute growth or increased solute-solute separation, which typically demand multiple intermediate simulation steps along the transformation. Analysis of radial distribution functions indicates that the flow generates physically meaningful solvent rearrangements, substantially enhancing configurational overlap between states in configuration space. These results suggest flow-based models as a promising alternative to traditional free energy estimation methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.18190</link>
<guid>https://arxiv.org/abs/2512.18190</guid>
<content:encoded><![CDATA[
arXiv:2512.18190v1 Announce Type: cross 
Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</title>
<link>https://arxiv.org/abs/2512.18228</link>
<guid>https://arxiv.org/abs/2512.18228</guid>
<content:encoded><![CDATA[
arXiv:2512.18228v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in handling graph-structured data; however, they exhibit failures after deployment, which can cause severe consequences. Hence, conducting thorough testing before deployment becomes imperative to ensure the reliability of GNNs. However, thorough testing requires numerous manually annotated test data. To mitigate the annotation cost, strategically prioritizing and labeling high-quality unlabeled inputs for testing becomes crucial, which facilitates uncovering more model failures with a limited labeling budget. Unfortunately, existing test input prioritization techniques either overlook the valuable information contained in graph structures or are overly reliant on attributes extracted from the target model, i.e., model-aware attributes, whose quality can vary significantly. To address these issues, we propose a novel test input prioritization framework, named GraphRank, for GNNs. GraphRank introduces model-agnostic attributes to compensate for the limitations of the model-aware ones. It also leverages the graph structure information to aggregate attributes from neighboring nodes, thereby enhancing the model-aware and model-agnostic attributes. Furthermore, GraphRank combines the above attributes with a binary classifier, using it as a ranking model to prioritize inputs. This classifier undergoes iterative training, which enables it to learn from each round's feedback and improve its performance accordingly. Extensive experiments demonstrate GraphRank's superiority over existing techniques.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dimensionality Reduction Considered Harmful (Some of the Time)</title>
<link>https://arxiv.org/abs/2512.18230</link>
<guid>https://arxiv.org/abs/2512.18230</guid>
<content:encoded><![CDATA[
arXiv:2512.18230v1 Announce Type: cross 
Abstract: Visual analytics now plays a central role in decision-making across diverse disciplines, but it can be unreliable: the knowledge or insights derived from the analysis may not accurately reflect the underlying data. In this dissertation, we improve the reliability of visual analytics with a focus on dimensionality reduction (DR). DR techniques enable visual analysis of high-dimensional data by reducing it to two or three dimensions, but they inherently introduce errors that can compromise the reliability of visual analytics. To this end, I investigate reliability challenges that practitioners face when using DR for visual analytics. Then, I propose technical solutions to address these challenges, including new evaluation metrics, optimization strategies, and interaction techniques. We conclude the thesis by discussing how our contributions lay the foundation for achieving more reliable visual analytics practices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation</title>
<link>https://arxiv.org/abs/2512.18232</link>
<guid>https://arxiv.org/abs/2512.18232</guid>
<content:encoded><![CDATA[
arXiv:2512.18232v1 Announce Type: cross 
Abstract: Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.18251</link>
<guid>https://arxiv.org/abs/2512.18251</guid>
<content:encoded><![CDATA[
arXiv:2512.18251v1 Announce Type: cross 
Abstract: Crystal structure prediction is a fundamental problem in materials science. We present CrystalFormer-CSP, an efficient framework that unifies data-driven heuristic and physics-driven optimization approaches to predict stable crystal structures for given chemical compositions. The approach combines pretrained generative models for space-group-informed structure generation and a universal machine learning force field for energy minimization. Reinforcement fine-tuning can be employed to further boost the accuracy of the framework. We demonstrate the effectiveness of CrystalFormer-CSP on benchmark problems and showcase its usage via web interface and language model integration.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition</title>
<link>https://arxiv.org/abs/2512.18263</link>
<guid>https://arxiv.org/abs/2512.18263</guid>
<content:encoded><![CDATA[
arXiv:2512.18263v1 Announce Type: cross 
Abstract: Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A two-stream network with global-local feature fusion for bone age assessment</title>
<link>https://arxiv.org/abs/2512.18331</link>
<guid>https://arxiv.org/abs/2512.18331</guid>
<content:encoded><![CDATA[
arXiv:2512.18331v1 Announce Type: cross 
Abstract: Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</title>
<link>https://arxiv.org/abs/2512.18333</link>
<guid>https://arxiv.org/abs/2512.18333</guid>
<content:encoded><![CDATA[
arXiv:2512.18333v1 Announce Type: cross 
Abstract: This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($\phi$) and Pitch ($\theta$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($\psi$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</title>
<link>https://arxiv.org/abs/2512.18336</link>
<guid>https://arxiv.org/abs/2512.18336</guid>
<content:encoded><![CDATA[
arXiv:2512.18336v1 Announce Type: cross 
Abstract: This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</title>
<link>https://arxiv.org/abs/2512.18365</link>
<guid>https://arxiv.org/abs/2512.18365</guid>
<content:encoded><![CDATA[
arXiv:2512.18365v1 Announce Type: cross 
Abstract: Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost. Code is available at https://github.com/YazidJanati/ding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior</title>
<link>https://arxiv.org/abs/2512.18367</link>
<guid>https://arxiv.org/abs/2512.18367</guid>
<content:encoded><![CDATA[
arXiv:2512.18367v1 Announce Type: cross 
Abstract: Diffusion models are highly expressive image priors for Bayesian inverse problems. However, most diffusion models cannot operate on large-scale, high-dimensional data due to high training and inference costs. In this work, we introduce a Plug-and-play algorithm for 3D stochastic inference with latent diffusion prior (PSI3D) to address massive ($1024\times 1024\times 128$) volumes. Specifically, we formulate a Markov chain Monte Carlo approach to reconstruct each two-dimensional (2D) slice by sampling from a 2D latent diffusion model. To enhance inter-slice consistency, we also incorporate total variation (TV) regularization stochastically along the concatenation axis. We evaluate our performance on optical coherence tomography (OCT) super-resolution. Our method significantly improves reconstruction quality for large-scale scientific imaging compared to traditional and learning-based baselines, while providing robust and credible reconstructions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Proofs for Sound Verification and Control of Complex Systems</title>
<link>https://arxiv.org/abs/2512.18389</link>
<guid>https://arxiv.org/abs/2512.18389</guid>
<content:encoded><![CDATA[
arXiv:2512.18389v1 Announce Type: cross 
Abstract: This informal contribution presents an ongoing line of research that is pursuing a new approach to the construction of sound proofs for the formal verification and control of complex stochastic models of dynamical systems, of reactive programs and, more generally, of models of Cyber-Physical Systems. Neural proofs are made up of two key components: 1) proof rules encode requirements entailing the verification of general temporal specifications over the models of interest; and 2) certificates that discharge such rules, namely they are constructed from said proof rules with an inductive (that is, cyclic, repetitive) approach; this inductive approach involves: 2a) accessing samples from the model's dynamics and accordingly training neural networks, whilst 2b) generalising such networks via SAT-modulo-theory (SMT) queries that leverage the full knowledge of the models. In the context of sequential decision making problems over complex stochastic models, it is possible to additionally generate provably-correct policies/strategies/controllers, namely state-feedback functions that, in conjunction with neural certificates, formally attain the given specifications for the models of interest.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2512.18406</link>
<guid>https://arxiv.org/abs/2512.18406</guid>
<content:encoded><![CDATA[
arXiv:2512.18406v1 Announce Type: cross 
Abstract: Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title>
<link>https://arxiv.org/abs/2512.18462</link>
<guid>https://arxiv.org/abs/2512.18462</guid>
<content:encoded><![CDATA[
arXiv:2512.18462v1 Announce Type: cross 
Abstract: Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on a hybrid LSTM-CNN-Attention model for text-based web content classification</title>
<link>https://arxiv.org/abs/2512.18475</link>
<guid>https://arxiv.org/abs/2512.18475</guid>
<content:encoded><![CDATA[
arXiv:2512.18475v1 Announce Type: cross 
Abstract: This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</title>
<link>https://arxiv.org/abs/2512.18500</link>
<guid>https://arxiv.org/abs/2512.18500</guid>
<content:encoded><![CDATA[
arXiv:2512.18500v1 Announce Type: cross 
Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NASTaR: NovaSAR Automated Ship Target Recognition Dataset</title>
<link>https://arxiv.org/abs/2512.18503</link>
<guid>https://arxiv.org/abs/2512.18503</guid>
<content:encoded><![CDATA[
arXiv:2512.18503v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence</title>
<link>https://arxiv.org/abs/2512.18531</link>
<guid>https://arxiv.org/abs/2512.18531</guid>
<content:encoded><![CDATA[
arXiv:2512.18531v1 Announce Type: cross 
Abstract: One-dimensional NMR spectroscopy is one of the most widely used techniques for the characterization of organic compounds and natural products. For molecules with up to 36 non-hydrogen atoms, the number of possible structures has been estimated to range from $10^{20} - 10^{60}$. The task of determining the structure (formula and connectivity) of a molecule of this size using only its one-dimensional $^1$H and/or $^{13}$C NMR spectrum, i.e. de novo structure generation, thus appears completely intractable. Here we show how it is possible to achieve this task for systems with up to 40 non-hydrogen atoms across the full elemental coverage typically encountered in organic chemistry (C, N, O, H, P, S, Si, B, and the halogens) using a deep learning framework, thus covering a vast portion of the drug-like chemical space. Leveraging insights from natural language processing, we show that our transformer-based architecture predicts the correct molecule with 55.2% accuracy within the first 15 predictions using only the $^1$H and $^{13}$C NMR spectra, thus overcoming the combinatorial growth of the chemical space while also being extensible to experimental data via fine-tuning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset</title>
<link>https://arxiv.org/abs/2512.18533</link>
<guid>https://arxiv.org/abs/2512.18533</guid>
<content:encoded><![CDATA[
arXiv:2512.18533v1 Announce Type: cross 
Abstract: The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard "Performance Ceiling", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive "Generalization Gap" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</title>
<link>https://arxiv.org/abs/2512.18540</link>
<guid>https://arxiv.org/abs/2512.18540</guid>
<content:encoded><![CDATA[
arXiv:2512.18540v1 Announce Type: cross 
Abstract: We study distributed control of networked systems through reinforcement learning, where neural policies must be simultaneously scalable, expressive and stabilizing. We introduce a policy parameterization that embeds Graph Neural Networks (GNNs) into a Youla-like magnitude-direction parameterization, yielding distributed stochastic controllers that guarantee network-level closed-loop stability by design. The magnitude is implemented as a stable operator consisting of a GNN acting on disturbance feedback, while the direction is a GNN acting on local observations. We prove robustness of the closed loop to perturbations in both the graph topology and model parameters, and show how to integrate our parameterization with Proximal Policy Optimization. Experiments on a multi-agent navigation task show that policies trained on small networks transfer directly to larger ones and unseen network topologies, achieve higher returns and lower variance than a state-of-the-art MARL baseline while preserving stability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title>
<link>https://arxiv.org/abs/2512.18542</link>
<guid>https://arxiv.org/abs/2512.18542</guid>
<content:encoded><![CDATA[
arXiv:2512.18542v1 Announce Type: cross 
Abstract: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).
  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.
  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Training Superintelligent Software Agents through Self-Play SWE-RL</title>
<link>https://arxiv.org/abs/2512.18552</link>
<guid>https://arxiv.org/abs/2512.18552</guid>
<content:encoded><![CDATA[
arXiv:2512.18552v1 Announce Type: cross 
Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</title>
<link>https://arxiv.org/abs/2512.18573</link>
<guid>https://arxiv.org/abs/2512.18573</guid>
<content:encoded><![CDATA[
arXiv:2512.18573v1 Announce Type: cross 
Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2512.18660</link>
<guid>https://arxiv.org/abs/2512.18660</guid>
<content:encoded><![CDATA[
arXiv:2512.18660v1 Announce Type: cross 
Abstract: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</title>
<link>https://arxiv.org/abs/2512.18699</link>
<guid>https://arxiv.org/abs/2512.18699</guid>
<content:encoded><![CDATA[
arXiv:2512.18699v1 Announce Type: cross 
Abstract: Recent advances in text-to-speech (TTS) have yielded remarkable improvements in naturalness and intelligibility. Building on these achievements, research has increasingly shifted toward enhancing the expressiveness of generated speech, such as dialectal and emotional TTS. However, cross-style synthesis combining both dialect and emotion remains challenging and largely unexplored, mainly due to the scarcity of dialectal data with emotional labels. To address this, we propose Hierarchical Expressive Vector (HE-Vector), a two-stage method for Emotional Dialectal TTS. In the first stage, we construct different task vectors to model dialectal and emotional styles independently, and then enhance single-style synthesis by adjusting their weights, a method we refer to as Expressive Vector (E-Vector). For the second stage, we hierarchically integrate these vectors to achieve controllable emotionally expressive dialect synthesis without requiring jointly labeled data, corresponding to Hierarchical Expressive Vector (HE-Vector). Experimental results demonstrate that HE-Vectors achieve superior performance in dialect synthesis, and promising results in synthesizing emotionally expressive dialectal speech in a zero-shot setting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning</title>
<link>https://arxiv.org/abs/2512.18720</link>
<guid>https://arxiv.org/abs/2512.18720</guid>
<content:encoded><![CDATA[
arXiv:2512.18720v1 Announce Type: cross 
Abstract: Effective feature selection is essential for high-dimensional data analysis and machine learning. Unsupervised feature selection (UFS) aims to simultaneously cluster data and identify the most discriminative features. Most existing UFS methods linearly project features into a pseudo-label space for clustering, but they suffer from two critical limitations: (1) an oversimplified linear mapping that fails to capture complex feature relationships, and (2) an assumption of uniform cluster distributions, ignoring outliers prevalent in real-world data. To address these issues, we propose the Robust Autoencoder-based Unsupervised Feature Selection (RAEUFS) model, which leverages a deep autoencoder to learn nonlinear feature representations while inherently improving robustness to outliers. We further develop an efficient optimization algorithm for RAEUFS. Extensive experiments demonstrate that our method outperforms state-of-the-art UFS approaches in both clean and outlier-contaminated data settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth</title>
<link>https://arxiv.org/abs/2512.18732</link>
<guid>https://arxiv.org/abs/2512.18732</guid>
<content:encoded><![CDATA[
arXiv:2512.18732v1 Announce Type: cross 
Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</title>
<link>https://arxiv.org/abs/2512.18745</link>
<guid>https://arxiv.org/abs/2512.18745</guid>
<content:encoded><![CDATA[
arXiv:2512.18745v1 Announce Type: cross 
Abstract: The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</title>
<link>https://arxiv.org/abs/2512.18784</link>
<guid>https://arxiv.org/abs/2512.18784</guid>
<content:encoded><![CDATA[
arXiv:2512.18784v1 Announce Type: cross 
Abstract: We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization</title>
<link>https://arxiv.org/abs/2512.18788</link>
<guid>https://arxiv.org/abs/2512.18788</guid>
<content:encoded><![CDATA[
arXiv:2512.18788v1 Announce Type: cross 
Abstract: This chapter overviews the concept of Smart Wireless Environments (SWEs) motivated by the emerging technology of Reconfigurable Intelligent Surfaces (RISs). The operating principles and state-of-the-art hardware architectures of programmable metasurfaces are first introduced. Subsequently, key performance objectives and use cases of RIS-enabled SWEs, including spectral and energy efficiency, physical-layer security, integrated sensing and communications, as well as the emerging paradigm of over-the-air computing, are discussed. Focusing on the recent trend of Beyond-Diagonal (BD) RISs, two distributed designs of respective SWEs are presented. The first deals with a multi-user Multiple-Input Single-Output (MISO) system operating within the area of influence of a SWE comprising multiple BD-RISs. A hybrid distributed and fusion machine learning framework based on multi-branch attention-based convolutional Neural Networks (NNs), NN parameter sharing, and neuroevolutionary training is presented, which enables online mapping of channel realizations to the BD-RIS configurations as well as the multi-user transmit precoder. Performance evaluation results showcase that the distributedly optimized RIS-enabled SWE achieves near-optimal sum-rate performance with low online computational complexity. The second design focuses on the wideband interference MISO broadcast channel, where each base station exclusively controls one BD-RIS to serve its assigned group of users. A cooperative optimization framework that jointly designs the base station transmit precoders as well as the tunable capacitances and switch matrices of all metasurfaces is presented. Numerical results demonstrating the superior sum-rate performance of the designed RIS-enabled SWE for multi-cell MISO networks over benchmark schemes, considering non-cooperative configuration and conventional diagonal metasurfaces, are presented.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2512.18857</link>
<guid>https://arxiv.org/abs/2512.18857</guid>
<content:encoded><![CDATA[
arXiv:2512.18857v1 Announce Type: cross 
Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of deep learning approaches for medieval historical documents transcription</title>
<link>https://arxiv.org/abs/2512.18865</link>
<guid>https://arxiv.org/abs/2512.18865</guid>
<content:encoded><![CDATA[
arXiv:2512.18865v1 Announce Type: cross 
Abstract: Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</title>
<link>https://arxiv.org/abs/2512.18892</link>
<guid>https://arxiv.org/abs/2512.18892</guid>
<content:encoded><![CDATA[
arXiv:2512.18892v1 Announce Type: cross 
Abstract: We present a new approach to formulating and solving heterogeneous agent models with aggregate risk. We replace the cross-sectional distribution with low-dimensional prices as state variables and let agents learn equilibrium price dynamics directly from simulated paths. To do so, we introduce a structural reinforcement learning (SRL) method which treats prices via simulation while exploiting agents' structural knowledge of their own individual dynamics. Our SRL method yields a general and highly efficient global solution method for heterogeneous agent models that sidesteps the Master equation and handles problems traditional methods struggle with, in particular nontrivial market-clearing conditions. We illustrate the approach in the Krusell-Smith model, the Huggett model with aggregate shocks, and a HANK model with a forward-looking Phillips curve, all of which we solve globally within minutes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18901</link>
<guid>https://arxiv.org/abs/2512.18901</guid>
<content:encoded><![CDATA[
arXiv:2512.18901v1 Announce Type: cross 
Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</title>
<link>https://arxiv.org/abs/2512.18956</link>
<guid>https://arxiv.org/abs/2512.18956</guid>
<content:encoded><![CDATA[
arXiv:2512.18956v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction</title>
<link>https://arxiv.org/abs/2512.18971</link>
<guid>https://arxiv.org/abs/2512.18971</guid>
<content:encoded><![CDATA[
arXiv:2512.18971v1 Announce Type: cross 
Abstract: Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $\sigma$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</title>
<link>https://arxiv.org/abs/2512.19001</link>
<guid>https://arxiv.org/abs/2512.19001</guid>
<content:encoded><![CDATA[
arXiv:2512.19001v1 Announce Type: cross 
Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.19004</link>
<guid>https://arxiv.org/abs/2512.19004</guid>
<content:encoded><![CDATA[
arXiv:2512.19004v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.
  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title>
<link>https://arxiv.org/abs/2512.19011</link>
<guid>https://arxiv.org/abs/2512.19011</guid>
<content:encoded><![CDATA[
arXiv:2512.19011v1 Announce Type: cross 
Abstract: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.
  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.
  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</title>
<link>https://arxiv.org/abs/2512.19020</link>
<guid>https://arxiv.org/abs/2512.19020</guid>
<content:encoded><![CDATA[
arXiv:2512.19020v1 Announce Type: cross 
Abstract: Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title>
<link>https://arxiv.org/abs/2512.19025</link>
<guid>https://arxiv.org/abs/2512.19025</guid>
<content:encoded><![CDATA[
arXiv:2512.19025v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \name, an automated stress-testing framework that generates a surrogate dataset, $\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$\beta$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recontextualization Mitigates Specification Gaming without Modifying the Specification</title>
<link>https://arxiv.org/abs/2512.19027</link>
<guid>https://arxiv.org/abs/2512.19027</guid>
<content:encoded><![CDATA[
arXiv:2512.19027v1 Announce Type: cross 
Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</title>
<link>https://arxiv.org/abs/2512.19037</link>
<guid>https://arxiv.org/abs/2512.19037</guid>
<content:encoded><![CDATA[
arXiv:2512.19037v1 Announce Type: cross 
Abstract: The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation</title>
<link>https://arxiv.org/abs/2512.19067</link>
<guid>https://arxiv.org/abs/2512.19067</guid>
<content:encoded><![CDATA[
arXiv:2512.19067v1 Announce Type: cross 
Abstract: We study a variant of cost-aware sequential hypothesis testing in which a single active Decision Maker (DM) selects actions with positive, random costs to identify the true hypothesis under an average error constraint, while minimizing the expected total cost. The DM may abort an in-progress action, yielding no sample, by truncating its realized cost at a smaller, tunable deterministic limit, which we term a per-action deadline. We analyze how this cancellation option can be exploited under two cost-revelation models: ex-post, where the cost is revealed only after the sample is obtained, and ex-ante, where the cost accrues before sample acquisition.
  In the ex-post model, per-action deadlines do not affect the expected total cost, and the cost-error tradeoffs coincide with the baseline obtained by replacing deterministic costs with cost means. In the ex-ante model, we show how per-action deadlines inflate the expected number of times actions are applied, and that the resulting expected total cost can be reduced to the constant-cost setting by introducing an effective per-action cost. We characterize when deadlines are beneficial and study several families in detail.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</title>
<link>https://arxiv.org/abs/2512.19091</link>
<guid>https://arxiv.org/abs/2512.19091</guid>
<content:encoded><![CDATA[
arXiv:2512.19091v1 Announce Type: cross 
Abstract: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</title>
<link>https://arxiv.org/abs/2512.19104</link>
<guid>https://arxiv.org/abs/2512.19104</guid>
<content:encoded><![CDATA[
arXiv:2512.19104v1 Announce Type: cross 
Abstract: Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAP: Syntactic Attention Pruning for Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2512.19125</link>
<guid>https://arxiv.org/abs/2512.19125</guid>
<content:encoded><![CDATA[
arXiv:2512.19125v1 Announce Type: cross 
Abstract: This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</title>
<link>https://arxiv.org/abs/2512.19131</link>
<guid>https://arxiv.org/abs/2512.19131</guid>
<content:encoded><![CDATA[
arXiv:2512.19131v1 Announce Type: cross 
Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite-sample guarantees for data-driven forward-backward operator methods</title>
<link>https://arxiv.org/abs/2512.19172</link>
<guid>https://arxiv.org/abs/2512.19172</guid>
<content:encoded><![CDATA[
arXiv:2512.19172v1 Announce Type: cross 
Abstract: We establish finite sample certificates on the quality of solutions produced by data-based forward-backward (FB) operator splitting schemes. As frequently happens in stochastic regimes, we consider the problem of finding a zero of the sum of two operators, where one is either unavailable in closed form or computationally expensive to evaluate, and shall therefore be approximated using a finite number of noisy oracle samples. Under the lens of algorithmic stability, we then derive probabilistic bounds on the distance between a true zero and the FB output without making specific assumptions about the underlying data distribution. We show that under weaker conditions ensuring the convergence of FB schemes, stability bounds grow proportionally to the number of iterations. Conversely, stronger assumptions yield stability guarantees that are independent of the iteration count. We then specialize our results to a popular FB stochastic Nash equilibrium seeking algorithm and validate our theoretical bounds on a control problem for smart grids, where the energy price uncertainty is approximated by means of historical data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</title>
<link>https://arxiv.org/abs/2512.19190</link>
<guid>https://arxiv.org/abs/2512.19190</guid>
<content:encoded><![CDATA[
arXiv:2512.19190v1 Announce Type: cross 
Abstract: Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</title>
<link>https://arxiv.org/abs/2512.19196</link>
<guid>https://arxiv.org/abs/2512.19196</guid>
<content:encoded><![CDATA[
arXiv:2512.19196v1 Announce Type: cross 
Abstract: Solving high-dimensional Fokker-Planck (FP) equations is a challenge in computational physics and stochastic dynamics, due to the curse of dimensionality (CoD) and the bottleneck of evaluating second-order diffusion terms. Existing deep learning approaches, such as Physics-Informed Neural Networks (PINNs), face computational challenges as dimensionality increases, driven by the $O(D^2)$ complexity of automatic differentiation for second-order derivatives. While recent probability flow approaches bypass this by learning score functions or matching velocity fields, they often involve serial computational operations or depend on sampling efficiency in complex distributions. To address these issues, we propose the Self-Consistent Probability Flow (SCPF) method. We reformulate the second-order FP equation into an equivalent first-order deterministic Probability Flow ODE (PF-ODE) constraint. Unlike score matching or velocity matching, SCPF solves this problem by minimizing the residual of the PF-ODE continuity equation, which avoids explicit Hessian computation. We leverage Continuous Normalizing Flows (CNF) combined with the Hutchinson Trace Estimator (HTE) to reduce the training complexity to linear scale $O(D)$, achieving an effective $O(1)$ wall-clock time on GPUs. To address data sparsity in high dimensions, we apply a generative adaptive sampling strategy and theoretically prove that dynamically aligning collocation points with the evolving probability mass is a necessary condition to bound the approximation error. Experiments on diverse benchmarks -- ranging from anisotropic Ornstein-Uhlenbeck (OU) processes and high-dimensional Brownian motions with time-varying diffusion terms, to Geometric OU processes featuring non-Gaussian solutions -- demonstrate that SCPF effectively mitigates the CoD, maintaining high accuracy and constant computational cost for problems up to 100 dimensions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title>
<link>https://arxiv.org/abs/2512.19238</link>
<guid>https://arxiv.org/abs/2512.19238</guid>
<content:encoded><![CDATA[
arXiv:2512.19238v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Flow to Policy via Hindsight Online Imitation</title>
<link>https://arxiv.org/abs/2512.19269</link>
<guid>https://arxiv.org/abs/2512.19269</guid>
<content:encoded><![CDATA[
arXiv:2512.19269v1 Announce Type: cross 
Abstract: Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GShield: Mitigating Poisoning Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2512.19286</link>
<guid>https://arxiv.org/abs/2512.19286</guid>
<content:encoded><![CDATA[
arXiv:2512.19286v1 Announce Type: cross 
Abstract: Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\% to 65\% after detecting malicious and low-quality clients.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models</title>
<link>https://arxiv.org/abs/2512.19334</link>
<guid>https://arxiv.org/abs/2512.19334</guid>
<content:encoded><![CDATA[
arXiv:2512.19334v1 Announce Type: cross 
Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</title>
<link>https://arxiv.org/abs/2512.19342</link>
<guid>https://arxiv.org/abs/2512.19342</guid>
<content:encoded><![CDATA[
arXiv:2512.19342v1 Announce Type: cross 
Abstract: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</title>
<link>https://arxiv.org/abs/2512.19349</link>
<guid>https://arxiv.org/abs/2512.19349</guid>
<content:encoded><![CDATA[
arXiv:2512.19349v1 Announce Type: cross 
Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning General Policies with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2512.19366</link>
<guid>https://arxiv.org/abs/2512.19366</guid>
<content:encoded><![CDATA[
arXiv:2512.19366v1 Announce Type: cross 
Abstract: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-Based Generalized Additive Models Informed by Random Fourier Features</title>
<link>https://arxiv.org/abs/2512.19373</link>
<guid>https://arxiv.org/abs/2512.19373</guid>
<content:encoded><![CDATA[
arXiv:2512.19373v1 Announce Type: cross 
Abstract: Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows</title>
<link>https://arxiv.org/abs/2512.19376</link>
<guid>https://arxiv.org/abs/2512.19376</guid>
<content:encoded><![CDATA[
arXiv:2512.19376v1 Announce Type: cross 
Abstract: Understanding intraventricular hemodynamics requires compact and physically interpretable representations of the underlying flow structures, as characteristic flow patterns are closely associated with cardiovascular conditions and can support early detection of cardiac deterioration. Conventional visualization of velocity or pressure fields, however, provides limited insight into the coherent mechanisms driving these dynamics. Reduced-order modeling techniques, like Proper Orthogonal Decomposition (POD) and Autoencoder (AE) architectures, offer powerful alternatives to extract dominant flow features from complex datasets. This study systematically compares POD with several AE variants (Linear, Nonlinear, Convolutional, and Variational) using left ventricular flow fields obtained from computational fluid dynamics simulations. We show that, for a suitably chosen latent dimension, AEs produce modes that become nearly orthogonal and qualitatively resemble POD modes that capture a given percentage of kinetic energy. As the number of latent modes increases, AE modes progressively lose orthogonality, leading to linear dependence, spatial redundancy, and the appearance of repeated modes with substantial high-frequency content. This degradation reduces interpretability and introduces noise-like components into AE-based reduced-order models, potentially complicating their integration with physics-based formulations or neural-network surrogates. The extent of interpretability loss varies across the AEs, with nonlinear, convolutional, and variational models exhibiting distinct behaviors in orthogonality preservation and feature localization. Overall, the results indicate that AEs can reproduce POD-like coherent structures under specific latent-space configurations, while highlighting the need for careful mode selection to ensure physically meaningful representations of cardiac flow dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Streamable Generative Speech Restoration with Flow Matching</title>
<link>https://arxiv.org/abs/2512.19442</link>
<guid>https://arxiv.org/abs/2512.19442</guid>
<content:encoded><![CDATA[
arXiv:2512.19442v1 Announce Type: cross 
Abstract: Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.
  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.
  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</title>
<link>https://arxiv.org/abs/2512.19469</link>
<guid>https://arxiv.org/abs/2512.19469</guid>
<content:encoded><![CDATA[
arXiv:2512.19469v1 Announce Type: cross 
Abstract: Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</title>
<link>https://arxiv.org/abs/2512.19540</link>
<guid>https://arxiv.org/abs/2512.19540</guid>
<content:encoded><![CDATA[
arXiv:2512.19540v1 Announce Type: cross 
Abstract: Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</title>
<link>https://arxiv.org/abs/2512.19576</link>
<guid>https://arxiv.org/abs/2512.19576</guid>
<content:encoded><![CDATA[
arXiv:2512.19576v1 Announce Type: cross 
Abstract: Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\"at W\"urzburg in cooperation with the Technische Universit\"at Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</title>
<link>https://arxiv.org/abs/2512.19687</link>
<guid>https://arxiv.org/abs/2512.19687</guid>
<content:encoded><![CDATA[
arXiv:2512.19687v1 Announce Type: cross 
Abstract: We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2301.11321</link>
<guid>https://arxiv.org/abs/2301.11321</guid>
<content:encoded><![CDATA[
arXiv:2301.11321v3 Announce Type: replace 
Abstract: Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditions for our operator in the tabular setting, establishing the first guarantees for several existing methods as well as many new ones. Finally, we introduce Recency-Bounded Importance Sampling (RBIS), which leverages trajectory awareness to perform robustly across $\lambda$-values in an off-policy control task.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Exploitation and Exploration of Contextual Bandits</title>
<link>https://arxiv.org/abs/2305.03784</link>
<guid>https://arxiv.org/abs/2305.03784</guid>
<content:encoded><![CDATA[
arXiv:2305.03784v2 Announce Type: replace 
Abstract: In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration. We provide an instance-based $\widetilde{\mathcal{O}}(\sqrt{T})$ regret upper bound for EE-Net and show that EE-Net outperforms related linear and neural contextual bandit baselines on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anti-Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances in Flat Directions</title>
<link>https://arxiv.org/abs/2306.05300</link>
<guid>https://arxiv.org/abs/2306.05300</guid>
<content:encoded><![CDATA[
arXiv:2306.05300v3 Announce Type: replace 
Abstract: Stochastic Gradient Descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the gradient noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: First, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional predictions of isotropic weight variance under stationarity, based on uncorrelated and curvature-proportional noise, are recovered. Anti-correlations have negligible effect here. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we present a numerical experiment where training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training may play a role in finding flatter minima that generalize better.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting</title>
<link>https://arxiv.org/abs/2307.14596</link>
<guid>https://arxiv.org/abs/2307.14596</guid>
<content:encoded><![CDATA[
arXiv:2307.14596v2 Announce Type: replace 
Abstract: Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks~(STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting (e.g., 1-h ahead), while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting (e.g., 1-day ahead). To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-Net TransFormer~(HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we {\color{black}propose} window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified Defense on the Fairness of Graph Neural Networks</title>
<link>https://arxiv.org/abs/2311.02757</link>
<guid>https://arxiv.org/abs/2311.02757</guid>
<content:encoded><![CDATA[
arXiv:2311.02757v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically shown that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes {\em any} GNN as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not make any assumptions over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs and parameter settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Averaging $n$-step Returns Reduces Variance in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2402.03903</link>
<guid>https://arxiv.org/abs/2402.03903</guid>
<content:encoded><![CDATA[
arXiv:2402.03903v4 Announce Type: replace 
Abstract: Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that compound returns often increase the sample efficiency of $n$-step deep RL agents like DQN and PPO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IT Intrusion Detection Using Statistical Learning and Testbed Measurements</title>
<link>https://arxiv.org/abs/2402.13081</link>
<guid>https://arxiv.org/abs/2402.13081</guid>
<content:encoded><![CDATA[
arXiv:2402.13081v2 Announce Type: replace 
Abstract: We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum Attention for Neural Operators</title>
<link>https://arxiv.org/abs/2406.06486</link>
<guid>https://arxiv.org/abs/2406.06486</guid>
<content:encoded><![CDATA[
arXiv:2406.06486v4 Announce Type: replace 
Abstract: Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces. In this paper, we state and prove the first universal approximation result for transformer neural operators, using only a slight modification of the architecture implemented in practice. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Transformers: A Survey</title>
<link>https://arxiv.org/abs/2407.09777</link>
<guid>https://arxiv.org/abs/2407.09777</guid>
<content:encoded><![CDATA[
arXiv:2407.09777v2 Announce Type: replace 
Abstract: Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning</title>
<link>https://arxiv.org/abs/2408.10566</link>
<guid>https://arxiv.org/abs/2408.10566</guid>
<content:encoded><![CDATA[
arXiv:2408.10566v5 Announce Type: replace 
Abstract: In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Reliable are Causal Probing Interventions?</title>
<link>https://arxiv.org/abs/2408.15510</link>
<guid>https://arxiv.org/abs/2408.15510</guid>
<content:encoded><![CDATA[
arXiv:2408.15510v5 Announce Type: replace 
Abstract: Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.
  Our project webpage is available at: https://ahdavies6.github.io/causal_probing_reliability/
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler</title>
<link>https://arxiv.org/abs/2409.11068</link>
<guid>https://arxiv.org/abs/2409.11068</guid>
<content:encoded><![CDATA[
arXiv:2409.11068v2 Announce Type: replace 
Abstract: Code optimization is a crucial task that aims to enhance code performance. However, this process is often tedious and complex, highlighting the necessity for automatic code optimization techniques. Reinforcement Learning (RL) has emerged as a promising approach for tackling such complex optimization problems. In this project, we introduce MLIR RL, an RL environment for the MLIR compiler, dedicated to facilitating MLIR compiler research and enabling automatic code optimization. We propose a multi-discrete formulation of the action space where the action space is the Cartesian product of simpler action subspaces. We also propose a new method, called level pointers, to reduce the size of the action space related to the loop interchange transformation. This enables more efficient and effective learning of the policy. To demonstrate the effectiveness of MLIR RL, we train an RL agent to optimize MLIR Linalg code, targeting CPU. The code is generated from two domain-specific frameworks: deep-learning models generated from PyTorch, and LQCD (Lattice Quantum Chromodynamics) code generated from an LQCD compiler. The result of this work is a research environment that allows the community to experiment with novel ideas in RL-driven loop-nest optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Polynomial Functional Networks</title>
<link>https://arxiv.org/abs/2410.04213</link>
<guid>https://arxiv.org/abs/2410.04213</guid>
<content:encoded><![CDATA[
arXiv:2410.04213v2 Announce Type: replace 
Abstract: Neural Functional Networks (NFNs) have gained increasing interest due to their wide range of applications, including extracting information from implicit representations of data, editing network weights, and evaluating policies. A key design principle of NFNs is their adherence to the permutation and scaling symmetries inherent in the connectionist structure of the input neural networks. Recent NFNs have been proposed with permutation and scaling equivariance based on either graph-based message-passing mechanisms or parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from high memory consumption and long running times. On the other hand, parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower memory consumption and faster running time, yet their expressivity is limited due to the large size of the symmetric group of the input neural networks. The challenge of designing a permutation and scaling equivariant NFN that maintains low memory consumption and running time while preserving expressivity remains unresolved. In this paper, we propose a novel solution with the development of MAGEP-NFN (Monomial mAtrix Group Equivariant Polynomial NFN). Our approach follows the parameter-sharing mechanism but differs from previous works by constructing a nonlinear equivariant layer represented as a polynomial in the input weights. This polynomial formulation enables us to incorporate additional relationships between weights from different input hidden layers, enhancing the model's expressivity while keeping memory consumption and running time low, thereby addressing the aforementioned challenge. We provide empirical evidence demonstrating that MAGEP-NFN achieves competitive performance and efficiency compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision</title>
<link>https://arxiv.org/abs/2411.01431</link>
<guid>https://arxiv.org/abs/2411.01431</guid>
<content:encoded><![CDATA[
arXiv:2411.01431v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs upon real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate the above computational gap and enable ubiquitous embedded intelligence, we, in this survey, focus on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Distribution Learning for Graph Classification</title>
<link>https://arxiv.org/abs/2411.15206</link>
<guid>https://arxiv.org/abs/2411.15206</guid>
<content:encoded><![CDATA[
arXiv:2411.15206v4 Announce Type: replace 
Abstract: Leveraging the diversity and quantity of data provided by various graph-structured data augmentations while preserving intrinsic semantic information is challenging. Additionally, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while graph contrastive learning aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of negative pairs via intraviews. In this paper, we propose a conditional distribution learning (CDL) method that learns graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment enables the CDL model to effectively preserve intrinsic semantic information when both weak and strong augmentations are applied to graph-structured data. To avoid the conflict between the MPM and the CL of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed CDL method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PowerMamba: A Deep State Space Model and Comprehensive Benchmark for Time Series Prediction in Electric Power Systems</title>
<link>https://arxiv.org/abs/2412.06112</link>
<guid>https://arxiv.org/abs/2412.06112</guid>
<content:encoded><![CDATA[
arXiv:2412.06112v2 Announce Type: replace 
Abstract: The electricity sector is undergoing substantial transformations due to the rising electrification of demand, enhanced integration of renewable energy resources, and the emergence of new technologies. These changes are rendering the electric grid more volatile and unpredictable, making it difficult to maintain reliable operations. In order to address these issues, advanced time series prediction models are needed for closing the gap between the forecasted and actual grid outcomes. In this paper, we introduce a multivariate time series prediction model that combines traditional state space models with deep learning methods to simultaneously capture and predict the underlying dynamics of multiple time series. Additionally, we design a time series processing module that incorporates high-resolution external forecasts into sequence-to-sequence prediction models, achieving this with negligible increases in size and no loss of accuracy. We also release an extended dataset spanning five years of load, electricity price, ancillary service price, and renewable generation. To complement this dataset, we provide an open-access toolbox that includes our proposed model, the dataset itself, and several state-of-the-art prediction models, thereby creating a unified framework for benchmarking advanced machine learning approaches. Our findings indicate that the proposed model outperforms existing models across various prediction tasks, improving state-of-the-art prediction error by an average of 7% and decreasing model parameters by 43%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2501.12046</link>
<guid>https://arxiv.org/abs/2501.12046</guid>
<content:encoded><![CDATA[
arXiv:2501.12046v3 Announce Type: replace 
Abstract: Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We theoretically analyze the privacy guarantee of CEPAM and investigate the trade-offs among user privacy and accuracy of CEPAM through experimental evaluations. Moreover, we assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-Aware DNN Compression for Homogeneous Edge Devices</title>
<link>https://arxiv.org/abs/2501.15240</link>
<guid>https://arxiv.org/abs/2501.15240</guid>
<content:encoded><![CDATA[
arXiv:2501.15240v2 Announce Type: replace 
Abstract: Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Experiments on ResNet50 and MobileNetV1 with the ImageNet dataset show that HDAP consistently achieves lower average inference latency compared with state-of-the-art methods, with substantial speedup gains (e.g., 2.86 $\times$ speedup at 1.0G FLOPs for ResNet50) on the homogeneous device clusters. HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Slice Discovery via Manifold Compactness</title>
<link>https://arxiv.org/abs/2501.19032</link>
<guid>https://arxiv.org/abs/2501.19032</guid>
<content:encoded><![CDATA[
arXiv:2501.19032v2 Announce Type: replace 
Abstract: Despite the great performance of deep learning models in many areas, they still make mistakes and underperform on certain subsets of data, i.e. error slices. Given a trained model, it is important to identify its semantically coherent error slices that are easy to interpret, which is referred to as the error slice discovery problem. However, there is no proper metric of slice coherence without relying on extra information like predefined slice labels. Current evaluation of slice coherence requires access to predefined slices formulated by metadata like attributes or subclasses. Its validity heavily relies on the quality and abundance of metadata, where some possible patterns could be ignored. Besides, current algorithms cannot directly incorporate the constraint of coherence into their optimization objective due to absence of an explicit coherence metric, which could potentially hinder their effectiveness. In this paper, we propose manifold compactness, a coherence metric without reliance on extra information by incorporating the data geometry property into its design, and experiments on typical datasets empirically validate the rationality of the metric. Then we develop Manifold Compactness based error Slice Discovery (MCSD), a novel algorithm that directly treats risk and coherence as the optimization objective, and is flexible to be applied to models of various tasks. Extensive experiments on the benchmark and case studies on other typical datasets demonstrate the superiority of MCSD.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning and Learning in Average Risk-aware MDPs</title>
<link>https://arxiv.org/abs/2503.17629</link>
<guid>https://arxiv.org/abs/2503.17629</guid>
<content:encoded><![CDATA[
arXiv:2503.17629v3 Announce Type: replace 
Abstract: For continuing tasks, average cost Markov decision processes have well-documented value and can be solved using efficient algorithms. However, it explicitly assumes that the agent is risk-neutral. In this work, we extend risk-neutral algorithms to accommodate the more general class of dynamic risk measures. Specifically, we propose a relative value iteration (RVI) algorithm for planning and design two model-free Q-learning algorithms, namely a generic algorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy algorithm dedicated to utility-based shortfall risk measures. Both the RVI and MLMC-based Q-learning algorithms are proven to converge to optimality. Numerical experiments validate our analysis, confirm empirically the convergence of the off-policy algorithm, and demonstrate that our approach enables the identification of policies that are finely tuned to the intricate risk-awareness of the agent that they serve.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicXGNN: Grounded Logical Rules for Explaining Graph Neural Networks</title>
<link>https://arxiv.org/abs/2503.19476</link>
<guid>https://arxiv.org/abs/2503.19476</guid>
<content:encoded><![CDATA[
arXiv:2503.19476v3 Announce Type: replace 
Abstract: Existing rule-based explanations for Graph Neural Networks (GNNs) provide global interpretability but often optimize and assess fidelity in an intermediate, uninterpretable concept space, overlooking grounding quality for end users in the final subgraph explanations. This gap yields explanations that may appear faithful yet be unreliable in practice. To this end, we propose LogicXGNN, a post-hoc framework that constructs logical rules over reliable predicates explicitly designed to capture the GNN's message-passing structure, thereby ensuring effective grounding. We further introduce data-grounded fidelity ($\textit{Fid}_{\mathcal{D}}$), a realistic metric that evaluates explanations in their final-graph form, along with complementary utility metrics such as coverage and validity. Across extensive experiments, LogicXGNN improves $\textit{Fid}_{\mathcal{D}}$ by over 20% on average relative to state-of-the-art methods while being 10-100 $\times$ faster. With strong scalability and utility performance, LogicXGNN produces explanations that are faithful to the model's logic and reliably grounded in observable data. Our code is available at https://github.com/allengeng123/LogicXGNN/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Online Mirror Descent for Robust Learning in Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2504.02618</link>
<guid>https://arxiv.org/abs/2504.02618</guid>
<content:encoded><![CDATA[
arXiv:2504.02618v4 Announce Type: replace 
Abstract: The Schr\"{o}dinger bridge (SB) has evolved into a universal class of probabilistic generative models. In practice, however, estimated learning signals are innately uncertain, and the reliability promised by existing methods is often based on speculative optimal case scenarios. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD (OMD) framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound for the novel OMD formulation of SB acquisition. As a result, we propose a simulation-free SB algorithm called Variational Mirrored Schr\"{o}dinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schr\"{o}dinger potentials. Based on the Wasserstein gradient flow theory, the algorithm offers tractable learning dynamics that precisely approximate each OMD step. In experiments, we validate the performance of the proposed VMSB algorithm across an extensive suite of benchmarks. VMSB consistently outperforms contemporary SB solvers on a wide range of SB problems, demonstrating the robustness as well as generality predicted by our OMD theory.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation</title>
<link>https://arxiv.org/abs/2504.04798</link>
<guid>https://arxiv.org/abs/2504.04798</guid>
<content:encoded><![CDATA[
arXiv:2504.04798v5 Announce Type: replace 
Abstract: Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient. Code is available at https://github.com/jacobyhsi/TabRep.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.01912</link>
<guid>https://arxiv.org/abs/2505.01912</guid>
<content:encoded><![CDATA[
arXiv:2505.01912v2 Announce Type: replace 
Abstract: Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\mathbf{BOOM}$, $\mathbf{b}$enchmarks for $\mathbf{o}$ut-$\mathbf{o}$f-distribution $\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at https://github.com/FLASK-LLNL/BOOM
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focus on Likely Classes for Test-Time Prediction</title>
<link>https://arxiv.org/abs/2505.03819</link>
<guid>https://arxiv.org/abs/2505.03819</guid>
<content:encoded><![CDATA[
arXiv:2505.03819v3 Announce Type: replace 
Abstract: We ask: Can focusing on likely classes of a single, in-domain sample improve model predictions? Prior work argued ``no''. We put forward a novel rationale in favor of ``yes'': Sharedness of features among classes indicates their reliability for a single sample. We aim for an affirmative answer without using hand-engineered augmentations or auxiliary tasks. We propose two novel test-time fine-tuning methods to improve uncertain model predictions. Instead of greedily selecting the most likely class, we introduce an additional step, \emph{focus on the likely classes}, to refine predictions. By applying a single gradient descent step with a large learning rate, we refine predictions when an initial forward pass indicates high uncertainty. The experimental evaluation demonstrates accuracy gains for one of our methods on average, which emphasizes shared features among likely classes. The gains are confirmed across diverse text and image domain models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
<link>https://arxiv.org/abs/2505.09017</link>
<guid>https://arxiv.org/abs/2505.09017</guid>
<content:encoded><![CDATA[
arXiv:2505.09017v2 Announce Type: replace 
Abstract: Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addition is almost all you need: Compressing neural networks with double binary factorization</title>
<link>https://arxiv.org/abs/2505.11076</link>
<guid>https://arxiv.org/abs/2505.11076</guid>
<content:encoded><![CDATA[
arXiv:2505.11076v3 Announce Type: replace 
Abstract: Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape it Up! Restoring LLM Safety during Finetuning</title>
<link>https://arxiv.org/abs/2505.17196</link>
<guid>https://arxiv.org/abs/2505.17196</guid>
<content:encoded><![CDATA[
arXiv:2505.17196v3 Announce Type: replace 
Abstract: Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAEs Are Good for Steering -- If You Select the Right Features</title>
<link>https://arxiv.org/abs/2505.20063</link>
<guid>https://arxiv.org/abs/2505.20063</guid>
<content:encoded><![CDATA[
arXiv:2505.20063v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v3 Announce Type: replace 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v4 Announce Type: replace 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOORL: A Framework for Integrating Offline-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09574</link>
<guid>https://arxiv.org/abs/2506.09574</guid>
<content:encoded><![CDATA[
arXiv:2506.09574v2 Announce Type: replace 
Abstract: Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design components and added computational complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without added complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining</title>
<link>https://arxiv.org/abs/2506.13274</link>
<guid>https://arxiv.org/abs/2506.13274</guid>
<content:encoded><![CDATA[
arXiv:2506.13274v3 Announce Type: replace 
Abstract: Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide theoretical and experimental analyzes to show that foundation model pretraining loss and its descent velocity are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, base learning rate scheduler choices, and hyperparameter settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Free Policy Optimization via Reward Partitioning</title>
<link>https://arxiv.org/abs/2506.13702</link>
<guid>https://arxiv.org/abs/2506.13702</guid>
<content:encoded><![CDATA[
arXiv:2506.13702v3 Announce Type: replace 
Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subspace-Boosted Model Merging</title>
<link>https://arxiv.org/abs/2506.16506</link>
<guid>https://arxiv.org/abs/2506.16506</guid>
<content:encoded><![CDATA[
arXiv:2506.16506v3 Announce Type: replace 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we empirically and theoretically analyze this limitation, proving that for Task Arithmetic-based methods, as more experts are merged, the common information dominates the task-specific information, leading to inevitable rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging. Code and models are available at https://github.com/ronskoro/Subspace-Boosting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESSA: Evolutionary Strategies for Scalable Alignment</title>
<link>https://arxiv.org/abs/2507.04453</link>
<guid>https://arxiv.org/abs/2507.04453</guid>
<content:encoded><![CDATA[
arXiv:2507.04453v3 Announce Type: replace 
Abstract: Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.12815</link>
<guid>https://arxiv.org/abs/2507.12815</guid>
<content:encoded><![CDATA[
arXiv:2507.12815v2 Announce Type: replace 
Abstract: Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent-environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network's embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that offers insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v4 Announce Type: replace 
Abstract: Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for "one prior, many embodiments": strong, inexpensive video priors together with minimal on-robot alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-Oriented Subsampling from Data Streams</title>
<link>https://arxiv.org/abs/2508.03868</link>
<guid>https://arxiv.org/abs/2508.03868</guid>
<content:encoded><![CDATA[
arXiv:2508.03868v2 Announce Type: replace 
Abstract: Data is often generated in streams, with new observations arriving over time. A key challenge for learning models from data streams is capturing relevant information while keeping computational costs manageable. We explore intelligent data subsampling for offline learning, and argue for an information-theoretic method centred on reducing uncertainty in downstream predictions of interest. Empirically, we demonstrate that this prediction-oriented approach performs better than a previously proposed information-theoretic technique on two widely studied problems. At the same time, we highlight that reliably achieving strong performance in practice requires careful model design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Regret and Low-Complexity Learning for Hierarchical Inference</title>
<link>https://arxiv.org/abs/2508.08985</link>
<guid>https://arxiv.org/abs/2508.08985</guid>
<content:encoded><![CDATA[
arXiv:2508.08985v3 Announce Type: replace 
Abstract: This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
<link>https://arxiv.org/abs/2508.09922</link>
<guid>https://arxiv.org/abs/2508.09922</guid>
<content:encoded><![CDATA[
arXiv:2508.09922v5 Announce Type: replace 
Abstract: Diffusion models achieve state-of-the-art image generation but remain computationally costly due to iterative denoising. Latent-space models like Stable Diffusion reduce overhead yet lose fine detail, while retrieval-augmented methods improve efficiency but rely on large memory banks, static similarity models, and rigid infrastructures. We introduce the Prototype Diffusion Model (PDM), which embeds prototype learning into the diffusion process to provide adaptive, memory-free conditioning. Instead of retrieving references, PDM learns compact visual prototypes from clean features via contrastive learning, then aligns noisy representations with semantically relevant patterns during denoising. Experiments demonstrate that PDM sustains high generation quality while lowering computational and storage costs, offering a scalable alternative to retrieval-based conditioning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems</title>
<link>https://arxiv.org/abs/2508.12569</link>
<guid>https://arxiv.org/abs/2508.12569</guid>
<content:encoded><![CDATA[
arXiv:2508.12569v2 Announce Type: replace 
Abstract: Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Spectral Clustering For GloVe-like Text Embeddings</title>
<link>https://arxiv.org/abs/2508.14075</link>
<guid>https://arxiv.org/abs/2508.14075</guid>
<content:encoded><![CDATA[
arXiv:2508.14075v2 Announce Type: replace 
Abstract: In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design</title>
<link>https://arxiv.org/abs/2509.07150</link>
<guid>https://arxiv.org/abs/2509.07150</guid>
<content:encoded><![CDATA[
arXiv:2509.07150v3 Announce Type: replace 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising approach to improve correctness in LLMs, however, in many scientific problems, the objective is not necessarily to produce the correct answer, but instead to produce a diverse array of candidates which satisfy a set of constraints. We study this challenge in the context of materials generation. To this end, we introduce PLaID++, an LLM post-trained for stable and property-guided crystal generation. We find that performance hinges on our crystallographic representation and reward formulation. First, we introduce a compact, symmetry-informed Wyckoff text representation which improves computational efficiency and encourages generalization from physical priors. Second, we demonstrate that temperature scaling acts as an entropy regularizer which counteracts mode collapse and encourages exploration. By encoding symmetry constraints directly into text and guiding model outputs towards desirable chemical space, PLaID++ generates structures that are thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than prior methods and conditionally generates structures with desired space group properties. Our work demonstrates the potential of adapting post-training techniques from natural language processing to materials design, paving the way for targeted and efficient discovery of novel materials.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition</title>
<link>https://arxiv.org/abs/2509.10729</link>
<guid>https://arxiv.org/abs/2509.10729</guid>
<content:encoded><![CDATA[
arXiv:2509.10729v3 Announce Type: replace 
Abstract: Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.16625</link>
<guid>https://arxiv.org/abs/2509.16625</guid>
<content:encoded><![CDATA[
arXiv:2509.16625v5 Announce Type: replace 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks</title>
<link>https://arxiv.org/abs/2509.21605</link>
<guid>https://arxiv.org/abs/2509.21605</guid>
<content:encoded><![CDATA[
arXiv:2509.21605v2 Announce Type: replace 
Abstract: Operator learning is a recently developed generalization of regression to mappings between functions. It promises to drastically reduce expensive numerical integration of PDEs to fast evaluations of mappings between functional states of a system, i.e., surrogate and reduced-order modeling. Operator learning has already found applications in several areas such as modeling sea ice, combustion, and atmospheric physics. Recent approaches towards integrating uncertainty quantification into the operator models have relied on likelihood based methods to infer parameter distributions from noisy data. However, stochastic operators may yield actions from which a likelihood is difficult or impossible to construct. In this paper, we introduce, GenUQ, a measure-theoretic approach to UQ that avoids constructing a likelihood by introducing a generative hyper-network model that produces parameter distributions consistent with observed data. We demonstrate that GenUQ outperforms other UQ methods in three example problems, recovering a manufactured operator, learning the solution operator to a stochastic elliptic PDE, and modeling the failure location of porous steel under tension.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-language fusion enables interactive neural readout and in-silico experimentation</title>
<link>https://arxiv.org/abs/2509.23941</link>
<guid>https://arxiv.org/abs/2509.23941</guid>
<content:encoded><![CDATA[
arXiv:2509.23941v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OAT-FM: Optimal Acceleration Transport for Improved Flow Matching</title>
<link>https://arxiv.org/abs/2509.24936</link>
<guid>https://arxiv.org/abs/2509.24936</guid>
<content:encoded><![CDATA[
arXiv:2509.24936v2 Announce Type: replace 
Abstract: As a powerful technique in generative modeling, Flow Matching (FM) aims to learn velocity fields from noise to data, which is often explained and implemented as solving Optimal Transport (OT) problems. In this study, we bridge FM and the recent theory of Optimal Acceleration Transport (OAT), developing an improved FM method called OAT-FM and exploring its benefits in both theory and practice. In particular, we demonstrate that the straightening objective hidden in existing OT-based FM methods is mathematically equivalent to minimizing the physical action associated with acceleration defined by OAT. Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the acceleration transport in the product space of sample and velocity, whose objective corresponds to a necessary and sufficient condition of flow straightness. An efficient algorithm is designed to achieve OAT-FM with low complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative model trained by an arbitrary FM method, whose velocity information has been relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm eliminates the risk of data distribution drift and the need to generate a large number of noise data pairs, which consistently improves model performance in various generative tasks. Code is available at: https://github.com/AngxiaoYue/OAT-FM
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</title>
<link>https://arxiv.org/abs/2509.25510</link>
<guid>https://arxiv.org/abs/2509.25510</guid>
<content:encoded><![CDATA[
arXiv:2509.25510v2 Announce Type: replace 
Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury</title>
<link>https://arxiv.org/abs/2510.03248</link>
<guid>https://arxiv.org/abs/2510.03248</guid>
<content:encoded><![CDATA[
arXiv:2510.03248v2 Announce Type: replace 
Abstract: Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPAS: Datasets for Learning the Learning with Errors Problem</title>
<link>https://arxiv.org/abs/2510.08797</link>
<guid>https://arxiv.org/abs/2510.08797</guid>
<content:encoded><![CDATA[
arXiv:2510.08797v2 Announce Type: replace 
Abstract: AI-powered attacks on Learning with Errors (LWE), an important hard math problem in post-quantum cryptography, rival or outperform "classical" attacks on LWE under certain parameter settings. Despite the promise of this approach, a dearth of accessible data limits AI practitioners' ability to study and improve these attacks. Creating LWE data for AI model training is time- and compute-intensive and requires significant domain expertise. To fill this gap and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a Toolkit for Analysis of Post-quantum cryptography using AI Systems. These datasets cover several LWE settings and can be used off-the-shelf by AI practitioners to prototype new approaches to cracking LWE. This work documents TAPAS dataset creation, establishes attack performance baselines, and lays out directions for future work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2510.12085</link>
<guid>https://arxiv.org/abs/2510.12085</guid>
<content:encoded><![CDATA[
arXiv:2510.12085v2 Announce Type: replace 
Abstract: Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\% accuracy improvements on citation networks and 7.63\% on social networks in zero-shot settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation</title>
<link>https://arxiv.org/abs/2510.22732</link>
<guid>https://arxiv.org/abs/2510.22732</guid>
<content:encoded><![CDATA[
arXiv:2510.22732v2 Announce Type: replace 
Abstract: Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-Controlled Group Relative Policy Optimization with Token-Level Probability Shaping</title>
<link>https://arxiv.org/abs/2511.00066</link>
<guid>https://arxiv.org/abs/2511.00066</guid>
<content:encoded><![CDATA[
arXiv:2511.00066v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a practical route to improve large language model reasoning, and Group Relative Policy Optimization (GRPO) is a widely used optimizer in this setting. This paper revisits GRPO from a generalization perspective. Recent analysis shows that population performance can be controlled by a robust empirical objective that decomposes into the training loss plus a sharpness term measured by the gradient norm. We develop a token-level view of this sharpness term and show that GRPO can be dominated by a small subset of tokens with disproportionately large per-token gradients, which increases sharpness and can harm generalization. Motivated by this view, we propose Token-Regulated GRPO (TR-GRPO), which introduces a monotone probability shaping function to assign token weights based on the model's own token probabilities, and integrates these weights into the standard GRPO. Our analysis yields a bound that isolates a probability dependent multiplicative factor in token-gradient magnitudes, explaining how probability-aware weighting suppresses sharp directions while preserving learning signal on semantically critical tokens. Experiments on logic puzzles, mathematical reasoning, and tool-augmented question answering show consistent improvements over GRPO, along with smoother gradient-norm trajectories, supporting TR-GRPO as a simple and effective generalization-oriented upgrade to GRPO for RLVR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Efficient Training with In-Place FFT Implementation</title>
<link>https://arxiv.org/abs/2511.01385</link>
<guid>https://arxiv.org/abs/2511.01385</guid>
<content:encoded><![CDATA[
arXiv:2511.01385v2 Announce Type: replace 
Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Graph Neural Networks for Healthcare</title>
<link>https://arxiv.org/abs/2511.02531</link>
<guid>https://arxiv.org/abs/2511.02531</guid>
<content:encoded><![CDATA[
arXiv:2511.02531v3 Announce Type: replace 
Abstract: Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Good GRACEs: Principled Teacher Selection for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02833</link>
<guid>https://arxiv.org/abs/2511.02833</guid>
<content:encoded><![CDATA[
arXiv:2511.02833v3 Announce Type: replace 
Abstract: Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title>
<link>https://arxiv.org/abs/2511.04659</link>
<guid>https://arxiv.org/abs/2511.04659</guid>
<content:encoded><![CDATA[
arXiv:2511.04659v3 Announce Type: replace 
Abstract: Extreme-precipitation nowcasting requires high spatial and temporal resolution together with extended lead times, yet current approaches remain constrained. Numerical weather prediction systems and their deep-learning emulators operate at relatively coarse space-time resolution and struggle to capture rapidly evolving convective systems. Radar extrapolation methods, which advect recent fields using estimated motion, have difficulty capturing the complex evolution of precipitation. Purely data-driven models often produce overly smoothed reflectivity fields and underestimate intensity. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce Nowcast3D, a gray-box, fully three-dimensional nowcasting framework that operates directly on volumetric radar reflectivity and couples physically constrained neural operators with data-driven learning. The model learns three fields that govern reflectivity evolution: a three-dimensional flow field for advective transport, a spatially varying diffusion field for local dispersive spreading, and a residual source term for unresolved microphysical effects. These learned operators advance the forecast in time under explicit physical constraints, while a conditional diffusion model, conditioned on both the observations and the physics-based forecast, generates ensembles of future radar volumes that quantify forecast uncertainty. In a blind evaluation by 160 meteorologists, Nowcast3D is preferred in 57% of post-hoc and 51% of prior assessments. By explicitly embedding three-dimensional dynamics and uncertainty into a single framework, Nowcast3D offers a scalable and robust approach for reliable nowcasting of extreme precipitation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers</title>
<link>https://arxiv.org/abs/2511.07213</link>
<guid>https://arxiv.org/abs/2511.07213</guid>
<content:encoded><![CDATA[
arXiv:2511.07213v2 Announce Type: replace 
Abstract: Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title>
<link>https://arxiv.org/abs/2511.07329</link>
<guid>https://arxiv.org/abs/2511.07329</guid>
<content:encoded><![CDATA[
arXiv:2511.07329v2 Announce Type: replace 
Abstract: It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title>
<link>https://arxiv.org/abs/2511.09392</link>
<guid>https://arxiv.org/abs/2511.09392</guid>
<content:encoded><![CDATA[
arXiv:2511.09392v4 Announce Type: replace 
Abstract: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models</title>
<link>https://arxiv.org/abs/2511.11646</link>
<guid>https://arxiv.org/abs/2511.11646</guid>
<content:encoded><![CDATA[
arXiv:2511.11646v2 Announce Type: replace 
Abstract: Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expressive Temporal Specifications for Reward Monitoring</title>
<link>https://arxiv.org/abs/2511.12808</link>
<guid>https://arxiv.org/abs/2511.12808</guid>
<content:encoded><![CDATA[
arXiv:2511.12808v3 Announce Type: replace 
Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
<link>https://arxiv.org/abs/2511.14455</link>
<guid>https://arxiv.org/abs/2511.14455</guid>
<content:encoded><![CDATA[
arXiv:2511.14455v3 Announce Type: replace 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
arXiv:2511.17582v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVAdam: A Full-Dimension Adaptive Optimizer</title>
<link>https://arxiv.org/abs/2511.20277</link>
<guid>https://arxiv.org/abs/2511.20277</guid>
<content:encoded><![CDATA[
arXiv:2511.20277v2 Announce Type: replace 
Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</title>
<link>https://arxiv.org/abs/2511.20944</link>
<guid>https://arxiv.org/abs/2511.20944</guid>
<content:encoded><![CDATA[
arXiv:2511.20944v3 Announce Type: replace 
Abstract: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Overhead Introspection for Adaptive Test-Time Compute</title>
<link>https://arxiv.org/abs/2512.01457</link>
<guid>https://arxiv.org/abs/2512.01457</guid>
<content:encoded><![CDATA[
arXiv:2512.01457v3 Announce Type: replace 
Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics</title>
<link>https://arxiv.org/abs/2512.03290</link>
<guid>https://arxiv.org/abs/2512.03290</guid>
<content:encoded><![CDATA[
arXiv:2512.03290v3 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate</title>
<link>https://arxiv.org/abs/2512.03578</link>
<guid>https://arxiv.org/abs/2512.03578</guid>
<content:encoded><![CDATA[
arXiv:2512.03578v2 Announce Type: replace 
Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.
  The code implementation and datasets are publicly available at https://github.com/FlorentF9/MAGNETS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v2 Announce Type: replace 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.07010</link>
<guid>https://arxiv.org/abs/2512.07010</guid>
<content:encoded><![CDATA[
arXiv:2512.07010v2 Announce Type: replace 
Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and model modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, requiring no model modification, enabling side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with 100M-1B parameters. We achieved 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures. All code is available at https://github.com/keeinlev/dynamicLRP .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renormalizable Spectral-Shell Dynamics as the Origin of Neural Scaling Laws</title>
<link>https://arxiv.org/abs/2512.10427</link>
<guid>https://arxiv.org/abs/2512.10427</guid>
<content:encoded><![CDATA[
arXiv:2512.10427v3 Announce Type: replace 
Abstract: Neural scaling laws and double-descent phenomena suggest that deep-network training obeys a simple macroscopic structure despite highly nonlinear optimization dynamics. We derive such structure directly from gradient descent in function space. For mean-squared error loss, the training error evolves as $\dot e_t=-M(t)e_t$ with $M(t)=J_{\theta(t)}J_{\theta(t)}^{\!*}$, a time-dependent self-adjoint operator induced by the network Jacobian. Using Kato perturbation theory, we obtain an exact system of coupled modewise ODEs in the instantaneous eigenbasis of $M(t)$.
  To extract macroscopic behavior, we introduce a logarithmic spectral-shell coarse-graining and track quadratic error energy across shells. Microscopic interactions within each shell cancel identically at the energy level, so shell energies evolve only through dissipation and external inter-shell interactions. We formalize this via a \emph{renormalizable shell-dynamics} assumption, under which cumulative microscopic effects reduce to a controlled net flux across shell boundaries.
  Assuming an effective power-law spectral transport in a relevant resolution range, the shell dynamics admits a self-similar solution with a moving resolution frontier and explicit scaling exponents. This framework explains neural scaling laws and double descent, and unifies lazy (NTK-like) training and feature learning as two limits of the same spectral-shell dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v2 Announce Type: replace 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure of Classifier Boundaries: Case Study for a Naive Bayes Classifier</title>
<link>https://arxiv.org/abs/2212.04382</link>
<guid>https://arxiv.org/abs/2212.04382</guid>
<content:encoded><![CDATA[
arXiv:2212.04382v3 Announce Type: replace-cross 
Abstract: Classifiers assign complex input data points to one of a small number of output categories. For a Bayes classifier whose input space is a graph, we study the structure of the \emph{boundary}, which comprises those points for which at least one neighbor is classified differently. The scientific setting is assignment of DNA reads produced by \NGSs\ to candidate source genomes. The boundary is both large and complicated in structure. We introduce a new measure of uncertainty, Neighbor Similarity, that compares the result for an input point to the distribution of results for its neighbors. This measure not only tracks two inherent uncertainty measures for the Bayes classifier, but also can be implemented for classifiers without inherent measures of uncertainty.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy</title>
<link>https://arxiv.org/abs/2302.09049</link>
<guid>https://arxiv.org/abs/2302.09049</guid>
<content:encoded><![CDATA[
arXiv:2302.09049v3 Announce Type: replace-cross 
Abstract: We construct multiperiodic processes -- a simple example of stationary ergodic (but not mixing) processes over natural numbers that enjoy the vanishing entropy rate under a mild condition. Multiperiodic processes are supported on randomly shifted deterministic sequences called multiperiodic sequences, which can be efficiently generated using an algorithm called the Infinite Clock. Under a suitable parameterization, multiperiodic sequences exhibit relative frequencies of particular numbers given by Zipf's law. Exactly in the same setting, the respective multiperiodic processes satisfy an asymptotic power-law growth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold for statistical language models, in particular.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training robust and generalizable quantum models</title>
<link>https://arxiv.org/abs/2311.11871</link>
<guid>https://arxiv.org/abs/2311.11871</guid>
<content:encoded><![CDATA[
arXiv:2311.11871v4 Announce Type: replace-cross 
Abstract: Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against data perturbations. Further, we derive a bound on the generalization error which explicitly involves the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings, as those frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. The practical implications of our theoretical findings are illustrated with numerical results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method</title>
<link>https://arxiv.org/abs/2402.08992</link>
<guid>https://arxiv.org/abs/2402.08992</guid>
<content:encoded><![CDATA[
arXiv:2402.08992v3 Announce Type: replace-cross 
Abstract: High-probability guarantees in stochastic optimization are often obtained only under strong noise assumptions such as sub-Gaussian tails. We show that such guarantees can also be achieved under the weaker assumption of bounded variance by developing a stochastic proximal point method. This method combines a proximal subproblem solver, which inherently reduces variance, with a probability booster that amplifies per-iteration reliability into high-confidence results. The analysis demonstrates convergence with low sample complexity, without restrictive noise assumptions or reliance on mini-batching.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any-Time Regret-Guaranteed Algorithm for Control of Linear Quadratic Systems</title>
<link>https://arxiv.org/abs/2406.07746</link>
<guid>https://arxiv.org/abs/2406.07746</guid>
<content:encoded><![CDATA[
arXiv:2406.07746v2 Announce Type: replace-cross 
Abstract: We propose a computationally efficient algorithm that achieves anytime regret of order $\mathcal{O}(\sqrt{t})$, with explicit dependence on the system dimensions and on the solution of the Discrete Algebraic Riccati Equation (DARE). Our approach uses an appropriately tuned regularization and a sufficiently accurate initial estimate to construct confidence ellipsoids for control design. A carefully designed input-perturbation mechanism is incorporated to ensure anytime performance. We develop two variants of the algorithm. The first enforces strong sequential stability, requiring each policy to be stabilizing and successive policies to remain close. This sequential condition helps prevent state explosion at policy update times; however, it results in a suboptimal regret scaling with respect to the DARE solution. Motivated by this limitation, we introduce a second class of algorithms that removes this requirement and instead requires only that each generated policy be stabilizing. Closed-loop stability is then preserved through a dwell-time inspired policy-update rule. This class of algorithms also addresses key shortcomings of most existing approaches which lack explicit high-probability bounds on the state trajectory expressed in system-theoretic terms. Our analysis shows that partially relaxing the sequential-stability requirement yields optimal regret. Finally, our method eliminates the need for any \emph{a priori} bound on the norm of the DARE solution, an assumption required by all existing computationally efficient OFU based algorithms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Retrieval with Few-shot Indexing</title>
<link>https://arxiv.org/abs/2408.02152</link>
<guid>https://arxiv.org/abs/2408.02152</guid>
<content:encoded><![CDATA[
arXiv:2408.02152v2 Announce Type: replace-cross 
Abstract: Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation</title>
<link>https://arxiv.org/abs/2408.11607</link>
<guid>https://arxiv.org/abs/2408.11607</guid>
<content:encoded><![CDATA[
arXiv:2408.11607v3 Announce Type: replace-cross 
Abstract: Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</title>
<link>https://arxiv.org/abs/2409.01382</link>
<guid>https://arxiv.org/abs/2409.01382</guid>
<content:encoded><![CDATA[
arXiv:2409.01382v2 Announce Type: replace-cross 
Abstract: The adoption of Large Language Models (LLMs) for code generation risks incorporating vulnerable code into software systems. Existing detectors face two critical limitations: a lack of systematic cross-model validation and opaque "black box" operation. We address this through a comparative study of code generated by four distinct LLMs: GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS.
  Analyzing 14,485 Python functions and 11,913 classes from the CodeSearchNet dataset, we generated corresponding code with all four LLMs. Using interpretable software metrics, we trained CatBoost classifiers for each configuration. Our analysis reveals that granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap, indicating that function-level and class-level detection rely on fundamentally disjoint structural signatures.
  We discover critical granularity-dependent inversions: while modern models (Claude, GPT-OSS) are more detectable at the class level, GPT-3.5 is an anomaly that uniquely excels at the function level. SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models, explaining why detectors trained on specific LLMs fail to generalize.
  Our findings demonstrate that GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80). Robust detection requires moving beyond single-model studies to account for substantial diversity in structural fingerprints across architectures and granularities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments</title>
<link>https://arxiv.org/abs/2410.15178</link>
<guid>https://arxiv.org/abs/2410.15178</guid>
<content:encoded><![CDATA[
arXiv:2410.15178v4 Announce Type: replace-cross 
Abstract: Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Convergence Guarantees for Variational Autoencoders</title>
<link>https://arxiv.org/abs/2410.16750</link>
<guid>https://arxiv.org/abs/2410.16750</guid>
<content:encoded><![CDATA[
arXiv:2410.16750v3 Announce Type: replace-cross 
Abstract: Variational Autoencoders (VAE) are popular generative models used to sample from complex data distributions. Despite their empirical success in various machine learning tasks, significant gaps remain in understanding their theoretical properties, particularly regarding convergence guarantees. This paper aims to bridge that gap by providing non-asymptotic convergence guarantees for VAE trained using both Stochastic Gradient Descent and Adam algorithms. We derive a convergence rate of $\mathcal{O}(\log n / \sqrt{n})$, where $n$ is the number of iterations of the optimization algorithm, with explicit dependencies on the batch size, the number of variational samples, and other key hyperparameters. Our theoretical analysis applies to both Linear VAE and Deep Gaussian VAE, as well as several VAE variants, including $\beta$-VAE and IWAE. Additionally, we empirically illustrate the impact of hyperparameters on convergence, offering new insights into the theoretical understanding of VAE training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</title>
<link>https://arxiv.org/abs/2410.24116</link>
<guid>https://arxiv.org/abs/2410.24116</guid>
<content:encoded><![CDATA[
arXiv:2410.24116v2 Announce Type: replace-cross 
Abstract: Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</title>
<link>https://arxiv.org/abs/2411.12150</link>
<guid>https://arxiv.org/abs/2411.12150</guid>
<content:encoded><![CDATA[
arXiv:2411.12150v3 Announce Type: replace-cross 
Abstract: We study the problem of robot navigation in dense and interactive crowds with static constraints such as corridors and furniture. Previous methods fail to consider all types of spatial and temporal interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different inputs and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous spatio-temporal graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success, navigation time, and generalization to domain shifts in challenging navigation scenarios. More information is available at https://sites.google.com/view/crowdnav-height/home.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks</title>
<link>https://arxiv.org/abs/2412.14031</link>
<guid>https://arxiv.org/abs/2412.14031</guid>
<content:encoded><![CDATA[
arXiv:2412.14031v5 Announce Type: replace-cross 
Abstract: In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Preference Elicitation with LLM-Based Proxies</title>
<link>https://arxiv.org/abs/2501.14625</link>
<guid>https://arxiv.org/abs/2501.14625</guid>
<content:encoded><![CDATA[
arXiv:2501.14625v2 Announce Type: replace-cross 
Abstract: Bidders in combinatorial auctions face significant challenges when describing their preferences to an auctioneer. Classical work on preference elicitation focuses on query-based techniques inspired from proper learning--often via proxies that interface between bidders and an auction mechanism--to incrementally learn bidder preferences as needed to compute efficient allocations. Although such elicitation mechanisms enjoy theoretical query efficiency, the amount of communication required may still be too cognitively taxing in practice.
  We propose a family of efficient LLM-based proxy designs for eliciting preferences from bidders using natural language. Our proposed mechanism combines LLM pipelines and DNF-proper-learning techniques to quickly approximate preferences when communication is limited. To validate our approach, we create a testing sandbox for elicitation mechanisms that communicate in natural language. In our experiments, our most promising LLM proxy design reaches approximately efficient outcomes with five times fewer queries than classical proper learning based elicitation mechanisms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimSort: A Data-Driven Framework for Spike Sorting by Large-Scale Electrophysiology Simulation</title>
<link>https://arxiv.org/abs/2502.03198</link>
<guid>https://arxiv.org/abs/2502.03198</guid>
<content:encoded><![CDATA[
arXiv:2502.03198v3 Announce Type: replace-cross 
Abstract: Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings. In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present SimSort, a pretraining framework for spike sorting. Trained solely on simulated data, SimSort demonstrates zero-shot generalizability to real-world spike sorting tasks, yielding consistent improvements over existing methods across multiple benchmarks. These results highlight the potential of simulation-driven pretraining to enhance the robustness and scalability of spike sorting in experimental neuroscience.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</title>
<link>https://arxiv.org/abs/2502.04226</link>
<guid>https://arxiv.org/abs/2502.04226</guid>
<content:encoded><![CDATA[
arXiv:2502.04226v2 Announce Type: replace-cross 
Abstract: In the era of pre-trained models, effective classification can often be achieved using simple linear probing or lightweight readout layers. In contrast, many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets, including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning</title>
<link>https://arxiv.org/abs/2503.02341</link>
<guid>https://arxiv.org/abs/2503.02341</guid>
<content:encoded><![CDATA[
arXiv:2503.02341v2 Announce Type: replace-cross 
Abstract: Recent great advances in video generation models have demonstrated their potential to produce high-quality videos, bringing challenges to effective evaluation. Unlike human evaluation, existing automated evaluation metrics lack highlevel semantic understanding and reasoning capabilities for video, thus making them infeasible and unexplainable. To fill this gap, we curate GRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset, including 3.3k videos from over 10 existing video generation models and multi-step reasoning assessments converted by 16k human annotations. We then introduce GRADEO, one of the first specifically designed video evaluation models, which grades AI-generated videos for explainable scores and assessments through multi-step reasoning. Experiments show that our method aligns better with human evaluations than existing methods. Furthermore, our benchmarking reveals that current video generation models struggle to produce content that aligns with human reasoning and complex real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density estimation via mixture discrepancy and moments</title>
<link>https://arxiv.org/abs/2504.01570</link>
<guid>https://arxiv.org/abs/2504.01570</guid>
<content:encoded><![CDATA[
arXiv:2504.01570v2 Announce Type: replace-cross 
Abstract: With the aim of generalizing histogram statistics to higher dimensional cases, density estimation via discrepancy based sequential partition (DSP) has been proposed to learn an adaptive piecewise constant approximation defined on a binary sequential partition of the underlying domain, where the star discrepancy is adopted to measure the uniformity of particle distribution. However, the calculation of the star discrepancy is NP-hard and it does not satisfy the reflection invariance and rotation invariance either. To this end, we use the mixture discrepancy and the comparison of moments as a replacement of the star discrepancy, leading to the density estimation via mixture discrepancy based sequential partition (DSP-mix) and density estimation via moment-based sequential partition (MSP), respectively. Both DSP-mix and MSP are computationally tractable and exhibit the reflection and rotation invariance. Numerical experiments in reconstructing Beta mixtures, Gaussian mixtures and heavy-tailed Cauchy mixtures up to 30 dimension are conducted, demonstrating that MSP can maintain the same accuracy compared with DSP, while gaining an increase in speed by a factor of two to twenty for large sample size, and DSP-mix can achieve satisfactory accuracy and boost the efficiency in low-dimensional tests ($d \le 6$), but might lose accuracy in high-dimensional problems due to a reduction in partition level.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Optimization with Optimal Importance Sampling</title>
<link>https://arxiv.org/abs/2504.03560</link>
<guid>https://arxiv.org/abs/2504.03560</guid>
<content:encoded><![CDATA[
arXiv:2504.03560v2 Announce Type: replace-cross 
Abstract: Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its effectiveness, the performance of IS is highly sensitive to the choice of the proposal distribution and often requires stochastic calibration. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a fundamental challenge: the decision variable and the importance sampling distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both convergence analysis and variance control. We consider convex stochastic optimization problems with linear constraints and propose a single-loop stochastic approximation algorithm, based on a joint variant of Nesterov's dual averaging, that jointly updates the decision variable and the importance sampling distribution, without time-scale separation or nested optimization. The method is globally convergent and achieves minimal asymptotic variance among stochastic gradient schemes, matching the performance of an oracle sampler adapted to the optimal solution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JITServe: SLO-aware LLM Serving with Imprecise Request Information</title>
<link>https://arxiv.org/abs/2504.20068</link>
<guid>https://arxiv.org/abs/2504.20068</guid>
<content:encoded><![CDATA[
arXiv:2504.20068v3 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into applications ranging from interactive chatbots to multi-agent systems has introduced a wide spectrum of service-level objectives (SLOs) for responsiveness. These include latency-sensitive requests emphasizing per-token latency in streaming chat, deadline-sensitive requests requiring rapid full responses to trigger external tools, and compound requests with evolving dependencies across multiple LLM calls. Despite-or perhaps, because of-this workload diversity and unpredictable request information (e.g., response lengths and dependencies), existing request schedulers have focused on aggregate performance, unable to ensure application-level SLO needs.
  This paper presents JITServe, the first SLO-aware LLM serving system designed to maximize service goodput (e.g., the number of tokens meeting request SLOs) across diverse workloads. JITServe novelly schedules requests using imprecise request information and gradually relaxes this conservatism by refining request information estimates as generation progresses. It applies a grouped margin goodput maximization algorithm to allocate just enough serving bandwidth to satisfy each request's SLO just-in-time (JIT), maximizing residual capacity for others, while deciding the composition of requests in a batch to maximize efficiency and goodput with provable guarantees. Our evaluation across diverse realistic workloads, including chat, deep research, and agentic pipelines, shows that JITServe improves service goodput by 1.4x-6.3x, alternatively achieving 28.5%-83.2% resource savings, compared to state-of-the-art designs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Nonlinear Model Predictive Control</title>
<link>https://arxiv.org/abs/2505.01353</link>
<guid>https://arxiv.org/abs/2505.01353</guid>
<content:encoded><![CDATA[
arXiv:2505.01353v2 Announce Type: replace-cross 
Abstract: The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v2 Announce Type: replace-cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized and Resilient Distributed Learning Through Opinion Dynamics</title>
<link>https://arxiv.org/abs/2505.14081</link>
<guid>https://arxiv.org/abs/2505.14081</guid>
<content:encoded><![CDATA[
arXiv:2505.14081v2 Announce Type: replace-cross 
Abstract: In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications</title>
<link>https://arxiv.org/abs/2505.14918</link>
<guid>https://arxiv.org/abs/2505.14918</guid>
<content:encoded><![CDATA[
arXiv:2505.14918v2 Announce Type: replace-cross 
Abstract: This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v2 Announce Type: replace-cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor. Previous work has shown that transfer learning can be used effectively with ESNs for single-orbit prediction. The novelty of our paper lies in our use of this pairing to predict the long-term statistical properties of spatiotemporally chaotic PDEs. We also show that transfer learning nontrivially improves the length of time that predictions of individual gKS trajectories remain accurate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units</title>
<link>https://arxiv.org/abs/2506.16289</link>
<guid>https://arxiv.org/abs/2506.16289</guid>
<content:encoded><![CDATA[
arXiv:2506.16289v2 Announce Type: replace-cross 
Abstract: This paper explores the relationship between the condition number of a neural network's weight tensor and the extent of information encoded by the associated processing unit, viewed through the lens of information theory. It argues that a high condition number, though not sufficient for effective knowledge encoding, may indicate that the unit has learned to selectively amplify and compress information. This intuition is formalized for linear units with Gaussian inputs, linking the condition number and the transformation's log-volume scaling factor to the characteristics of the output entropy and the geometric properties of the learned transformation. The analysis demonstrates that for a fixed weight norm, a concentrated distribution of singular values (high condition number) corresponds to reduced overall information transfer, indicating a specialized and efficient encoding strategy. Furthermore, the linear stage entropy bound provides an upper limit on post-activation information for contractive, element-wise nonlinearities, supporting the condition number as a scale-invariant proxy for encoding capacity in practical neural networks. An empirical case study applies these principles to guide selective fine-tuning of Large Language Models for both a new task and a new input modality. The experiments show that the proposed method, named KappaTune, effectively mitigates catastrophic forgetting. Unlike many existing catastrophic forgetting mitigation methods that rely on access to pre-training statistics, which are often unavailable, this selective fine-tuning approach offers a way to bypass this common requirement.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication</title>
<link>https://arxiv.org/abs/2506.22714</link>
<guid>https://arxiv.org/abs/2506.22714</guid>
<content:encoded><![CDATA[
arXiv:2506.22714v2 Announce Type: replace-cross 
Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used in deep learning and scientific computing. Modern accelerators are commonly equipped with Tensor Core Units (TCUs) and CUDA cores to accelerate sparse operators. The former excels at structured matrix computations, whereas the latter offers greater programming flexibility. However, how to combine these two resources to maximize sparse-operator performance remains unclear. In this work, we first identify the source of performance gains in hybrid computation and systematically analyze their complementary strengths. Motivated by this, we propose Libra, a holistic framework that efficiently leverages heterogeneous computing resources to accelerate both SpMM and SDDMM operators. Specifically, Libra introduces a 2D-aware (locality and utilization) workload distribution method to precisely identify the optimal task mapping, simultaneously leveraging the data reuse capabilities of TCUs and the flexibility of CUDA cores to minimize computational redundancy. Libra further incorporates hybrid load balancing, occupancy-aware task scheduling, and efficient kernel implementations to maximize execution efficiency. Extensive experiments on H100 and RTX 4090 GPUs demonstrate that Libra surpasses all the 12 up-to-date baselines significantly, e.g., on average 1.77x speedup over FlashSparse, 1.73x over RoDe, and 2.9x over DGL for end-to-end GNN applications. Libra opens up a new perspective for sparse operator acceleration by fully unleashing the power of heterogeneous GPU resources.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cVLA: Towards Efficient Camera-Space VLAs</title>
<link>https://arxiv.org/abs/2507.02190</link>
<guid>https://arxiv.org/abs/2507.02190</guid>
<content:encoded><![CDATA[
arXiv:2507.02190v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models offer a compelling framework for tackling complex robotic manipulation tasks, but they are often expensive to train. In this paper, we propose a novel VLA approach that leverages the competitive performance of Vision Language Models (VLMs) on 2D images to directly infer robot end-effector poses in image frame coordinates. Unlike prior VLA models that output low-level controls, our model predicts trajectory waypoints, making it both more efficient to train and robot embodiment agnostic. Despite its lightweight design, our next-token prediction architecture effectively learns meaningful and executable robot trajectories. We further explore the underutilized potential of incorporating depth images, inference-time techniques such as decoding strategies, and demonstration-conditioned action generation. Our model is trained on a simulated dataset and exhibits strong sim-to-real transfer capabilities. We evaluate our approach using a combination of simulated and real data, demonstrating its effectiveness on a real robotic system.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models</title>
<link>https://arxiv.org/abs/2507.10934</link>
<guid>https://arxiv.org/abs/2507.10934</guid>
<content:encoded><![CDATA[
arXiv:2507.10934v2 Announce Type: replace-cross 
Abstract: Data quality remains an important challenge in data-driven systems, as errors in tabular data can severely compromise downstream analytics and machine learning performance. Although numerous error detection algorithms have been proposed, the lack of diverse, real-world error datasets limits comprehensive evaluation. Manual error annotation is both time-consuming and inconsistent, motivating the exploration of synthetic error generation as an alternative. In this work, we introduce TableEG, a framework that leverages large language models (LLMs) to generate authentic errors. By employing a table fine-tuning strategy and a triplet representation $(I, T, O)$ to model error generation, detection, and correction tasks, TableEG captures the complex dependencies inherent in two-dimensional tables. Trained on 12 real-world datasets spanning 10 diverse domains, TableEG ensures that the synthesized errors faithfully reflect authentic error distributions. Experimental results indicate that errors generated by TableEG exhibit superior pattern and distribution similarity compared to both rule-based methods and LLM-generated errors without fine-tuning. Furthermore, performance metrics on TableEG-generated errors closely align with those on real-world errors across nearly all datasets and detection algorithms, particularly for machine learning based detection techniques. Overall, TableEG not only bridges the gap between synthetic and real-world errors but also establishes a robust benchmark for subsequent error detection and correction tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence Calibration in Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2507.17383</link>
<guid>https://arxiv.org/abs/2507.17383</guid>
<content:encoded><![CDATA[
arXiv:2507.17383v2 Announce Type: replace-cross 
Abstract: Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present a first-of-its-kind study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural language instructions to low-level robot motor commands. We establish a confidence baseline for VLAs, examine how task success relates to calibration error and how calibration evolves over time, and introduce two lightweight techniques to remedy the miscalibration we observe: prompt ensembles and action-wise Platt scaling. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASR-Synchronized Speaker-Role Diarization</title>
<link>https://arxiv.org/abs/2507.17765</link>
<guid>https://arxiv.org/abs/2507.17765</guid>
<content:encoded><![CDATA[
arXiv:2507.17765v3 Announce Type: replace-cross 
Abstract: Speaker-role diarization (RD), such as doctor vs. patient or lawyer vs. client, is practically often more useful than conventional speaker diarization (SD), which assigns only generic labels (speaker-1, speaker-2). The state-of-the-art end-to-end ASR+RD approach uses a single transducer that serializes word and role predictions (role at the end of a speaker's turn), but at the cost of degraded ASR performance. To address this, we adapt a recent joint ASR+SD framework to ASR+RD by freezing the ASR transducer and training an auxiliary RD transducer in parallel to assign a role to each ASR-predicted word. For this, we first show that SD and RD are fundamentally different tasks, exhibiting different dependencies on acoustic and linguistic information. Motivated by this, we propose (1) task-specific predictor networks and (2) using higher-layer ASR encoder features as input to the RD encoder. Additionally, we replace the blank-shared RNNT loss by cross-entropy loss along the 1-best forced-alignment path to further improve performance while reducing computational and memory requirements during RD training. Experiments on a public and a private dataset of doctor-patient conversations demonstrate that our method outperforms the best baseline with relative reductions of 6.2% and 4.5% in role-based word diarization error rate (R-WDER), respectively
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Variational Free Energy Calculation of Hydrogen Hugoniot</title>
<link>https://arxiv.org/abs/2507.18540</link>
<guid>https://arxiv.org/abs/2507.18540</guid>
<content:encoded><![CDATA[
arXiv:2507.18540v2 Announce Type: replace-cross 
Abstract: We develop a deep variational free energy framework to compute the equation of state of hydrogen in the warm dense matter region. This method parameterizes the variational density matrix of hydrogen nuclei and electrons at finite temperature using three deep generative models: a normalizing flow model for the Boltzmann distribution of the classical nuclei, an autoregressive transformer for the distribution of electrons in excited states, and a permutational equivariant flow model for the unitary backflow transformation of electron coordinates in Hartree-Fock states. By jointly optimizing the three neural networks to minimize the variational free energy, we obtain the equation of state and related thermodynamic properties of dense hydrogen for the temperature range where electrons occupy excited states. We compare our results with other theoretical and experimental results on the deuterium Hugoniot curve, aiming to resolve existing discrepancies. Our results bridge the gap between the results obtained by path-integral Monte Carlo calculations at high temperature and ground-state electronic methods at low temperature, thus providing a valuable benchmark for hydrogen in the warm dense matter region.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies</title>
<link>https://arxiv.org/abs/2507.22782</link>
<guid>https://arxiv.org/abs/2507.22782</guid>
<content:encoded><![CDATA[
arXiv:2507.22782v3 Announce Type: replace-cross 
Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title>
<link>https://arxiv.org/abs/2508.20072</link>
<guid>https://arxiv.org/abs/2508.20072</guid>
<content:encoded><![CDATA[
arXiv:2508.20072v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic Communications</title>
<link>https://arxiv.org/abs/2509.08139</link>
<guid>https://arxiv.org/abs/2509.08139</guid>
<content:encoded><![CDATA[
arXiv:2509.08139v2 Announce Type: replace-cross 
Abstract: Future AI-native wireless networks are moving from reactive optimization to agentic decision-making that can sense, predict, and plan under fast-varying channels. This calls for wireless world models that can predict and roll out channel dynamics, for which multi-step channel state information (CSI) prediction offers a practical short-horizon look-ahead. Recent advances in foundation sequence models further motivate large language models (LLMs) as general-purpose dynamics learners when suitably adapted to non-text time-series signals. However, bridging CSI to LLMs is non-trivial because an effective adapter must expose informative spectral and temporal evolution patterns, while prior designs provide limited inductive bias to capture such channel structures. To this end, we propose SCA-LLM, a spectral-attentive LLM-based wireless world modeling framework that bridges CSI to LLMs via a spectral-channel attention (SCA) adapter. Specifically, the SCA adapter performs multi-spectral representation learning to extract informative channel features and align CSI with the LLM's sequence modeling capability, enabling parameter-efficient adaptation while keeping the LLM backbone largely frozen. Extensive simulations show that SCA-LLM achieves state-of-the-art prediction performance and strong zero-shot generalization, yielding up to -2.4 dB normalized mean squared error (NMSE) advantage over the previous LLM based method. Our ablation studies further confirm the effectiveness of the proposed SCA adapter in mitigating domain mismatch.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v3 Announce Type: replace-cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</title>
<link>https://arxiv.org/abs/2510.09595</link>
<guid>https://arxiv.org/abs/2510.09595</guid>
<content:encoded><![CDATA[
arXiv:2510.09595v2 Announce Type: replace-cross 
Abstract: Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Optimised Spiking Neural Network (NOS) Scheduling for 6G O-RAN: Spectral Margin and Delay-Tail Control</title>
<link>https://arxiv.org/abs/2510.11291</link>
<guid>https://arxiv.org/abs/2510.11291</guid>
<content:encoded><![CDATA[
arXiv:2510.11291v2 Announce Type: replace-cross 
Abstract: This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler for 6G radio access. The scheme couples a bounded two-state kernel to a clique-feasible proportional-fair (PF) grant head: the excitability state acts as a finite-buffer proxy, the recovery state suppresses repeated grants, and neighbour pressure is injected along the interference graph via delayed spikes. A small-signal analysis yields a delay-dependent threshold $k_\star(\Delta)$ and a spectral margin $\delta = k_\star(\Delta) - gH\rho(W)$ that compress topology, controller gain, and delay into a single design parameter. Under light assumptions on arrivals, we prove geometric ergodicity for $\delta>0$ and derive sub-Gaussian backlog and delay tail bounds with exponents proportional to $\delta$. A numerical study, aligned with the analysis and a DU compute budget, compares NOS with PF and delayed backpressure (BP) across interference topologies over a $5$--$20$\,ms delay sweep. With a single gain fixed at the worst spectral radius, NOS sustains higher utilisation and a smaller 99.9th-percentile delay while remaining clique-feasible on integer PRBs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polarization based direction of arrival estimation using a radio interferometric array</title>
<link>https://arxiv.org/abs/2510.15116</link>
<guid>https://arxiv.org/abs/2510.15116</guid>
<content:encoded><![CDATA[
arXiv:2510.15116v2 Announce Type: replace-cross 
Abstract: Direction of arrival (DOA) estimation is mostly performed using specialized arrays that have carefully designed receiver spacing and layouts to match the operating frequency range. In contrast, radio interferometric arrays are designed to optimally sample the Fourier space data for making high quality images of the sky.
  Therefore, using existing radio interferometric arrays (with arbitrary geometry and wide frequency variation) for DOA estimation is practically infeasible except by using images made by such interferometers. In this paper, we focus on low cost DOA estimation without imaging, using a subset of a radio interferometric array, using a fraction of the data collected by the full array, and, enabling early determination of DOAs. The proposed method is suitable for transient and low duty cycle source detection. Moreover, the proposed method is an ideal follow-up step to online radio frequency interference (RFI) mitigation, enabling the early estimation of the DOA of the detected RFI.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena</title>
<link>https://arxiv.org/abs/2510.17638</link>
<guid>https://arxiv.org/abs/2510.17638</guid>
<content:encoded><![CDATA[
arXiv:2510.17638v2 Announce Type: replace-cross 
Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call "LLM-as-a-Prophet". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel Optimization</title>
<link>https://arxiv.org/abs/2510.24710</link>
<guid>https://arxiv.org/abs/2510.24710</guid>
<content:encoded><![CDATA[
arXiv:2510.24710v2 Announce Type: replace-cross 
Abstract: We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Configuration-First Framework for Reproducible, Low-Code Localization</title>
<link>https://arxiv.org/abs/2510.25692</link>
<guid>https://arxiv.org/abs/2510.25692</guid>
<content:encoded><![CDATA[
arXiv:2510.25692v2 Announce Type: replace-cross 
Abstract: Machine learning is increasingly permeating radio-based localization services. To keep results credible and comparable, everyday workflows should make rigorous experiment specification and exact repeatability the default, without blocking advanced experimentation. However, in practice, researchers face a three-way gap that could be filled by a framework that offers (i) low coding effort for end-to-end studies, (ii) reproducibility by default, including versioned code, data, and configurations, controlled randomness, isolated runs, and recorded artifacts, and (iii) built-in extensibility so new models, metrics, and stages can be added with minimal integration effort. Existing tools rarely deliver all three for machine learning in general and localization workflows in particular. In this paper, we introduce LOCALIZE, a low-code, configuration-first framework for radio localization in which experiments are declared in human-readable configuration files, a workflow orchestrator executes standardized pipelines from data preparation to reporting, and all artifacts, such as datasets, models, metrics, and reports, are versioned. Preconfigured, versioned datasets reduce initial setup effort and boilerplate, thereby accelerating model development and evaluation. The design, with explicit extension points, allows experts to add components without reworking the underlying infrastructure. Through a qualitative comparison and a head-to-head study against a plain Jupyter notebook baseline, we show that the framework reduces authoring effort while maintaining comparable runtime and memory behavior. Furthermore, using a Bluetooth Low Energy dataset, we demonstrate that scaling the training data from 1x to 10x keeps orchestration overheads bounded as data grows. Overall, the framework makes reproducible machine-learning-based localization experimentation practical, accessible, and extensible.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned Static Function Data Structures</title>
<link>https://arxiv.org/abs/2510.27588</link>
<guid>https://arxiv.org/abs/2510.27588</guid>
<content:encoded><![CDATA[
arXiv:2510.27588v2 Announce Type: replace-cross 
Abstract: We consider the task of constructing a data structure for associating a static set of keys with values, while allowing arbitrary output values for queries involving keys outside the set. Compared to hash tables, these so-called static function data structures do not need to store the key set and thus use significantly less memory. Several techniques are known, with compressed static functions approaching the zero-order empirical entropy of the value sequence. In this paper, we introduce learned static functions, which use machine learning to capture correlations between keys and values. For each key, a model predicts a probability distribution over the values, from which we derive a key-specific prefix code to compactly encode the true value. The resulting codeword is stored in a classic static function data structure. This design allows learned static functions to break the zero-order entropy barrier while still supporting point queries. Our experiments show substantial space savings: up to one order of magnitude on real data, and up to three orders of magnitude on synthetic data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
arXiv:2511.02376v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Guidance through Calibration and Regularization</title>
<link>https://arxiv.org/abs/2511.05844</link>
<guid>https://arxiv.org/abs/2511.05844</guid>
<content:encoded><![CDATA[
arXiv:2511.05844v3 Announce Type: replace-cross 
Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Source-Optimal Training is Transfer-Suboptimal</title>
<link>https://arxiv.org/abs/2511.08401</link>
<guid>https://arxiv.org/abs/2511.08401</guid>
<content:encoded><![CDATA[
arXiv:2511.08401v3 Announce Type: replace-cross 
Abstract: We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $\tau_0^* \neq \tau_S^*$. We characterize the transfer-optimal source penalty $\tau_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0<\rho<1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($\rho>1$), transfer benefits from weaker regularization. In isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present CIFAR-10 experiments as evidence that the source-optimal versus transfer-optimal mismatch can persist in nonlinear networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2511.18562</link>
<guid>https://arxiv.org/abs/2511.18562</guid>
<content:encoded><![CDATA[
arXiv:2511.18562v2 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue</title>
<link>https://arxiv.org/abs/2511.21728</link>
<guid>https://arxiv.org/abs/2511.21728</guid>
<content:encoded><![CDATA[
arXiv:2511.21728v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-time reinforcement learning for optimal switching over multiple regimes</title>
<link>https://arxiv.org/abs/2512.04697</link>
<guid>https://arxiv.org/abs/2512.04697</guid>
<content:encoded><![CDATA[
arXiv:2512.04697v2 Announce Type: replace-cross 
Abstract: This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v2 Announce Type: replace-cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent Density Estimation</title>
<link>https://arxiv.org/abs/2512.10067</link>
<guid>https://arxiv.org/abs/2512.10067</guid>
<content:encoded><![CDATA[
arXiv:2512.10067v2 Announce Type: replace-cross 
Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Compositional Data Generation for Robot Control</title>
<link>https://arxiv.org/abs/2512.10891</link>
<guid>https://arxiv.org/abs/2512.10891</guid>
<content:encoded><![CDATA[
arXiv:2512.10891v3 Announce Type: replace-cross 
Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
<link>https://arxiv.org/abs/2512.13123</link>
<guid>https://arxiv.org/abs/2512.13123</guid>
<content:encoded><![CDATA[
arXiv:2512.13123v3 Announce Type: replace-cross 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
<link>https://arxiv.org/abs/2512.13491</link>
<guid>https://arxiv.org/abs/2512.13491</guid>
<content:encoded><![CDATA[
arXiv:2512.13491v2 Announce Type: replace-cross 
Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
<link>https://arxiv.org/abs/2512.13634</link>
<guid>https://arxiv.org/abs/2512.13634</guid>
<content:encoded><![CDATA[
arXiv:2512.13634v2 Announce Type: replace-cross 
Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this limit is the same whenever the data is drawn from mixtures of arbitrary product distributions whose first two moments match the corresponding Gaussian distribution, provided the initialization and ground truth vectors are coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title>
<link>https://arxiv.org/abs/2512.12218</link>
<guid>https://arxiv.org/abs/2512.12218</guid>
<content:encoded><![CDATA[
<div> Reasoning-augmented vision language models, visual faithfulness, perception steps, self-reflection, multimodal reasoning<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating reasoning-augmented vision language models (VLMs) that generate explicit chains of thought. Such chains enhance model capability and transparency but introduce failure modes where models might produce correct answers through intermediate steps that are visually unfaithful or correctly reason but err on the final prediction. The authors argue that standard evaluations focusing solely on final-answer accuracy fail to capture these nuanced behaviors. They introduce the concept of visual faithfulness as a distinct evaluation dimension, which assesses whether the perception steps within reasoning chains are grounded accurately in the input image. Their proposed framework is training- and reference-free, decomposing reasoning chains into perception and reasoning components. It employs off-the-shelf VLM judges to evaluate step-level faithfulness and is validated through human meta-evaluation. Furthermore, leveraging this metric, they develop a lightweight self-reflection mechanism that detects and regenerates unfaithful perception steps without additional training. Experimental results across multiple reasoning-trained VLMs and perception-heavy benchmarks demonstrate that this approach effectively reduces the Unfaithful Perception Rate while maintaining final-answer accuracy, thereby improving the reliability and transparency of multimodal reasoning systems. <div>
arXiv:2512.12218v2 Announce Type: replace-cross 
Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-Resolution Reasoning, ambiguity retention, Multi-Vector Embeddings, Non-Collapsing Attention, Contextual Identity Tracking<br /><br />Summary:<br /><br />Current AI systems face a fundamental limitation where they prematurely resolve ambiguity, collapsing multiple valid interpretations into a single output. This issue arises from classical identity assumptions in standard neural architectures. The paper introduces Non-Resolution Reasoning (NRR), a new computational framework that treats ambiguity retention as a valuable reasoning mode rather than a flaw. NRR is built on three core principles: Non-Identity ($A \neq A$), where the same symbol can represent different entities depending on context; Approximate Identity ($A \approx A$), which allows partial but not full overlap between entities; and Non-Resolution, allowing conflicting interpretations to coexist simultaneously without forced convergence. To implement these principles, the authors propose three architectural components: Multi-Vector Embeddings for representing context-dependent meanings, Non-Collapsing Attention to maintain parallel interpretations, and Contextual Identity Tracking (CIT) to enforce the Non-Identity principle during inference. The paper demonstrates NRR’s benefits through case studies in handling paradoxes, enhancing creative generation, and improving context-dependent reasoning. Empirical validation on a synthetic context-shift task reveals that an NRR-lite model achieves 90.9% out-of-distribution accuracy, vastly outperforming standard architectures with 9.1%. This work reframes the role of ambiguity in AI, suggesting the critical question is not whether to resolve ambiguity, but when, how, and controlled by whom. <div>
arXiv:2512.13478v4 Announce Type: replace-cross 
Abstract: Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dion2: A Simple Method to Shrink Matrix in Muon</title>
<link>https://arxiv.org/abs/2512.16928</link>
<guid>https://arxiv.org/abs/2512.16928</guid>
<content:encoded><![CDATA[
<div> Muon optimizer, orthonormalization, scalability, Dion2, sparse update<br /><br />Summary:<br /><br />1. The Muon optimizer is known for its strong empirical performance and solid theoretical foundation, yet it faces scalability challenges due to the super-linear complexity of its orthonormalization step. 2. The orthonormalization step becomes increasingly costly as the size of the matrix it processes grows, creating computational and communication bottlenecks in large-scale applications. 3. Prior research efforts have attempted to address this issue by reducing the size of the matrix involved in orthonormalization, but these methods tend to be complex. 4. The paper introduces Dion2, a simpler approach which improves scalability by shrinking the matrix through sampling, selectively choosing a fraction of rows or columns for orthonormalization at each iteration. 5. This selective sampling results in sparse updates that significantly reduce computational load and communication overhead, thereby enhancing the scalability and efficiency of the Muon optimizer in large-scale environments. <div>
arXiv:2512.16928v1 Announce Type: new 
Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control</title>
<link>https://arxiv.org/abs/2512.16929</link>
<guid>https://arxiv.org/abs/2512.16929</guid>
<content:encoded><![CDATA[
<div> Keywords: upper-limb prostheses, neuro-muscular control, EEG, EMG, low-cost

<br /><br />Summary:  
This project addresses the limitations of affordable upper-limb prostheses, which often lack intuitive control, particularly impacting amputees in low-resource settings. The proposed solution is a low-cost dual-mode neuro-muscular control system that integrates EEG and EMG signals to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG data is captured using a NeuroSky MindWave Mobile 2 headset and processed by an ESP32 microcontroller running a lightweight classification model trained on 1500 seconds of data with specific filtering and data splits. The EEG classifier detects strong blink events that toggle the prosthetic hand between open and closed states. EMG signals, acquired via a MyoWare 2.0 sensor, are sent to a second ESP32 microcontroller to perform threshold-based detection across three activation bands, which control elbow movements with added stability via consecutive frame confirmation. The EEG-controlled ESP32 operates four finger servos, while the EMG-controlled ESP32 manages two elbow servos. A functional prototype was built for around $240, primarily due to the cost of the EEG headset. Future improvements aim to include a 3D-printed chassis, reduced EMG latency through auto-regressive models, and enhanced servo torque for better load capacity and grip strength. This system offers a promising, accessible approach to biologically intuitive prosthetic control for underserved populations and global health applications. <div>
arXiv:2512.16929v1 Announce Type: new 
Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification</title>
<link>https://arxiv.org/abs/2512.16960</link>
<guid>https://arxiv.org/abs/2512.16960</guid>
<content:encoded><![CDATA[
<div> Quantum-inspired machine learning, Kernel Trick, Pretty Good Measurement, Quantum SMOTE, classification performance

<br /><br />Summary:  
This work investigates quantum-inspired machine learning (QiML) approaches that utilize quantum theory concepts to enhance classical algorithms, focusing on inner product structures in high-dimensional feature spaces. It compares two main methods: the kernelized Pretty Good Measurement (KPGM) and direct PGM-based classifiers, both grounded in Hilbert space geometry. The study applies synthetic oversampling techniques using Quantum SMOTE (QSMOTE) variants to evaluate performance under different data augmentation scenarios. Experimental results demonstrate that both PGM and KPGM classifiers outperform classical random forest baselines, especially when multiple quantum copies are used. Notably, PGM with stereo encoding and two quantum copies achieved the highest classification accuracy (0.8512) and F1-score (0.8234). Meanwhile, KPGM showed competitive accuracy with slightly lower peak values but greater stability across QSMOTE variants, particularly in stereo and amplitude encodings. The analysis highlights complementary advantages: PGM benefits from encoding-specific improvements, whereas KPGM offers robustness to variations in synthetic data sampling. These insights advance theoretical understanding and provide practical guidance for choosing between kernel-based and measurement-based QiML methods depending on data features and computational resource considerations. <div>
arXiv:2512.16960v1 Announce Type: new 
Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models</title>
<link>https://arxiv.org/abs/2512.16963</link>
<guid>https://arxiv.org/abs/2512.16963</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Compression, Mixture-of-Experts, Transformer Autoencoder, Distribution Fingerprint<br /><br />Summary:<br /><br />This paper identifies three major challenges faced by current Large Language Models (LLMs): limited context length, high inference costs, and catastrophic forgetting during continual learning. Traditional Mixture-of-Experts (MoE) architectures alleviate some issues but depend on complex auxiliary classifiers for routing, which reduces interpretability. The authors introduce an innovative concept: "Compression is Routing," leveraging compression as a means of expert scheduling. They develop an 87M-parameter Transformer Autoencoder that compresses sequences 64-fold (512 tokens into 8 latent vectors). The model exhibits strong domain discriminative ability, achieving 99.47% reconstruction accuracy on in-domain code data, but accuracy sharply falls to 47.76% on semi-out-of-domain Wiki texts and 0.57% on fully out-of-distribution random sequences. This demonstrates that reconstruction error acts as an intrinsic distribution fingerprint, enabling expert modules to be selected automatically based on residuals rather than explicit gating networks. This approach promises improved scalability and offers a novel perspective on VRAM compression to handle ultra-long contexts efficiently. The study validates the underlying architecture physically and proposes a new research direction for scalable, modular neural networks for future LLM designs. <div>
arXiv:2512.16963v1 Announce Type: new 
Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes</title>
<link>https://arxiv.org/abs/2512.16967</link>
<guid>https://arxiv.org/abs/2512.16967</guid>
<content:encoded><![CDATA[
<div> Keywords: nowcasting, aviation meteorology, physics-guided machine learning, explainable AI, METAR<br /><br />Summary: This study addresses the challenge of short-term prediction (nowcasting) of low-visibility and precipitation events crucial for aviation safety and efficiency. Traditional operational methods depend on numerical weather prediction models and human-issued Terminal Aerodrome Forecasts (TAF), which suffer from conservative biases and limited temporal resolution. The authors propose a lightweight gradient boosting model (XGBoost) trained solely on surface observation data (METAR) and enhanced through physics-guided feature engineering rooted in thermodynamic principles. The framework is tested across 11 global airports encompassing different climate zones, using historical data spanning 2000 to 2024. Results demonstrate that the model effectively captures local physical processes automatically, without manual tuning. In a blind evaluation against operational TAFs, the model significantly outperformed TAFs at tactical horizons (3 hours), improving recall by 2.5 to 4 times while reducing false alarms. Additionally, SHAP (SHapley Additive exPlanations) analysis indicates the model implicitly reconstructs key physical processes such as advection, radiation, and subsidence, offering explainability and actionable insights for operational situational awareness. This work emphasizes the value of integrating physics-guided machine learning and explainable AI to enhance real-time weather nowcasting relevant to aviation. <div>
arXiv:2512.16967v1 Announce Type: new 
Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</title>
<link>https://arxiv.org/abs/2512.17008</link>
<guid>https://arxiv.org/abs/2512.17008</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Group Relative Policy Optimization, Proximal Policy Optimization, turn-PPO, multi-turn tasks<br /><br />Summary:<br /><br />This paper addresses limitations of the Group Relative Policy Optimization (GRPO) algorithm when applied to reinforcement learning (RL) in multi-turn interactive tasks involving long-horizon reasoning. The authors argue that GRPO struggles with stability and effectiveness in these scenarios. To improve performance, they investigate alternative advantage estimation methods and identify Proximal Policy Optimization (PPO) as a more robust solution compared to GRPO. Further advancing PPO, they propose a novel variant called turn-PPO, which reframes the problem using a turn-level Markov Decision Process (MDP) instead of the conventional token-level MDP, making it better suited for multi-turn environments. Experimental evaluations on the WebShop and Sokoban benchmarks show that turn-PPO achieves superior results over existing methods, demonstrating improved stability and effectiveness both in tasks that require long reasoning chains and those that do not. This work contributes to the development of more reliable RL algorithms for training large language model agents capable of handling complex, multi-turn interactions in real-world environments. <div>
arXiv:2512.17008v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.17034</link>
<guid>https://arxiv.org/abs/2512.17034</guid>
<content:encoded><![CDATA[
<div> Keywords: non-stationary environments, deep reinforcement learning, Gradient-Boosted Deep Q-Networks, ensemble learning, Bellman residual<br /><br />Summary:<br /><br />This paper addresses the challenge of non-stationary environments in deep reinforcement learning, where changes in dynamics or rewards disrupt learned value functions and lead to catastrophic forgetting. The authors propose Gradient-Boosted Deep Q-Networks (GB-DQN), an ensemble-based adaptive method that mitigates model drift by incrementally learning residuals rather than retraining a single Q-network from scratch. GB-DQN builds an additive ensemble where each new learner approximates the Bellman residual of the current ensemble after environmental changes. Theoretical analysis demonstrates that each boosting iteration systematically reduces the empirical Bellman residual, and under standard assumptions, the ensemble converges to the post-drift optimal value function. Empirical evaluations on a range of control tasks with controlled dynamics alterations show that GB-DQN enables faster adaptation, improved stability, and enhanced robustness compared to the classic DQN and existing non-stationary reinforcement learning baselines. This work contributes both a principled theoretical framework and practical algorithmic advances for reinforcement learning agents operating in evolving environments. <div>
arXiv:2512.17034v1 Announce Type: new 
Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples</title>
<link>https://arxiv.org/abs/2512.17051</link>
<guid>https://arxiv.org/abs/2512.17051</guid>
<content:encoded><![CDATA[
<div> Keywords: distribution restoration, noisy samples, entropic optimal transport, SFBD-OMNI, black-box corruption model<br /><br />Summary:<br /><br />This paper addresses the challenge of restoring the true underlying data distribution when only noisy, partially observed samples are available, a common scenario in many real-world applications where fully observed data is costly or infeasible to collect. The authors propose framing the problem as a one-sided entropic optimal transport task and present an EM-like algorithm for its solution. They introduce a test criterion to assess whether the original distribution can be recovered despite per-sample information loss, highlighting conditions for recoverability. For cases where recovery is not possible from noisy data alone, the study shows that incorporating a small number of clean samples can substantially improve reconstruction accuracy. Based on these theoretical insights, the authors develop SFBD-OMNI, a novel bridge model-based framework that generalizes prior Stochastic Forward-Backward Deconvolution (SFBD) methods, extending applicability beyond Gaussian noise to arbitrary corruption models treated as black-box generators. Experiments conducted on multiple benchmark datasets under a variety of measurement models demonstrate that SFBD-OMNI achieves notable qualitative and quantitative improvements in restoring true distributions compared to existing approaches, validating the practical effectiveness of their method. <div>
arXiv:2512.17051v1 Announce Type: new 
Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Tool Dependency Retrieval for Efficient Function Calling</title>
<link>https://arxiv.org/abs/2512.17052</link>
<guid>https://arxiv.org/abs/2512.17052</guid>
<content:encoded><![CDATA[
<div> Function calling, Large Language Models, tool retrieval, dynamic context, task automation<br /><br />Summary:<br /><br />This paper addresses the challenge of tool selection for function calling agents powered by Large Language Models (LLMs), focusing on improving on-device agent efficiency through better retrieval methods. Existing approaches rely on static, limited inputs which fail to capture multi-step dependencies among tools and do not adapt to an evolving task context, often resulting in irrelevant tool retrieval and decreased performance. The authors propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial user query and the unfolding execution context, enabling adaptive and context-aware selection of relevant tools. DTDR leverages function calling demonstrations to learn tool dependencies and dynamically updates the retrieval process as the plan develops. The method is benchmarked against state-of-the-art static retrieval techniques across multiple datasets and different LLM backbones, with evaluations measuring retrieval precision, downstream task accuracy, and computational efficiency. Results demonstrate that DTDR significantly improves function calling success rates, with gains ranging from 23% to 104% compared to existing static retrievers. Additionally, the study explores strategies for effectively integrating the retrieved tools into LLM prompts to further boost performance in task automation scenarios. <div>
arXiv:2512.17052v1 Announce Type: new 
Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III</title>
<link>https://arxiv.org/abs/2512.17058</link>
<guid>https://arxiv.org/abs/2512.17058</guid>
<content:encoded><![CDATA[
<div> Keywords: k-nearest neighbour classifier, universal consistency, Lebesgue--Besicovitch differentiation, sigma-finite dimensionality, Nagata dimension<br /><br />Summary:<br /><br />This article establishes the final missing link in proving the equivalence between three important properties in the context of complete separable metric spaces \( X \). First, it deals with the universal weak consistency of the \( k \)-nearest neighbour classifier in \( X \). Second, it considers the strong Lebesgue--Besicovitch differentiation property holding for every locally finite Borel measure on \( X \). Third, it addresses the characterization of \( X \) being sigma-finite dimensional in the sense defined by Nagata. Previous works had shown the equivalence between the differentiation property and sigma-finite dimensionality, as well as the differentiation property implying universal consistency. The key contribution of this article is proving the last missing implication: that if the \( k \)-nearest neighbour classifier is universally weakly consistent, then the space \( X \) must be sigma-finite dimensional. This completes the proof of equivalence of all three conditions. The paper also revisits and corrects an incorrect assertion made in a recent follow-up article from 2024, and thereby clarifies the conjecture initially posed in 2020 by the same authors. The results unify and deepen the understanding of the geometric and measure-theoretic properties underpinning universal consistency of classifiers. <div>
arXiv:2512.17058v1 Announce Type: new 
Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by C\'erou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation</title>
<link>https://arxiv.org/abs/2512.17073</link>
<guid>https://arxiv.org/abs/2512.17073</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, offloading, static uniform quantization, low-rank compensators, bandwidth-accuracy trade-off<br /><br />Summary:<br /><br />1. Mixture-of-Experts (MoE) models achieve scalable capacity through sparse activation but face challenges in memory usage and bandwidth demands. 2. Offloading techniques help reduce GPU memory pressure by fetching experts only when needed; however, token-level routing introduces irregular data transfers, leading to inference that is I/O-bound. 3. Existing static uniform quantization methods reduce communication traffic but tend to degrade accuracy significantly under aggressive compression because they do not account for the heterogeneity among experts. 4. The paper introduces Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, a method that performs router-guided precision restoration by leveraging precomputed low-rank compensators. 5. During inference, this approach transfers compact low-rank factors for the top-n experts per token (where n is smaller than k) and applies compensation to them, while keeping other experts in low-bit precision, effectively balancing bandwidth and accuracy. 6. When integrated with offloading on GPU and GPU-NDP systems, this method demonstrates a superior trade-off between bandwidth usage and model accuracy, alongside improved throughput in inference tasks. <div>
arXiv:2512.17073v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</title>
<link>https://arxiv.org/abs/2512.17079</link>
<guid>https://arxiv.org/abs/2512.17079</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought prompting, error recovery, reinforcement learning, mathematical reasoning, large language models<br /><br />Summary:<br /><br />1. Chain-of-thought (CoT) prompting is essential for enabling large language models (LLMs) to perform mathematical reasoning, but these models are vulnerable to early errors that can cause entire solutions to fail.  
2. The study explores whether training LLMs with intentionally flawed reasoning traces can help the models detect and recover from mistakes without reducing their problem-solving capabilities.  
3. Using competition-level math problems from MATH-lighteval, the researchers generated CoT prefixes containing exactly one controlled error, either a calculation error (e.g., sign flips, dropped terms) or a reasoning error (e.g., misapplied rules, unjustified steps).  
4. The Qwen3-4B model was fine-tuned with a reinforcement learning method (GRPO) using a binary reward based on the final answer's correctness.  
5. The Mixed-CoT-RL model, trained on both clean and flawed reasoning, achieved comparable performance on clean problems (41%) to standard RL fine-tuning but significantly outperformed on problems with flawed reasoning traces (24% vs. 19%).  
6. Conversely, conventional RL trained only on clean examples reduced robustness on flawed prefills below the baseline (19% vs. 20%), showing increased sensitivity to errors.  
7. Exposure to reasoning errors during training offered greater robustness improvements than exposure to calculation errors alone, with combined training producing the best outcomes.  
8. The results indicate that deliberately training on flawed reasoning can improve LLMs' ability to recover from mistakes, paving the way for more reliable mathematical reasoning in large language models. <div>
arXiv:2512.17079v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Square Tensor Networks and Circuits Without Squaring Them</title>
<link>https://arxiv.org/abs/2512.17090</link>
<guid>https://arxiv.org/abs/2512.17090</guid>
<content:encoded><![CDATA[
<div> Keywords: squared tensor networks, squared circuits, marginalization, unitary parameterization, distribution estimation<br /><br />Summary:<br /><br />This article addresses the complexity challenges in computing partition functions and marginalizing variables in squared tensor networks (TNs) and their extension, squared circuits, which are used for expressive distribution estimation. The squaring operation in these models adds computational overhead, limiting their practical use in machine learning tasks. To mitigate this, canonical forms of TNs parameterized via unitary matrices have been developed to simplify marginalization. However, these canonical forms do not extend naturally to squared circuits due to their ability to represent factorizations that do not correspond directly to known TN structures. The authors introduce a novel approach inspired by orthogonality principles in canonical forms and determinism in circuits, enabling efficient parameterization of squared circuits. This new parameterization reduces the computational burden of marginalization even for factorizations encoded by circuits with otherwise intractable structure. Experimental results on distribution estimation demonstrate that these conditions on squared circuits do not reduce expressiveness but significantly improve learning efficiency. Thus, the work presents a key advance in making squared circuits more computationally feasible without compromising their representational power. <div>
arXiv:2512.17090v1 Announce Type: new 
Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</title>
<link>https://arxiv.org/abs/2512.17091</link>
<guid>https://arxiv.org/abs/2512.17091</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical planning, reinforcement learning, MPC, MPPI sampler, adaptive sampling<br /><br />Summary:  
This work introduces a novel approach that integrates reinforcement learning (RL) with Model Predictive Control (MPC) to address hierarchical planning problems. The method establishes a tight coupling between RL actions and the MPPI (Model Predictive Path Integral) sampler, allowing RL to guide the sampling process. It then adaptively aggregates MPPI samples to enhance value estimation, focusing exploration efforts where uncertainty is higher. This adaptive interplay between RL and MPC yields improved training robustness and better policy outcomes. The approach is validated across multiple challenging domains, including race driving, a modified Acrobot, and the Lunar Lander with added obstacles, demonstrating versatility. Experimental results highlight significant improvements in data efficiency and overall task performance, showcasing a 72% increase in success rates compared to existing methods. Moreover, the proposed adaptive sampling accelerates convergence by over twofold relative to non-adaptive techniques. Overall, the study delivers a robust hierarchical planning framework that efficiently balances exploration and exploitation, adapts seamlessly to diverse applications, and outperforms current state-of-the-art approaches in both reward metrics and task success. <div>
arXiv:2512.17091v1 Announce Type: new 
Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</title>
<link>https://arxiv.org/abs/2512.17100</link>
<guid>https://arxiv.org/abs/2512.17100</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactual explanations, multivariate time series, model interpretability, ECG classification, deep learning  

<br /><br />Summary:  
This study introduces UniCoMTE, a novel model-agnostic framework designed to generate counterfactual explanations specifically for multivariate time series classifiers. UniCoMTE aims to improve trust and transparency in deep neural networks by identifying the temporal features that most significantly influence classification decisions. The framework works by modifying input samples and analyzing the effect on model predictions, and is versatile enough to be applied across diverse model architectures while operating directly on raw time series data. The authors evaluate UniCoMTE using an ECG classification task, a critical healthcare application, comparing its explanation quality with established methods like LIME and SHAP. The comparison emphasizes comprehensibility and the ability of explanations to generalize across similar samples. Clinical relevance is further validated through expert feedback collected via questionnaires, where medical professionals reviewed counterfactual explanations alongside original ECG data. Results demonstrate that UniCoMTE provides concise, stable, and human-aligned explanations that surpass existing approaches in clarity and practical use. By linking predictive outcomes to meaningful signal patterns, UniCoMTE advances the interpretability of deep learning models, facilitating their adoption in high-stakes time series applications such as healthcare diagnostics. <div>
arXiv:2512.17100v1 Announce Type: new 
Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models</title>
<link>https://arxiv.org/abs/2512.17107</link>
<guid>https://arxiv.org/abs/2512.17107</guid>
<content:encoded><![CDATA[
<div> Fault Diagnosis, Photovoltaic Arrays, Differentiable Simulation, Gradient-Based Identification, I-V Curve Reconstruction<br /><br />Summary:<br /><br />1. The paper addresses the critical need for accurate fault diagnosis and quantification to ensure the reliable operation and intelligent maintenance of photovoltaic (PV) arrays.<br /><br />2. It identifies limitations in existing fault quantification methods, particularly in terms of efficiency and interpretability.<br /><br />3. To overcome these challenges, the authors propose a novel fault quantification approach based on a Differentiable Fast Fault Simulation Model (DFFSM), which accurately simulates I-V characteristics under multiple fault conditions.<br /><br />4. The DFFSM provides analytical gradients with respect to fault parameters, enabling the use of a gradient-based fault parameter identification (GFPI) method.<br /><br />5. The GFPI employs the Adahessian optimizer to efficiently quantify common faults such as partial shading, short-circuit, and series-resistance degradation.<br /><br />6. Experimental validation on both simulated and real measured I-V curve data demonstrates that the proposed method achieves high accuracy, maintaining I-V reconstruction errors below 3%.<br /><br />7. The results confirm the feasibility and effectiveness of using differentiable physical simulators combined with gradient-based optimization for fault diagnosis in PV systems. <div>
arXiv:2512.17107v1 Announce Type: new 
Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse</title>
<link>https://arxiv.org/abs/2512.17108</link>
<guid>https://arxiv.org/abs/2512.17108</guid>
<content:encoded><![CDATA[
<div> Keywords: video-language models, on-device system, module reuse, latency reduction, mobile execution efficiency<br /><br />Summary: Recent developments in video-language models have enabled complex tasks such as video retrieval, captioning, and assembly but face challenges when executed on mobile devices due to inefficient pipeline management. The paper presents Atom, an innovative on-device system designed to optimize video-language pipelines by decomposing large billion-parameter models into modular components like visual encoders and language decoders. Atom focuses on module reuse across various subtasks, including captioning, reasoning, and indexing, which eliminates redundant model loading and enables parallel execution. This reuse-centric approach significantly reduces end-to-end latency while maintaining performance quality. Empirical results demonstrate that Atom achieves 27–33% faster execution on commodity smartphones compared to traditional non-reuse methods, with only slight performance drops observed in key metrics such as Recall@1 for retrieval (≤ 2.3) and CIDEr for captioning (≤ 1.5). Consequently, Atom offers a practical and scalable solution for efficient video-language understanding specifically tailored for edge devices, addressing both speed and resource constraints without compromising accuracy or capability. <div>
arXiv:2512.17108v1 Announce Type: new 
Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Training and Merging Through Momentum-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.17109</link>
<guid>https://arxiv.org/abs/2512.17109</guid>
<content:encoded><![CDATA[
<div> Low-rank structure, parameter importance, curvature-aware merging, model composition, optimization trajectory  

<br /><br />Summary:  
This paper addresses the inefficiency and isolation in current workflows that handle large neural network training and task-specific model merging by separately computing curvature information. The authors propose a unified framework that maintains factorized momentum and curvature statistics throughout training, allowing this valuable information to be reused for geometry-aware model composition without redundant computation. Their method matches the memory efficiency of state-of-the-art approaches while accumulating task saliency scores, enabling curvature-aware merging without the need for additional Fisher information post-training. They provide convergence guarantees for non-convex objectives, with approximation errors linked to gradient singular value decay. Experiments on natural language understanding benchmarks demonstrate that curvature-aware parameter selection outperforms magnitude-based baselines at all sparsity levels and that multi-task model merging improves upon strong existing baselines. The framework also achieves rank-invariant convergence and shows greater robustness to hyperparameter settings compared to other low-rank optimizers. By conceptualizing the optimization trajectory as a reusable asset rather than discarding it, this approach eliminates redundant computational steps and enables more principled, effective model merging and parameter selection. <div>
arXiv:2512.17109v1 Announce Type: new 
Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts</title>
<link>https://arxiv.org/abs/2512.17111</link>
<guid>https://arxiv.org/abs/2512.17111</guid>
<content:encoded><![CDATA[
<div> Old Nepali, Handwritten Text Recognition, Encoder-Decoder, Character Error Rate, Low-resource Language<br /><br />Summary: This paper introduces the first comprehensive end-to-end pipeline designed for Handwritten Text Recognition (HTR) specifically tailored to Old Nepali, an important but under-resourced historical language. The approach focuses on line-level transcription, allowing for more contextual recognition compared to isolated character recognition. The authors systematically investigate different encoder-decoder architectures, exploring how various model designs impact the overall recognition accuracy. Complementing architecture choices, data-centric techniques are applied to enhance performance further. The best performing model achieves a remarkable Character Error Rate (CER) of 4.9%, indicating high accuracy in recognizing handwritten Old Nepali text. To gain deeper insights into the model’s behavior and error distribution, the study includes a detailed analysis of token-level confusions and evaluates different decoding strategies. Although the dataset used for evaluation remains confidential, the authors contribute to the research community by releasing their training code, model configurations, and evaluation scripts. This enables other researchers to replicate the study or extend it to other low-resource historical scripts. Overall, the work represents a significant step forward in applying modern HTR techniques to ancient and low-resource languages, promoting further development and preservation of historical scripts. <div>
arXiv:2512.17111v1 Announce Type: new 
Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</title>
<link>https://arxiv.org/abs/2512.17121</link>
<guid>https://arxiv.org/abs/2512.17121</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, medical imaging, negation, chest X-ray retrieval, fine tuning  

<br /><br />Summary:  
This study focuses on the challenges faced by large vision-language models like CLIP in medical imaging tasks, specifically their difficulty in interpreting negated phrases—a critical issue for accurate medical diagnosis. The researchers evaluated the Stanford AIMI CheXagent model’s performance in retrieving chest X-ray images using queries both with and without negation. The primary objective was to identify failure points in handling negation and subsequently improve retrieval accuracy by applying fine tuning methods derived from prior research. The results indicate that fine tuning enhances the model’s ability to process negation, although there is a minor reduction in accuracy when responding to positive (non-negated) prompts. Beyond retrieval accuracy, the study analyzed the internal behavior of the text encoder through methods such as token attribution, t-SNE visualization, and attention-head ablation, shedding light on how fine tuning alters the model’s internal representation of negated clinical language. This work aims to deepen understanding of CLIP’s internal mechanisms and improve its reliability for clinical applications by making it better suited for handling negated statements in medical text, ultimately contributing to more dependable AI-assisted medical devices. <div>
arXiv:2512.17121v1 Announce Type: new 
Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</title>
<link>https://arxiv.org/abs/2512.17129</link>
<guid>https://arxiv.org/abs/2512.17129</guid>
<content:encoded><![CDATA[
<div> Keywords: morphogenesis, SE(3)-equivariant graph neural network, 3D Zernike polynomials, shape-matching loss, implicit differentiation<br /><br />Summary: Biological systems form complex 3D structures through distributed control among identical agents without central coordination. This work introduces DiffeoMorph, an end-to-end differentiable framework that learns morphogenesis protocols guiding agents to form desired 3D shapes. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network influenced by signals from peers, ensuring spatial and rotational equivariance. To compare predicted and target shapes continuously rather than as point clouds, the authors develop a novel shape-matching loss based on 3D Zernike polynomials, which is invariant to agent ordering, population size, and rigid transformations. They enforce full SO(3) invariance (rotation invariant but sensitive to reflections) by including an alignment step that optimally rotates the predicted Zernike spectrum to minimize the loss. This creates a bilevel optimization problem with an inner loop optimizing a unit quaternion for alignment and an outer loop updating the agent model. Gradients through the alignment step are computed using implicit differentiation. Extensive benchmarks show the superiority of their shape-matching loss over standard metrics. Finally, DiffeoMorph demonstrates the ability to form a variety of shapes, from simple ellipsoids to complex morphologies, using only minimal spatial cues. <div>
arXiv:2512.17129v1 Announce Type: new 
Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</title>
<link>https://arxiv.org/abs/2512.17131</link>
<guid>https://arxiv.org/abs/2512.17131</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalized Primal Averaging, Nesterov's method, averaging-based optimizers, AdamW, convergence guarantee<br /><br />Summary:<br /><br />1. The paper introduces Generalized Primal Averaging (GPA), an extension of Nesterov's primal averaging method designed to overcome limitations in recent averaging-based optimizers like single-worker DiLoCo and Schedule-Free (SF) in non-distributed training.<br /><br />2. Existing methods such as Schedule-Free maintain uniform averages of past weights, while single-worker DiLoCo applies implicit averaging through periodic pseudo-gradient aggregation, which incurs increased memory use and extra hyperparameters due to a two-loop structure.<br /><br />3. GPA addresses these drawbacks by decoupling the interpolation constant in Nesterov's primal averaging, allowing smooth averaging at every iteration and generalizing the single-worker DiLoCo approach.<br /><br />4. Empirical results demonstrate that GPA consistently outperforms single-worker DiLoCo, simplifies hyperparameter tuning, eliminates the two-loop overhead, and reduces memory consumption to just one additional buffer.<br /><br />5. On benchmark models such as Llama-160M and ImageNet ViT, GPA achieves significant speedups (up to ~27%) in reaching validation performance equivalent to AdamW. Theoretical analysis also guarantees that GPA can match or improve upon the base optimizer’s convergence rate, characterized by $O(\sqrt{T})$ regret, through appropriate interpolation constants. <div>
arXiv:2512.17131v1 Announce Type: new 
Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing</title>
<link>https://arxiv.org/abs/2512.17161</link>
<guid>https://arxiv.org/abs/2512.17161</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed learning, spectrum access, Gale-Shapley stability, restless bandits, interference graphs<br /><br />Summary:<br /><br />This paper addresses the problem of distributed spectrum access and sharing among multiple cognitive communication entities (called cells) in wireless networks constrained by interference modeled via graphs. The aim is to achieve a globally stable and interference-aware channel allocation defined by a generalized Gale-Shapley multi-to-one matching framework. The setting considers L cells sharing S orthogonal channels, where neighboring cells cannot use the same channel simultaneously. Each channel is modeled as an unknown restless Markov process with cell-specific rewards, making the problem stochastic and temporally varying. The authors propose SMILE (Stable Multi-matching with Interference-aware Learning), a novel distributed learning algorithm that combines restless bandit approaches with coordination constrained by the interference graph. SMILE is communication-efficient and balances exploration versus exploitation of channels in a distributed fashion. The paper proves that SMILE converges to the globally optimal stable channel allocation and achieves logarithmic regret compared to an oracle with full knowledge of expected utilities. Finally, simulations confirm the theoretical findings and demonstrate SMILE’s robustness, scalability, and efficiency in various spectrum-sharing scenarios. <div>
arXiv:2512.17161v1 Announce Type: new 
Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions</title>
<link>https://arxiv.org/abs/2512.17198</link>
<guid>https://arxiv.org/abs/2512.17198</guid>
<content:encoded><![CDATA[
<div> Keywords: BumpNet, PDE numerical solution, meshless basis functions, physics-informed neural networks, operator learning<br /><br />Summary:  
1. The article introduces BumpNet, a sparse neural network framework designed for solving partial differential equations (PDEs) and learning operators associated with PDEs.  
2. BumpNet utilizes meshless basis function expansions similar to radial-basis function (RBF) networks but uniquely constructs basis functions from ordinary sigmoid activations, enabling the use of advanced training techniques optimized for these functions.  
3. All parameters of each basis function—shape, location, and amplitude—are fully trainable, and the model enforces parsimony and h-adaptivity by dynamically pruning basis functions during training.  
4. BumpNet serves as a flexible framework that can be integrated with various neural architectures for PDE tasks, such as Bump-PINNs for general PDE solutions using collocation training, Bump-EDNNs for time-evolution PDEs combining BumpNet spatial representation with evolutionary deep neural networks for temporal advancement, and Bump-DeepONets incorporating BumpNet as the trunk network in Deep Operator Networks for operator learning.  
5. Extensive numerical experiments confirm that BumpNet-based architectures achieve significant improvements in both efficiency and accuracy compared to existing methods for PDE numerical solution and operator learning. <div>
arXiv:2512.17198v1 Announce Type: new 
Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning solution operator of dynamical systems with diffusion maps kernel ridge regression</title>
<link>https://arxiv.org/abs/2512.17203</link>
<guid>https://arxiv.org/abs/2512.17203</guid>
<content:encoded><![CDATA[
<div> Keywords: kernel ridge regression, diffusion maps, dynamical systems, long-term prediction, data-driven modeling  

<br /><br />Summary:  
This paper addresses the challenge of predicting complex nonlinear dynamical systems over long time horizons, where traditional data-driven models often struggle due to unknown or poorly represented geometric structures governing system behavior. The authors propose a novel method called Diffusion Maps Kernel Ridge Regression (DM-KRR), which integrates kernel ridge regression with a kernel derived from diffusion maps, capturing the intrinsic geometry of the system’s invariant set without requiring explicit manifold reconstruction or attractor modeling. A key innovation is the use of a dynamics-aware validation strategy that ensures model selection respects the underlying geometry of the data. Extensive experiments demonstrate that DM-KRR outperforms cutting-edge approaches, including random feature models, neural networks, and operator-learning methods, across a variety of systems such as smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows. The study highlights that long-term predictive ability is not solely dependent on model complexity but significantly benefits from incorporating geometric constraints encoded in the data. Ultimately, the paper shows that a simple, geometry-informed model can deliver superior accuracy and data efficiency, pointing towards a promising direction for the reliable and efficient learning of complex dynamical systems. <div>
arXiv:2512.17203v1 Announce Type: new 
Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</title>
<link>https://arxiv.org/abs/2512.17257</link>
<guid>https://arxiv.org/abs/2512.17257</guid>
<content:encoded><![CDATA[
<div> electric vehicles, charging demand, time series forecasting, urban datasets, spatial-temporal analysis  

<br /><br />Summary:  
This article addresses the emerging challenge of predicting electric vehicle (EV) charging demand, which is increasingly vital due to the rising adoption of EVs and their impact on electric grid management. The study systematically compares five time series forecasting models, including traditional statistical, machine learning, and deep learning approaches, to evaluate their effectiveness. Forecasts are examined across multiple temporal horizons—short-term (minutes), mid-term (hours), and long-term (days)—to assess model performance at different timescales. Additionally, the research investigates spatial aggregation levels from single charging stations to regional and city-wide aggregations, thereby capturing demand patterns at various scales. The evaluation is conducted on four publicly available, real-world datasets from diverse urban environments, allowing for a comprehensive and comparative analysis. The results are reported independently for each dataset, ensuring detailed insights into model robustness and applicability. Importantly, this study is the first to systematically assess EV charging demand forecasting across such a broad spectrum of temporal and spatial dimensions using multiple datasets, providing valuable guidance for both researchers and practitioners aiming to improve grid management and planning in the context of growing EV penetration. <div>
arXiv:2512.17257v1 Announce Type: new 
Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS</title>
<link>https://arxiv.org/abs/2512.17262</link>
<guid>https://arxiv.org/abs/2512.17262</guid>
<content:encoded><![CDATA[
<div> QoS prediction, service-oriented computing, hyperbolic convolution, multi-task learning, loss balancing<br /><br />Summary:<br /><br />1. The article addresses the challenge of predicting multiple Quality of Service (QoS) parameters simultaneously in service-oriented computing, where data is sparse, noisy, and influenced by hierarchical and contextual dependencies such as network and geographic factors.  
2. Existing methods commonly predict QoS parameters independently, leading to increased computational costs and poor generalization, while recent joint prediction approaches suffer from negative transfer due to inconsistent loss scaling and insufficient representation learning.  
3. The paper proposes SHARP-QoS, a unified strategy combining three core components: hyperbolic convolution in the Poincaré ball for capturing hierarchical QoS and contextual features; an adaptive feature-sharing mechanism with gated fusion to dynamically select and share informative features across QoS and context domains; and an EMA-based loss balancing approach to stabilize joint optimization and mitigate negative transfer.  
4. SHARP-QoS is evaluated on three datasets involving two to four QoS parameters, consistently outperforming single-task and multi-task baseline models in accuracy and robustness.  
5. The comprehensive analysis confirms SHARP-QoS effectively tackles challenges such as data sparsity, outliers, and cold-start scenarios while maintaining moderate computational overhead, making it a dependable solution for joint QoS prediction. <div>
arXiv:2512.17262v1 Announce Type: new 
Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincar\'e ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Analysis of State Similarity Between Markov Decision Processes</title>
<link>https://arxiv.org/abs/2512.17265</link>
<guid>https://arxiv.org/abs/2512.17265</guid>
<content:encoded><![CDATA[
<div> bisimulation metric, Markov decision process, state similarity, reinforcement learning, policy transfer  

<br /><br />Summary:  
This paper introduces a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of Markov decision processes (MDPs). Unlike the traditional bisimulation metric (BSM), which focuses on state similarity within a single MDP, GBSM extends this concept to multiple MDPs, addressing prior challenges due to the lack of mathematical rigor in earlier attempts. The authors rigorously prove that GBSM satisfies three fundamental metric properties: symmetry, the inter-MDP triangle inequality, and a distance bound on identical state spaces. Leveraging these properties, the paper provides theoretical analysis and explicit bounds for key reinforcement learning tasks involving multiple MDPs, including policy transfer, state aggregation, and sampling-based estimation. These bounds are shown to be tighter than those derived from the standard BSM, thereby improving theoretical guarantees for multi-MDP scenarios. Furthermore, GBSM offers a closed-form sample complexity for estimation, enhancing practical utility compared to existing asymptotic analyses. Numerical experiments presented support and validate the theoretical contributions, demonstrating GBSM's effectiveness in improving state similarity evaluations and downstream RL tasks across multiple MDPs. This work thus represents a significant advancement in extending bisimulation concepts to broader, multi-environment reinforcement learning frameworks. <div>
arXiv:2512.17265v1 Announce Type: new 
Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Generalization in Role-Playing Models via Information Theory</title>
<link>https://arxiv.org/abs/2512.17270</link>
<guid>https://arxiv.org/abs/2512.17270</guid>
<content:encoded><![CDATA[
<div> Role-playing models, distribution shifts, R-EMID, reinforcement learning, generalization performance  

<br /><br />Summary:  
Role-playing models (RPMs) commonly applied in real-world scenarios tend to underperform when deployed outside their original training environments due to distribution shifts involving users, characters, and dialogue compositions. Existing evaluation methods such as LLM-as-a-judge are inadequate for diagnosing how these shifts specifically impact RPM generalization, and there is a lack of formal frameworks to characterize this degradation. To address these challenges, the paper introduces a novel information-theoretic metric called reasoning-based effective mutual information difference (R-EMID), which quantifies RPM performance degradation in an interpretable manner. The authors derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically analyze how different types of shifts contribute to performance loss. Additionally, they propose a co-evolving reinforcement learning framework designed to model the interconnected dynamics among user, character, and dialogue context; this framework improves the estimation of dialogue response generation probabilities, a key component for calculating R-EMID. Empirical evaluation using R-EMID reveals that user shifts pose the most significant risk to RPM generalization, while reinforcement learning methods prove to be the most effective strategy for improving RPM robustness and performance across distributional changes. <div>
arXiv:2512.17270v1 Announce Type: new 
Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2512.17273</link>
<guid>https://arxiv.org/abs/2512.17273</guid>
<content:encoded><![CDATA[
<div> Keywords: integro-differential equations, nonlocal operators, neural networks, Kolmogorov-Arnold Networks, computational efficiency<br /><br />Summary:<br /><br />This paper addresses the computational challenges in solving integro-differential equations (IDEs) that describe nonlocal spatiotemporal physical systems, where classical methods suffer from high costs due to repeatedly evaluating convolution integrals. The authors propose a novel framework called Memory-Informed Neural Pseudo-Operator (MINPO) that unifies modeling nonlocal dynamics caused by long-range spatial interactions and temporal memory effects. MINPO employs neural network encoders—either Kolmogorov-Arnold Networks (KANs) or multilayer perceptrons (MLPs)—to learn both the nonlocal operator and its inverse directly, enabling explicit reconstruction of unknown solution fields. To ensure consistency between the learned operators and solutions, a lightweight nonlocal consistency loss term is introduced during training. This approach effectively captures and resolves a wide variety of IDEs and fractional partial differential equations (PDEs), naturally handling diverse kernel types and dimensionalities. The efficacy of MINPO is validated by comparison against classical numerical methods and recent neural-based approaches such as A-PINN, fPINN, and their KAN variants (A-PIKAN, fPIKAN), demonstrating improved accuracy and robustness. Additionally, MINPO manages the computational demands associated with repeated integral evaluations efficiently. Overall, MINPO offers a generalized, unified framework that extends beyond problem-specific solvers for nonlocal operator-driven systems. <div>
arXiv:2512.17273v1 Announce Type: new 
Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alzheimer's Disease Brain Network Mining</title>
<link>https://arxiv.org/abs/2512.17276</link>
<guid>https://arxiv.org/abs/2512.17276</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, semi-supervised learning, neuroimaging, optimal transport, label propagation<br /><br />Summary:<br /><br />1. The study addresses the challenge of limited ground truth labels in Alzheimer's disease (AD) diagnosis due to expensive and invasive clinical assessments.  
2. It proposes MATCH-AD (Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease), a semi-supervised framework combining deep representation learning, graph-based label propagation, and optimal transport theory.  
3. MATCH-AD leverages the manifold structure in neuroimaging data to effectively propagate labels from a small labeled set to a large unlabeled population.  
4. The framework uses Wasserstein distances to quantify disease progression across different cognitive states, enhancing diagnostic precision.  
5. Evaluated on nearly 5,000 subjects from the National Alzheimer's Coordinating Center, using structural MRI, cerebrospinal fluid biomarkers, and clinical variables, MATCH-AD achieves near-perfect diagnostic accuracy despite fewer than one-third of subjects being labeled.  
6. The method significantly outperforms baseline models, moving from weak agreement to almost perfect agreement, indicating a qualitative improvement in diagnostic reliability.  
7. MATCH-AD maintains clinical usefulness even with severe label scarcity, backed by theoretical guarantees on label propagation error and transport stability.  
8. The results demonstrate that principled semi-supervised learning can unlock diagnostic potential from vast partially annotated neuroimaging datasets, reducing annotation burdens while preserving clinical accuracy suitable for deployment. <div>
arXiv:2512.17276v1 Announce Type: new 
Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</title>
<link>https://arxiv.org/abs/2512.17299</link>
<guid>https://arxiv.org/abs/2512.17299</guid>
<content:encoded><![CDATA[
<div> Continual learning, edge platforms, mixed-signal architecture, minion recurrent unit, energy efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of performing continual learning on edge platforms, where traditional recurrent networks are hindered by energy-intensive training and frequent data movement, making them unsuitable for embedded environments. It introduces M2RU, a novel mixed-signal architecture implementing the minion recurrent unit designed for efficient temporal processing with on-chip continual learning capabilities. The architecture incorporates weighted-bit streaming, allowing multi-bit digital inputs to be processed directly in crossbar arrays without requiring high-resolution conversions, reducing complexity and power usage. To stabilize learning amidst domain shifts, M2RU integrates an experience replay mechanism. Performance metrics demonstrate that M2RU achieves 15 GOPS while consuming only 48.62 mW, resulting in an energy efficiency of 312 GOPS per watt. On standard benchmarks such as sequential MNIST and CIFAR-10, M2RU retains accuracy within 5% of software baselines, showcasing its practical efficacy. When compared to a conventional CMOS digital design, this accelerator offers a 29-fold improvement in energy efficiency. Furthermore, device-aware analysis projects an operational lifetime of approximately 12.2 years under continual learning workloads. Collectively, these results position M2RU as a scalable, energy-efficient, and real-time adaptable solution for temporal intelligence in edge-level deployments. <div>
arXiv:2512.17299v1 Announce Type: new 
Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</title>
<link>https://arxiv.org/abs/2512.17316</link>
<guid>https://arxiv.org/abs/2512.17316</guid>
<content:encoded><![CDATA[
<div> Keywords: inherent explainability, graph theory, structure-local explanation, Cox proportional hazards model, regulatory compliance  

<br /><br />Summary:  
This work addresses the lack of a consistent definition and test for inherent explainability in Explainable Artificial Intelligence (XAI). The authors propose a globally applicable criterion grounded in graph theory, which represents and decomposes models into structure-local explanations and then recomposes these into comprehensive global explanations. These structure-local explanations are formed as annotations, creating a verifiable hypothesis-evidence framework that supports various explanatory methods. The criterion aligns with existing intuitions about explainability and explains why some models, like large regression models, may not be inherently explainable, whereas sparse neural networks can be. A key distinction is made between models that are explainable (allow explanations) and those that are explained (have verified explanations). The paper demonstrates this by providing a full explanation of PREDICT, a Cox proportional hazards model used clinically in New Zealand for cardiovascular disease risk, thereby establishing that PREDICT is inherently explainable. This framework offers a structured approach to formalize explainability, which can assist regulators by providing a flexible yet rigorous test suitable for compliance and governance in AI deployment. <div>
arXiv:2512.17316v1 Announce Type: new 
Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Schema and Binding: A Double Dissociation Study of In-Context Learning</title>
<link>https://arxiv.org/abs/2512.17325</link>
<guid>https://arxiv.org/abs/2512.17325</guid>
<content:encoded><![CDATA[
<div> Keywords: In-Context Learning, Task Schema, Binding Mechanism, Transformer Models, Activation Patching

<br /><br />Summary:  
This study causally validates that in-context learning (ICL) is driven by two distinct mechanisms: Task Schema (recognition of abstract task types) and Binding (associating specific inputs to outputs). First, through activation patching experiments on nine models spanning seven Transformer families plus a non-Transformer model (Mamba, 370M-13B parameters), the authors demonstrate a double dissociation: Task Schema transfers perfectly (100%) via late MLP patching, whereas Binding transfers at a lower rate (62%) via residual stream patching, confirming these mechanisms are separable. Second, there is a significant inverse correlation between the reliance on Task Schema and prior knowledge (Spearman rho = -0.596, p < 0.001), showing a prior-schema trade-off in model behavior. Third, this dual-mechanism framework applies broadly across all evaluated architectures including Mamba, signifying architectural generality. The findings challenge prior monolithic interpretations of ICL by delivering causal evidence for dual-process theories and revealing that models resort to Task Schema when prior knowledge is lacking. Furthermore, prior knowledge disrupts Binding through attentional mis-routing (72.7% recency bias), rather than output competition. This clarifies why arbitrary mappings succeed while overriding factual knowledge is difficult. Practically, recognizing these dual mechanisms can improve prompt engineering reliability by promoting schema transfer and addressing binding failures in scenarios with substantial prior knowledge. <div>
arXiv:2512.17325v1 Announce Type: new 
Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</title>
<link>https://arxiv.org/abs/2512.17352</link>
<guid>https://arxiv.org/abs/2512.17352</guid>
<content:encoded><![CDATA[
<div> Spatio-Temporal Graph Neural Networks, adaptive pruning, communication overhead, Sudden Event Prediction Accuracy, traffic prediction<br /><br />Summary:<br /><br />1. This paper addresses the high communication overhead in deploying Spatio-Temporal Graph Neural Networks (ST-GNNs) across distributed cloudlets in smart mobility systems due to overlapping node feature transmissions.<br />2. The authors propose an adaptive pruning algorithm that dynamically filters redundant neighboring node features while retaining the most informative spatial context, adjusting pruning rates based on recent model performance.<br />3. The pruning method enables each cloudlet to focus computational resources on regions experiencing significant traffic changes without degrading prediction accuracy.<br />4. The paper introduces Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to rapid traffic slowdowns and recoveries, which conventional error metrics often fail to capture.<br />5. The evaluation is conducted in an online semi-decentralized setting using frameworks such as traditional federated learning, server-free federated learning, and Gossip Learning on two large-scale traffic datasets (PeMS-BAY and PeMSD7-M) over various prediction horizons.<br />6. Results demonstrate that SEPA effectively reveals the importance of spatial connectivity in predicting dynamic traffic events, while the adaptive pruning algorithm significantly reduces communication costs without compromising prediction accuracy or responsiveness.<br />7. The findings suggest that communication overhead in distributed ST-GNN deployments can be lowered substantially without sacrificing model performance, particularly in critical traffic event prediction. <div>
arXiv:2512.17352v1 Announce Type: new 
Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</title>
<link>https://arxiv.org/abs/2512.17367</link>
<guid>https://arxiv.org/abs/2512.17367</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial robustness, harmful content detection, large language models, ensemble learning, adversarial training  

<br /><br />Summary:  
This study addresses the challenge of detecting harmful content such as hate speech, misinformation, and extremist rhetoric on social media platforms, particularly under adversarial attacks where malicious users subtly modify text to evade detection. It introduces a novel framework named Large Language Model-based Sample Generation and Aggregation (LLM-SGA) that identifies key invariances in textual adversarial attacks to enhance the generalizability of detectors. Within this framework, the authors develop the Adversarially Robust Harmful Online Content Detector (ARHOCD), which incorporates three innovative components to boost detection accuracy: (1) an ensemble of multiple base detectors leveraging their complementary strengths; (2) a dynamic weight assignment method that adjusts weights based on sample predictability and base detector capability, initialized with domain knowledge and refined via Bayesian inference; and (3) an adversarial training strategy that iteratively optimizes both base detectors and weight assignor for continual robustness enhancement. The study overcomes several limitations found in prior research on adversarial robustness. Empirical evaluations carried out on three diverse datasets covering hate speech, rumors, and extremist content demonstrate that ARHOCD achieves strong generalizability and significantly improved detection accuracy under various adversarial conditions. <div>
arXiv:2512.17367v1 Announce Type: new 
Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</title>
<link>https://arxiv.org/abs/2512.17375</link>
<guid>https://arxiv.org/abs/2512.17375</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward models, LLM-as-a-Judge, adversarial tokens, reward hacking, adversarial training<br /><br />Summary:<br /><br />This paper investigates a vulnerability in reward models and LLM-as-a-Judge systems that are widely used in model fine-tuning methods like RLHF, DPO, and RLAIF. These judge systems provide scalar feedback and binary decisions crucial for guiding model selection and reinforcement learning. The authors identify that short sequences of low-perplexity control tokens can manipulate these judges by flipping judgments from correct "No" to incorrect "Yes," exploiting the last-layer logit gaps. Unlike worst-case adversarial attacks, these control tokens are plausible outputs from policy models during post-training, representing a realistic reward-hacking threat. To find such tokens, the authors develop AdvJudge-Zero, an approach leveraging next-token distributions and beam search to discover diverse control sequences from scratch. Their analysis reveals that the induced hidden-state perturbations concentrate in a low-rank "soft mode," which is anti-aligned with the judge's refusal direction. Empirical tests show these tokens cause significantly increased false positive rates when judging incorrect answers, especially in large open-weight and specialized judge models on math and reasoning benchmarks. Lastly, the paper demonstrates that LoRA-based adversarial training using small sets of control-token-augmented examples can greatly reduce false positives while maintaining overall evaluation quality. <div>
arXiv:2512.17375v1 Announce Type: new 
Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference</title>
<link>https://arxiv.org/abs/2512.17398</link>
<guid>https://arxiv.org/abs/2512.17398</guid>
<content:encoded><![CDATA[
<div> Private Inference, DReLU, ReLU optimization, prototype channels, image segmentation<br /><br />Summary:<br /><br />1. The paper addresses Private Inference (PI), a technique that uses cryptography to enable privacy-preserving machine learning, allowing neural network inference on client data without revealing the data or the model.  
2. A key computational challenge in PI is evaluating non-linear activation functions, especially ReLU gates, which are computationally expensive.  
3. The authors focus on the derivative ReLU (DReLU), a non-linear step function closely related to ReLU, and demonstrate that a single DReLU operation can support multiple ReLU activations.  
4. They propose a novel activation module where DReLU is computed only on a subset of channels called prototype channels, while the other replicate channels copy the DReLU outputs from their corresponding prototype neurons. This design significantly reduces the number of DReLU computations.  
5. The concept is further extended across multiple layers to enhance efficiency in ResNet-like architectures.  
6. Theoretical analysis shows the approach can solve an extended XOR problem with only one non-linearity and two neurons, outperforming traditional methods and some PI-specific techniques.  
7. Experimental results demonstrate state-of-the-art (SOTA) performance on several classification tasks and image segmentation benchmarks, showcasing both computational efficiency and accuracy benefits. <div>
arXiv:2512.17398v1 Announce Type: new 
Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</title>
<link>https://arxiv.org/abs/2512.17409</link>
<guid>https://arxiv.org/abs/2512.17409</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, performance metrics, subgroup analysis, medical imaging, statistical toolbox  

<br /><br />Summary:  
1. This article addresses the growing practice of analyzing machine learning model performance by patient and recording properties to uncover important model failure modes.  
2. It highlights the statistical challenges involved, including selecting appropriate performance metrics for valid comparisons across groups with varying sample sizes and base rates, estimating metric uncertainty, and correcting for multiple comparisons to avoid chance findings.  
3. The paper emphasizes the complexity of intersectional analyses, where numerous subgroup combinations require methods to identify the most statistically significant or “interesting” subgroups.  
4. To tackle these issues, the authors introduce a comprehensive statistical toolbox designed to enable practitioners to rigorously and easily evaluate potential subgroup performance disparities in machine learning models.  
5. Although generally applicable, the toolbox is tailored for medical imaging contexts, as demonstrated through two case studies: skin lesion malignancy classification using the ISIC2020 dataset and chest X-ray-based disease classification with the MIMIC-CXR dataset.  
6. These case studies illustrate the practical utility of the toolbox in detecting subgroup performance issues that may otherwise be overlooked, thereby enhancing the robustness and fairness of medical imaging AI models. <div>
arXiv:2512.17409v1 Announce Type: new 
Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.17444</link>
<guid>https://arxiv.org/abs/2512.17444</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, electricity market, decarbonization, market design, policy mechanisms<br /><br />Summary:<br /><br />1. The article addresses the importance of long-term electricity market mechanisms such as auctions and support schemes in steering the electricity generation mix toward a carbon-free economy.<br /><br />2. It introduces a novel multi-agent reinforcement learning (MARL) model that simulates profit-maximizing generation companies making investment decisions within a decentralized, competitive wholesale electricity market.<br /><br />3. The model utilizes independent proximal policy optimization, chosen for its suitability in decentralized, multi-agent environments, and employs extensive hyperparameter tuning to ensure realistic competitive market outcomes despite the challenges of independent learning.<br /><br />4. The framework is tested on a stylized representation of the Italian electricity system under varying conditions of market competition, design, and policy scenarios.<br /><br />5. Results demonstrate the critical influence of market design on successful decarbonization and reduction of price volatility, highlighting the framework’s ability to simultaneously evaluate complex interactions among multiple policy and market mechanisms as participants adapt to decarbonization pathways. <div>
arXiv:2512.17444v1 Announce Type: new 
Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</title>
<link>https://arxiv.org/abs/2512.17452</link>
<guid>https://arxiv.org/abs/2512.17452</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context LLM, KV cache management, Write-Gated KV, memory efficiency, inference speedup<br /><br />Summary: This paper addresses the inefficiency in long-context large language model (LLM) inference caused by quadratic attention complexity and linear growth of the key-value (KV) cache. Traditional methods mitigate these challenges by post-hoc selection or eviction but fail to tackle the core issue of indiscriminate caching of tokens. The authors formalize KV cache management as a causal system involving three key operations: KV Admission, Selection, and Eviction. They propose a novel mechanism called Write-Gated KV to implement KV Admission, which learns to predict the utility of tokens before writing them into the cache. By filtering out low-utility tokens early, the approach maintains a compact global cache alongside a sliding local cache, thereby significantly reducing memory usage by 46-57%. This methodology results in substantial speedups, achieving 3.03-3.45× faster prefill times and 1.89-2.56× faster decoding speeds on the Llama model with minimal impact on accuracy. Additionally, Write-Gated KV integrates smoothly with existing technologies like FlashAttention and paged-KV systems. The findings demonstrate that selectively writing tokens to cache is an effective and practical formula for enhancing efficiency in long-context LLM inference. The authors have also made their code publicly available for further research and application. <div>
arXiv:2512.17452v1 Announce Type: new 
Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.17453</link>
<guid>https://arxiv.org/abs/2512.17453</guid>
<content:encoded><![CDATA[
<div> Lite-STGNN, spatial-temporal graph neural network, long-term forecasting, sparse graph structure, trend-seasonal decomposition<br /><br />Summary:  
The paper introduces Lite-STGNN, a lightweight spatial-temporal graph neural network designed for long-term multivariate time series forecasting. It effectively integrates decomposition-based temporal modeling with a learnable sparse graph structure to improve performance. The temporal module implements trend-seasonal decomposition, separating time series patterns into trend and seasonal components to enhance forecasting accuracy. The spatial module employs message passing along with low-rank Top-K adjacency learning and conservative horizon-wise gating, which allow spatial corrections that complement a strong linear baseline model. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets, forecasting up to 720 steps into the future. It is parameter-efficient and trains substantially faster compared to transformer-based methods, making it computationally attractive. Ablation studies highlight the contribution of the spatial module, which yields a 4.6% accuracy improvement over a purely temporal baseline. Additionally, the Top-K adjacency learning improves locality by 3.3%, ensuring the model focuses on the most relevant spatial connections. Furthermore, the learned adjacency matrices reveal meaningful domain-specific interaction dynamics, enhancing interpretability. Overall, Lite-STGNN provides a compact, interpretable, and efficient framework ideally suited for long-term multivariate time series forecasting tasks. <div>
arXiv:2512.17453v1 Announce Type: new 
Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</title>
<link>https://arxiv.org/abs/2512.17477</link>
<guid>https://arxiv.org/abs/2512.17477</guid>
<content:encoded><![CDATA[
<div> Keywords: Inconel 625, creep deformation, deep learning surrogate models, BiLSTM Variational Autoencoder, BiLSTM Transformer<br /><br />Summary:<br /><br />1. This study addresses the computational challenges of simulating time-dependent creep deformation in high-temperature alloys, focusing on Inconel 625, which is critical for aerospace and energy system components. <br /><br />2. Traditional finite-element creep simulations using software like ANSYS are accurate but computationally expensive, often needing 30 to 40 minutes to simulate a 10,000-hour creep event under varying stress and temperature conditions.<br /><br />3. To overcome these limitations, the authors develop two deep learning-based surrogate models trained on temporal creep strain data generated in ANSYS using the Norton creep law under uniaxial stresses (50–150 MPa) and temperatures (700–1000 °C).<br /><br />4. The first model, a BiLSTM Variational Autoencoder (VAE), provides uncertainty-aware and generative predictions, offering probabilistic outputs suitable for reliable forecasting.<br /><br />5. The second model, a BiLSTM Transformer hybrid, uses self-attention mechanisms to capture long-range temporal dependencies and delivers highly accurate deterministic predictions.<br /><br />6. Evaluation metrics, including RMSE, MAE, and R², confirm that the BiLSTM-VAE ensures stability and robustness while the BiLSTM-Transformer achieves superior accuracy throughout the simulation period.<br /><br />7. Latency testing reveals that the surrogate models reduce processing time from tens of minutes to mere seconds, enabling rapid creep assessment.<br /><br />8. This framework facilitates design optimization and structural health monitoring in high-temperature alloy applications by providing scalable, fast, and reliable creep deformation predictions. <div>
arXiv:2512.17477v1 Announce Type: new 
Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</title>
<link>https://arxiv.org/abs/2512.17527</link>
<guid>https://arxiv.org/abs/2512.17527</guid>
<content:encoded><![CDATA[
<div> protein design, biosecurity, sequence screening, homology clustering, calibrated models<br /><br />Summary: This work introduces SafeBench-Seq, a reproducible benchmark and baseline classifier for screening protein sequences for biosecurity hazards. It addresses the lack of simple, reproducible sequence-level hazard screening methods that are explicitly evaluated under homology control and can run efficiently on commodity CPUs. SafeBench-Seq is constructed entirely from public datasets, combining SafeProtein hazard annotations and UniProt benign sequences, and uses interpretable global physicochemical descriptors and amino acid composition features. To simulate never-before-seen threats, the dataset undergoes homology clustering at 40% sequence identity, with cluster-level holdouts ensuring no cluster overlap between training and testing sets. The evaluation reports standard discrimination metrics such as AUROC and AUPRC, along with screening operating points (TPR@1% FPR, FPR@95% TPR), each with 95% bootstrap confidence intervals from 200 replicates. Calibrated probability estimates are provided using CalibratedClassifierCV, applying isotonic regression for logistic regression and random forests, and Platt scaling for linear SVMs. The model’s probability quality is assessed via Brier score, Expected Calibration Error with 15 bins, and reliability diagrams. Robustness is tested against shortcuts by shuffling residues while preserving composition and through ablations based on length and composition features. Results show that random splits overestimate performance versus homology-controlled splits, calibrated linear models demonstrate superior calibration, and tree ensembles have slightly worse calibration errors. SafeBench-Seq releases only metadata (accessions, cluster IDs, split labels) to enable rigorous, CPU-only, reproducible hazard screening evaluations without distributing dangerous sequences. <div>
arXiv:2512.17527v1 Announce Type: new 
Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks</title>
<link>https://arxiv.org/abs/2512.17531</link>
<guid>https://arxiv.org/abs/2512.17531</guid>
<content:encoded><![CDATA[
<div> Forward-Forward algorithm, inter-layer collaboration, Collaborative Forward-Forward (CFF), adaptive learning, neuromorphic computing  

<br /><br />Summary:  
The paper addresses limitations in the Forward-Forward (FF) learning algorithm, which eliminates backpropagation's biological implausibility and memory issues by processing positive and negative data through dual forward passes. Conventional FF implementations isolate layers, preventing collaborative learning across the network and reducing convergence efficiency, especially in deep architectures. To overcome this, the authors propose Collaborative Forward-Forward (CFF) learning, which maintains forward-only computation while enabling inter-layer cooperation and global context integration. Two variants are introduced: Fixed CFF (F-CFF), employing constant inter-layer coupling, and Adaptive CFF (A-CFF), which uses learnable parameters that evolve with training to modulate collaboration strength. The framework's collaborative goodness function aggregates weighted contributions from all layers, fostering coordinated feature representation. Evaluations on MNIST and Fashion-MNIST datasets reveal that both CFF variants outperform the baseline FF approach, indicating enhanced learning dynamics and better convergence. The study highlights inter-layer collaboration as a crucial advancement for FF learning, with promising applications in neuromorphic hardware and AI systems where energy efficiency and memory constraints are critical. This work paves the way for more biologically plausible and scalable learning algorithms suitable for next-generation computing platforms. <div>
arXiv:2512.17531v1 Announce Type: new 
Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimisation: Which Constraints Matter?</title>
<link>https://arxiv.org/abs/2512.17569</link>
<guid>https://arxiv.org/abs/2512.17569</guid>
<content:encoded><![CDATA[
<div> Bayesian optimisation, Knowledge Gradient, decoupled constraints, black-box optimisation, constraint evaluation<br /><br />Summary:<br /><br />1. The paper introduces new variants of the Knowledge Gradient acquisition function tailored for Bayesian optimisation problems with decoupled black-box constraints, where objective and constraint functions can be evaluated independently.<br /><br />2. These variants are designed to efficiently handle settings where only a subset of constraints are binding at the optimum, which allows selective evaluation of relevant constraints rather than all constraints at every step.<br /><br />3. By doing so, the methods reduce the computational cost associated with evaluating unnecessary constraints during optimisation.<br /><br />4. The proposed algorithms are empirically benchmarked against existing state-of-the-art methods for constrained Bayesian optimisation.<br /><br />5. Experimental results show that the new approaches outperform prior methods, demonstrating superior optimisation performance and efficiency on problems with decoupled constraint structures. <div>
arXiv:2512.17569v1 Announce Type: new 
Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</title>
<link>https://arxiv.org/abs/2512.17570</link>
<guid>https://arxiv.org/abs/2512.17570</guid>
<content:encoded><![CDATA[
<div> SSD-offloaded training, vertical scheduling, GreedySnake, training throughput, optimization overlap<br /><br />Summary:<br /><br />This paper presents GreedySnake, a novel SSD-offloaded training system designed to improve the cost-effectiveness of large language model (LLM) training. Unlike existing systems that rely on horizontal scheduling—executing micro-batches sequentially—GreedySnake employs vertical scheduling, executing all micro-batches of a specific layer before moving on to the next. This approach boosts training throughput even with smaller batch sizes, aligning performance closer to the ideal predicted by the roofline model. To address the I/O bottleneck commonly encountered during SSD-offloaded training, GreedySnake overlaps parts of the optimization step with the forward pass of the subsequent iteration, further enhancing efficiency. Experimental evaluations on NVIDIA A100 GPUs demonstrate significant throughput improvements compared to ZeRO-Infinity: nearly 2x on single and 4 GPU setups for GPT-65B models and over 2.5x on a single GPU for GPT-175B. These results indicate GreedySnake’s capability to accelerate SSD-offloaded training substantially while maintaining system responsiveness. The authors have open-sourced GreedySnake at the provided GitHub repository, encouraging adoption and further research in the community. <div>
arXiv:2512.17570v1 Announce Type: new 
Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Static and Single-Event Dynamic Complex Network Analysis</title>
<link>https://arxiv.org/abs/2512.17577</link>
<guid>https://arxiv.org/abs/2512.17577</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Representation Learning, Latent Space Models, Latent Distance Model, Structural-aware Network Representations, Unified Network Embeddings<br /><br />Summary: The thesis focuses on developing novel algorithmic approaches for Graph Representation Learning specifically targeting static and single-event dynamic networks. It concentrates on the family of Latent Space Models, with an emphasis on the Latent Distance Model to capture key network properties such as homophily, transitivity, and balance theory. The work aims to produce structural-aware network representations that allow for hierarchical mapping of network structures, community detection, identification of extreme profiles, and analysis of impact dynamics in temporal networks. A significant contribution is the design of unified learning methods that avoid heuristic-based and multi-stage procedures, including eliminating the need for post-processing steps. The overall objective is to advance unified and powerful network embeddings that comprehensively characterize network structures and support a wide variety of graph analysis tasks efficiently and effectively. This approach is expected to facilitate deeper insights into network behavior and improve performance in applications relying on graph data. <div>
arXiv:2512.17577v1 Announce Type: new 
Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</title>
<link>https://arxiv.org/abs/2512.17586</link>
<guid>https://arxiv.org/abs/2512.17586</guid>
<content:encoded><![CDATA[
<div> Keywords: Safe reinforcement learning, autonomous driving, safety representations, policy optimization, generalization  

<br /><br />Summary:  
This paper investigates Safe Reinforcement Learning (SafeRL) within autonomous driving, focusing on balancing safety and performance—a key challenge where conservative policies reduce efficiency and aggressive exploration risks safety. The Safety Representations for Safer Policy Learning (SRPL) framework equips agents with models predicting future constraint violations, previously proven effective in controlled settings. The study examines SRPL’s applicability to real-world datasets, specifically the Waymo Open Motion Dataset (WOMD) and NuPlan. Results show SRPL improves the reward-safety tradeoff, achieving statistically significant gains in success rates (effect sizes r = 0.65-0.86) and cost reductions (effect sizes r = 0.70-0.83) with p-values < 0.05. However, SRPL’s effectiveness varies depending on the policy optimizer used and dataset distributions. Furthermore, predictive safety representations enhance agent robustness against observation noise. Cross-dataset zero-shot evaluations reveal that SRPL-augmented agents generalize better than baseline non-SRPL methods. Collectively, the findings highlight the potential of predictive safety representations to improve robustness, safety, and generalization in SafeRL for autonomous driving applications. <div>
arXiv:2512.17586v1 Announce Type: new 
Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</title>
<link>https://arxiv.org/abs/2512.17592</link>
<guid>https://arxiv.org/abs/2512.17592</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, federated learning, asynchronous collaboration, model stitching, multi-objective performance<br /><br />Summary:<br />1. Deep learning excels at many real-world tasks but often depends on large, diverse datasets that are fragmented or not shareable, especially in sensitive domains like medicine.<br />2. Federated learning addresses data fragmentation by requiring parties to train a single model synchronously, sharing model weights during training.<br />3. The paper explores asynchronous collaboration, where only fully trained models are shared (e.g., via publications), rather than synchronously training together.<br />4. The authors propose using stitching layers to combine intermediate representations of individually trained models to improve performance across datasets.<br />5. Their multi-objective analysis shows that training on a single party’s data yields good performance on that data but poor generalization to other parties’ data.<br />6. Ensembles of individually trained models generalize better but reduce performance on each party’s own dataset.<br />7. Utilizing stitching layers enables recovery of party-specific performance while maintaining the advantages of improved generalization through asynchronous collaboration.<br />8. This method demonstrates that asynchronous sharing of trained models can competitively approximate the benefits of federated training without requiring synchronous model updates. <div>
arXiv:2512.17592v1 Announce Type: new 
Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Representation of Neural Networks Architectures</title>
<link>https://arxiv.org/abs/2512.17593</link>
<guid>https://arxiv.org/abs/2512.17593</guid>
<content:encoded><![CDATA[
arXiv:2512.17593v1 Announce Type: new 
Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systems-Theoretic View on the Convergence of Algorithms under Disturbances</title>
<link>https://arxiv.org/abs/2512.17598</link>
<guid>https://arxiv.org/abs/2512.17598</guid>
<content:encoded><![CDATA[
arXiv:2512.17598v1 Announce Type: new 
Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Consistent Accuracy PINN via Alternating Easy-Hard Training</title>
<link>https://arxiv.org/abs/2512.17607</link>
<guid>https://arxiv.org/abs/2512.17607</guid>
<content:encoded><![CDATA[
arXiv:2512.17607v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Sequential Causal Optimization of Process Interventions</title>
<link>https://arxiv.org/abs/2512.17629</link>
<guid>https://arxiv.org/abs/2512.17629</guid>
<content:encoded><![CDATA[
arXiv:2512.17629v1 Announce Type: new 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust-Region Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2512.17636</link>
<guid>https://arxiv.org/abs/2512.17636</guid>
<content:encoded><![CDATA[
arXiv:2512.17636v1 Announce Type: new 
Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Spatially Resolved Radiation Fields Using Neural Networks</title>
<link>https://arxiv.org/abs/2512.17654</link>
<guid>https://arxiv.org/abs/2512.17654</guid>
<content:encoded><![CDATA[
arXiv:2512.17654v1 Announce Type: new 
Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polyharmonic Cascade</title>
<link>https://arxiv.org/abs/2512.17671</link>
<guid>https://arxiv.org/abs/2512.17671</guid>
<content:encoded><![CDATA[
arXiv:2512.17671v1 Announce Type: new 
Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once: Differentiable Subset Selection for Omics Data</title>
<link>https://arxiv.org/abs/2512.17678</link>
<guid>https://arxiv.org/abs/2512.17678</guid>
<content:encoded><![CDATA[
arXiv:2512.17678v1 Announce Type: new 
Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2512.17688</link>
<guid>https://arxiv.org/abs/2512.17688</guid>
<content:encoded><![CDATA[
arXiv:2512.17688v1 Announce Type: new 
Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</title>
<link>https://arxiv.org/abs/2512.17696</link>
<guid>https://arxiv.org/abs/2512.17696</guid>
<content:encoded><![CDATA[
arXiv:2512.17696v1 Announce Type: new 
Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Forgetting in Low Rank Adaptation</title>
<link>https://arxiv.org/abs/2512.17720</link>
<guid>https://arxiv.org/abs/2512.17720</guid>
<content:encoded><![CDATA[
arXiv:2512.17720v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</title>
<link>https://arxiv.org/abs/2512.17762</link>
<guid>https://arxiv.org/abs/2512.17762</guid>
<content:encoded><![CDATA[
arXiv:2512.17762v1 Announce Type: new 
Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2512.17771</link>
<guid>https://arxiv.org/abs/2512.17771</guid>
<content:encoded><![CDATA[
arXiv:2512.17771v1 Announce Type: new 
Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</title>
<link>https://arxiv.org/abs/2512.17788</link>
<guid>https://arxiv.org/abs/2512.17788</guid>
<content:encoded><![CDATA[
arXiv:2512.17788v1 Announce Type: new 
Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2512.17820</link>
<guid>https://arxiv.org/abs/2512.17820</guid>
<content:encoded><![CDATA[
arXiv:2512.17820v1 Announce Type: new 
Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</title>
<link>https://arxiv.org/abs/2512.17878</link>
<guid>https://arxiv.org/abs/2512.17878</guid>
<content:encoded><![CDATA[
arXiv:2512.17878v1 Announce Type: new 
Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</title>
<link>https://arxiv.org/abs/2512.17884</link>
<guid>https://arxiv.org/abs/2512.17884</guid>
<content:encoded><![CDATA[
arXiv:2512.17884v1 Announce Type: new 
Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach</title>
<link>https://arxiv.org/abs/2512.16927</link>
<guid>https://arxiv.org/abs/2512.16927</guid>
<content:encoded><![CDATA[
arXiv:2512.16927v1 Announce Type: cross 
Abstract: In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</title>
<link>https://arxiv.org/abs/2512.16956</link>
<guid>https://arxiv.org/abs/2512.16956</guid>
<content:encoded><![CDATA[
arXiv:2512.16956v1 Announce Type: cross 
Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</title>
<link>https://arxiv.org/abs/2512.16962</link>
<guid>https://arxiv.org/abs/2512.16962</guid>
<content:encoded><![CDATA[
arXiv:2512.16962v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.16964</link>
<guid>https://arxiv.org/abs/2512.16964</guid>
<content:encoded><![CDATA[
arXiv:2512.16964v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</title>
<link>https://arxiv.org/abs/2512.16969</link>
<guid>https://arxiv.org/abs/2512.16969</guid>
<content:encoded><![CDATA[
arXiv:2512.16969v1 Announce Type: cross 
Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAACE: A Plan-Aware Automated Agent Context Engineering Framework</title>
<link>https://arxiv.org/abs/2512.16970</link>
<guid>https://arxiv.org/abs/2512.16970</guid>
<content:encoded><![CDATA[
arXiv:2512.16970v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Women's Health Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2512.17028</link>
<guid>https://arxiv.org/abs/2512.17028</guid>
<content:encoded><![CDATA[
arXiv:2512.17028v1 Announce Type: cross 
Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perturb Your Data: Paraphrase-Guided Training Data Watermarking</title>
<link>https://arxiv.org/abs/2512.17075</link>
<guid>https://arxiv.org/abs/2512.17075</guid>
<content:encoded><![CDATA[
arXiv:2512.17075v1 Announce Type: cross 
Abstract: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled representations via score-based variational autoencoders</title>
<link>https://arxiv.org/abs/2512.17127</link>
<guid>https://arxiv.org/abs/2512.17127</guid>
<content:encoded><![CDATA[
arXiv:2512.17127v1 Announce Type: cross 
Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</title>
<link>https://arxiv.org/abs/2512.17146</link>
<guid>https://arxiv.org/abs/2512.17146</guid>
<content:encoded><![CDATA[
arXiv:2512.17146v1 Announce Type: cross 
Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of machine learning to predict food processing level using Open Food Facts</title>
<link>https://arxiv.org/abs/2512.17169</link>
<guid>https://arxiv.org/abs/2512.17169</guid>
<content:encoded><![CDATA[
arXiv:2512.17169v1 Announce Type: cross 
Abstract: Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</title>
<link>https://arxiv.org/abs/2512.17185</link>
<guid>https://arxiv.org/abs/2512.17185</guid>
<content:encoded><![CDATA[
arXiv:2512.17185v1 Announce Type: cross 
Abstract: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.
  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.
  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Foundational Audio Encoders Understand Music Structure?</title>
<link>https://arxiv.org/abs/2512.17209</link>
<guid>https://arxiv.org/abs/2512.17209</guid>
<content:encoded><![CDATA[
arXiv:2512.17209v1 Announce Type: cross 
Abstract: In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title>
<link>https://arxiv.org/abs/2512.17213</link>
<guid>https://arxiv.org/abs/2512.17213</guid>
<content:encoded><![CDATA[
arXiv:2512.17213v1 Announce Type: cross 
Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function</title>
<link>https://arxiv.org/abs/2512.17245</link>
<guid>https://arxiv.org/abs/2512.17245</guid>
<content:encoded><![CDATA[
arXiv:2512.17245v1 Announce Type: cross 
Abstract: Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</title>
<link>https://arxiv.org/abs/2512.17251</link>
<guid>https://arxiv.org/abs/2512.17251</guid>
<content:encoded><![CDATA[
arXiv:2512.17251v1 Announce Type: cross 
Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</title>
<link>https://arxiv.org/abs/2512.17254</link>
<guid>https://arxiv.org/abs/2512.17254</guid>
<content:encoded><![CDATA[
arXiv:2512.17254v1 Announce Type: cross 
Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</title>
<link>https://arxiv.org/abs/2512.17259</link>
<guid>https://arxiv.org/abs/2512.17259</guid>
<content:encoded><![CDATA[
arXiv:2512.17259v1 Announce Type: cross 
Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest</title>
<link>https://arxiv.org/abs/2512.17277</link>
<guid>https://arxiv.org/abs/2512.17277</guid>
<content:encoded><![CDATA[
arXiv:2512.17277v1 Announce Type: cross 
Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</title>
<link>https://arxiv.org/abs/2512.17281</link>
<guid>https://arxiv.org/abs/2512.17281</guid>
<content:encoded><![CDATA[
arXiv:2512.17281v1 Announce Type: cross 
Abstract: Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
<link>https://arxiv.org/abs/2512.17340</link>
<guid>https://arxiv.org/abs/2512.17340</guid>
<content:encoded><![CDATA[
arXiv:2512.17340v1 Announce Type: cross 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Structure-Agnostic Lower Bounds for General Functional Estimation</title>
<link>https://arxiv.org/abs/2512.17341</link>
<guid>https://arxiv.org/abs/2512.17341</guid>
<content:encoded><![CDATA[
arXiv:2512.17341v1 Announce Type: cross 
Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Timely Information Updating for Mobile Devices Without and With ML Advice</title>
<link>https://arxiv.org/abs/2512.17381</link>
<guid>https://arxiv.org/abs/2512.17381</guid>
<content:encoded><![CDATA[
arXiv:2512.17381v1 Announce Type: cross 
Abstract: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</title>
<link>https://arxiv.org/abs/2512.17419</link>
<guid>https://arxiv.org/abs/2512.17419</guid>
<content:encoded><![CDATA[
arXiv:2512.17419v1 Announce Type: cross 
Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing</title>
<link>https://arxiv.org/abs/2512.17426</link>
<guid>https://arxiv.org/abs/2512.17426</guid>
<content:encoded><![CDATA[
arXiv:2512.17426v1 Announce Type: cross 
Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title>
<link>https://arxiv.org/abs/2512.17450</link>
<guid>https://arxiv.org/abs/2512.17450</guid>
<content:encoded><![CDATA[
arXiv:2512.17450v1 Announce Type: cross 
Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction</title>
<link>https://arxiv.org/abs/2512.17460</link>
<guid>https://arxiv.org/abs/2512.17460</guid>
<content:encoded><![CDATA[
arXiv:2512.17460v1 Announce Type: cross 
Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</title>
<link>https://arxiv.org/abs/2512.17462</link>
<guid>https://arxiv.org/abs/2512.17462</guid>
<content:encoded><![CDATA[
arXiv:2512.17462v1 Announce Type: cross 
Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</title>
<link>https://arxiv.org/abs/2512.17466</link>
<guid>https://arxiv.org/abs/2512.17466</guid>
<content:encoded><![CDATA[
arXiv:2512.17466v1 Announce Type: cross 
Abstract: Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating the Rashomon Effect to Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2512.17470</link>
<guid>https://arxiv.org/abs/2512.17470</guid>
<content:encoded><![CDATA[
arXiv:2512.17470v1 Announce Type: cross 
Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions</title>
<link>https://arxiv.org/abs/2512.17473</link>
<guid>https://arxiv.org/abs/2512.17473</guid>
<content:encoded><![CDATA[
arXiv:2512.17473v1 Announce Type: cross 
Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</title>
<link>https://arxiv.org/abs/2512.17488</link>
<guid>https://arxiv.org/abs/2512.17488</guid>
<content:encoded><![CDATA[
arXiv:2512.17488v1 Announce Type: cross 
Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-efficient medical image classification for edge devices</title>
<link>https://arxiv.org/abs/2512.17515</link>
<guid>https://arxiv.org/abs/2512.17515</guid>
<content:encoded><![CDATA[
arXiv:2512.17515v1 Announce Type: cross 
Abstract: Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</title>
<link>https://arxiv.org/abs/2512.17517</link>
<guid>https://arxiv.org/abs/2512.17517</guid>
<content:encoded><![CDATA[
arXiv:2512.17517v1 Announce Type: cross 
Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</title>
<link>https://arxiv.org/abs/2512.17534</link>
<guid>https://arxiv.org/abs/2512.17534</guid>
<content:encoded><![CDATA[
arXiv:2512.17534v1 Announce Type: cross 
Abstract: Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems</title>
<link>https://arxiv.org/abs/2512.17562</link>
<guid>https://arxiv.org/abs/2512.17562</guid>
<content:encoded><![CDATA[
arXiv:2512.17562v1 Announce Type: cross 
Abstract: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</title>
<link>https://arxiv.org/abs/2512.17574</link>
<guid>https://arxiv.org/abs/2512.17574</guid>
<content:encoded><![CDATA[
arXiv:2512.17574v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.17585</link>
<guid>https://arxiv.org/abs/2512.17585</guid>
<content:encoded><![CDATA[
arXiv:2512.17585v1 Announce Type: cross 
Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</title>
<link>https://arxiv.org/abs/2512.17594</link>
<guid>https://arxiv.org/abs/2512.17594</guid>
<content:encoded><![CDATA[
arXiv:2512.17594v1 Announce Type: cross 
Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</title>
<link>https://arxiv.org/abs/2512.17630</link>
<guid>https://arxiv.org/abs/2512.17630</guid>
<content:encoded><![CDATA[
arXiv:2512.17630v1 Announce Type: cross 
Abstract: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</title>
<link>https://arxiv.org/abs/2512.17659</link>
<guid>https://arxiv.org/abs/2512.17659</guid>
<content:encoded><![CDATA[
arXiv:2512.17659v1 Announce Type: cross 
Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2512.17660</link>
<guid>https://arxiv.org/abs/2512.17660</guid>
<content:encoded><![CDATA[
arXiv:2512.17660v1 Announce Type: cross 
Abstract: Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidarc: Embodied Video Diffusion Model for Closed-loop Control</title>
<link>https://arxiv.org/abs/2512.17661</link>
<guid>https://arxiv.org/abs/2512.17661</guid>
<content:encoded><![CDATA[
arXiv:2512.17661v1 Announce Type: cross 
Abstract: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imputation Uncertainty in Interpretable Machine Learning Methods</title>
<link>https://arxiv.org/abs/2512.17689</link>
<guid>https://arxiv.org/abs/2512.17689</guid>
<content:encoded><![CDATA[
arXiv:2512.17689v1 Announce Type: cross 
Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study</title>
<link>https://arxiv.org/abs/2512.17703</link>
<guid>https://arxiv.org/abs/2512.17703</guid>
<content:encoded><![CDATA[
arXiv:2512.17703v1 Announce Type: cross 
Abstract: The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</title>
<link>https://arxiv.org/abs/2512.17759</link>
<guid>https://arxiv.org/abs/2512.17759</guid>
<content:encoded><![CDATA[
arXiv:2512.17759v1 Announce Type: cross 
Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.17774</link>
<guid>https://arxiv.org/abs/2512.17774</guid>
<content:encoded><![CDATA[
arXiv:2512.17774v1 Announce Type: cross 
Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Aware Quantum Circuit for QML</title>
<link>https://arxiv.org/abs/2512.17800</link>
<guid>https://arxiv.org/abs/2512.17800</guid>
<content:encoded><![CDATA[
arXiv:2512.17800v1 Announce Type: cross 
Abstract: Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
<link>https://arxiv.org/abs/2512.17875</link>
<guid>https://arxiv.org/abs/2512.17875</guid>
<content:encoded><![CDATA[
arXiv:2512.17875v1 Announce Type: cross 
Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning vertical coordinates via automatic differentiation of a dynamical core</title>
<link>https://arxiv.org/abs/2512.17877</link>
<guid>https://arxiv.org/abs/2512.17877</guid>
<content:encoded><![CDATA[
arXiv:2512.17877v1 Announce Type: cross 
Abstract: Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
<link>https://arxiv.org/abs/2512.17897</link>
<guid>https://arxiv.org/abs/2512.17897</guid>
<content:encoded><![CDATA[
arXiv:2512.17897v1 Announce Type: cross 
Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</title>
<link>https://arxiv.org/abs/2512.17899</link>
<guid>https://arxiv.org/abs/2512.17899</guid>
<content:encoded><![CDATA[
arXiv:2512.17899v1 Announce Type: cross 
Abstract: Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
<link>https://arxiv.org/abs/2512.17908</link>
<guid>https://arxiv.org/abs/2512.17908</guid>
<content:encoded><![CDATA[
arXiv:2512.17908v1 Announce Type: cross 
Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection</title>
<link>https://arxiv.org/abs/2306.09158</link>
<guid>https://arxiv.org/abs/2306.09158</guid>
<content:encoded><![CDATA[
arXiv:2306.09158v2 Announce Type: replace 
Abstract: Modern machine learning models deployed in the wild can encounter both covariate and semantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and OOD detection respectively. While both problems have received significant research attention lately, they have been pursued independently. This may not be surprising, since the two tasks have seemingly conflicting goals. This paper provides a new unified approach that is capable of simultaneously generalizing to covariate shifts while robustly detecting semantic shifts. We propose a margin-based learning framework that exploits freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. We show both empirically and theoretically that the proposed margin constraint is the key to achieving both OOD generalization and detection. Extensive experiments show the superiority of our framework, outperforming competitive baselines that specialize in either OOD generalization or OOD detection. Code is publicly available at https://github.com/deeplearning-wisc/scone.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse, Efficient and Explainable Data Attribution with DualXDA</title>
<link>https://arxiv.org/abs/2402.12118</link>
<guid>https://arxiv.org/abs/2402.12118</guid>
<content:encoded><![CDATA[
arXiv:2402.12118v3 Announce Type: replace 
Abstract: Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs</title>
<link>https://arxiv.org/abs/2405.00645</link>
<guid>https://arxiv.org/abs/2405.00645</guid>
<content:encoded><![CDATA[
arXiv:2405.00645v3 Announce Type: replace 
Abstract: Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Identification of Temporally Causal Representation with Instantaneous Dependence</title>
<link>https://arxiv.org/abs/2405.15325</link>
<guid>https://arxiv.org/abs/2405.15325</guid>
<content:encoded><![CDATA[
arXiv:2405.15325v3 Announce Type: replace 
Abstract: Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications</title>
<link>https://arxiv.org/abs/2405.15877</link>
<guid>https://arxiv.org/abs/2405.15877</guid>
<content:encoded><![CDATA[
arXiv:2405.15877v4 Announce Type: replace 
Abstract: Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</title>
<link>https://arxiv.org/abs/2409.03735</link>
<guid>https://arxiv.org/abs/2409.03735</guid>
<content:encoded><![CDATA[
arXiv:2409.03735v3 Announce Type: replace 
Abstract: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Filtering and Smoothing for Sequential Deep Learning</title>
<link>https://arxiv.org/abs/2410.06800</link>
<guid>https://arxiv.org/abs/2410.06800</guid>
<content:encoded><![CDATA[
arXiv:2410.06800v2 Announce Type: replace 
Abstract: Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks. Regularizing network parameters is a common approach, but it rarely incorporates prior knowledge about task relationships, and limits information flow to future tasks only. We propose a Bayesian framework that treats the network's parameters as the state space of a nonlinear Gaussian model, unlocking two key capabilities: (1) A principled way to encode domain knowledge about task relationships, allowing, e.g., control over which layers should adapt between tasks. (2) A novel application of Bayesian smoothing, allowing task-specific models to also incorporate knowledge from models learned later. This does not require direct access to their data, which is crucial, e.g., for privacy-critical applications. These capabilities rely on efficient filtering and smoothing operations, for which we propose diagonal plus low-rank approximations of the precision matrix in the Laplace approximation (LR-LGF). Empirical results demonstrate the efficiency of LR-LGF and the benefits of the unlocked capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification</title>
<link>https://arxiv.org/abs/2410.18686</link>
<guid>https://arxiv.org/abs/2410.18686</guid>
<content:encoded><![CDATA[
arXiv:2410.18686v2 Announce Type: replace 
Abstract: Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness via Independence: A (Conditional) Distance Covariance Framework</title>
<link>https://arxiv.org/abs/2412.00720</link>
<guid>https://arxiv.org/abs/2412.00720</guid>
<content:encoded><![CDATA[
arXiv:2412.00720v2 Announce Type: replace 
Abstract: We explore fairness from a statistical perspective by selectively utilizing either conditional distance covariance or distance covariance statistics as measures to assess the independence between predictions and sensitive attributes. We boost fairness with independence by adding a distance covariance-based penalty to the model's training. Additionally, we present the matrix form of empirical (conditional) distance covariance for parallel calculations to enhance computational efficiency. Theoretically, we provide a proof for the convergence between empirical and population (conditional) distance covariance, establishing necessary guarantees for batch computations. Through experiments conducted on a range of real-world datasets, we have demonstrated that our method effectively bridges the fairness gap in machine learning. Our code is available at \url{https://github.com/liuhaixias1/Fair_dc/}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</title>
<link>https://arxiv.org/abs/2412.15184</link>
<guid>https://arxiv.org/abs/2412.15184</guid>
<content:encoded><![CDATA[
arXiv:2412.15184v2 Announce Type: replace 
Abstract: The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners''), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of "motivated proof", introduced by G. P\'olya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy</title>
<link>https://arxiv.org/abs/2501.10290</link>
<guid>https://arxiv.org/abs/2501.10290</guid>
<content:encoded><![CDATA[
arXiv:2501.10290v3 Announce Type: replace 
Abstract: Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a reward constraint. For example, we may seek to make decisions that have at least the reward of a reference ``default'' decision, with as low a cost as possible. This problem was recently introduced in the Multi-Armed Bandits with Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem domains where a primary metric (cost) is constrained by a secondary metric (reward), and the rewards are unknown. In our work, we address variants of MAB-CS including ones with reward constrained by the reward of a known reference arm or by the subsidized best reward. We introduce the Pairwise-Elimination (PE) algorithm for the known reference arm variant and generalize PE to PE-CS for the subsidized best reward variant. Our instance-dependent analysis of PE and PE-CS reveals that both algorithms have an order-wise logarithmic upper bound on Cost and Quality Regret, making our policies the first with such a guarantee. Moreover, by comparing our upper and lower bound results we establish that PE is order-optimal for all known reference arm problem instances. Finally, experiments are conducted using the MovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the effectiveness of PE and the superior balance between performance and reliability offered by PE-CS compared to baselines from the literature.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Human-Guided, Data-Centric LLM Co-Pilots</title>
<link>https://arxiv.org/abs/2501.10321</link>
<guid>https://arxiv.org/abs/2501.10321</guid>
<content:encoded><![CDATA[
arXiv:2501.10321v3 Announce Type: replace 
Abstract: Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized Langevin Dynamics for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2502.00277</link>
<guid>https://arxiv.org/abs/2502.00277</guid>
<content:encoded><![CDATA[
arXiv:2502.00277v4 Announce Type: replace 
Abstract: This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Samples to Probe Trained Models</title>
<link>https://arxiv.org/abs/2502.06658</link>
<guid>https://arxiv.org/abs/2502.06658</guid>
<content:encoded><![CDATA[
arXiv:2502.06658v3 Announce Type: replace 
Abstract: There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Agnostic PAC Learning in the Small Error Regime</title>
<link>https://arxiv.org/abs/2502.09496</link>
<guid>https://arxiv.org/abs/2502.09496</guid>
<content:encoded><![CDATA[
arXiv:2502.09496v2 Announce Type: replace 
Abstract: Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\mathcal{D}$ are simply more difficult to learn than realizable distributions -- even when one discounts a learner's error by $\mathrm{err}(h^*_{\mathcal{D}})$, the error of the best hypothesis in $\mathcal{H}$ for $\mathcal{D}$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\mathcal{D}$ for which $\tau = \mathrm{err}(h^*_{\mathcal{D}})$ is small.
  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this shortcoming by including $\tau$ itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound $\tau + \Omega \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ in a regime where $\tau > d/m$, and leave open the question of whether there may be a higher lower bound when $\tau \approx d/m$, with $d$ denoting $\mathrm{VC}(\mathcal{H})$. In this work, we resolve this question by exhibiting a learner which achieves error $c \cdot \tau + O \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lower bound when $\tau \approx d/m$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS `24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preconditioned Inexact Stochastic ADMM for Deep Model</title>
<link>https://arxiv.org/abs/2502.10784</link>
<guid>https://arxiv.org/abs/2502.10784</guid>
<content:encoded><![CDATA[
arXiv:2502.10784v5 Announce Type: replace 
Abstract: The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effect of Sampling Diversity in Scaling LLM Inference</title>
<link>https://arxiv.org/abs/2502.11027</link>
<guid>https://arxiv.org/abs/2502.11027</guid>
<content:encoded><![CDATA[
arXiv:2502.11027v4 Announce Type: replace 
Abstract: Large language model (LLM) scaling inference is key to unlocking greater performance, and leveraging diversity has proven an effective way to enhance it. Motivated by the observed relationship between solution accuracy and meaningful response diversity, we systematically study the effect of prompt diversity in scaling inference. We theoretically explain why diversified sampling improves Best-of-N scaling, showing that responses generated from diverse prompts after Best-of-N selection exhibit significantly lower error rates than those produced from stationary prompts. Building on this analysis, we derive a diversity-fidelity trade-off principle, that guides the design of sampling strategies introducing diversity. From this guidance, we instantiate a family of effective perturbation styles. We theoretically and empirically characterize when diversified exploration remains effective, demonstrating that it works under a variety of conditions, and we further show that under majority voting, diversity may vanish. Finally, we systematically evaluate how effective sampling diversity is and show that, when applied appropriately in different contexts, it yields relative gains of 10.8% in EM@100 for reasoning, 9.6% for mathematics, and 9.5% in Pass@100 for code generation. Overall, this work provides a systematic analysis that offers a theoretical and empirical foundation for understanding how sampling diversity affects LLM inference-time scaling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to use score-based diffusion in earth system science: A satellite nowcasting example</title>
<link>https://arxiv.org/abs/2505.10432</link>
<guid>https://arxiv.org/abs/2505.10432</guid>
<content:encoded><![CDATA[
arXiv:2505.10432v2 Announce Type: replace 
Abstract: Machine learning (ML) is used for many earth science applications; however, traditional ML methods trained with squared errors often create blurry forecasts. Diffusion models are an emerging generative ML technique with the ability to produce sharper, more realistic images by learning the underlying data distribution. Diffusion models are becoming more prevalent, yet adapting them for earth science applications can be challenging because most articles focus on theoretical aspects of the approach, rather than making the method widely accessible. This work illustrates score-based diffusion models with a well-known problem in atmospheric science: cloud nowcasting (zero-to-three-hour forecast). After discussing the background and intuition of score-based diffusion models using examples from geostationary satellite infrared imagery, we experiment with three types of diffusion models: a standard score-based diffusion model (Diff); a residual correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our results show that the diffusion models not only advect existing clouds, but also generate and decay clouds, including convective initiation. A case study qualitatively shows the preservation of high-resolution features longer into the forecast than a conventional U-Net. The best of the three diffusion models tested was the CorrDiff approach, outperforming all other diffusion models, the conventional U-Net, and persistence. The diffusion models also enable out-of-the-box ensemble generation with skillful calibration. By explaining and exploring diffusion models for a common problem and ending with lessons learned from adapting diffusion models for our task, this work provides a starting point for the community to utilize diffusion models for a variety of earth science applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEAR: Equal Area Weather Forecasting on the Sphere</title>
<link>https://arxiv.org/abs/2505.17720</link>
<guid>https://arxiv.org/abs/2505.17720</guid>
<content:encoded><![CDATA[
arXiv:2505.17720v2 Announce Type: replace 
Abstract: Artificial intelligence is rapidly reshaping the natural sciences, with weather forecasting emerging as a flagship AI4Science application where machine learning models can now rival and even surpass traditional numerical simulations. Following the success of the landmark models Pangu Weather and Graphcast, outperforming traditional numerical methods for global medium-range forecasting, many novel data-driven methods have emerged. A common limitation shared by many of these models is their reliance on an equiangular discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on an equiangular grid without any computational overhead.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
arXiv:2505.22255v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training and interpreting SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Certified Unlearning Approach without Access to Source Data</title>
<link>https://arxiv.org/abs/2506.06486</link>
<guid>https://arxiv.org/abs/2506.06486</guid>
<content:encoded><![CDATA[
arXiv:2506.06486v3 Announce Type: replace 
Abstract: With the growing adoption of data privacy regulations, the ability to erase private or copyrighted information from trained models has become a crucial requirement. Traditional unlearning methods often assume access to the complete training dataset, which is unrealistic in scenarios where the source data is no longer available. To address this challenge, we propose a certified unlearning framework that enables effective data removal \final{without access to the original training data samples}. Our approach utilizes a surrogate dataset that approximates the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two. \updated{While our theoretical guarantees assume knowledge of the exact statistical distance, practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees.} This ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility. We establish theoretical bounds, introduce practical noise calibration techniques, and validate our method through extensive experiments on both synthetic and real-world datasets. The results demonstrate the effectiveness and reliability of our approach in privacy-sensitive settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Diffusion Duality</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
arXiv:2506.10892v3 Announce Type: replace 
Abstract: Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Representation Learning and Fusion</title>
<link>https://arxiv.org/abs/2506.20494</link>
<guid>https://arxiv.org/abs/2506.20494</guid>
<content:encoded><![CDATA[
arXiv:2506.20494v2 Announce Type: replace 
Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It tries to help machines understand complex things by combining information from different sources, like images, text, and audio. By using the strengths of each modality, multi-modal learning allows AI systems to build stronger and richer internal representations. These help machines better interpretation, reasoning, and making decisions in real-life situations. This field includes core techniques such as representation learning (to get shared features from different data types), alignment methods (to match information across modalities), and fusion strategies (to combine them by deep learning models). Although there has been good progress, some major problems still remain. Like dealing with different data formats, missing or incomplete inputs, and defending against adversarial attacks. Researchers now are exploring new methods, such as unsupervised or semi-supervised learning, AutoML tools, to make models more efficient and easier to scale. And also more attention on designing better evaluation metrics or building shared benchmarks, make it easier to compare model performance across tasks and domains. As the field continues to grow, multi-modal learning is expected to improve many areas: computer vision, natural language processing, speech recognition, and healthcare. In the future, it may help to build AI systems that can understand the world in a way more like humans, flexible, context aware, and able to deal with real-world complexity.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The kernel of graph indices for vector search</title>
<link>https://arxiv.org/abs/2506.20584</link>
<guid>https://arxiv.org/abs/2506.20584</guid>
<content:encoded><![CDATA[
arXiv:2506.20584v2 Announce Type: replace 
Abstract: The most popular graph indices for vector search use principles from computational geometry to build the graph. Hence, their formal graph navigability guarantees are only valid in Euclidean space. In this work, we show that machine learning can be used to build graph indices for vector search in metric and non-metric vector spaces (e.g., for inner product similarity). From this novel perspective, we introduce the Support Vector Graph (SVG), a new type of graph index that leverages kernel methods to establish the graph connectivity and that comes with formal navigability guarantees valid in metric and non-metric vector spaces. In addition, we interpret the most popular graph indices, including HNSW and DiskANN, as particular specializations of SVG and show that new navigable indices can be derived from the principles behind this specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$ sparsity constraint into the SVG kernel method to build graphs with a bounded out-degree. This yields a principled way of implementing this practical requirement, in contrast to the traditional heuristic of simply truncating the out edges of each node. Additionally, we show that SVG-L0 has a self-tuning property that avoids the heuristic of using a set of candidates to find the out-edges of each node and that keeps its computational complexity in check.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v4 Announce Type: replace 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods</title>
<link>https://arxiv.org/abs/2507.18242</link>
<guid>https://arxiv.org/abs/2507.18242</guid>
<content:encoded><![CDATA[
arXiv:2507.18242v3 Announce Type: replace 
Abstract: Despite their theoretical appeal, totally corrective boosting methods based on linear programming have received limited empirical attention. In this paper, we conduct the first large-scale experimental study of six LP-based boosting formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20 diverse datasets. We evaluate the use of both heuristic and optimal base learners within these formulations, and analyze not only accuracy, but also ensemble sparsity, margin distribution, anytime performance, and hyperparameter sensitivity. We show that totally corrective methods can outperform or match state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees, while producing significantly sparser ensembles. We further show that these methods can thin pre-trained ensembles without sacrificing performance, and we highlight both the strengths and limitations of using optimal decision trees in this context.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</title>
<link>https://arxiv.org/abs/2508.17702</link>
<guid>https://arxiv.org/abs/2508.17702</guid>
<content:encoded><![CDATA[
arXiv:2508.17702v2 Announce Type: replace 
Abstract: AI-driven molecular generation is reshaping drug discovery and materials design, yet the lack of protection mechanisms leaves AI-generated molecules vulnerable to unauthorized reuse and provenance ambiguity. Such limitation undermines both scientific reproducibility and intellectual property security. To address this challenge, we propose the first deep learning based watermarking framework for molecules (MolMark), which is exquisitely designed to embed high-fidelity digital signatures into molecules without compromising molecular functionalities. MolMark learns to modulate the chemically meaningful atom-level representations and enforce geometric robustness through SE(3)-invariant features, maintaining robustness under rotation, translation, and reflection. Additionally, MolMark integrates seamlessly with AI-based molecular generative models, enabling watermarking to be treated as a learned transformation with minimal interference to molecular structures. Experiments on benchmark datasets (QM9, GEOM-DRUG) and state-of-the-art molecular generative models (GeoBFN, GeoLDM) demonstrate that MolMark can embed 16-bit watermarks while retaining more than 90% of essential molecular properties, preserving downstream performance, and enabling >95% extraction accuracy under SE(3) transformations. MolMark establishes a principled pathway for unifying molecular generation with verifiable authorship, supporting trustworthy and accountable AI-driven molecular discovery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes</title>
<link>https://arxiv.org/abs/2508.19009</link>
<guid>https://arxiv.org/abs/2508.19009</guid>
<content:encoded><![CDATA[
arXiv:2508.19009v3 Announce Type: replace 
Abstract: Heterogeneous Federated Learning (HFL) has gained significant attention for its capacity to handle both model and data heterogeneity across clients. Prototype-based HFL methods emerge as a promising solution to address statistical and model heterogeneity as well as privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing class-representative prototypes among heterogeneous clients. However, aggregating these prototypes via standard weighted averaging often yields sub-optimal global knowledge. Specifically, the averaging approach induces a shrinking of the aggregated prototypes' decision margins, thereby degrading model performance in scenarios with model heterogeneity and non-IID data distributions. The propose FedProtoKD in a Heterogeneous Federated Learning setting, utilizing an enhanced dual-knowledge distillation mechanism to enhance system performance by leveraging clients' logits and prototype feature representations. The proposed framework aims to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, the framework assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD improved test accuracy by an average of 1.13% and up to 34.13% across various settings, significantly outperforming existing state-of-the-art HFL methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems</title>
<link>https://arxiv.org/abs/2508.19011</link>
<guid>https://arxiv.org/abs/2508.19011</guid>
<content:encoded><![CDATA[
arXiv:2508.19011v3 Announce Type: replace 
Abstract: Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Aved{\o}re), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGDM: Learning EEG Representation with Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2508.20705</link>
<guid>https://arxiv.org/abs/2508.20705</guid>
<content:encoded><![CDATA[
arXiv:2508.20705v2 Announce Type: replace 
Abstract: Recent advances in self-supervised learning for EEG representation have largely relied on masked reconstruction, where models are trained to recover randomly masked signal segments. While effective at modeling local dependencies, such objectives are inherently limited in capturing the global dynamics and long-range dependencies essential for characterizing neural activity. To address this limitation, we propose EEGDM, a novel self-supervised framework that leverages latent diffusion models to generate EEG signals as an objective. Unlike masked reconstruction, diffusion-based generation progressively denoises signals from noise to realism, compelling the model to capture holistic temporal patterns and cross-channel relationships. Specifically, EEGDM incorporates an EEG encoder that distills raw signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) reconstructs high-quality EEG signals, (2) learns robust representations, and (3) achieves competitive performance across diverse downstream tasks, thus exploring a new direction for self-supervised EEG representation learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration</title>
<link>https://arxiv.org/abs/2509.25977</link>
<guid>https://arxiv.org/abs/2509.25977</guid>
<content:encoded><![CDATA[
arXiv:2509.25977v2 Announce Type: replace 
Abstract: The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Masked Diffusion for Provable Self-Correction</title>
<link>https://arxiv.org/abs/2510.01384</link>
<guid>https://arxiv.org/abs/2510.01384</guid>
<content:encoded><![CDATA[
arXiv:2510.01384v3 Announce Type: replace 
Abstract: A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generic Machine Learning Framework for Radio Frequency Fingerprinting</title>
<link>https://arxiv.org/abs/2510.09775</link>
<guid>https://arxiv.org/abs/2510.09775</guid>
<content:encoded><![CDATA[
arXiv:2510.09775v3 Announce Type: replace 
Abstract: Fingerprinting radio frequency (RF) emitters typically involves finding unique characteristics that are featured in their received signal. These fingerprints are nuanced, but sufficiently detailed, motivating the pursuit of methods that can successfully extract them. The downstream task that requires the most meticulous RF fingerprinting (RFF) is known as specific emitter identification (SEI), which entails recognising each individual transmitter. RFF and SEI have a long history, with numerous defence and civilian applications such as signal intelligence, electronic surveillance, physical-layer authentication of wireless devices, to name a few. In recent years, data-driven RFF approaches have become popular due to their ability to automatically learn intricate fingerprints. They generally deliver superior performance when compared to traditional RFF techniques that are often labour-intensive, inflexible, and only applicable to a particular emitter type or transmission scheme. In this paper, we present a generic and versatile machine learning (ML) framework for data-driven RFF with several popular downstream tasks such as SEI, data association (EDA) and RF emitter clustering (RFEC). It is emitter-type agnostic. We then demonstrate the introduced framework for several tasks using real RF datasets for spaceborne surveillance, signal intelligence and countering drones applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASecond-Order SpikingSSM for Wearables</title>
<link>https://arxiv.org/abs/2510.14386</link>
<guid>https://arxiv.org/abs/2510.14386</guid>
<content:encoded><![CDATA[
arXiv:2510.14386v2 Announce Type: replace 
Abstract: Spiking neural networks have garnered increasing attention due to their energy efficiency, multiplication-free computation, and sparse event-based processing. In parallel, state space models have emerged as scalable alternatives to transformers for long-range sequence modelling by avoiding quadratic dependence on sequence length. We propose SHaRe-SSM (Spiking Harmonic Resonate-and-Fire State Space Model), a second-order spiking SSM for classification and regression on ultra-long sequences. SHaRe-SSM outperforms transformers and first-order SSMs on average while eliminating matrix multiplications, making it highly suitable for resource-constrained applications. To ensure fast computation over tens of thousands of time steps, we leverage a parallel scan formulation of the underlying dynamical system. Furthermore, we introduce a kernel-based spiking regressor, which enables the accurate modelling of dependencies in sequences of up to 50k steps. Our results demonstrate that SHaRe-SSM achieves superior long-range modelling capability with energy efficiency (52.1x less than ANN-based second order SSM), positioning it as a strong candidate for resource-constrained devices such as wearables
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2510.16882</link>
<guid>https://arxiv.org/abs/2510.16882</guid>
<content:encoded><![CDATA[
arXiv:2510.16882v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction</title>
<link>https://arxiv.org/abs/2510.23117</link>
<guid>https://arxiv.org/abs/2510.23117</guid>
<content:encoded><![CDATA[
arXiv:2510.23117v3 Announce Type: replace 
Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Deep Physics-Informed Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2510.23501</link>
<guid>https://arxiv.org/abs/2510.23501</guid>
<content:encoded><![CDATA[
arXiv:2510.23501v2 Announce Type: replace 
Abstract: Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed machine learning (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on nine standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[
arXiv:2511.00040v2 Announce Type: replace 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Causal Market Simulators</title>
<link>https://arxiv.org/abs/2511.04469</link>
<guid>https://arxiv.org/abs/2511.04469</guid>
<content:encoded><![CDATA[
arXiv:2511.04469v3 Announce Type: replace 
Abstract: Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incremental Generation is Necessary and Sufficient for Universality in Flow-Based Modelling</title>
<link>https://arxiv.org/abs/2511.09902</link>
<guid>https://arxiv.org/abs/2511.09902</guid>
<content:encoded><![CDATA[
arXiv:2511.09902v2 Announce Type: replace 
Abstract: Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.
  Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $O(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[
arXiv:2511.11571v2 Announce Type: replace 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.12817</link>
<guid>https://arxiv.org/abs/2511.12817</guid>
<content:encoded><![CDATA[
arXiv:2511.12817v2 Announce Type: replace 
Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look-Ahead Reasoning on Learning Platforms</title>
<link>https://arxiv.org/abs/2511.14745</link>
<guid>https://arxiv.org/abs/2511.14745</guid>
<content:encoded><![CDATA[
arXiv:2511.14745v2 Announce Type: replace 
Abstract: On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-k thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. Look-ahead reasoning can be seen as a generalization of algorithmic collective action; we thus offer the first results characterizing the utility trade-offs of coordination when contesting algorithmic systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Gaussian Process Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2511.18214</link>
<guid>https://arxiv.org/abs/2511.18214</guid>
<content:encoded><![CDATA[
arXiv:2511.18214v2 Announce Type: replace 
Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xGR: Efficient Generative Recommendation Serving at Scale</title>
<link>https://arxiv.org/abs/2512.11529</link>
<guid>https://arxiv.org/abs/2512.11529</guid>
<content:encoded><![CDATA[
arXiv:2512.11529v2 Announce Type: replace 
Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially private Bayesian tests</title>
<link>https://arxiv.org/abs/2401.15502</link>
<guid>https://arxiv.org/abs/2401.15502</guid>
<content:encoded><![CDATA[
arXiv:2401.15502v3 Announce Type: replace-cross 
Abstract: Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showcased via several numerical experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning</title>
<link>https://arxiv.org/abs/2402.04114</link>
<guid>https://arxiv.org/abs/2402.04114</guid>
<content:encoded><![CDATA[
arXiv:2402.04114v3 Announce Type: replace-cross 
Abstract: In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy $\epsilon$. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?</title>
<link>https://arxiv.org/abs/2408.07588</link>
<guid>https://arxiv.org/abs/2408.07588</guid>
<content:encoded><![CDATA[
arXiv:2408.07588v5 Announce Type: replace-cross 
Abstract: Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question "How big is big enough?" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Perturbative Trivializing Flows for Lattice Gauge Theories</title>
<link>https://arxiv.org/abs/2410.13161</link>
<guid>https://arxiv.org/abs/2410.13161</guid>
<content:encoded><![CDATA[
arXiv:2410.13161v2 Announce Type: replace-cross 
Abstract: Continuous normalizing flows are known to be highly expressive and flexible, which allows for easier incorporation of large symmetries and makes them a powerful computational tool for lattice field theories. Building on previous work, we present a general continuous normalizing flow architecture for matrix Lie groups that is equivariant under group transformations. We apply this to lattice gauge theories in two dimensions as a proof of principle and demonstrate competitive performance, showing its potential as a tool for future lattice computations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic PET Image Prediction Using a Network Combining Reversible and Irreversible Modules</title>
<link>https://arxiv.org/abs/2410.22674</link>
<guid>https://arxiv.org/abs/2410.22674</guid>
<content:encoded><![CDATA[
arXiv:2410.22674v2 Announce Type: replace-cross 
Abstract: Dynamic positron emission tomography (PET) images can reveal the distribution of tracers in the organism and the dynamic processes involved in biochemical reactions, and it is widely used in clinical practice. Despite the high effectiveness of dynamic PET imaging in studying the kinetics and metabolic processes of radiotracers. Pro-longed scan times can cause discomfort for both patients and medical personnel. This study proposes a dynamic frame prediction method for dynamic PET imaging, reduc-ing dynamic PET scanning time by applying a multi-module deep learning framework composed of reversible and irreversible modules. The network can predict kinetic parameter images based on the early frames of dynamic PET images, and then generate complete dynamic PET images. In validation experiments with simulated data, our network demonstrated good predictive performance for kinetic parameters and was able to reconstruct high-quality dynamic PET images. Additionally, in clinical data experiments, the network exhibited good generalization performance and attached that the proposed method has promising clinical application prospects.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Learning for Variable Importance</title>
<link>https://arxiv.org/abs/2411.02221</link>
<guid>https://arxiv.org/abs/2411.02221</guid>
<content:encoded><![CDATA[
arXiv:2411.02221v2 Announce Type: replace-cross 
Abstract: Variable importance is one of the most widely used measures for interpreting machine learning with significant interest from both statistics and machine learning communities. Recently, increasing attention has been directed toward uncertainty quantification in these metrics. Current approaches largely rely on one-step procedures, which, while asymptotically efficient, can present higher sensitivity and instability in finite sample settings. To address these limitations, we propose a novel method by employing the targeted learning (TL) framework, designed to enhance robustness in inference for variable importance metrics. Our approach is particularly suited for conditional permutation variable importance. We show that it (i) retains the asymptotic efficiency of traditional methods, (ii) maintains comparable computational complexity, and (iii) delivers improved accuracy, especially in finite sample contexts. We further support these findings with numerical experiments that illustrate the practical advantages of our method and validate the theoretical results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refined Analysis of Federated Averaging and Federated Richardson-Romberg</title>
<link>https://arxiv.org/abs/2412.01389</link>
<guid>https://arxiv.org/abs/2412.01389</guid>
<content:encoded><![CDATA[
arXiv:2412.01389v2 Announce Type: replace-cross 
Abstract: In this paper, we present a novel analysis of \FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution. We provide a first-order bias expansion in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement</title>
<link>https://arxiv.org/abs/2412.12667</link>
<guid>https://arxiv.org/abs/2412.12667</guid>
<content:encoded><![CDATA[
arXiv:2412.12667v2 Announce Type: replace-cross 
Abstract: This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence</title>
<link>https://arxiv.org/abs/2502.01890</link>
<guid>https://arxiv.org/abs/2502.01890</guid>
<content:encoded><![CDATA[
arXiv:2502.01890v4 Announce Type: replace-cross 
Abstract: 3D cell segmentation methods are often hindered by \emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2502.01956</link>
<guid>https://arxiv.org/abs/2502.01956</guid>
<content:encoded><![CDATA[
arXiv:2502.01956v3 Announce Type: replace-cross 
Abstract: Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate a 100% success rate (vs. 90% baseline). We also present an offline variant that achieves state-of-the-art results on OGBench benchmarks, with up to 71% absolute gains on giant HumanoidMaze tasks, demonstrating our core contributions are architecture-agnostic. The method also generalizes to momentum-based control tasks and requires only log N steps for replanning. Theoretical analysis and ablations validate our design choices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling</title>
<link>https://arxiv.org/abs/2502.03480</link>
<guid>https://arxiv.org/abs/2502.03480</guid>
<content:encoded><![CDATA[
arXiv:2502.03480v2 Announce Type: replace-cross 
Abstract: Evaluating the predictive performance of species distribution models (SDMs) under realistic deployment scenarios requires careful handling of spatial and temporal dependencies in the data. Cross-validation (CV) is the standard approach for model evaluation, but its design strongly influences the validity of performance estimates. When SDMs are intended for spatial or temporal transfer, random CV can lead to overoptimistic results due to spatial autocorrelation (SAC) among neighboring observations.
  We benchmark four machine learning algorithms (GBM, XGBoost, LightGBM, Random Forest) on two real-world presence-absence datasets, a temperate plant and an anadromous fish, using multiple CV designs: random, spatial, spatio-temporal, environmental, and forward-chaining. Two training data usage strategies (LAST FOLD and RETRAIN) are evaluated, with hyperparameter tuning performed within each CV scheme. Model performance is assessed on independent out-of-time test sets using AUC, MAE, and correlation metrics.
  Random CV overestimates AUC by up to 0.16 and produces MAE values up to 80 percent higher than spatially blocked alternatives. Blocking at the empirical SAC range substantially reduces this bias. Training strategy affects evaluation outcomes: LAST FOLD yields smaller validation-test discrepancies under strong SAC, while RETRAIN achieves higher test AUC when SAC is weaker. Boosted ensemble models consistently perform best under spatially structured CV designs. We recommend a robust SDM workflow based on SAC-aware blocking, blocked hyperparameter tuning, and external temporal validation to improve reliability under spatial and temporal shifts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing</title>
<link>https://arxiv.org/abs/2502.09652</link>
<guid>https://arxiv.org/abs/2502.09652</guid>
<content:encoded><![CDATA[
arXiv:2502.09652v3 Announce Type: replace-cross 
Abstract: Shape deviation modeling and compensation in additive manufacturing are pivotal for achieving high geometric accuracy and enabling industrial-scale production. Critical challenges persist, including generalizability across complex geometries and adaptability to position-dependent variations in batch production. Traditional methods of controlling geometric deviations often rely on complex parameterized models and repetitive metrology, which can be time-consuming yet not applicable for batch production. In this paper, we present a novel, process-agnostic approach to address the challenge of ensuring geometric precision and accuracy in position-dependent AM production. The proposed GraphCompNet presents a novel computational framework integrating graph-based neural networks with a GAN inspired training paradigm. The framework leverages point cloud representations and dynamic graph convolutional neural networks (DGCNNs) to model intricate geometries while incorporating position-specific thermal and mechanical variations. A two-stage adversarial training process iteratively refines compensated designs using a compensator-predictor architecture, enabling real-time feedback and optimization. Experimental validation across various shapes and positions demonstrates the framework's ability to predict deviations in freeform geometries and adapt to position-dependent batch production conditions, significantly improving compensation accuracy (35 to 65 percent) across the entire printing space, addressing position-dependent variabilities within the print chamber. The proposed method advances the development of a Digital Twin for AM, offering scalable, real-time monitoring and compensation capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v4 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets</title>
<link>https://arxiv.org/abs/2503.21526</link>
<guid>https://arxiv.org/abs/2503.21526</guid>
<content:encoded><![CDATA[
arXiv:2503.21526v3 Announce Type: replace-cross 
Abstract: In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title>
<link>https://arxiv.org/abs/2504.03790</link>
<guid>https://arxiv.org/abs/2504.03790</guid>
<content:encoded><![CDATA[
arXiv:2504.03790v2 Announce Type: replace-cross 
Abstract: Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Archetypal Analysis</title>
<link>https://arxiv.org/abs/2504.12392</link>
<guid>https://arxiv.org/abs/2504.12392</guid>
<content:encoded><![CDATA[
arXiv:2504.12392v2 Announce Type: replace-cross 
Abstract: Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and Leo Breiman as a computational procedure for extracting distinct aspects, so-called archetypes, from observations, with each observational record approximated as a mixture (i.e., convex combination) of these archetypes. AA thereby provides straightforward, interpretable, and explainable representations for feature extraction and dimensionality reduction, facilitating the understanding of the structure of high-dimensional data and enabling wide applications across the sciences. However, AA also faces challenges, particularly as the associated optimization problem is non-convex. This is the first survey that provides researchers and data mining practitioners with an overview of the methodologies and opportunities that AA offers, surveying the many applications of AA across disparate fields of science, as well as best practices for modeling data with AA and its limitations. The survey concludes by explaining crucial future research directions concerning AA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Stochastic Occupation Kernel (SOCK) Method for Learning Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2505.11622</link>
<guid>https://arxiv.org/abs/2505.11622</guid>
<content:encoded><![CDATA[
arXiv:2505.11622v2 Announce Type: replace-cross 
Abstract: We present a novel kernel-based method for learning multivariate stochastic differential equations (SDEs). The method follows a two-step procedure: we first estimate the drift term function, then the (matrix-valued) diffusion function given the drift. Occupation kernels are integral functionals on a reproducing kernel Hilbert space (RKHS) that aggregate information over a trajectory. Our approach leverages vector-valued occupation kernels for estimating the drift component of the stochastic process. For diffusion estimation, we extend this framework by introducing operator-valued occupation kernels, enabling the estimation of an auxiliary matrix-valued function as a positive semi-definite operator, from which we readily derive the diffusion estimate. This enables us to avoid common challenges in SDE learning, such as intractable likelihoods, by optimizing a reconstruction-error-based objective. We propose a simple learning procedure that retains strong predictive accuracy while using Fenchel duality to promote efficiency. We validate the method on simulated benchmarks and a real-world dataset of Amyloid imaging in healthy and Alzheimer's disease subjects.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions</title>
<link>https://arxiv.org/abs/2505.12289</link>
<guid>https://arxiv.org/abs/2505.12289</guid>
<content:encoded><![CDATA[
arXiv:2505.12289v2 Announce Type: replace-cross 
Abstract: Efficient matrix trace estimation is essential for scalable computation of log-determinants, matrix norms, and distributional divergences. In many large-scale applications, the matrices involved are too large to store or access in full, making even a single matrix-vector (mat-vec) product infeasible. Instead, one often has access only to small subblocks of the matrix or localized matrix-vector products on restricted index sets. Hutch++ achieves optimal convergence rate but relies on randomized SVD and assumes full mat-vec access, making it difficult to apply in these constrained settings. We propose the Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches Hutch++ accuracy with a simpler implementation based on orthonormal block probes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature (SLQ) framework, which combines random probing with Krylov subspace methods to efficiently approximate traces of matrix functions, and performs better than Hutch++ in near flat-spectrum regimes. To address memory limitations and partial access constraints, we introduce Subblock SLQ, a variant of BOLT that operates only on small principal submatrices. As a result, this framework yields a proxy KL divergence estimator and an efficient method for computing the Wasserstein-2 distance between Gaussians - both compatible with low-memory and partial-access regimes. We provide theoretical guarantees and demonstrate strong empirical performance across a range of high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering and Pruning in Causal Data Fusion</title>
<link>https://arxiv.org/abs/2505.15215</link>
<guid>https://arxiv.org/abs/2505.15215</guid>
<content:encoded><![CDATA[
arXiv:2505.15215v2 Announce Type: replace-cross 
Abstract: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiological boundary conditions</title>
<link>https://arxiv.org/abs/2506.11683</link>
<guid>https://arxiv.org/abs/2506.11683</guid>
<content:encoded><![CDATA[
arXiv:2506.11683v2 Announce Type: replace-cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Uncertainty in the Presence of Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.18283</link>
<guid>https://arxiv.org/abs/2506.18283</guid>
<content:encoded><![CDATA[
arXiv:2506.18283v2 Announce Type: replace-cross 
Abstract: Neural networks make accurate predictions but often fail to provide reliable uncertainty estimates, especially under covariate distribution shifts between training and testing. To address this problem, we propose a Bayesian framework for uncertainty estimation that explicitly accounts for covariate shifts. While conventional approaches rely on fixed priors, the key idea of our method is an adaptive prior, conditioned on both training and new covariates. This prior naturally increases uncertainty for inputs that lie far from the training distribution in regions where predictive performance is likely to degrade. To efficiently approximate the resulting posterior predictive distribution, we employ amortized variational inference. Finally, we construct synthetic environments by drawing small bootstrap samples from the training data, simulating a range of plausible covariate shift using only the original dataset. We evaluate our method on both synthetic and real-world data. It yields substantially improved uncertainty estimates under distribution shifts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models</title>
<link>https://arxiv.org/abs/2506.20915</link>
<guid>https://arxiv.org/abs/2506.20915</guid>
<content:encoded><![CDATA[
arXiv:2506.20915v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[
arXiv:2507.01939v4 Announce Type: replace-cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</title>
<link>https://arxiv.org/abs/2507.17860</link>
<guid>https://arxiv.org/abs/2507.17860</guid>
<content:encoded><![CDATA[
arXiv:2507.17860v3 Announce Type: replace-cross 
Abstract: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title>
<link>https://arxiv.org/abs/2508.10501</link>
<guid>https://arxiv.org/abs/2508.10501</guid>
<content:encoded><![CDATA[
arXiv:2508.10501v4 Announce Type: replace-cross 
Abstract: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice</title>
<link>https://arxiv.org/abs/2508.20717</link>
<guid>https://arxiv.org/abs/2508.20717</guid>
<content:encoded><![CDATA[
arXiv:2508.20717v2 Announce Type: replace-cross 
Abstract: Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning-Driven Predictive Resource Management in Complex Science Workflows</title>
<link>https://arxiv.org/abs/2509.11512</link>
<guid>https://arxiv.org/abs/2509.11512</guid>
<content:encoded><![CDATA[
arXiv:2509.11512v2 Announce Type: replace-cross 
Abstract: The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair</title>
<link>https://arxiv.org/abs/2509.16187</link>
<guid>https://arxiv.org/abs/2509.16187</guid>
<content:encoded><![CDATA[
arXiv:2509.16187v2 Announce Type: replace-cross 
Abstract: Code translation transforms source code from one programming language (PL) to another. Validating the functional equivalence of translation and repairing, if necessary, are critical steps in code translation. Existing automated validation and repair approaches struggle to generalize to many PLs due to high engineering overhead, and they rely on existing and often inadequate test suites, which results in false claims of equivalence and ineffective translation repair. We develop MatchFixAgent, a large language model (LLM)-based, PL-agnostic framework for equivalence validation and repair of translations. MatchFixAgent features a multi-agent architecture that divides equivalence validation into several sub-tasks to ensure thorough and consistent semantic analysis of the translation. Then it feeds this analysis to test agent to write and execute tests. Upon observing a test failure, the repair agent attempts to fix the translation bug. The final (in)equivalence decision is made by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four repository-level code translation techniques. We use 2,219 translation pairs from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub projects totaling over 900K lines of code. Our results demonstrate that MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs, with the same equivalence validation result as prior work on 72.8% of them. When MatchFixAgent's result disagrees with prior work, we find that 60.7% of the time MatchFixAgent's result is actually correct. In addition, we show that MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to many PL pairs than prior work, while producing highly accurate validation results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Machine Learning Pipeline: Large Language Models-Assisted Automated Dataset Generation for Training Machine-Learned Interatomic Potentials</title>
<link>https://arxiv.org/abs/2509.21647</link>
<guid>https://arxiv.org/abs/2509.21647</guid>
<content:encoded><![CDATA[
arXiv:2509.21647v2 Announce Type: replace-cross 
Abstract: Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/{\AA} in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-{\AA} accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Impact of Structured Output Format on Large Language Models through Causal Inference</title>
<link>https://arxiv.org/abs/2509.21791</link>
<guid>https://arxiv.org/abs/2509.21791</guid>
<content:encoded><![CDATA[
arXiv:2509.21791v3 Announce Type: replace-cross 
Abstract: Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions. Further experiments show that OpenAI-o3 are more resilient to output formats than general-purpose GPT-4o and GPT-4.1, highlighting an unaware advantage of reasoning models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[
arXiv:2510.24830v2 Announce Type: replace-cross 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2511.04461</link>
<guid>https://arxiv.org/abs/2511.04461</guid>
<content:encoded><![CDATA[
arXiv:2511.04461v2 Announce Type: replace-cross 
Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions of a high-speed catamaran, namely the Delft 372 model. Experimental measurements (time histories) of wave elevation at the longitudinal center of gravity, heave, pitch, notional flight-deck velocity, notional bridge acceleration, and total resistance were collected from irregular wave basin tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5 conditions at Fr = 0.425, and organized into training, validation, and test sets. The HDMDc algorithm constructs an equation-free linear reduced-order model of the seakeeping vessel by augmenting states and inputs with their time-lagged copies to capture nonlinear and memory effects. Two ensembling strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters considered stochastic variables with prior distribution to produce posterior mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which aggregates multiple model obtained over data subsets, are compared in providing seakeeping prediction and uncertainty quantification. The FHDMDc approach is found to improve the accuracy of the predictions compared to the deterministic counterpart, also providing robust uncertainty estimation; whereas the application of BHDMDc to the present test case is not found beneficial in comparison to the deterministic model. FHDMDc-derived probability density functions for the motions closely match both experimental data and URANS results, demonstrating reliable and computationally efficient seakeeping prediction for design and operational support.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized infinite dimensional Alpha-Procrustes based geometries</title>
<link>https://arxiv.org/abs/2511.09801</link>
<guid>https://arxiv.org/abs/2511.09801</guid>
<content:encoded><![CDATA[
arXiv:2511.09801v2 Announce Type: replace-cross 
Abstract: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[
arXiv:2511.12346v2 Announce Type: replace-cross 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients</title>
<link>https://arxiv.org/abs/2512.02342</link>
<guid>https://arxiv.org/abs/2512.02342</guid>
<content:encoded><![CDATA[
arXiv:2512.02342v2 Announce Type: replace-cross 
Abstract: The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. On non-smooth convex benchmarks, our experiments are consistent with the theoretical predictions on how the safeguard affects the convergence neighborhood. On deep neural networks the proposed step size achieves competitive performance to existing adaptive baselines and exhibits stable behavior across a wide range of problem settings. Moreover, in these experiments, the gradient norms under our step size do not collapse to (near) zero, indicating robustness to vanishing gradients.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[
arXiv:2512.06699v2 Announce Type: replace-cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[
arXiv:2512.07540v2 Announce Type: replace-cross 
Abstract: Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
arXiv:2512.11575v2 Announce Type: replace-cross 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Estimate and Convergence Analysis for Data Valuation</title>
<link>https://arxiv.org/abs/2511.06463</link>
<guid>https://arxiv.org/abs/2511.06463</guid>
<content:encoded><![CDATA[
<div> Data valuation, neural dynamic data valuation, error estimation, convergence analysis, sublinear convergence<br /><br />Summary:<br /><br />This paper focuses on data valuation, which quantifies the importance of data in machine learning, highlighting that existing methods lack guarantees of validity within a single training process. It builds upon the neural dynamic data valuation (NDDV) method as a foundation. The authors are the first to explore error estimation and convergence analysis specifically in the context of data valuation using NDDV. Under assumptions of Lipschitz continuity and smoothness, they derive quadratic error bounds for the difference in loss values, which inversely scale with the number of time steps and scale quadratically with control variations, thus ensuring stability of the valuation process. Furthermore, they demonstrate theoretically that the expected squared norm of the gradient of the training loss approaches zero asymptotically, indicating vanishing gradients in the long run of training. Additionally, the meta loss, reflecting the quality of data valuation, is shown to converge sublinearly over iterations. Overall, the analysis proves that the NDDV method achieves sublinear convergence rates, providing both stability and theoretical convergence guarantees in single training processes for data valuation. <div>
arXiv:2511.06463v3 Announce Type: replace 
Abstract: Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression</title>
<link>https://arxiv.org/abs/2512.15721</link>
<guid>https://arxiv.org/abs/2512.15721</guid>
<content:encoded><![CDATA[
<div> Keywords: DiscoverDCP, symbolic regression, Disciplined Convex Programming, convex models, system identification<br /><br />Summary:<br /><br />1. The paper introduces DiscoverDCP, a novel data-driven framework integrating symbolic regression with Disciplined Convex Programming (DCP) rules for system identification.<br /><br />2. By enforcing that all candidate model expressions comply with DCP composition rules, the method guarantees that discovered expressions are globally convex by design, avoiding the difficult problem of verifying convexity after model discovery.<br /><br />3. This approach enables the identification of convex surrogate models that go beyond standard fixed-parameter convex forms like quadratic functions, allowing for more flexible and accurate representations.<br /><br />4. The models produced by DiscoverDCP are interpretable and verifiable, making them well-suited for applications in safety-critical control and optimization where guarantees on convexity and model behavior are crucial.<br /><br />5. Overall, DiscoverDCP advances symbolic regression by embedding convexity constraints directly into the model discovery process, producing convex, flexible, and trustworthy models for system identification tasks. <div>
arXiv:2512.15721v1 Announce Type: new 
Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Ensemble Learning for S\&amp;P 500 Directional Prediction</title>
<link>https://arxiv.org/abs/2512.15738</link>
<guid>https://arxiv.org/abs/2512.15738</guid>
<content:encoded><![CDATA[
<div> Keywords: financial market prediction, ensemble learning, quantum sentiment analysis, Decision Transformer, model selection

<br /><br />Summary:  
This study presents a hybrid ensemble framework for financial market prediction that achieves a directional accuracy of 60.14% on S&amp;P 500 data, surpassing typical model performance by about 3.10%. The framework integrates quantum-enhanced sentiment analysis using a 4-qubit variational quantum circuit, which contributes an accuracy gain of 0.8% to 1.5% per model. It combines diverse learning architectures—including LSTM, Decision Transformer, XGBoost, Random Forest, and Logistic Regression—on the same dataset, proving more effective than training identical models on multiple datasets (60.14% vs. 52.80%), supported by correlation analysis showing higher agreement among same-architecture models (r > 0.6). The approach employs strategic model selection by filtering out weaker predictors with accuracy below 52%, which improves ensemble results significantly (top 7 models achieve 60.14% compared to 51.2% when using all 35 models). The evaluation spans seven market instruments from 2020 to 2023, covering various regimes, including the COVID-19 crash and inflation-driven downturn, with McNemar’s test confirming statistical significance (p < 0.05). Preliminary backtesting with confidence-based filtering (requiring 6+ model consensus) demonstrates practical trading value, yielding a Sharpe ratio of 1.2 compared to 0.8 for a buy-and-hold strategy. <div>
arXiv:2512.15738v1 Announce Type: new 
Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&amp;P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</title>
<link>https://arxiv.org/abs/2512.15742</link>
<guid>https://arxiv.org/abs/2512.15742</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, memory wall, holographic topology, vector quantization, hardware-aware compiler<br /><br />Summary:<br /><br />1. Kolmogorov-Arnold Networks (KANs) suffer from a "memory wall" due to a large number of learned basis functions that create heavy parameter counts, demanding high memory bandwidth and making deployment in memory-limited environments challenging.  
2. Vision KANs display a holographic topology where information is not localized but distributed across spline interference patterns, which causes traditional pruning methods to perform poorly, as evidenced by a drastic accuracy drop (from 85.23% to 45%) at only 10% sparsity.  
3. To overcome this, the authors propose SHARe-KAN, a novel compression framework using Gain-Shape-Bias Vector Quantization that exploits functional redundancy while preserving the dense and distributed nature of the holographic topology.  
4. Together with LUTHAM, a hardware-aware compiler performing static memory planning, the approach achieves an 88× reduction in runtime memory footprint (reducing usage from 1.13 GB to 12.91 MB) without sacrificing accuracy on the PASCAL VOC benchmark.  
5. Profiling on NVIDIA's Ampere GPU architecture shows that the optimized workload maintains over 90% L2 cache residency, effectively decoupling the execution from costly DRAM bandwidth limitations typical in spline-based architectures. <div>
arXiv:2512.15742v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals</title>
<link>https://arxiv.org/abs/2512.15744</link>
<guid>https://arxiv.org/abs/2512.15744</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral graph neural networks, low-frequency signals, high-frequency signals, recommendation systems, graph embeddings<br /><br />Summary:  
This paper investigates the role of low-frequency and high-frequency graph signals in recommendation systems using spectral graph neural networks (GNNs). Firstly, it challenges the conventional emphasis on low-pass filtering by demonstrating that both low-frequency and high-frequency signals equivalently contribute to improving recommendation performance by smoothing user-item pair similarities. Secondly, the authors propose a frequency signal scaler, a flexible, plug-and-play module that adjusts the filtering function in GNNs to fine-tune the smoothness of graph signals, compatible with various GNN architectures. Thirdly, the paper identifies limitations in graph embedding-based methods, proving they cannot fully capture graph signal characteristics. To overcome this, a novel space flip method is introduced, which restores the expressive power of graph embeddings in representing graph signals. Fourthly, the study reveals that using solely low-frequency or high-frequency signals is sufficient for effective recommendation, simplifying the design of GNN-based recommenders. Finally, extensive experiments on four public datasets validate the theoretical insights and demonstrate the superior effectiveness of the proposed methods. The paper also provides public code for reproducibility at https://github.com/mojosey/SimGCF. <div>
arXiv:2512.15744v1 Announce Type: new 
Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA2.0: Scaling Up Diffusion Language Models to 100B</title>
<link>https://arxiv.org/abs/2512.15745</link>
<guid>https://arxiv.org/abs/2512.15745</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, large language model, auto-regressive conversion, Mixture-of-Experts, instruction tuning  

<br /><br />Summary:  
This paper introduces LLaDA2.0, a novel suite of discrete diffusion large language models (dLLMs) scaling up to 100 billion parameters. The key innovation lies in systematically converting pre-trained auto-regressive (AR) models into dLLMs, bypassing the need for expensive training from scratch while preserving inherited knowledge. The training follows a unique 3-phase block-level WSD (word sequence diffusion) scheme: starting with progressively increasing block sizes (warm-up), followed by large-scale full-sequence diffusion (stable phase), and concluding with reverting to compact block sizes (decay phase). This progressive method ensures stable training and effective adaptation of AR models to dLLMs. Post-training alignment is conducted using supervised fine-tuning (SFT) and direct preference optimization (DPO), producing two instruction-tuned Mixture-of-Experts (MoE) models: LLaDA2.0-mini at 16 billion parameters and LLaDA2.0-flash at 100 billion parameters. These models maintain the advantage of parallel decoding inherent in diffusion models, providing superior performance and efficiency for large-scale deployment. Both variants have been open-sourced, aiming to establish a new paradigm for frontier-scale LLM deployment that balances efficiency, performance, and practical usability. <div>
arXiv:2512.15745v1 Announce Type: new 
Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Generative-Predictive Framework for Deterministic Inverse Design</title>
<link>https://arxiv.org/abs/2512.15746</link>
<guid>https://arxiv.org/abs/2512.15746</guid>
<content:encoded><![CDATA[
<div> Inverse design, heterogeneous microstructures, generative model, latent manifold, physics-informed inversion  

<br /><br />Summary:  
This paper addresses the challenging problem of inverse design for heterogeneous material microstructures, known for its ill-posed nature and high computational demand due to high-dimensional design spaces and complex physics. The authors introduce Janus, a unified generative-predictive framework that combines a deep encoder-decoder architecture with a separable KHRONOS predictive head. Janus learns a latent manifold that is simultaneously isometric for generative inversion and optimized for physical prediction, promoting disentanglement in the latent space. The model is first validated on MNIST, showing high-fidelity image reconstruction, accurate classification, and diverse generative inversion across all classes. It is then applied to the inverse design of microstructures with targeted thermal conductivity, achieving high forward prediction accuracy (R² = 0.98) and low pixelwise reconstruction error (under 5%). Inverse solutions meet target properties within 1% relative error. Traversing the latent space reveals smooth transitions consistent with physical variations, supported by UMAP visualizations showing a low-dimensional, disentangled manifold. By integrating prediction and generation within one latent space, Janus enables real-time, physics-informed inverse microstructure design at a significantly lower computational cost compared to classical optimization methods. <div>
arXiv:2512.15746v1 Announce Type: new 
Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</title>
<link>https://arxiv.org/abs/2512.15747</link>
<guid>https://arxiv.org/abs/2512.15747</guid>
<content:encoded><![CDATA[
<div> Keywords: image classification, demographic bias, zero-shot learning, multimodal models, data generation  

<br /><br />Summary:  
This paper addresses challenges in zero-shot image classification, particularly focusing on demographic bias present in models like CLIP. First, the authors highlight that image classification remains difficult, especially for fine-grained tasks where models with limited capacity struggle due to underfitting. Second, the quality and diversity of cross-modal data is crucial, as biased or unbalanced datasets lead to skewed predictions favoring overrepresented demographics. Third, the study emphasizes the risks of harmful demographic bias in zero-shot classification setups and the need for mitigation strategies. Fourth, the authors propose Diverse Demographic Data Generation (D3G), a novel, training-free, zero-shot approach that enhances classification accuracy while reducing demographic bias by introducing diverse synthetic demographic data at inference time. Fifth, the method leverages CLIP as the base model combined with Stable Diffusion XL to generate diverse demographic imagery, improving balanced representation without additional model training. Finally, experiments demonstrate that D3G boosts performance and fairness, with further analysis exploring the impact of individual demographics on accuracy, underscoring the importance of demographic diversity for robust multimodal image classification. <div>
arXiv:2512.15747v1 Announce Type: new 
Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?</title>
<link>https://arxiv.org/abs/2512.15748</link>
<guid>https://arxiv.org/abs/2512.15748</guid>
<content:encoded><![CDATA[
<div> Visual Species Recognition, Few-shot Learning, Large Multimodal Models, Post-hoc Correction, Biodiversity Assessment<br /><br />Summary:<br /><br />Visual Species Recognition (VSR) is crucial for biodiversity assessment, ecology, and conservation research but typically requires extensive expert-annotated images, which are costly and limited. To address this, few-shot learning (FSL) expert models have been adopted, leveraging only a small number of labeled examples to train accurate species classifiers. Despite the success of Large Multimodal Models (LMMs) in general recognition tasks, they surprisingly underperform compared to simple fine-tuned FSL expert models on specialized VSR tasks, even with advanced prompting. However, LMMs demonstrate an ability to effectively correct errors made by FSL expert models when prompted with their top predictions. Building on this observation, the authors propose a Post-hoc Correction (POC) method that prompts LMMs to re-rank the top FSL predictions by incorporating softmax confidence scores and few-shot visual examples into enriched prompts. The POC method significantly improves classification accuracy by +6.4% across five challenging VSR benchmarks without requiring additional training, validation, or manual intervention. Furthermore, POC generalizes well to different pretrained backbones and LMM architectures, making it a versatile, plug-and-play enhancement for existing FSL approaches in species recognition. <div>
arXiv:2512.15748v1 Announce Type: new 
Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel</title>
<link>https://arxiv.org/abs/2512.15749</link>
<guid>https://arxiv.org/abs/2512.15749</guid>
<content:encoded><![CDATA[
<div> ReLU MLP, Neural Tangent Kernel, extrapolation, origin, quadratic extrapolation<br /><br />Summary:<br /><br />1. The article focuses on the extrapolation behavior of ReLU multilayer perceptrons (MLPs), particularly under the Neural Tangent Kernel (NTK) regime. <br />2. It is well-established that ReLU MLPs tend to extrapolate linearly when evaluating points out-of-distribution, but analysis near the origin remains less explored. <br />3. The NTK induces an infinite-dimensional feature map which is not translationally invariant, meaning extrapolation at points far from the origin differs fundamentally from that near the origin. <br />4. Due to the rotational invariance of the NTK’s feature map, studying points close to or far from the origin represents two extreme cases of extrapolation behavior. <br />5. The article discovers that, contrary to the classical linear extrapolation for distant points, there is a quadratic extrapolation characteristic when evaluating points near the origin under the ReLU NTK framework. <div>
arXiv:2512.15749v1 Announce Type: new 
Abstract: It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction</title>
<link>https://arxiv.org/abs/2512.15751</link>
<guid>https://arxiv.org/abs/2512.15751</guid>
<content:encoded><![CDATA[
<div> Agentic Workflows, Performance Prediction, Graph Neural Networks, Large Language Models, Contrastive Alignment  

<br /><br />Summary:  
Agentic Workflows (AWs) are an emerging approach designed to tackle complex tasks through structured, multi-step processes. The primary challenge lies in the scalability of automating AW generation due to costly and time-consuming execution-based evaluations. Current prediction methods attempt to estimate AW performance without actual execution but struggle to simultaneously capture both the complex topological dependencies among workflow components and the rich semantic logic embedded within. To overcome these limitations, the paper introduces GLOW, a unified predictive framework that synergizes Graph Neural Networks (GNNs) and Large Language Models (LLMs). GLOW employs a graph-oriented LLM, fine-tuned specifically on graph-related tasks, to extract semantic features that are sensitive to workflow topology. These semantic features are then fused with structural representations encoded by GNNs, creating a comprehensive understanding of AWs. Additionally, a contrastive alignment strategy is applied to refine the latent space, enhancing the distinction between high- and low-quality workflows. Extensive experimental validation on the FLORA-Bench benchmark demonstrates that GLOW significantly outperforms existing state-of-the-art methods in both prediction accuracy and the ranking utility of AWs. This work highlights the benefits of combining graph-based and semantic reasoning techniques for efficient workflow performance prediction. <div>
arXiv:2512.15751v1 Announce Type: new 
Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification</title>
<link>https://arxiv.org/abs/2512.15753</link>
<guid>https://arxiv.org/abs/2512.15753</guid>
<content:encoded><![CDATA[
<div> Keywords: encrypted traffic classification, Out-of-Distribution (OOD), TAO-Net, transformer, large language models  

<br /><br />Summary:  
This paper addresses the challenge of classifying encrypted network traffic, particularly the detection and fine-grained categorization of Out-of-Distribution (OOD) traffic generated by emerging applications that are not covered by existing models. Traditional traffic classification methods depend on predefined categories, limiting their ability to handle unknown traffic types, and often resort to grouping all unknown traffic into a single "Other" category, which lacks detail. To overcome this, the authors propose TAO-Net, a Two-stage Adaptive OOD classification Network. The first stage uses a hybrid OOD detection mechanism that combines transformer-based inter-layer transformation smoothness with feature analysis to accurately discern between In-Distribution (ID) and OOD encrypted traffic. The second stage employs large language models enhanced with a novel semantic prompt strategy, converting the OOD classification problem into a generation task that supports flexible and fine-grained classification without predefined labels. Experiments conducted on three datasets show that TAO-Net achieves superior performance, with macro-precision between 96.81% and 97.70% and macro-F1 scores between 96.77% and 97.68%, significantly outperforming previous methods that only achieve 44.73% to 86.30% macro-precision, especially excelling in recognizing newly emerging network applications. <div>
arXiv:2512.15753v1 Announce Type: new 
Abstract: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single "Other" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight</title>
<link>https://arxiv.org/abs/2512.15755</link>
<guid>https://arxiv.org/abs/2512.15755</guid>
<content:encoded><![CDATA[
<div> Keywords: Kolmogorov-Arnold Networks, interpretability, nonlinear associations, feature ranking, data visualization  

<br /><br />Summary:  
1. The article addresses the challenge of interpreting complex datasets, which is often hindered by high dimensionality and collinearity among variables.  
2. It introduces a novel use of Kolmogorov-Arnold Networks (KANs) to improve interpretability and model parsimony beyond what traditional correlation methods can achieve.  
3. Two new visualization tools are proposed: the Pairwise KAN Matrix (PKAN), which captures nonlinear relationships between pairs of variables, and the Multivariate KAN Contribution Matrix (MKAN), which ranks features based on their nonlinear contributions to predicting a target variable.  
4. These tools aid both pre-processing tasks such as feature selection and redundancy analysis, and post-processing uses including model explanation and extraction of physical insights.  
5. Through experiments, PKAN and MKAN are demonstrated to provide more robust and informative results than conventional metrics like Pearson Correlation and Mutual Information, by effectively revealing the strength and functional nature of underlying relationships. This promotes the discovery of hidden physical patterns and supports domain-informed decision making in model development. <div>
arXiv:2512.15755v1 Announce Type: new 
Abstract: Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning</title>
<link>https://arxiv.org/abs/2512.15756</link>
<guid>https://arxiv.org/abs/2512.15756</guid>
<content:encoded><![CDATA[
<div> Keywords: ReactorFold, nuclear reactor core design, language models, Direct Preference Optimization, asymmetric configurations  

<br /><br />Summary:  
The paper introduces ReactorFold, a novel generative framework that tackles nuclear reactor core design by framing fuel-assembly layout as a sequence modeling challenge suited for language models. Unlike traditional methods constrained by fixed, human-defined configuration spaces, ReactorFold leverages Monte Carlo simulation data and parameter-efficient fine-tuning to learn the underlying structure of pressurized-water-reactor assemblies. The model is trained with Direct Preference Optimization (DPO), enabling it to generate candidate fuel assembly designs in a single forward pass. A significant finding is the model’s emergent ability to expand the design space: although trained solely on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously varies Gd content to meet stringent power-peaking constraints. Additionally, ReactorFold discovers high-performing asymmetric configurations that challenge conventional symmetric design heuristics, thus revealing new design regimes that traditional search methods cannot access. This demonstrates that language models can not only internalize complex causal physical relationships but also surpass artificial human-imposed constraints, opening pathways for innovative nuclear reactor core topologies and improved design optimization strategies. <div>
arXiv:2512.15756v1 Announce Type: new 
Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twin Restricted Kernel Machines for Multiview Classification</title>
<link>https://arxiv.org/abs/2512.15757</link>
<guid>https://arxiv.org/abs/2512.15757</guid>
<content:encoded><![CDATA[
<div> Multi-view learning, kernel machines, twin restricted kernel machine, multiview support vector machine, classification performance<br /><br />Summary:<br /><br />Multi-view learning (MVL) aims to enhance generalization by utilizing complementary data from multiple views. Traditional multi-view support vector machines (MvSVMs) succeed in various tasks but face challenges in high-dimensional spaces and are prone to view inconsistencies. This paper introduces the Twin Multiview Restricted Kernel Machine (TMvRKM), a novel model combining kernel machines’ strengths with the multiview framework to overcome these challenges. TMvRKM avoids computationally expensive quadratic programming problems by using a regularized least squares method to find optimal separating hyperplanes, improving efficiency and accuracy. The model’s primal objective incorporates a coupling term to effectively balance errors across different views. TMvRKM employs both early and late fusion strategies, leveraging collective multiview information during training while maintaining flexibility to individual view differences. The approach is thoroughly evaluated on UCI, KEEL, and AwA benchmark datasets. Experimental results and statistical analyses consistently demonstrate that TMvRKM outperforms baseline models, showing superior generalization and classification performance across all tested scenarios. <div>
arXiv:2512.15757v1 Announce Type: new 
Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yantra AI -- An intelligence platform which interacts with manufacturing operations</title>
<link>https://arxiv.org/abs/2512.15758</link>
<guid>https://arxiv.org/abs/2512.15758</guid>
<content:encoded><![CDATA[
<div> Keywords: Industry 4.0, intelligent production system, machine learning, predictive maintenance, AI-powered decision support<br /><br />Summary:<br /><br />Industry 4.0 advancements have rapidly transformed smart manufacturing by integrating real-time tracking, machine learning, and AI-driven systems to streamline operations. This dissertation focuses on the design and evaluation of an intelligent production system tailored for XRIT, addressing critical issues such as energy management, predictive maintenance, and AI-enhanced decision support. The system incorporates machine learning models like the Random Forest Classifier for proactive maintenance scheduling and the Isolation Forest algorithm for anomaly detection, aiding in informed decision-making and minimizing downtime. Real-time data visualization is achieved through Streamlit dashboards, enabling workers to interact with live operational insights. The system's effectiveness was validated using simulated data and emphasizes scalability to support real-time implementation within XRIT's production environment. Additionally, the integration of a GPT-4-based AI virtual assistant allows workers to obtain real-time, relevant answers to complex queries, enhancing operational decisions. Testing results demonstrate improvements in work efficiency, energy usage optimization, and repair planning capabilities. Future work will prioritize transitioning to live data integration and exploring additional enhancements to further optimize the intelligent production system. <div>
arXiv:2512.15758v1 Announce Type: new 
Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning</title>
<link>https://arxiv.org/abs/2512.15759</link>
<guid>https://arxiv.org/abs/2512.15759</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, non-IID data, semantic constraints, convergence rate, differential privacy<br /><br />Summary:  
Federated learning facilitates collaborative model training across distributed data sources but often experiences slow convergence when data is non-IID (not independently and identically distributed). This paper introduces Semantic-Constrained Federated Aggregation (SCFA), a novel, theoretically-grounded framework that integrates domain knowledge constraints into the federated learning process to enhance optimization. SCFA achieves a proven convergence rate of O(1/√T + ρ), where ρ measures the rate of constraint violation, marking the first convergence guarantee for constraint-based federated learning. The inclusion of semantic constraints effectively reduces data heterogeneity by 41%, thereby improving training stability and model consistency. Privacy-utility tradeoffs are also enhanced since the hypothesis space is reduced by a factor of θ=0.37, meaning models retain higher accuracy under differential privacy constraints. Specifically, under (ε, δ)-differential privacy with ε=10, SCFA limits utility degradation to 3.7%, a significant improvement over the 12.1% loss observed with standard federated learning, translating to a 2.7× gain. The framework was empirically validated on a manufacturing predictive maintenance task using Bosch production data (1.18 million samples, 968 sensor features) with knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Results showed 22% faster convergence, 41.3% reduction in model divergence, and that maintaining constraint violation ρ below 0.05 preserves 90% optimal performance, while values above 0.18 lead to failure. The theoretical analysis strongly correlates with experimental outcomes (R² > 0.90) across metrics of convergence, privacy impact, and constraint violations. <div>
arXiv:2512.15759v1 Announce Type: new 
Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tutorial on Dimensionless Learning: Geometric Interpretation and the Effect of Noise</title>
<link>https://arxiv.org/abs/2512.15760</link>
<guid>https://arxiv.org/abs/2512.15760</guid>
<content:encoded><![CDATA[
<div> Dimensionless learning, dimensional analysis, machine learning, scaling laws, experimental data<br /><br />Summary:<br /><br />This article introduces "dimensionless learning," a data-driven framework designed to discover dimensionless numbers and scaling laws from experimental measurements. The method integrates classical dimensional analysis with modern machine learning techniques, transforming physical quantity measurements into compact and interpretable physical laws demonstrating dimensional invariance. It identifies fundamental combinations of variables into dimensionless groups and employs neural networks to determine which combinations best predict experimental outcomes. A novel regularization technique promotes coefficients to take simple, interpretable values such as integers or half-integers, ensuring the discovered laws are both accurate and physically meaningful. The authors systematically examine the impact of measurement noise and discrete sampling on the discovery process, showing that their regularization approach enhances robustness against experimental uncertainties. The method effectively handles scenarios with one or multiple dimensionless numbers, revealing that different valid representations can express the same underlying physics. Despite successes, challenges remain, including high computational costs when identifying multiple dimensionless groups, understanding how data characteristics influence the process, automating the selection of relevant variables, and developing user-friendly tools for experimentalists. This tutorial aims to serve both as an educational resource and a practical guide for researchers interested in applying dimensionless learning to experimental data analysis. <div>
arXiv:2512.15760v1 Announce Type: new 
Abstract: Dimensionless learning is a data-driven framework for discovering dimensionless numbers and scaling laws from experimental measurements. This tutorial introduces the method, explaining how it transforms experimental data into compact physical laws that reveal compact dimensional invariance between variables. The approach combines classical dimensional analysis with modern machine learning techniques. Starting from measurements of physical quantities, the method identifies the fundamental ways to combine variables into dimensionless groups, then uses neural networks to discover which combinations best predict the experimental output. A key innovation is a regularization technique that encourages the learned coefficients to take simple, interpretable values like integers or half-integers, making the discovered laws both accurate and physically meaningful. We systematically investigate how measurement noise and discrete sampling affect the discovery process, demonstrating that the regularization approach provides robustness to experimental uncertainties. The method successfully handles cases with single or multiple dimensionless numbers, revealing how different but equivalent representations can capture the same underlying physics. Despite recent progress, key challenges remain, including managing the computational cost of identifying multiple dimensionless groups, understanding the influence of data characteristics, automating the selection of relevant input variables, and developing user-friendly tools for experimentalists. This tutorial serves as both an educational resource and a practical guide for researchers seeking to apply dimensionless learning to their experimental data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps</title>
<link>https://arxiv.org/abs/2512.15761</link>
<guid>https://arxiv.org/abs/2512.15761</guid>
<content:encoded><![CDATA[
<div> Keywords: thrombosis, rotary blood pumps, machine learning, computational fluid dynamics, risk prediction  

<br /><br />Summary:  
This study addresses the challenge of predicting thrombosis in rotary blood pumps, which arises from complex flow conditions difficult to characterize with traditional computational models. The authors propose an interpretable machine learning framework that utilizes flow features directly derived from computational fluid dynamics (CFD) simulations. A logistic regression model, combined with a structured feature-selection pipeline, is developed to extract a compact and physically meaningful feature set, including nonlinear combinations of flow parameters. The framework is trained on spatial thrombosis risk patterns generated by a validated macro-scale thrombosis model under two representative scenarios. The trained model successfully replicates labeled risk distributions and identifies specific flow features correlated with elevated thrombosis risk. Notably, when the model is applied to a centrifugal pump—despite being trained on data from an axial pump at a single operating point—it predicts plausible thrombosis-prone regions, highlighting its generalizability. This approach provides a transparent mechanistic link between local flow characteristics and thrombosis risk, maintaining computational efficiency. The low computational overhead facilitates rapid thrombogenicity screening without the need for repeated or computationally expensive simulations. The framework thus complements existing physics-based thrombosis modeling and establishes a foundation for integrating interpretable machine learning into CFD-driven thrombosis assessment and medical device design workflows. <div>
arXiv:2512.15761v1 Announce Type: new 
Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction</title>
<link>https://arxiv.org/abs/2512.15762</link>
<guid>https://arxiv.org/abs/2512.15762</guid>
<content:encoded><![CDATA[
<div> Keywords: intraoperative hypotension, test-time adaptation, cross-sample augmentation, time series forecasting, masked reconstruction<br /><br />Summary:<br /><br />Intraoperative hypotension (IOH) significantly increases surgical risk but predicting it accurately remains difficult due to variability between patients. To improve personalized prediction, the authors propose CSA-TTA, a Cross-Sample Augmented Test-Time Adaptation framework. CSA-TTA addresses the challenge of sparse IOH events by augmenting test-time training data with hypotension samples from other patients. This is achieved through a cross-sample bank that categorizes historical data into hypotensive and non-hypotensive segments. The framework employs a coarse-to-fine retrieval method starting with K-Shape clustering to find representative centers and then retrieving the top-K semantically similar samples to the current patient. Further, CSA-TTA integrates self-supervised masked reconstruction and retrospective sequence forecasting during training, enhancing model sensitivity to rapid, subtle intraoperative changes. The method was evaluated on the VitalDB and a real-world clinical dataset, integrated with leading time series forecasting models such as TimesFM and UniTS. Results show consistent performance improvements, including Recall and F1 score increases of +1.33% and +1.13% under fine-tuning, and larger gains of +7.46% and +5.07% in zero-shot testing scenarios. This demonstrates CSA-TTA’s robustness, adaptability, and potential for clinical application in real-time IOH prediction. <div>
arXiv:2512.15762v1 Announce Type: new 
Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</title>
<link>https://arxiv.org/abs/2512.15764</link>
<guid>https://arxiv.org/abs/2512.15764</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Parameter-Efficient Fine-Tuning, AdaGradSelect, transformer blocks, resource efficiency<br /><br />Summary:<br /><br />Large Language Models (LLMs) require substantial computational resources for full fine-tuning, which can be costly and memory-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA mitigate this by applying small, low-rank updates to frozen weights but limit the trainable subspace, potentially lowering performance. For Small Language Models (SLMs), which demand even greater efficiency, the authors propose AdaGradSelect, an adaptive fine-tuning approach that intelligently selects transformer blocks to update based on gradient information. Initial observations revealed that updating only the blocks with the highest gradient norms yields performance close to full fine-tuning. AdaGradSelect builds on this via Dirichlet-based sampling, accounting for how often blocks were updated, and employs an epsilon-greedy exploration to balance discovering important blocks early on and exploiting them later. Experimental results demonstrate that AdaGradSelect trains approximately 12% faster and consumes 35% less GPU memory while maintaining comparable accuracy. On the GSM8K dataset, it surpasses LoRA (rank 256) by roughly 3% on average across several models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B, and shows similar performance gains on the MATH dataset. Overall, AdaGradSelect offers a more effective and resource-efficient alternative to conventional fine-tuning approaches for SLMs. <div>
arXiv:2512.15764v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.
  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.
  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.
  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic</title>
<link>https://arxiv.org/abs/2512.15765</link>
<guid>https://arxiv.org/abs/2512.15765</guid>
<content:encoded><![CDATA[
<div> Data valuation, large language models, Shapley value, Direct Preference Optimization, cooperative game theory<br /><br />Summary:  
This paper addresses the critical role of data in training large language models (LLMs), highlighting the challenges faced by data owners when deciding how to invest in data curation and acquisition strategies. It emphasizes the importance of fair and effective collaboration among multiple data owners who want to pool their datasets to develop superior models and equitably distribute the resulting benefits. The concept of data valuation is explored through the framework of cooperative game theory, with a particular focus on the Shapley value as a method for evaluating the contribution of individual datasets. However, the high computational cost of calculating Shapley values due to multiple model retrainings poses a significant barrier, especially for large-scale models. The authors propose a solution by leveraging the unique mathematical structure of Direct Preference Optimization (DPO), a training paradigm for LLMs, which drastically reduces the complexity of Shapley value computations. This development opens up new possibilities for scalable and practical data valuation in LLMs, potentially facilitating more informed data investment decisions and encouraging collaborative data sharing initiatives through a fair benefit distribution mechanism. The work bridges data valuation theory and LLM training, unlocking practical applications at this intersection. <div>
arXiv:2512.15765v1 Announce Type: new 
Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework</title>
<link>https://arxiv.org/abs/2512.15767</link>
<guid>https://arxiv.org/abs/2512.15767</guid>
<content:encoded><![CDATA[
<div> Keywords: Ignorance model, Hybrid twin, Graph Neural Networks, Sparse spatial data, Nonlinear heat transfer<br /><br />Summary:  
This paper addresses the challenge of simulating complex unsteady physical phenomena through mathematical models such as the Finite Element Method (FEM), which often suffer from inaccuracies due to unmodeled effects referred to as the ignorance model. Traditional data-driven approaches that aim to learn full system behavior require extensive high-quality spatial and temporal data, which is typically unavailable in practical scenarios, limiting their reliability. The authors propose a hybrid twin approach that models only the ignorance component rather than the entire physical response, leveraging the fact that the ignorance is less complex and thus can be learned with fewer data. A key challenge they overcome is the sparsity of spatial measurements and difficulty in acquiring data across varied spatial configurations. To tackle this, they utilize Graph Neural Networks (GNNs) to learn spatial patterns of missing physics from limited measurement locations. This enables enriching physics-based models with data-driven corrections without demanding dense data coverage across space, time, or parameters. The method is demonstrated on nonlinear heat transfer problems involving different mesh layouts, geometries, and load positions. Results indicate that the GNN-based hybrid twin effectively captures the ignorance, generalizes corrections across spatial configurations, enhances simulation accuracy, and improves interpretability while reducing data requirements. <div>
arXiv:2512.15767v1 Announce Type: new 
Abstract: Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions</title>
<link>https://arxiv.org/abs/2512.15771</link>
<guid>https://arxiv.org/abs/2512.15771</guid>
<content:encoded><![CDATA[
<div> Keywords: Partial Differential Equations, Physics-Informed Neural Networks, Time-Evolving Natural Gradient, Dirichlet Boundary Conditions, Numerical Time-Stepping  

<br /><br />Summary:  
This paper addresses the limitations of traditional numerical methods and Physics-Informed Neural Networks (PINNs) in solving complex Partial Differential Equations (PDEs), particularly in enforcing boundary conditions with high accuracy. The authors extend the Time-Evolving Natural Gradient (TENG) framework to incorporate Dirichlet boundary conditions by embedding penalty terms into the loss function, ensuring precise constraint satisfaction. The approach integrates natural gradient optimization with classical time-stepping schemes such as Euler and Heun methods, balancing computational stability and solution accuracy. Experimental results on the heat equation indicate that the Heun method achieves superior accuracy due to second-order correction terms, whereas the Euler method provides computational efficiency for simpler problems. This enhancement broadens the applicability of neural network-based PDE solvers and sets groundwork for future extensions to other boundary types, including Neumann and mixed conditions, as well as more diverse classes of PDEs. Overall, the work demonstrates the potential of combining physics-informed deep learning frameworks with advanced optimization and numerical techniques to tackle real-world PDE problems more effectively. <div>
arXiv:2512.15771v1 Announce Type: new 
Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</title>
<link>https://arxiv.org/abs/2512.15773</link>
<guid>https://arxiv.org/abs/2512.15773</guid>
<content:encoded><![CDATA[
<div> Diffusion Policy, Speculative Decoding, Reinforcement Learning, Embodied Control, Temporal Adaptivity<br /><br />Summary:<br /><br />This paper addresses the high inference latency and computational cost of Diffusion Policy (DP) in embodied control tasks, which arise due to multiple iterative denoising steps. Unlike static acceleration methods such as quantization, which fail to adapt to dynamic task difficulties, the authors propose leveraging speculative decoding as a lossless and adaptive acceleration technique for DP. They introduce Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework to combine speculative decoding with temporal adaptivity for dynamic embodied environments. TS-DP involves two main components: a Transformer-based drafter distilled to imitate the base model and reduce expensive denoising calls, and a reinforcement learning (RL) based scheduler that dynamically adjusts speculative decoding parameters according to varying task difficulty, balancing accuracy and efficiency. Extensive experiments across various embodied environments demonstrate that TS-DP can achieve up to 4.17 times faster inference with more than 94% of drafts accepted by the base model. This corresponds to an inference speed of 25 Hz, enabling real-time diffusion-based control without sacrificing performance. The proposed approach thus effectively adapts computation in response to temporal variations in task complexity, substantially improving the practicality of DP for real-world embodied control applications. <div>
arXiv:2512.15773v1 Announce Type: new 
Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</title>
<link>https://arxiv.org/abs/2512.15780</link>
<guid>https://arxiv.org/abs/2512.15780</guid>
<content:encoded><![CDATA[
<div> adversarial robustness, tabular data, financial decision making, credit scoring, fraud detection<br /><br />Summary:<br /><br />This study investigates the adversarial robustness of machine learning models applied to tabular data within financial decision-making contexts, such as credit scoring and fraud detection. The authors utilize gradient-based attack methods to introduce small perturbations to the input data in order to assess the vulnerability of these models. Key performance metrics analyzed include discrimination (the model’s ability to distinguish between different classes), calibration (the alignment of predicted probabilities with actual outcomes), and financial risk measures pertinent to real-world applications. The results reveal that even minimal adversarial perturbations can significantly degrade model performance across these metrics, posing concerns about the reliability and trustworthiness of such models when deployed in sensitive financial environments. To counteract this vulnerability, the study also explores adversarial training as a defense mechanism, demonstrating that it can partially restore model robustness and mitigate some of the adverse effects induced by the attacks. However, recovery is noted to be incomplete, indicating ongoing challenges remain in fully safeguarding financial machine learning systems against adversarial threats. Overall, this work highlights the importance of considering adversarial robustness in the deployment of tabular machine learning models for financial decision-making tasks. <div>
arXiv:2512.15780v1 Announce Type: new 
Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection</title>
<link>https://arxiv.org/abs/2512.15900</link>
<guid>https://arxiv.org/abs/2512.15900</guid>
<content:encoded><![CDATA[
<div> kernel selection, t-SNE, biological sequences, cosine similarity, dimensionality reduction<br /><br />Summary:<br /><br />This study evaluates the effectiveness of different kernel functions in t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction of high-dimensional biological sequencing data. Traditionally, the Gaussian kernel is used in t-SNE to compute pairwise similarities, but it has limitations regarding scalability and its ability to capture categorical sequence data characteristics. The isolation kernel has been proposed as an alternative, but may not optimally measure sequence similarities. The authors examine nine kernel functions applied to molecular sequences with three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Using subjective visualization and objective metrics such as neighborhood preservation scores, the results show the cosine similarity kernel generally outperforms other kernels including Gaussian and isolation kernels. The cosine kernel provides better runtime efficiency and preserves pairwise distances more accurately in low-dimensional embeddings. The study further validates these findings through classification and clustering experiments on six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer) using various machine learning algorithms and evaluation metrics. Overall, kernel choice significantly impacts visualization quality and downstream analysis performance, with the cosine similarity kernel proving the most robust and suitable for large-scale biological sequence analysis across multiple embedding methods and dataset types. <div>
arXiv:2512.15900v1 Announce Type: new 
Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introduction to Symbolic Regression in the Physical Sciences</title>
<link>https://arxiv.org/abs/2512.15920</link>
<guid>https://arxiv.org/abs/2512.15920</guid>
<content:encoded><![CDATA[
<div> Keywords: symbolic regression, physical sciences, equation discovery, empirical modelling, AI integration  

<br /><br />Summary:  
This article introduces a Special Issue on Symbolic Regression (SR) for the Physical Sciences, inspired by the Royal Society discussion meeting in April 2025. SR is presented as a powerful tool for deriving interpretable mathematical relationships directly from data, providing new pathways for scientific discovery and efficient empirical modelling. The issue showcases a variety of applications, including automated equation discovery, modeling emergent phenomena, and building compact surrogate models to speed up computationally expensive simulations. The introductory review explains the core concepts of SR and compares it with traditional regression techniques. It highlights key use cases in physical sciences, such as generating effective theories and empirical functional forms. Methodologically, the article discusses important factors like search-space design, operator choice, controlling model complexity, feature selection, and how SR can be combined with modern AI methods. Challenges remain in scaling SR algorithms, ensuring robustness against noise, avoiding overfitting, and managing computational costs. Finally, the article points to promising future directions, emphasizing the integration of theoretical information such as symmetry constraints and asymptotic behavior. Altogether, the Special Issue reflects rapid advancements in SR and its increasing significance for research across the physical sciences. <div>
arXiv:2512.15920v1 Announce Type: new 
Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unification of Discrete, Gaussian, and Simplicial Diffusion</title>
<link>https://arxiv.org/abs/2512.15923</link>
<guid>https://arxiv.org/abs/2512.15923</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, Wright-Fisher model, Gaussian diffusion, simplicial diffusion, DNA sequence modeling<br /><br />Summary: This paper addresses the challenge of modeling discrete sequences such as DNA, proteins, and language using diffusion methods. It identifies three main approaches: discrete diffusion in its native space, Gaussian diffusion in Euclidean space, and diffusion on the simplex. Each approach has its own advantages and disadvantages, with discrete diffusion naturally fitting the domain, Gaussian methods benefiting from mature algorithms, and simplicial diffusion theoretically combining the strengths but suffering from numerical instability. The authors propose a unified theoretical framework based on the Wright-Fisher population genetics model, under which all three diffusion methods emerge as different parametric limits. Specifically, Gaussian and simplicial diffusions correspond to large-population limits of this underlying process. This framework bridges gaps in prior theories by formally connecting the models' likelihoods and hyperparameters, while also resolving stability issues in simplicial diffusion. Practically, the paper demonstrates that a single trained model can perform diffusion across any of these three domains at test time, removing the need for practitioners to choose between trade-offs. Experiments highlight that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial models in conditional DNA generation tasks. Furthermore, models trained jointly on multiple diffusion domains achieve performance competitive with those specialized to any single domain. <div>
arXiv:2512.15923v1 Announce Type: new 
Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSO: Direct Steering Optimization for Bias Mitigation</title>
<link>https://arxiv.org/abs/2512.15926</link>
<guid>https://arxiv.org/abs/2512.15926</guid>
<content:encoded><![CDATA[
<div> Bias Mitigation, Vision-Language Models, Activation Steering, Reinforcement Learning, Fairness Control<br /><br />Summary:<br /><br />1. The paper addresses bias in generative models, specifically vision-language models (VLMs), which can produce biased decisions based on perceived demographic attributes, negatively affecting outcomes such as identifying professionals like doctors.  
2. It highlights the challenge users face in balancing bias reduction with maintaining overall model performance, emphasizing the need for inference-time methods that allow controllable trade-offs.  
3. Activation steering is identified as a promising technique for controllable model behavior during inference but is limited in effectively correcting biases where equal outcomes across demographics are required.  
4. To overcome these limitations, the authors propose Direct Steering Optimization (DSO), a novel approach that uses reinforcement learning to learn linear transformations of model activations aimed specifically at mitigating bias while preserving model capabilities.  
5. Experiments demonstrate that DSO achieves a superior fairness-performance trade-off on both VLMs and large language models (LLMs) compared to existing methods, providing practitioners with practical control over bias mitigation during inference. The work underscores the importance of directly optimized steering strategies for more effective bias intervention than heuristic-based methods. <div>
arXiv:2512.15926v1 Announce Type: new 
Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research</title>
<link>https://arxiv.org/abs/2512.15931</link>
<guid>https://arxiv.org/abs/2512.15931</guid>
<content:encoded><![CDATA[
<div> Keywords: fungal taxonomy, DNA barcodes, foundation model, state-space model, hierarchical label smoothing<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurate taxonomic classification of fungi from DNA barcodes, a critical task for global biodiversity monitoring complicated by sparse labeling and long-tailed taxa distributions.  
2. Traditional supervised learning struggles in this domain notably due to difficulties in generalizing to unseen species and managing the hierarchical nature of fungal taxonomy.  
3. To overcome these issues, the authors introduce BarcodeMamba+, a foundation model based on an efficient state-space model architecture designed specifically for fungal barcode classification.  
4. A pretrain and fine-tune training paradigm leveraging partially labeled data is employed, demonstrated to be significantly more effective than conventional fully supervised approaches in data-sparse scenarios.  
5. During fine-tuning, the model integrates key enhancements including hierarchical label smoothing, a weighted loss function, and a multi-head output layer inherited from MycoAI, each contributing notable performance improvements.  
6. Evaluation on a challenging fungal classification benchmark with distinct taxonomic distribution shifts confirms that BarcodeMamba+ outperforms existing methods at all taxonomic levels.  
7. The work establishes a powerful, scalable framework for genomics-based biodiversity research and provides an open-source implementation available at https://github.com/bioscan-ml/BarcodeMamba. <div>
arXiv:2512.15931v1 Announce Type: new 
Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.15934</link>
<guid>https://arxiv.org/abs/2512.15934</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, in-context learning, semi-supervised learning, unlabeled context, representation learning<br /><br />Summary:<br />1. The paper addresses the capacity of Transformers for in-context learning (ICL), highlighting that most prior theoretical work focuses on supervised settings where labeled example pairs are explicitly provided.<br />2. It recognizes that in practical scenarios, Transformers often perform well even when labels are sparse or absent, implying that there is significant structure in the unlabeled contextual demonstrations.<br />3. To explore this, the authors introduce the concept of in-context semi-supervised learning (IC-SSL), which combines a small number of labeled examples with many unlabeled points within the context.<br />4. The study demonstrates that Transformers can leverage the unlabeled context effectively to learn robust, context-dependent representations that improve prediction accuracy.<br />5. This approach particularly benefits low-label regimes, providing foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework, thus advancing understanding of semi-supervised learning in transformer models. <div>
arXiv:2512.15934v1 Announce Type: new 
Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoder, Model Interpretability, Feature Editing, Grad-FAM, Robustness Diagnostics<br /><br />Summary:<br /><br />1. The paper introduces SALVE (Sparse Autoencoder-Latent Vector Editing), a unified framework aimed at discovering, validating, and controlling neural network features for improved interpretability and model editing.  
2. SALVE utilizes an $\ell_1$-regularized autoencoder to learn sparse, model-native feature bases without requiring supervision, enabling effective feature discovery.  
3. Validation of these features is performed using Grad-FAM, a new feature-level saliency mapping technique that visually ties latent features back to input data, facilitating mechanistic interpretability.  
4. SALVE supports precise and permanent weight-space interventions within the autoencoder structure, permitting continuous modulation of both class-specific and cross-class features, allowing actionable control over model behavior.  
5. The framework introduces a critical suppression threshold, $\alpha_{crit}$, which quantifies how much each class depends on its dominant feature, providing fine-grained diagnostics of model robustness.  
6. Experiments demonstrate SALVE's applicability and effectiveness on both convolutional neural networks (ResNet-18) and transformer models (ViT-B/16), confirming its generality and consistent control over model behavior.  
7. Overall, this work presents a principled methodology that bridges feature discovery with direct model editing, advancing transparent, interpretable, and controllable AI systems. <div>
arXiv:2512.15938v1 Announce Type: new 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</title>
<link>https://arxiv.org/abs/2512.15946</link>
<guid>https://arxiv.org/abs/2512.15946</guid>
<content:encoded><![CDATA[
<div> arXiv, AMD Versal AI Engine, AIE4ML framework, neural network execution, on-chip parallelization<br /><br />Summary:  
This paper addresses the challenges of performing efficient AI inference on AMD’s Versal AI Engine (AIE), focusing on its complex VLIW execution, explicit datapaths, and local memory management. The authors introduce AIE4ML, the first comprehensive framework designed to automatically convert AI models into optimized firmware specifically for AIE-ML generation devices, with forward compatibility for AIE-MLv2 architectures. At the single-kernel level, AIE4ML achieves performance near the hardware's peak. For full neural network execution, the framework provides a structured parallelization strategy that scales across the 2D array of AIE tiles, utilizing dedicated memory tiles to maintain all data movement on-chip. They developed a generalized linear-layer implementation supporting fused bias addition and ReLU activation. The framework also systematically generates multi-layer implementations through a novel graph placement and search algorithm that optimizes the physical 2D layout of the device for maximum efficiency. It supports quantized models imported from tools like hls4ml or PyTorch while preserving bit-exactness. Benchmarks show up to 98.6% efficiency relative to single-kernel baselines, leveraging 97.4% of the device’s tiles. The framework delivers GPU-class throughput with microsecond latency, making it highly suitable for ultra-low-latency applications such as trigger systems in particle physics. <div>
arXiv:2512.15946v1 Announce Type: new 
Abstract: Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Governance by Evidence: Regulated Predictors in Decision-Tree Models</title>
<link>https://arxiv.org/abs/2512.15955</link>
<guid>https://arxiv.org/abs/2512.15955</guid>
<content:encoded><![CDATA[
<div> Keywords: decision trees, privacy laws, regulated data, healthcare data, machine learning practice<br /><br />Summary:<br /><br />1. Decision-tree methods are extensively employed for structured tabular data analysis and valued for their interpretability in various sectors. 2. Many studies using decision trees report predictors such as age, diagnosis codes, and location, which fall under categories regulated by privacy laws. 3. The authors compiled a corpus of decision-tree studies and categorized each predictor according to regulated data types—including health data, biometric identifiers, children’s data, financial attributes, location traces, and government IDs. 4. Each category of data was then linked to specific excerpts from European Union and United States privacy legislation to highlight legal governance. 5. Findings reveal that numerous predictors applied in decision-tree research are governed by privacy regulations, with healthcare data being the most heavily represented and notable variations observed across different industries. 6. The study analyzes prevalence, industry composition, and temporal trends of these regulated predictors, aligning them to legal reference years in privacy frameworks. 7. The evidence underscores the importance of incorporating privacy-preserving techniques and governance checks in machine learning models, extending its implications beyond decision trees to broader ML practice. <div>
arXiv:2512.15955v1 Announce Type: new 
Abstract: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling</title>
<link>https://arxiv.org/abs/2512.15956</link>
<guid>https://arxiv.org/abs/2512.15956</guid>
<content:encoded><![CDATA[
<div> Keywords: RFID localization, Gaussian Process, wildfire response, signal attenuation, passive tracking<br /><br />Summary:<br /><br />1. This paper introduces a novel, scalable, and cost-effective method for tracking multiple assets in forested environments using commodity Radio Frequency Identification (RFID) systems specifically aimed at wildfire response scenarios. <br /><br />2. Traditional RFID localization techniques struggle in forested areas due to signal attenuation, multipath effects, and environmental variability, and often require fingerprinting approaches where tags must be placed at known locations beforehand. <br /><br />3. The authors address the challenging scenario where known tag locations are unavailable and demonstrate that it is still possible to achieve localization accuracy comparable to GPS without this constraint. <br /><br />4. Their approach uses Gaussian Processes to model environmental variations based solely on RF signal response signatures, eliminating the need for supplementary sensors like GPS or cameras. Localization is achieved by matching unknown RF signatures to the closest environment model in a previously constructed dictionary. <br /><br />5. A new weighted log-likelihood method is introduced to associate unknown environmental signals with the most similar environment model, enabling accurate localization. The results show that passive commodity RFID can localize assets with GPS-like accuracy at a fraction of the cost, allowing simultaneous tracking of dozens of wildfire assets near mobile readers without requiring pre-tagged known positions. <div>
arXiv:2512.15956v1 Announce Type: new 
Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</title>
<link>https://arxiv.org/abs/2512.15973</link>
<guid>https://arxiv.org/abs/2512.15973</guid>
<content:encoded><![CDATA[
<div> Dynamic Rank Reinforcement Learning, Multi-Head Self-Attention, Low-rank factorization, Reinforcement learning, Matrix perturbation<br /><br />Summary:<br /><br />This paper introduces Dynamic Rank Reinforcement Learning (DR-RL), a framework designed to optimize the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models adaptively. Unlike traditional static low-rank approximations, DR-RL dynamically adjusts the rank based on sequence dynamics, layer-specific sensitivities, and hardware constraints, enhancing flexibility and efficiency. The key innovation is an RL agent that treats rank selection as a sequential policy optimization problem, balancing the trade-off between attention fidelity and computational latency via a carefully designed reward function. By leveraging online matrix perturbation theory, the framework allows incremental rank updates without expensive full decompositions at inference time, maintaining computational efficiency. The method integrates a lightweight Transformer-based policy network and batched Singular Value Decomposition operations for scalable deployment on GPUs. Experimental results show that DR-RL achieves accuracy comparable to full-rank attention models while significantly reducing floating point operations, especially in processing long sequences exceeding 4,096 tokens. This approach offers a mathematically rigorous and adaptive alternative to heuristic rank reduction techniques, advancing resource-efficient deep learning for MHSA. The authors have made their source code and experiment logs publicly available to facilitate further research and adoption. <div>
arXiv:2512.15973v1 Announce Type: new 
Abstract: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Extracting the Features from a General Superposition</title>
<link>https://arxiv.org/abs/2512.15987</link>
<guid>https://arxiv.org/abs/2512.15987</guid>
<content:encoded><![CDATA[
<div> Features, Superposition, Black-box learning, Fourier analysis, Overcomplete representations<br /><br />Summary:<br /><br />1. The paper addresses the challenge of learning feature representations encoded in superposition within complex machine learning models, focusing on settings where the number of features exceeds the underlying dimension (overcomplete regime).<br /><br />2. The problem is formulated as recovering feature directions \(v_i\) and their associated response functions \(\sigma_i\) from noisy black-box query access to a function \(f(x) = \sum_{i=1}^n a_i \sigma_i(v_i^\top x)\), where each \(v_i\) is a unit vector representing a feature direction.<br /><br />3. Unlike prior work restricted to simpler or specialized forms, the proposed method allows for arbitrary response functions \(\sigma_i\) and only requires that the feature directions \(v_i\) are not nearly identical, enabling recovery from general superpositions.<br /><br />4. The main contribution is an efficient query algorithm that, through an innovative iterative search in Fourier space, progressively refines the search domain to uncover all significant feature directions and reconstruct the original function \(f\), even under noise.<br /><br />5. This approach significantly advances algorithmic capabilities for feature learning in overcomplete models by relaxing limitations of linearity and identifiability conditions commonly assumed, thus deepening theoretical understanding and practical applicability of feature recovery in complex ML systems. <div>
arXiv:2512.15987v1 Announce Type: new 
Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,\sigma_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $\sigma_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $\sigma_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives</title>
<link>https://arxiv.org/abs/2512.15997</link>
<guid>https://arxiv.org/abs/2512.15997</guid>
<content:encoded><![CDATA[
<div> Keywords: partial differential equations, reduced-order models, finite-difference scheme, Rollout loss, 2D Burgers equation<br /><br />Summary:<br />1. The article tackles the challenge of solving complex partial differential equations (PDEs), which are fundamental in physical sciences but typically require heavy computational resources. <br />2. Reduced-order models (ROMs) provide a solution by applying dimensionality reduction techniques to enable faster approximations of PDEs. <br />3. Although current ROMs can handle parameterized PDE families, their accuracy tends to deteriorate over long time horizons, limiting their long-term predictive capability. <br />4. To overcome this, the authors introduce a novel approach consisting of a flexible, high-order, yet computationally inexpensive finite-difference scheme designed to improve accuracy and efficiency. <br />5. They also propose a new training method called the Rollout loss, which optimizes ROMs to maintain prediction accuracy over arbitrarily long time frames. <br />6. The effectiveness of this combined approach is demonstrated through experiments on the 2D Burgers equation, a standard nonlinear PDE benchmark, showing improved long-term predictive performance. <div>
arXiv:2512.15997v1 Announce Type: new 
Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate Neural Architecture Codesign Package (SNAC-Pack)</title>
<link>https://arxiv.org/abs/2512.15998</link>
<guid>https://arxiv.org/abs/2512.15998</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Search, FPGA, Multi-objective Optimization, Resource Utilization, Latency Estimator<br /><br />Summary:<br /><br />1. The paper presents SNAC-Pack, a Surrogate Neural Architecture Codesign Package designed to automate neural network discovery and optimization specifically for FPGA deployment.<br /><br />2. SNAC-Pack integrates multi-stage neural architecture codesign search with a Resource Utilization and Latency Estimator, allowing multi-objective optimization on accuracy, FPGA resource use, and latency without expensive synthesis for every candidate.<br /><br />3. The framework is demonstrated on a high energy physics jet classification task, achieving 63.84% accuracy using resource estimation.<br /><br />4. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, models designed with SNAC-Pack match baseline accuracy and maintain resource utilization comparable to models optimized by traditional bit operations (BOPs) metrics.<br /><br />5. This work highlights the effectiveness of hardware-aware neural architecture search for resource-constrained FPGA deployments and provides an open-source framework to streamline the design of efficient FPGA-accelerated neural networks. <div>
arXiv:2512.15998v1 Announce Type: new 
Abstract: Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</title>
<link>https://arxiv.org/abs/2512.16013</link>
<guid>https://arxiv.org/abs/2512.16013</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, spatial variability, machine learning, agroecosystem carbon cycle, site calibration<br /><br />Summary:<br /><br />1. The article addresses the need for accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales, which is crucial for climate mitigation and sustainable agriculture.  
2. It highlights challenges in this domain due to heterogeneous data and complex cross-scale dependencies, noting that traditional methods often ignore spatial heterogeneity and transfer learning benefits by relying on location-independent parameterizations and independent training.  
3. The authors propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a novel framework that integrates pretraining, fine-tuning, spatial variability awareness, and knowledge guidance to improve carbon emission estimation.  
4. FTBSC-KGML leverages remote sensing data for GPP, climate, and soil covariates collected from multiple sites in the U.S. Midwest, employing a spatial-heterogeneity-aware transfer learning scheme which globally pretrains a model and fine-tunes it per site or state for improved local accuracy and interpretability under limited data scenarios.  
5. Empirical results show that FTBSC-KGML outperforms purely global models by achieving lower validation error and greater consistency in explanatory power, thereby better capturing spatial variability and extending the previous SDSA-KGML framework. <div>
arXiv:2512.16013v1 Announce Type: new 
Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization</title>
<link>https://arxiv.org/abs/2512.16032</link>
<guid>https://arxiv.org/abs/2512.16032</guid>
<content:encoded><![CDATA[
<div> Keywords: Heat-pipe microreactors, optimization, levelized cost of electricity, reinforcement learning, surrogate models<br /><br />Summary: This article addresses the economic challenges of microreactors, specifically heat-pipe microreactors (HPMRs), which are compact and transportable power systems suitable for remote areas heavily reliant on fossil fuels. The study highlights the issue of diseconomies of scale and insufficient financial viability of these reactors. To overcome this, the authors propose a novel geometric design optimization framework integrating both economic and physical constraints into early-stage reactor design. The method involves generating random samples to train surrogate models including Gaussian processes and multi-layer perceptrons. These surrogates are then embedded within a reinforcement learning-based optimization algorithm targeting the reduction of the levelized cost of electricity (LCOE), while enforcing constraints such as fuel lifetime, shutdown margin, peak heat flux, and rod-integrated peaking factor. Two cost scenarios are analyzed: one with high-cost axial reflectors and another with inexpensive axial reflectors. Results reveal that operation, maintenance, and capital costs—especially axial reflector and control drum material costs—dominate overall LCOE. The optimization successfully reduces LCOE by over 57% by adjusting design parameters to minimize these costs without violating constraints. Future work aims to integrate fuel and heat-pipe performance with multi-objective optimization to better understand cost-constraint interactions. <div>
arXiv:2512.16032v1 Announce Type: new 
Abstract: Microreactors, particularly heat-pipe microreactors (HPMRs), are compact, transportable, self-regulated power systems well-suited for access-challenged remote areas where costly fossil fuels dominate. However, they suffer from diseconomies of scale, and their financial viability remains unconvincing. One step in addressing this shortcoming is to design these reactors with comprehensive economic and physics analyses informing early-stage design iteration. In this work, we present a novel unifying geometric design optimization approach that accounts for techno-economic considerations. We start by generating random samples to train surrogate models, including Gaussian processes (GPs) and multi-layer perceptrons (MLPs). We then deploy these surrogates within a reinforcement learning (RL)-based optimization framework to optimize the levelized cost of electricity (LCOE), all the while imposing constraints on the fuel lifetime, shutdown margin (SDM), peak heat flux, and rod-integrated peaking factor. We study two cases: one in which the axial reflector cost is very high, and one in which it is inexpensive. We found that the operation and maintenance and capital costs are the primary contributors to the overall LCOE particularly the cost of the axial reflectors (for the first case) and the control drum materials. The optimizer cleverly changes the design parameters so as to minimize one of them while still satisfying the constraints, ultimately reducing the LCOE by more than 57% in both instances. A comprehensive integration of fuel and HP performance with multi-objective optimization is currently being pursued to fully understand the interaction between constraints and cost performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI in Big Data Fraud Detection</title>
<link>https://arxiv.org/abs/2512.16037</link>
<guid>https://arxiv.org/abs/2512.16037</guid>
<content:encoded><![CDATA[
<div> Keywords: Big Data, explainable AI (XAI), fraud detection, risk management, scalability  

<br /><br />Summary:  
This paper explores the integration of explainable artificial intelligence (XAI) within Big Data analytics pipelines focused on fraud detection and risk management. It begins by reviewing the characteristics of Big Data and the major analytical tools commonly employed, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based techniques, and ensemble classifiers. The study then provides a structured review of popular XAI methods like LIME, SHAP, counterfactual explanations, and attention mechanisms, evaluating their strengths and limitations especially in large-scale deployments. Key research gaps are identified in the areas of scalability, real-time processing capabilities, and explainability for graph and temporal data models. To tackle these issues, the authors propose a conceptual framework that combines scalable Big Data infrastructure with context-aware explanation methods and incorporates human feedback to enhance interpretability. Finally, the paper outlines open research directions including the development of scalable XAI solutions, privacy-preserving explanation techniques, and the establishment of standardized evaluation metrics specifically for explainable fraud detection systems. This comprehensive review underlines the importance of transparency, regulatory compliance, and trust in automated analytics for financial, insurance, and cybersecurity applications. <div>
arXiv:2512.16037v1 Announce Type: new 
Abstract: Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
<link>https://arxiv.org/abs/2512.16046</link>
<guid>https://arxiv.org/abs/2512.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: streamflow forecasting, causal learning, spatiotemporal modeling, runoff causal graph, hydrological interpretability<br /><br />Summary: Streamflow forecasting is vital for effective water resource management and risk mitigation, yet existing deep learning models often lack interpretability and generalization due to neglecting underlying physical processes. CauStream addresses these limitations by proposing a unified framework that integrates causal learning with spatiotemporal forecasting. The framework jointly learns two key causal structures: a runoff causal graph that models relationships among meteorological forcings and a routing graph that captures dynamic dependencies across streamflow monitoring stations. CauStream establishes identifiability conditions for these causal graphs within a nonparametric context, enhancing the theoretical rigor of its approach. Evaluated on three major U.S. river basins across multiple forecast horizons, CauStream consistently outperforms prior state-of-the-art models, especially showing increased performance gains at longer forecast windows, indicating superior generalization to unseen conditions. Beyond predictive accuracy, CauStream’s inferred causal graphs closely align with established hydrological domain knowledge, providing interpretable insights into watershed dynamics. This combination of accurate forecasting and interpretable causal structure modeling makes CauStream a principled foundation for causal spatiotemporal modeling with promising applications extending beyond hydrology to broader scientific and environmental domains. <div>
arXiv:2512.16046v1 Announce Type: new 
Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Multi-Operator Learning with DeepOSets</title>
<link>https://arxiv.org/abs/2512.16074</link>
<guid>https://arxiv.org/abs/2512.16074</guid>
<content:encoded><![CDATA[
<div> Keywords: In-context Learning, DeepOSets, Partial Differential Equations, Operator Learning, Universal Approximation  

<br /><br />Summary:  
1. The paper investigates In-context Learning (ICL), which enables machine learning models to learn from prompt examples without weight updates.  
2. DeepOSets, a neural architecture combining DeepSets for set learning and Deep Operator Networks (DeepONets) for operator learning, is examined for its ICL capabilities beyond autoregressive transformer models.  
3. The authors demonstrate that with modifications, DeepOSets can act as a multi-operator in-context learner, recovering solution operators of new PDEs not seen during training using example parameter-solution pairs in prompts.  
4. They prove that DeepOSets serves as a universal uniform approximator for a class of continuous operators, meaning one DeepOSets architecture can approximate any continuous operator in that class to any desired accuracy, given enough prompt examples.  
5. Experimental results on Poisson and reaction-diffusion PDE forward and inverse boundary-value problems validate that DeepOSets can accurately predict PDE solutions for parameter queries outside the training distribution by leveraging in-context examples.  
This work highlights the first universal approximation result for in-context learning of continuous operators in scientific machine learning, potentially enabling more flexible and efficient PDE solvers. <div>
arXiv:2512.16074v1 Announce Type: new 
Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Blur: Quantifying Privacy and Utility for Image Data Release</title>
<link>https://arxiv.org/abs/2512.16086</link>
<guid>https://arxiv.org/abs/2512.16086</guid>
<content:encoded><![CDATA[
<div> Privacy, Gaussian blurring, pixelization, data obfuscation, reversal attacks  

<br /><br />Summary:  
The paper addresses the challenge of protecting private information such as faces and license plates in image datasets collected "in the wild" while maintaining the utility of such data for training machine learning models. It critiques the widely used standard method of Gaussian blurring for private information obfuscation, demonstrating that practical implementations of Gaussian blur can be reversed, thus compromising privacy. The study evaluates three alternative obfuscation techniques: pixelization, pixelization combined with noise addition (DP-Pix), and cropping. Privacy is measured through the effectiveness of reversal and discrimination attacks, while utility is assessed based on the quality of model-learned representations trained on images with obfuscated faces. Results indicate that Gaussian blur is the least effective privacy-preserving method because of its susceptibility to reversal attacks, especially in low-precision implementations commonly used in practice. On the other hand, pixelization and pixelization with noise addition, when applied at appropriate granularity levels, offer a stronger balance between privacy protection and maintaining data utility across various computer vision tasks. To facilitate the adoption of these techniques, the authors provide a software package called Privacy Blur that includes their proposed methods and recommended parameter settings. <div>
arXiv:2512.16086v1 Announce Type: new 
Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation</title>
<link>https://arxiv.org/abs/2512.16103</link>
<guid>https://arxiv.org/abs/2512.16103</guid>
<content:encoded><![CDATA[
<div> Market manipulation, social media, retail investors, AIMM, GME<br /><br />Summary:<br /><br />This article introduces AIMM, an AI-driven framework designed to detect market manipulation originating from coordinated social media campaigns rather than isolated trades. AIMM combines Reddit activity, bot and coordination indicators, and OHLCV (open-high-low-close-volume) market data into a daily Manipulation Risk Score for each stock ticker. The system features a parquet-native data pipeline and a Streamlit dashboard that enables analysts to examine suspicious periods, review related social media posts and corresponding price action, and track model outputs over time. Due to limitations in accessing Reddit's API, calibrated synthetic social features are used, while real historical market data is sourced from Yahoo Finance. This work contributes three key elements: first, the creation of the AIMM Ground Truth dataset (AIMM-GT), which includes 33 labeled ticker-days covering eight equities, integrating SEC enforcement cases, community-verified manipulations, and controls; second, the implementation of forward-walk evaluation and prospective prediction logging to assess performance both retrospectively and in deployment contexts; third, an analysis showing that AIMM successfully flagged GameStop (GME) 22 days before its January 2021 price squeeze peak. Although the labeled dataset is small with only three positive events, preliminary results demonstrate promising discriminatory power and early warning capability. The authors release the code, dataset schema, and dashboard design to facilitate further research on social media-driven market surveillance. <div>
arXiv:2512.16103v1 Announce Type: new 
Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BUILD with Precision: Bottom-Up Inference of Linear DAGs</title>
<link>https://arxiv.org/abs/2512.16111</link>
<guid>https://arxiv.org/abs/2512.16111</guid>
<content:encoded><![CDATA[
<div> Keywords: DAG learning, causal discovery, precision matrix, structural equation model, algorithm BUILD<br /><br />Summary:<br /><br />1. The paper addresses the problem of learning the structure of directed acyclic graphs (DAGs) from observational data, which is important for causal discovery and machine learning.<br />2. It focuses on linear Gaussian structural equation models (SEMs) with equal noise variances, a setting where the DAG structure is identifiable.<br />3. The authors identify a distinctive structure in the ensemble precision matrix of the observed data that helps facilitate the recovery of the DAG.<br />4. They propose a novel deterministic stepwise algorithm named BUILD (Bottom-Up Inference of Linear DAGs), which reconstructs the DAG by iteratively identifying leaf nodes and their parents, pruning leaves, and repeating the process until the entire DAG is recovered from the true precision matrix.<br />5. Recognizing that precision matrices need to be estimated from finite samples in practice, which can cause error accumulation due to ill-conditioning, the method includes a mitigation strategy of periodically re-estimating the precision matrix on a reduced set of variables to improve robustness.<br />6. Experimental results on challenging synthetic datasets demonstrate that BUILD performs favorably compared to existing state-of-the-art DAG learning algorithms while providing explicit control over algorithmic complexity. <div>
arXiv:2512.16111v1 Announce Type: new 
Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title>
<link>https://arxiv.org/abs/2512.16126</link>
<guid>https://arxiv.org/abs/2512.16126</guid>
<content:encoded><![CDATA[
<div> Machine Unlearning, Privacy Risks, Dual-View Setting, Membership Inference, Likelihood Ratio

<br /><br />Summary: This paper investigates a novel privacy vulnerability introduced by machine unlearning, a technique designed to remove specific training data from a model to comply with data deletion requests. Unlike prior work focusing on the privacy of deleted data, this study highlights risks to the retained data when an adversary has access to both the original and unlearned models, termed the dual-view setting. The authors introduce the concept of privacy knowledge gain from an information-theoretic standpoint, showing that querying both models simultaneously amplifies privacy leakage beyond what is possible with either model alone. To demonstrate this threat, they propose DVIA, a Dual-View Inference Attack that efficiently extracts membership information related to the retained data through black-box queries without requiring a trained attack model. DVIA utilizes a lightweight likelihood ratio inference module to perform these attacks effectively. Experimental results on various datasets and model architectures confirm the practicality and severity of this new privacy risk, urging the research community to consider the implications of dual-view access scenarios in machine unlearning. <div>
arXiv:2512.16126v1 Announce Type: new 
Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INTELLECT-3: Technical Report</title>
<link>https://arxiv.org/abs/2512.16144</link>
<guid>https://arxiv.org/abs/2512.16144</guid>
<content:encoded><![CDATA[
<div> Keywords: INTELLECT-3, Mixture-of-Experts, reinforcement learning, prime-rl, large-scale training<br /><br />Summary:<br /><br />1. INTELLECT-3 is a large-scale Mixture-of-Experts (MoE) model with 106 billion parameters, of which 12 billion are active during inference.<br />2. It is trained using large-scale reinforcement learning (RL) via a comprehensive end-to-end RL infrastructure stack built by the authors.<br />3. The model achieves state-of-the-art performance on benchmarks related to mathematics, coding, science, and reasoning, outperforming many larger models.<br />4. The authors have open-sourced INTELLECT-3 along with the full infrastructure stack, including RL frameworks, training recipes, and a collection of environments developed with the verifiers library and hosted on their Environments Hub community platform.<br />5. A new open framework called prime-rl is introduced for scalable, asynchronous RL training, designed to work efficiently from single-node setups up to thousands of GPUs and optimized for agentic RL featuring multi-turn interactions and tool usage.<br />6. Training was performed by fine-tuning and RL on top of the GLM-4.5-Air-Base model, scaling up to 512 NVIDIA H200 GPUs with high efficiency. <div>
arXiv:2512.16144v1 Announce Type: new 
Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments</title>
<link>https://arxiv.org/abs/2512.16184</link>
<guid>https://arxiv.org/abs/2512.16184</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, cube copying, graph neural networks, multimodal integration, SHAP interpretability<br /><br />Summary:<br /><br />1. This study addresses the challenge of early and accessible detection of Alzheimer's disease (AD) by focusing on cube-copying tasks, which assess visuospatial function through simple hand-drawn sketches.<br /><br />2. The proposed framework transforms these cube drawings into graph-structured data, capturing both geometric and topological properties via node features such as spatial coordinates, local graphlet-based topology, and angular geometry.<br /><br />3. These graph representations are processed using graph neural networks (GNNs) and combined with demographic details (age, education) and neuropsychological test (NPT) scores through a late-fusion multimodal model.<br /><br />4. Experimental results reveal that graph-based representations alone outperform traditional pixel-based convolutional neural networks, providing a strong baseline, while their fusion with demographic and test data further enhances classification performance and robustness, especially under class imbalance conditions.<br /><br />5. An interpretability analysis using SHAP highlights that specific graphlet motifs and geometric distortions in the cube drawings are key predictors of AD, consistent with clinical observations of visuospatial disorganization in affected patients.<br /><br />Overall, the study presents a non-invasive, interpretable, and scalable graph-based methodology for AD screening using simple hand-drawn tasks integrated with clinical data. <div>
arXiv:2512.16184v1 Announce Type: new 
Abstract: Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-scale Fused Graph Neural Network with Inter-view Contrastive Learning for Spatial Transcriptomics Data Clustering</title>
<link>https://arxiv.org/abs/2512.16188</link>
<guid>https://arxiv.org/abs/2512.16188</guid>
<content:encoded><![CDATA[
arXiv:2512.16188v1 Announce Type: new 
Abstract: Spatial transcriptomics enables genome-wide expression analysis within native tissue context, yet identifying spatial domains remains challenging due to complex gene-spatial interactions. Existing methods typically process spatial and feature views separately, fusing only at output level - an "encode-separately, fuse-late" paradigm that limits multi-scale semantic capture and cross-view interaction. Accordingly, stMFG is proposed, a multi-scale interactive fusion graph network that introduces layer-wise cross-view attention to dynamically integrate spatial and gene features after each convolution. The model combines cross-view contrastive learning with spatial constraints to enhance discriminability while maintaining spatial continuity. On DLPFC and breast cancer datasets, stMFG outperforms state-of-the-art methods, achieving up to 14% ARI improvement on certain slices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions</title>
<link>https://arxiv.org/abs/2512.16200</link>
<guid>https://arxiv.org/abs/2512.16200</guid>
<content:encoded><![CDATA[
arXiv:2512.16200v1 Announce Type: new 
Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $\mu$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}{\mu}\log\!\frac{dL}{\mu\delta}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-\delta$ with $0<\delta<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural emulation of gravity-driven geohazard runout</title>
<link>https://arxiv.org/abs/2512.16221</link>
<guid>https://arxiv.org/abs/2512.16221</guid>
<content:encoded><![CDATA[
arXiv:2512.16221v1 Announce Type: new 
Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16244</link>
<guid>https://arxiv.org/abs/2512.16244</guid>
<content:encoded><![CDATA[
arXiv:2512.16244v1 Announce Type: new 
Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Federated Graph Learning</title>
<link>https://arxiv.org/abs/2512.16247</link>
<guid>https://arxiv.org/abs/2512.16247</guid>
<content:encoded><![CDATA[
arXiv:2512.16247v1 Announce Type: new 
Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data</title>
<link>https://arxiv.org/abs/2512.16277</link>
<guid>https://arxiv.org/abs/2512.16277</guid>
<content:encoded><![CDATA[
arXiv:2512.16277v1 Announce Type: new 
Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</title>
<link>https://arxiv.org/abs/2512.16282</link>
<guid>https://arxiv.org/abs/2512.16282</guid>
<content:encoded><![CDATA[
arXiv:2512.16282v1 Announce Type: new 
Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Selective Representation Misdirection for Machine Unlearning</title>
<link>https://arxiv.org/abs/2512.16297</link>
<guid>https://arxiv.org/abs/2512.16297</guid>
<content:encoded><![CDATA[
arXiv:2512.16297v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
<link>https://arxiv.org/abs/2512.16334</link>
<guid>https://arxiv.org/abs/2512.16334</guid>
<content:encoded><![CDATA[
arXiv:2512.16334v1 Announce Type: new 
Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Uncertainty Quantification with Tomographic Quantile Forests</title>
<link>https://arxiv.org/abs/2512.16383</link>
<guid>https://arxiv.org/abs/2512.16383</guid>
<content:encoded><![CDATA[
arXiv:2512.16383v1 Announce Type: new 
Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Verification of Fairness in Tree Ensembles</title>
<link>https://arxiv.org/abs/2512.16386</link>
<guid>https://arxiv.org/abs/2512.16386</guid>
<content:encoded><![CDATA[
arXiv:2512.16386v1 Announce Type: new 
Abstract: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2512.16391</link>
<guid>https://arxiv.org/abs/2512.16391</guid>
<content:encoded><![CDATA[
arXiv:2512.16391v1 Announce Type: new 
Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16408</link>
<guid>https://arxiv.org/abs/2512.16408</guid>
<content:encoded><![CDATA[
arXiv:2512.16408v1 Announce Type: new 
Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Laplace Neural Operator</title>
<link>https://arxiv.org/abs/2512.16409</link>
<guid>https://arxiv.org/abs/2512.16409</guid>
<content:encoded><![CDATA[
arXiv:2512.16409v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning mappings between function spaces, enabling efficient solutions to partial differential equations across varying inputs and domains. Despite the success, existing methods often struggle with non-periodic excitations, transient responses, and signals defined on irregular or non-Euclidean geometries. To address this, we propose a generalized operator learning framework based on a pole-residue decomposition enriched with exponential basis functions, enabling expressive modeling of aperiodic and decaying dynamics. Building on this formulation, we introduce the Geometric Laplace Neural Operator (GLNO), which embeds the Laplace spectral representation into the eigen-basis of the Laplace-Beltrami operator, extending operator learning to arbitrary Riemannian manifolds without requiring periodicity or uniform grids. We further design a grid-invariant network architecture (GLNONet) that realizes GLNO in practice. Extensive experiments on PDEs/ODEs and real-world datasets demonstrate our robust performance over other state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks</title>
<link>https://arxiv.org/abs/2512.16430</link>
<guid>https://arxiv.org/abs/2512.16430</guid>
<content:encoded><![CDATA[
arXiv:2512.16430v1 Announce Type: new 
Abstract: Inverse uncertainty quantification (UQ) tasks such as parameter estimation are computationally demanding whenever dealing with physics-based models, and typically require repeated evaluations of complex numerical solvers. When partial differential equations are involved, full-order models such as those based on the Finite Element Method can make traditional sampling approaches like Markov Chain Monte Carlo (MCMC) computationally infeasible. Although data-driven surrogate models may help reduce evaluation costs, their utility is often limited by the expense of generating high-fidelity data. In contrast, low-fidelity data can be produced more efficiently, although relying on them alone may degrade the accuracy of the inverse UQ solution.
  To address these challenges, we propose a Multi-Fidelity Delayed Acceptance scheme for Bayesian inverse problems. Extending the Multi-Level Delayed Acceptance framework, the method introduces multi-fidelity neural networks that combine the predictions of solvers of varying fidelity, with high fidelity evaluations restricted to an offline training stage. During the online phase, likelihood evaluations are obtained by evaluating the coarse solvers and passing their outputs to the trained neural networks, thereby avoiding additional high-fidelity simulations.
  This construction allows heterogeneous coarse solvers to be incorporated consistently within the hierarchy, providing greater flexibility than standard Multi-Level Delayed Acceptance. The proposed approach improves the approximation accuracy of the low fidelity solvers, leading to longer sub-chain lengths, better mixing, and accelerated posterior inference. The effectiveness of the strategy is demonstrated on two benchmark inverse problems involving (i) steady isotropic groundwater flow, (ii) an unsteady reaction-diffusion system, for which substantial computational savings are obtained.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bias and Fairness in Multi-Agent Decision Systems</title>
<link>https://arxiv.org/abs/2512.16433</link>
<guid>https://arxiv.org/abs/2512.16433</guid>
<content:encoded><![CDATA[
arXiv:2512.16433v1 Announce Type: new 
Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Proposal in Wind Turbine Blade Failure Detection: An Integrated Approach to Energy Efficiency and Sustainability</title>
<link>https://arxiv.org/abs/2512.16437</link>
<guid>https://arxiv.org/abs/2512.16437</guid>
<content:encoded><![CDATA[
arXiv:2512.16437v1 Announce Type: new 
Abstract: This paper presents a novel methodology for detecting faults in wind turbine blades using com-putational learning techniques. The study evaluates two models: the first employs logistic regression, which outperformed neural networks, decision trees, and the naive Bayes method, demonstrating its effectiveness in identifying fault-related patterns. The second model leverages clustering and achieves superior performance in terms of precision and data segmentation. The results indicate that clustering may better capture the underlying data characteristics compared to supervised methods. The proposed methodology offers a new approach to early fault detection in wind turbine blades, highlighting the potential of integrating different computational learning techniques to enhance system reliability. The use of accessible tools like Orange Data Mining underscores the practical application of these advanced solutions within the wind energy sector. Future work will focus on combining these methods to improve detection accuracy further and extend the application of these techniques to other critical components in energy infrastructure.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Modelling Black Box Optimization</title>
<link>https://arxiv.org/abs/2512.16445</link>
<guid>https://arxiv.org/abs/2512.16445</guid>
<content:encoded><![CDATA[
arXiv:2512.16445v1 Announce Type: new 
Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value</title>
<link>https://arxiv.org/abs/2512.16448</link>
<guid>https://arxiv.org/abs/2512.16448</guid>
<content:encoded><![CDATA[
arXiv:2512.16448v1 Announce Type: new 
Abstract: The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning</title>
<link>https://arxiv.org/abs/2512.16476</link>
<guid>https://arxiv.org/abs/2512.16476</guid>
<content:encoded><![CDATA[
arXiv:2512.16476v1 Announce Type: new 
Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Multiscale Density-based Clustering</title>
<link>https://arxiv.org/abs/2512.16558</link>
<guid>https://arxiv.org/abs/2512.16558</guid>
<content:encoded><![CDATA[
arXiv:2512.16558v1 Announce Type: new 
Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling</title>
<link>https://arxiv.org/abs/2512.16581</link>
<guid>https://arxiv.org/abs/2512.16581</guid>
<content:encoded><![CDATA[
arXiv:2512.16581v1 Announce Type: new 
Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</title>
<link>https://arxiv.org/abs/2512.16626</link>
<guid>https://arxiv.org/abs/2512.16626</guid>
<content:encoded><![CDATA[
arXiv:2512.16626v1 Announce Type: new 
Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario</title>
<link>https://arxiv.org/abs/2512.16648</link>
<guid>https://arxiv.org/abs/2512.16648</guid>
<content:encoded><![CDATA[
arXiv:2512.16648v1 Announce Type: new 
Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</title>
<link>https://arxiv.org/abs/2512.16676</link>
<guid>https://arxiv.org/abs/2512.16676</guid>
<content:encoded><![CDATA[
arXiv:2512.16676v1 Announce Type: new 
Abstract: The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification</title>
<link>https://arxiv.org/abs/2512.16687</link>
<guid>https://arxiv.org/abs/2512.16687</guid>
<content:encoded><![CDATA[
arXiv:2512.16687v1 Announce Type: new 
Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies</title>
<link>https://arxiv.org/abs/2512.16700</link>
<guid>https://arxiv.org/abs/2512.16700</guid>
<content:encoded><![CDATA[
arXiv:2512.16700v1 Announce Type: new 
Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library</title>
<link>https://arxiv.org/abs/2512.16715</link>
<guid>https://arxiv.org/abs/2512.16715</guid>
<content:encoded><![CDATA[
arXiv:2512.16715v1 Announce Type: new 
Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering</title>
<link>https://arxiv.org/abs/2512.16717</link>
<guid>https://arxiv.org/abs/2512.16717</guid>
<content:encoded><![CDATA[
arXiv:2512.16717v1 Announce Type: new 
Abstract: In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation</title>
<link>https://arxiv.org/abs/2512.16718</link>
<guid>https://arxiv.org/abs/2512.16718</guid>
<content:encoded><![CDATA[
arXiv:2512.16718v1 Announce Type: new 
Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.16723</link>
<guid>https://arxiv.org/abs/2512.16723</guid>
<content:encoded><![CDATA[
arXiv:2512.16723v1 Announce Type: new 
Abstract: Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\% accuracy while baselines drop below 20\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis</title>
<link>https://arxiv.org/abs/2512.16742</link>
<guid>https://arxiv.org/abs/2512.16742</guid>
<content:encoded><![CDATA[
arXiv:2512.16742v1 Announce Type: new 
Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NRGPT: An Energy-based Alternative for GPT</title>
<link>https://arxiv.org/abs/2512.16762</link>
<guid>https://arxiv.org/abs/2512.16762</guid>
<content:encoded><![CDATA[
arXiv:2512.16762v1 Announce Type: new 
Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern recognition in complex systems via vector-field representations of spatio-temporal data</title>
<link>https://arxiv.org/abs/2512.16763</link>
<guid>https://arxiv.org/abs/2512.16763</guid>
<content:encoded><![CDATA[
arXiv:2512.16763v1 Announce Type: new 
Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEPIC: Memory Efficient Position Independent Caching for LLM Serving</title>
<link>https://arxiv.org/abs/2512.16822</link>
<guid>https://arxiv.org/abs/2512.16822</guid>
<content:encoded><![CDATA[
arXiv:2512.16822v1 Announce Type: new 
Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control</title>
<link>https://arxiv.org/abs/2512.16824</link>
<guid>https://arxiv.org/abs/2512.16824</guid>
<content:encoded><![CDATA[
arXiv:2512.16824v1 Announce Type: new 
Abstract: Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-RL Induces Exploration in Language Agents</title>
<link>https://arxiv.org/abs/2512.16848</link>
<guid>https://arxiv.org/abs/2512.16848</guid>
<content:encoded><![CDATA[
arXiv:2512.16848v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</title>
<link>https://arxiv.org/abs/2512.16866</link>
<guid>https://arxiv.org/abs/2512.16866</guid>
<content:encoded><![CDATA[
arXiv:2512.16866v1 Announce Type: new 
Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2512.16871</link>
<guid>https://arxiv.org/abs/2512.16871</guid>
<content:encoded><![CDATA[
arXiv:2512.16871v1 Announce Type: new 
Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: new 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Racial Bias in Historical Training Data for News AI</title>
<link>https://arxiv.org/abs/2512.16901</link>
<guid>https://arxiv.org/abs/2512.16901</guid>
<content:encoded><![CDATA[
arXiv:2512.16901v1 Announce Type: new 
Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</title>
<link>https://arxiv.org/abs/2512.16911</link>
<guid>https://arxiv.org/abs/2512.16911</guid>
<content:encoded><![CDATA[
arXiv:2512.16911v1 Announce Type: new 
Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
<link>https://arxiv.org/abs/2512.16912</link>
<guid>https://arxiv.org/abs/2512.16912</guid>
<content:encoded><![CDATA[
arXiv:2512.16912v1 Announce Type: new 
Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random matrix theory of sparse neuronal networks with heterogeneous timescales</title>
<link>https://arxiv.org/abs/2512.12767</link>
<guid>https://arxiv.org/abs/2512.12767</guid>
<content:encoded><![CDATA[
arXiv:2512.12767v1 Announce Type: cross 
Abstract: Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</title>
<link>https://arxiv.org/abs/2512.15729</link>
<guid>https://arxiv.org/abs/2512.15729</guid>
<content:encoded><![CDATA[
arXiv:2512.15729v1 Announce Type: cross 
Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading</title>
<link>https://arxiv.org/abs/2512.15732</link>
<guid>https://arxiv.org/abs/2512.15732</guid>
<content:encoded><![CDATA[
arXiv:2512.15732v1 Announce Type: cross 
Abstract: The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the "Holy Grail" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of "Galaxy Empire," a hybrid framework coupling LSTM/Transformer-based perception with a genetic "Time-is-Life" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\%$) and live performance (Capital Decay $>70\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \textit{Aleatoric Uncertainty} in low-entropy time-series, the \textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments</title>
<link>https://arxiv.org/abs/2512.15736</link>
<guid>https://arxiv.org/abs/2512.15736</guid>
<content:encoded><![CDATA[
arXiv:2512.15736v1 Announce Type: cross 
Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</title>
<link>https://arxiv.org/abs/2512.15739</link>
<guid>https://arxiv.org/abs/2512.15739</guid>
<content:encoded><![CDATA[
arXiv:2512.15739v1 Announce Type: cross 
Abstract: A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&amp;P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling</title>
<link>https://arxiv.org/abs/2512.15768</link>
<guid>https://arxiv.org/abs/2512.15768</guid>
<content:encoded><![CDATA[
arXiv:2512.15768v1 Announce Type: cross 
Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title>
<link>https://arxiv.org/abs/2512.15769</link>
<guid>https://arxiv.org/abs/2512.15769</guid>
<content:encoded><![CDATA[
arXiv:2512.15769v1 Announce Type: cross 
Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</title>
<link>https://arxiv.org/abs/2512.15774</link>
<guid>https://arxiv.org/abs/2512.15774</guid>
<content:encoded><![CDATA[
arXiv:2512.15774v1 Announce Type: cross 
Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</title>
<link>https://arxiv.org/abs/2512.15775</link>
<guid>https://arxiv.org/abs/2512.15775</guid>
<content:encoded><![CDATA[
arXiv:2512.15775v1 Announce Type: cross 
Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization</title>
<link>https://arxiv.org/abs/2512.15778</link>
<guid>https://arxiv.org/abs/2512.15778</guid>
<content:encoded><![CDATA[
arXiv:2512.15778v1 Announce Type: cross 
Abstract: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2512.15779</link>
<guid>https://arxiv.org/abs/2512.15779</guid>
<content:encoded><![CDATA[
arXiv:2512.15779v1 Announce Type: cross 
Abstract: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Na\"ive Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Tuning Safety Guardrails for Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2512.15782</link>
<guid>https://arxiv.org/abs/2512.15782</guid>
<content:encoded><![CDATA[
arXiv:2512.15782v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title>
<link>https://arxiv.org/abs/2512.15783</link>
<guid>https://arxiv.org/abs/2512.15783</guid>
<content:encoded><![CDATA[
arXiv:2512.15783v1 Announce Type: cross 
Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM</title>
<link>https://arxiv.org/abs/2512.15784</link>
<guid>https://arxiv.org/abs/2512.15784</guid>
<content:encoded><![CDATA[
arXiv:2512.15784v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consensus dimension reduction via multi-view learning</title>
<link>https://arxiv.org/abs/2512.15802</link>
<guid>https://arxiv.org/abs/2512.15802</guid>
<content:encoded><![CDATA[
arXiv:2512.15802v1 Announce Type: cross 
Abstract: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative</title>
<link>https://arxiv.org/abs/2512.15803</link>
<guid>https://arxiv.org/abs/2512.15803</guid>
<content:encoded><![CDATA[
arXiv:2512.15803v1 Announce Type: cross 
Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications</title>
<link>https://arxiv.org/abs/2512.15823</link>
<guid>https://arxiv.org/abs/2512.15823</guid>
<content:encoded><![CDATA[
arXiv:2512.15823v1 Announce Type: cross 
Abstract: Immersive formats such as 360{\deg} and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Story Frames: Contextual Reasoning about Narrative Intent and Reception</title>
<link>https://arxiv.org/abs/2512.15925</link>
<guid>https://arxiv.org/abs/2512.15925</guid>
<content:encoded><![CDATA[
arXiv:2512.15925v1 Announce Type: cross 
Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Neural Surfaces for 3D Mesh Compression</title>
<link>https://arxiv.org/abs/2512.15985</link>
<guid>https://arxiv.org/abs/2512.15985</guid>
<content:encoded><![CDATA[
arXiv:2512.15985v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) have been demonstrated to achieve state-of-the-art compression of a broad range of modalities such as images, videos, 3D surfaces, and audio. Most studies have focused on building neural counterparts of traditional implicit representations of 3D geometries, such as signed distance functions. However, the triangle mesh-based representation of geometry remains the most widely used representation in the industry, while building INRs capable of generating them has been sparsely studied. In this paper, we present a method for building compact INRs of zero-genus 3D manifolds. Our method relies on creating a spherical parameterization of a given 3D mesh - mapping the surface of a mesh to that of a unit sphere - then constructing an INR that encodes the displacement vector field defined continuously on its surface that regenerates the original shape. The compactness of our representation can be attributed to its hierarchical structure, wherein it first recovers the coarse structure of the encoded surface before adding high-frequency details to it. Once the INR is computed, 3D meshes of arbitrary resolution/connectivity can be decoded from it. The decoding can be performed in real time while achieving a state-of-the-art trade-off between reconstruction quality and the size of the compressed representations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Frequency Analysis for Neural Networks</title>
<link>https://arxiv.org/abs/2512.15992</link>
<guid>https://arxiv.org/abs/2512.15992</guid>
<content:encoded><![CDATA[
arXiv:2512.15992v1 Announce Type: cross 
Abstract: We develop a quantitative approximation theory for shallow neural networks using tools from time-frequency analysis. Working in weighted modulation spaces $M^{p,q}_m(\mathbf{R}^{d})$, we prove dimension-independent approximation rates in Sobolev norms $W^{n,r}(\Omega)$ for networks whose units combine standard activations with localized time-frequency windows. Our main result shows that for $f \in M^{p,q}_m(\mathbf{R}^{d})$ one can achieve \[ \|f - f_N\|_{W^{n,r}(\Omega)} \lesssim N^{-1/2}\,\|f\|_{M^{p,q}_m(\mathbf{R}^{d})}, \] on bounded domains, with explicit control of all constants. We further obtain global approximation theorems on $\mathbf{R}^{d}$ using weighted modulation dictionaries, and derive consequences for Feichtinger's algebra, Fourier-Lebesgue spaces, and Barron spaces. Numerical experiments in one and two dimensions confirm that modulation-based networks achieve substantially better Sobolev approximation than standard ReLU networks, consistent with the theoretical estimates.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information theory and discriminative sampling for model discovery</title>
<link>https://arxiv.org/abs/2512.16000</link>
<guid>https://arxiv.org/abs/2512.16000</guid>
<content:encoded><![CDATA[
arXiv:2512.16000v1 Announce Type: cross 
Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concurrence: A dependence criterion for time series, applied to biological data</title>
<link>https://arxiv.org/abs/2512.16001</link>
<guid>https://arxiv.org/abs/2512.16001</guid>
<content:encoded><![CDATA[
arXiv:2512.16001v1 Announce Type: cross 
Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks for Interferometer Simulations</title>
<link>https://arxiv.org/abs/2512.16051</link>
<guid>https://arxiv.org/abs/2512.16051</guid>
<content:encoded><![CDATA[
arXiv:2512.16051v1 Announce Type: cross 
Abstract: In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</title>
<link>https://arxiv.org/abs/2512.16075</link>
<guid>https://arxiv.org/abs/2512.16075</guid>
<content:encoded><![CDATA[
arXiv:2512.16075v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</title>
<link>https://arxiv.org/abs/2512.16083</link>
<guid>https://arxiv.org/abs/2512.16083</guid>
<content:encoded><![CDATA[
arXiv:2512.16083v1 Announce Type: cross 
Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
arXiv:2512.16093v1 Announce Type: cross 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BayesSum: Bayesian Quadrature in Discrete Spaces</title>
<link>https://arxiv.org/abs/2512.16105</link>
<guid>https://arxiv.org/abs/2512.16105</guid>
<content:encoded><![CDATA[
arXiv:2512.16105v1 Announce Type: cross 
Abstract: This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</title>
<link>https://arxiv.org/abs/2512.16134</link>
<guid>https://arxiv.org/abs/2512.16134</guid>
<content:encoded><![CDATA[
arXiv:2512.16134v1 Announce Type: cross 
Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence-Enabled Holistic Design of Catalysts Tailored for Semiconducting Carbon Nanotube Growth</title>
<link>https://arxiv.org/abs/2512.16151</link>
<guid>https://arxiv.org/abs/2512.16151</guid>
<content:encoded><![CDATA[
arXiv:2512.16151v1 Announce Type: cross 
Abstract: Catalyst design is crucial for materials synthesis, especially for complex reaction networks. Strategies like collaborative catalytic systems and multifunctional catalysts are effective but face challenges at the nanoscale. Carbon nanotube synthesis contains complicated nanoscale catalytic reactions, thus achieving high-density, high-quality semiconducting CNTs demands innovative catalyst design. In this work, we present a holistic framework integrating machine learning into traditional catalyst design for semiconducting CNT synthesis. It combines knowledge-based insights with data-driven techniques. Three key components, including open-access electronic structure databases for precise physicochemical descriptors, pre-trained natural language processing-based embedding model for higher-level abstractions, and physical - driven predictive models based on experiment data, are utilized. Through this framework, a new method for selective semiconducting CNT synthesis via catalyst - mediated electron injection, tuned by light during growth, is proposed. 54 candidate catalysts are screened, and three with high potential are identified. High-throughput experiments validate the predictions, with semiconducting selectivity exceeding 91% and the FeTiO3 catalyst reaching 98.6%. This approach not only addresses semiconducting CNT synthesis but also offers a generalizable methodology for global catalyst design and nanomaterials synthesis, advancing materials science in precise control.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Science Consultant Agent</title>
<link>https://arxiv.org/abs/2512.16171</link>
<guid>https://arxiv.org/abs/2512.16171</guid>
<content:encoded><![CDATA[
arXiv:2512.16171v1 Announce Type: cross 
Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Modeling the Martian Induced Magnetosphere</title>
<link>https://arxiv.org/abs/2512.16175</link>
<guid>https://arxiv.org/abs/2512.16175</guid>
<content:encoded><![CDATA[
arXiv:2512.16175v1 Announce Type: cross 
Abstract: Understanding the magnetic field environment around Mars and its response to upstream solar wind conditions provide key insights into the processes driving atmospheric ion escape. To date, global models of Martian induced magnetosphere have been exclusively physics-based, relying on computationally intensive simulations. For the first time, we develop a data-driven model of the Martian induced magnetospheric magnetic field using Physics-Informed Neural Network (PINN) combined with MAVEN observations and physical laws. Trained under varying solar wind conditions, including B_IMF, P_SW, and {\theta}_cone, the data-driven model accurately reconstructs the three-dimensional magnetic field configuration and its variability in response to upstream solar wind drivers. Based on the PINN results, we identify key dependencies of magnetic field configuration on solar wind parameters, including the hemispheric asymmetries of the draped field line strength in the Mars-Solar-Electric coordinates. These findings demonstrate the capability of PINNs to reconstruct complex magnetic field structures in the Martian induced magnetosphere, thereby offering a promising tool for advancing studies of solar wind-Mars interactions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAG Learning from Zero-Inflated Count Data Using Continuous Optimization</title>
<link>https://arxiv.org/abs/2512.16233</link>
<guid>https://arxiv.org/abs/2512.16233</guid>
<content:encoded><![CDATA[
arXiv:2512.16233v1 Announce Type: cross 
Abstract: We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2512.16251</link>
<guid>https://arxiv.org/abs/2512.16251</guid>
<content:encoded><![CDATA[
arXiv:2512.16251v1 Announce Type: cross 
Abstract: We introduce the \textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.16266</link>
<guid>https://arxiv.org/abs/2512.16266</guid>
<content:encoded><![CDATA[
arXiv:2512.16266v1 Announce Type: cross 
Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Probing for Membership Inference in Fine-Tuned Language Models</title>
<link>https://arxiv.org/abs/2512.16292</link>
<guid>https://arxiv.org/abs/2512.16292</guid>
<content:encoded><![CDATA[
arXiv:2512.16292v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Transformers overcome the lack of data in the simulation of history-dependent flows?</title>
<link>https://arxiv.org/abs/2512.16305</link>
<guid>https://arxiv.org/abs/2512.16305</guid>
<content:encoded><![CDATA[
arXiv:2512.16305v1 Announce Type: cross 
Abstract: It is well known that the lack of information about certain variables necessary for the description of a dynamical system leads to the introduction of historical dependence (lack of Markovian character of the model) and noise. Traditionally, scientists have made up for these shortcomings by designing phenomenological variables that take into account this historical dependence (typically, conformational tensors in fluids). Often, these phenomenological variables are not easily measurable experimentally. In this work, we study to what extent Transformer architectures are able to cope with the lack of experimental data on these variables. The methodology is evaluated on three benchmark problems: a cylinder flow with no history dependence, a viscoelastic Couette flow modeled via the Oldroyd-B formalism, and a non-linear polymeric fluid described by the FENE model. Our results show that the Transformer outperforms a thermodynamically consistent, structure-preserving neural network with metriplectic bias in systems with missing experimental data, providing lower errors even in low-dimensional latent spaces. In contrast, for systems whose state variables can be fully known, the metriplectic model achieves superior performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global universal approximation with Brownian signatures</title>
<link>https://arxiv.org/abs/2512.16396</link>
<guid>https://arxiv.org/abs/2512.16396</guid>
<content:encoded><![CDATA[
arXiv:2512.16396v1 Announce Type: cross 
Abstract: We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning</title>
<link>https://arxiv.org/abs/2512.16489</link>
<guid>https://arxiv.org/abs/2512.16489</guid>
<content:encoded><![CDATA[
arXiv:2512.16489v1 Announce Type: cross 
Abstract: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders</title>
<link>https://arxiv.org/abs/2512.16519</link>
<guid>https://arxiv.org/abs/2512.16519</guid>
<content:encoded><![CDATA[
arXiv:2512.16519v1 Announce Type: cross 
Abstract: This paper introduces a cepstrum-based pitch modification method that can be applied to any mel-spectrogram representation. As a result, this method is compatible with any mel-based vocoder without requiring any additional training or changes to the model. This is achieved by directly modifying the cepstrum feature space in order to shift the harmonic structure to the desired target. The spectrogram magnitude is computed via the pseudo-inverse mel transform, then converted to the cepstrum by applying DCT. In this domain, the cepstral peak is shifted without having to estimate its position and the modified mel is recomputed by applying IDCT and mel-filterbank. These pitch-shifted mel-spectrogram features can be converted to speech with any compatible vocoder. The proposed method is validated experimentally with objective and subjective metrics on various state-of-the-art neural vocoders as well as in comparison with traditional pitch modification methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</title>
<link>https://arxiv.org/abs/2512.16538</link>
<guid>https://arxiv.org/abs/2512.16538</guid>
<content:encoded><![CDATA[
arXiv:2512.16538v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Inorganic Synthesis based on Machine Learning using Small Data sets: a case study of size-controlled Cu Nanoparticles</title>
<link>https://arxiv.org/abs/2512.16545</link>
<guid>https://arxiv.org/abs/2512.16545</guid>
<content:encoded><![CDATA[
arXiv:2512.16545v1 Announce Type: cross 
Abstract: Copper nanoparticles (Cu NPs) have a broad applicability, yet their synthesis is sensitive to subtle changes in reaction parameters. This sensitivity, combined with the time- and resource-intensive nature of experimental optimization, poses a major challenge in achieving reproducible and size-controlled synthesis. While Machine Learning (ML) shows promise in materials research, its application is often limited by scarcity of large high-quality experimental data sets. This study explores ML to predict the size of Cu NPs from microwave-assisted polyol synthesis using a small data set of 25 in-house performed syntheses. Latin Hypercube Sampling is used to efficiently cover the parameter space while creating the experimental data set. Ensemble regression models, built with the AMADEUS framework, successfully predict particle sizes with high accuracy ($R^2 = 0.74$), outperforming classical statistical approaches ($R^2 = 0.60$). Overall, this study highlights that, for lab-scale synthesis optimization, high-quality small datasets combined with classical, interpretable ML models outperform traditional statistical methods and are fully sufficient for quantitative synthesis prediction. This approach provides a sustainable and experimentally realistic pathway toward data-driven inorganic synthesis design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Global Convergence of PPO-Clip</title>
<link>https://arxiv.org/abs/2512.16565</link>
<guid>https://arxiv.org/abs/2512.16565</guid>
<content:encoded><![CDATA[
arXiv:2512.16565v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a {\L}ojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Muon is Provably Faster with Momentum Variance Reduction</title>
<link>https://arxiv.org/abs/2512.16598</link>
<guid>https://arxiv.org/abs/2512.16598</guid>
<content:encoded><![CDATA[
arXiv:2512.16598v1 Announce Type: cross 
Abstract: Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\cal O} (\frac{1}{K^{1/4}})$ to ${\cal O} (\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Riemannian Stochastic Interpolants for Amorphous Particle Systems</title>
<link>https://arxiv.org/abs/2512.16607</link>
<guid>https://arxiv.org/abs/2512.16607</guid>
<content:encoded><![CDATA[
arXiv:2512.16607v1 Announce Type: cross 
Abstract: Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARMAE: Masked Autoencoder for SAR Representation Learning</title>
<link>https://arxiv.org/abs/2512.16635</link>
<guid>https://arxiv.org/abs/2512.16635</guid>
<content:encoded><![CDATA[
arXiv:2512.16635v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How accurate are foundational machine learning interatomic potentials for heterogeneous catalysis?</title>
<link>https://arxiv.org/abs/2512.16702</link>
<guid>https://arxiv.org/abs/2512.16702</guid>
<content:encoded><![CDATA[
arXiv:2512.16702v1 Announce Type: cross 
Abstract: Foundational machine learning interatomic potentials (MLIPs) are being developed at a rapid pace, promising closer and closer approximation to ab initio accuracy. This unlocks the possibility to simulate much larger length and time scales. However, benchmarks for these MLIPs are usually limited to ordered, crystalline and bulk materials. Hence, reported performance does not necessarily accurately reflect MLIP performance in real applications such as heterogeneous catalysis. Here, we systematically analyze zero-shot performance of 80 different MLIPs, evaluating tasks typical for heterogeneous catalysis across a range of different data sets, including adsorption and reaction on surfaces of alloyed metals, oxides, and metal-oxide interfacial systems. We demonstrate that current-generation foundational MLIPs can already perform at high accuracy for applications such as predicting vacancy formation energies of perovskite oxides or zero-point energies of supported nanoclusters. However, limitations also exist. We find that many MLIPs catastrophically fail when applied to magnetic materials, and structure relaxation in the MLIP generally increases the energy prediction error compared to single-point evaluation of a previously optimized structure. Comparing low-cost task-specific models to foundational MLIPs, we highlight some core differences between these model approaches and show that -- if considering only accuracy -- these models can compete with the current generation of best-performing MLIPs. Furthermore, we show that no single MLIP universally performs best, requiring users to investigate MLIP suitability for their desired application.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Olaf: Bringing an Animated Character to Life in the Physical World</title>
<link>https://arxiv.org/abs/2512.16705</link>
<guid>https://arxiv.org/abs/2512.16705</guid>
<content:encoded><![CDATA[
arXiv:2512.16705v1 Announce Type: cross 
Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Hidden Biases of Flow Matching Samplers</title>
<link>https://arxiv.org/abs/2512.16768</link>
<guid>https://arxiv.org/abs/2512.16768</guid>
<content:encoded><![CDATA[
arXiv:2512.16768v1 Announce Type: cross 
Abstract: We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences</title>
<link>https://arxiv.org/abs/2512.16778</link>
<guid>https://arxiv.org/abs/2512.16778</guid>
<content:encoded><![CDATA[
arXiv:2512.16778v1 Announce Type: cross 
Abstract: Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_\gamma$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</title>
<link>https://arxiv.org/abs/2512.16786</link>
<guid>https://arxiv.org/abs/2512.16786</guid>
<content:encoded><![CDATA[
arXiv:2512.16786v1 Announce Type: cross 
Abstract: Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16813</link>
<guid>https://arxiv.org/abs/2512.16813</guid>
<content:encoded><![CDATA[
arXiv:2512.16813v1 Announce Type: cross 
Abstract: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16861</link>
<guid>https://arxiv.org/abs/2512.16861</guid>
<content:encoded><![CDATA[
arXiv:2512.16861v1 Announce Type: cross 
Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Universal Representation Property of Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.16872</link>
<guid>https://arxiv.org/abs/2512.16872</guid>
<content:encoded><![CDATA[
arXiv:2512.16872v1 Announce Type: cross 
Abstract: Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: cross 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
<link>https://arxiv.org/abs/2512.16875</link>
<guid>https://arxiv.org/abs/2512.16875</guid>
<content:encoded><![CDATA[
arXiv:2512.16875v1 Announce Type: cross 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta^{\gamma d})$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma < \alpha$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</title>
<link>https://arxiv.org/abs/2512.16881</link>
<guid>https://arxiv.org/abs/2512.16881</guid>
<content:encoded><![CDATA[
arXiv:2512.16881v1 Announce Type: cross 
Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion</title>
<link>https://arxiv.org/abs/2512.16882</link>
<guid>https://arxiv.org/abs/2512.16882</guid>
<content:encoded><![CDATA[
arXiv:2512.16882v1 Announce Type: cross 
Abstract: Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: cross 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Algebra</title>
<link>https://arxiv.org/abs/2512.16902</link>
<guid>https://arxiv.org/abs/2512.16902</guid>
<content:encoded><![CDATA[
arXiv:2512.16902v1 Announce Type: cross 
Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFTok: Bridging the Performance Gap in Discrete Tokenizers</title>
<link>https://arxiv.org/abs/2512.16910</link>
<guid>https://arxiv.org/abs/2512.16910</guid>
<content:encoded><![CDATA[
arXiv:2512.16910v1 Announce Type: cross 
Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16917</link>
<guid>https://arxiv.org/abs/2512.16917</guid>
<content:encoded><![CDATA[
arXiv:2512.16917v1 Announce Type: cross 
Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural networks for dengue forecasting: a systematic review</title>
<link>https://arxiv.org/abs/2106.12905</link>
<guid>https://arxiv.org/abs/2106.12905</guid>
<content:encoded><![CDATA[
arXiv:2106.12905v2 Announce Type: replace 
Abstract: Background: Early forecasts of dengue are an important tool for disease mitigation. Neural networks are powerful predictive models that have made contributions to many areas of public health. In this study, we reviewed the application of neural networks in the dengue forecasting literature, with the objective of informing model design for future work.
  Methods: Following PRISMA guidelines, we conducted a systematic search of studies that use neural networks to forecast dengue in human populations. We summarized the relative performance of neural networks and comparator models, architectures and hyper-parameters, choices of input features, geographic spread, and model transparency.
  Results: Sixty two papers were included. Most studies implemented shallow feed-forward neural networks, using historical dengue incidence and climate variables. Prediction horizons varied greatly, as did the model selection and evaluation approach. Building on the strengths of neural networks, most studies used granular observations at the city level, or on its subdivisions, while also commonly employing weekly data. Performance of neural networks relative to comparators, such as tree-based supervised models, varied across study contexts, and we found that 63% of all studies do include at least one such model as a baseline, and in those cases about half of the studies report neural networks as the best performing model.
  Conclusions: The studies suggest that neural networks can provide competitive forecasts for dengue, and can reliably be included in the set of candidate models for future dengue prediction efforts. The use of deep networks is relatively unexplored but offers promising avenues for further research, as does the use of a broader set of input features and prediction in light of structural changes in the data generation mechanism.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization with Access to Auxiliary Information</title>
<link>https://arxiv.org/abs/2206.00395</link>
<guid>https://arxiv.org/abs/2206.00395</guid>
<content:encoded><![CDATA[
arXiv:2206.00395v5 Announce Type: replace 
Abstract: We investigate the fundamental optimization question of minimizing a target function $f$, whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance, such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, Et cetera. We propose two generic new algorithms that apply in all these settings; we also prove that we can benefit from this framework under the Hessian similarity assumption between the target and side information. A benefit is obtained when this similarity measure is small; we also show a potential benefit from stochasticity when the auxiliary noise is correlated with that of the target function.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Four-hour thunderstorm nowcasting using a deep diffusion model of satellite data</title>
<link>https://arxiv.org/abs/2404.10512</link>
<guid>https://arxiv.org/abs/2404.10512</guid>
<content:encoded><![CDATA[
arXiv:2404.10512v5 Announce Type: replace 
Abstract: Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to infrastructure and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose a deep diffusion model for satellite data (DDMS) to establish an AI-based convection nowcasting system. Specifically, DDMS employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, achieving more accurate forecasts of convective growth and dissipation over longer lead times. Additionally, it combines geostationary satellite brightness temperature data and domain knowledge from meteorological experts, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution Mismatch</title>
<link>https://arxiv.org/abs/2405.02594</link>
<guid>https://arxiv.org/abs/2405.02594</guid>
<content:encoded><![CDATA[
arXiv:2405.02594v2 Announce Type: replace 
Abstract: Traditional online learning models are typically initialized from scratch. By contrast, contemporary real-world applications often have access to historical datasets that can potentially enhanced the online learning processes. We study how offline data can be leveraged to facilitate online learning in stochastic multi-armed bandits and combinatorial bandits. In our study, the probability distributions that govern the offline data and the online rewards can be different. We first show that, without a non-trivial upper bound on their difference, no non-anticipatory policy can outperform the classical Upper Confidence Bound (UCB) policy, even with the access to offline data. In complement, we propose an online policy MIN-UCB for multi-armed bandits. MIN-UCB outperforms the UCB when such an upper bound is available. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. We establish that MIN-UCB achieves tight regret bounds, in both instance independent and dependent settings. We generalize our approach to the combinatorial bandit setting by introducing MIN-COMB-UCB, and we provide corresponding instance dependent and instance independent regret bounds. We illustrate how various factors, such as the biases and the size of offline datasets, affect the utility of offline data in online learning. We discuss several applications and conduct numerical experiments to validate our findings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Models That Prove Their Own Correctness</title>
<link>https://arxiv.org/abs/2405.15722</link>
<guid>https://arxiv.org/abs/2405.15722</guid>
<content:encoded><![CDATA[
arXiv:2405.15722v4 Announce Type: replace 
Abstract: How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured on average over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train Self-Proving models that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over an input sampled from a given distribution, the model generates a correct output and successfully proves its correctness to $V$. The soundness property of $V$ guarantees that, for every input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while all incorrect outputs (of any model) are detected by $V$. We devise and analyze two generic methods for learning Self-Proving models: Transcript Learning (TL) which relies on access to transcripts of accepting interactions, and Reinforcement Learning from Verifier Feedback (RLVF) which trains a model by emulating interactions with the verifier.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PILA: Physics-Informed Low Rank Augmentation for Interpretable Earth Observation</title>
<link>https://arxiv.org/abs/2405.18953</link>
<guid>https://arxiv.org/abs/2405.18953</guid>
<content:encoded><![CDATA[
arXiv:2405.18953v2 Announce Type: replace 
Abstract: Physically meaningful representations are essential for Earth Observation (EO), yet existing physical models are often simplified and incomplete. This leads to discrepancies between simulation and observations that hinder reliable forward model inversion. Common approaches to EO inversion either ignored this incompleteness or relied on case-specific preprocessing. More recent methods use physics-informed autoencoders but depend on auxiliary variables that are difficult to interpret and multiple regularizers that are difficult to balance. We propose Physics-Informed Low-Rank Augmentation (PILA), a framework that augments incomplete physical models using a learnable low-rank residual to improve flexibility, while remaining close to the governing physics.
  We evaluate PILA on two EO inverse problems involving diverse physical processes: forest radiative transfer inversion from optical remote sensing; and volcanic deformation inversion from Global Navigation Satellite Systems (GNSS) displacement data. Across different domains, PILA yields more accurate and interpretable physical variables. For forest spectral inversion, it improves the separation of tree species and, compared to ground measurements, reduces prediction errors by 40-71\% relative to the state-of-the-art. For volcanic deformation, PILA's recovery of variables captures a major inflation event at the Akutan volcano in 2008, and estimates source depth, volume change, and displacement patterns that are consistent with prior studies that however required substantial additional preprocessing. Finally, we analyse the effects of model rank, observability, and physical priors, and suggest that PILA may offer an effective general pathway for inverting incomplete physical models even beyond the domain of Earth Observation. The code is available at https://github.com/yihshe/PILA.git.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title>
<link>https://arxiv.org/abs/2406.16745</link>
<guid>https://arxiv.org/abs/2406.16745</guid>
<content:encoded><![CDATA[
arXiv:2406.16745v3 Announce Type: replace 
Abstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs</title>
<link>https://arxiv.org/abs/2408.06966</link>
<guid>https://arxiv.org/abs/2408.06966</guid>
<content:encoded><![CDATA[
arXiv:2408.06966v2 Announce Type: replace 
Abstract: Dynamic graph modeling aims to uncover evolutionary patterns in real-world systems, enabling accurate social recommendation and early detection of cancer cells. Inspired by the success of recent state space models in efficiently capturing long-term dependencies, we propose DyG-Mamba by translating dynamic graph modeling into a long-term sequence modeling problem. Specifically, inspired by Ebbinghaus' forgetting curve, we treat the irregular timespans between events as control signals, allowing DyG-Mamba to dynamically adjust the forgetting of historical information. This mechanism ensures effective usage of irregular timespans, thereby improving both model effectiveness and inductive capability. In addition, inspired by Ebbinghaus' review cycle, we redefine core parameters to ensure that DyG-Mamba selectively reviews historical information and filters out noisy inputs, further enhancing the model's robustness. Through exhaustive experiments on 12 datasets covering dynamic link prediction and node classification tasks, we show that DyG-Mamba achieves state-of-the-art performance on most datasets, while demonstrating significantly improved computational and memory efficiency. Code is available at https://github.com/Clearloveyuan/DyG-Mamba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised discovery of the shared and private geometry in multi-view data</title>
<link>https://arxiv.org/abs/2408.12091</link>
<guid>https://arxiv.org/abs/2408.12091</guid>
<content:encoded><![CDATA[
arXiv:2408.12091v3 Announce Type: replace 
Abstract: Studying complex real-world phenomena often involves data from multiple views (e.g. sensor modalities or brain regions), each capturing different aspects of the underlying system. Within neuroscience, there is growing interest in large-scale simultaneous recordings across multiple brain regions. Understanding the relationship between views (e.g., the neural activity in each region recorded) can reveal fundamental insights into each view and the system as a whole. However, existing methods to characterize such relationships lack the expressivity required to capture nonlinear relationships, describe only shared sources of variance, or discard geometric information that is crucial to drawing insights from data. Here, we present SPLICE: a neural network-based method that infers disentangled, interpretable representations of private and shared latent variables from paired samples of high-dimensional views. Compared to competing methods, we demonstrate that SPLICE 1) disentangles shared and private representations more effectively, 2) yields more interpretable representations by preserving geometry, and 3) is more robust to incorrect a priori estimates of latent dimensionality. We propose our approach as a general-purpose method for finding succinct and interpretable descriptions of paired data sets in terms of disentangled shared and private latent variables.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2408.14491</link>
<guid>https://arxiv.org/abs/2408.14491</guid>
<content:encoded><![CDATA[
arXiv:2408.14491v2 Announce Type: replace 
Abstract: Recent technological advancements in multimodal machine learning--including the rise of large language models (LLMs)--have improved our ability to collect, process, and analyze diverse multimodal data such as speech, video, and eye gaze in learning and training contexts. While prior reviews have addressed individual components of the multimodal pipeline (e.g., conceptual models, data fusion), a comprehensive review of empirical methods in applied multimodal environments remains notably absent. This review addresses that, introducing a taxonomy and framework that capture both established practices and recent innovations driven by LLMs and generative AI. We identify five modality groups: Natural Language, Vision, Physiological Signals, Human-Centered Evidence, and Environment Logs. Our analysis reveals that integrating modalities enables richer insights into learner and trainee behaviors, revealing latent patterns often overlooked by unimodal approaches. However, persistent challenges in multimodal data collection and integration continue to hinder the adoption of these systems in real-time classroom settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensembles provably learn equivariance through data augmentation</title>
<link>https://arxiv.org/abs/2410.01452</link>
<guid>https://arxiv.org/abs/2410.01452</guid>
<content:encoded><![CDATA[
arXiv:2410.01452v2 Announce Type: replace 
Abstract: Recently, it was proved that group equivariance emerges in ensembles of neural networks as the result of full augmentation in the limit of infinitely wide neural networks (neural tangent kernel limit). In this paper, we extend this result significantly. We provide a proof that this emergence does not depend on the neural tangent kernel limit at all. We also consider stochastic settings, and furthermore general architectures. For the latter, we provide a simple sufficient condition on the relation between the architecture and the action of the group for our results to hold. We validate our findings through simple numeric experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable optimal transport with transformers: The essence of depth and prompt engineering</title>
<link>https://arxiv.org/abs/2410.19931</link>
<guid>https://arxiv.org/abs/2410.19931</guid>
<content:encoded><![CDATA[
arXiv:2410.19931v3 Announce Type: replace 
Abstract: Despite their empirical success, the internal mechanism by which transformer models align tokens during language processing remains poorly understood. This paper provides a mechanistic and theoretical explanation of token alignment in LLMs. We first present empirical evidences showing that, in machine translation, attention weights progressively align translated word pairs across layers, closely approximating Optimal Transport (OT) between word embeddings. Building on this observation, we prove that softmax self-attention layers can simulate gradient descent on the dual of the entropy-regularized OT problem, providing a theoretical foundation for the alignment. Our analysis yields a constructive convergence bound showing that transformer depth controls OT approximation accuracy. A direct implication is that standard transformers can sort lists of varying lengths without any parameter adjustment, up to an error term vanishing with transformers depth.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v4 Announce Type: replace 
Abstract: Adversarial examples exhibit cross-model transferability, enabling threatening black-box attacks on commercial models. Model ensembling, which attacks multiple surrogate models, is a known strategy to improve this transferability. However, prior studies typically use small, fixed ensembles, which leaves open an intriguing question of whether scaling the number of surrogate models can further improve black-box attacks. In this work, we conduct the first large-scale empirical study of this question. We show that by resolving gradient conflict with advanced optimizers, we discover a robust and universal log-linear scaling law through both theoretical analysis and empirical evaluations: the Attack Success Rate (ASR) scales linearly with the logarithm of the ensemble size $T$. We rigorously verify this law across standard classifiers, SOTA defenses, and MLLMs, and find that scaling distills robust, semantic features of the target class. Consequently, we apply this fundamental insight to benchmark SOTA MLLMs. This reveals both the attack's devastating power and a clear robustness hierarchy: we achieve 80\%+ transfer attack success rate on proprietary models like GPT-4o, while also highlighting the exceptional resilience of Claude-3.5-Sonnet. Our findings urge a shift in focus for robustness evaluation: from designing intricate algorithms on small ensembles to understanding the principled and powerful threat of scaling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
arXiv:2412.08435v5 Announce Type: replace 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Feature Exclusion Ranking for Deep Tabular Learning</title>
<link>https://arxiv.org/abs/2412.16442</link>
<guid>https://arxiv.org/abs/2412.16442</guid>
<content:encoded><![CDATA[
arXiv:2412.16442v2 Announce Type: replace 
Abstract: Tabular data is a common format for storing information in rows and columns to represent data entries and their features. Although deep neural networks have become the main approach for modeling a wide range of domains including computer vision and NLP, many of them are not well-suited for tabular data. Recently, a few deep learning models have been proposed for deep tabular learning, featuring an internal feature selection mechanism with end-to-end gradient-based optimization. However, their feature selection mechanisms are unidimensional, and hence fail to account for the contextual dependence of feature importance, potentially overlooking crucial interactions that govern complex tasks. In addition, they overlook the bias of high-impact features and the risk associated with the limitations of attention generalization. To address this limitation, this study proposes a novel iterative feature exclusion module that enhances the feature importance ranking in tabular data. The proposed module iteratively excludes each feature from the input data and computes the attention scores, which represent the impact of the features on the prediction. By aggregating the attention scores from each iteration, the proposed module generates a refined representation of feature importance that captures both global and local interactions between features. The effectiveness of the proposed module is evaluated on four public datasets. The results demonstrate that the proposed module consistently outperforms state-of-the-art methods and baseline models in feature ranking and classification tasks. The code is publicly available at https://github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module and https://github.com/mohalim/IFENet
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modality Collaborative Learning for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.12424</link>
<guid>https://arxiv.org/abs/2501.12424</guid>
<content:encoded><![CDATA[
arXiv:2501.12424v2 Announce Type: replace 
Abstract: Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Form Feedback-Free Learning with Forward Projection</title>
<link>https://arxiv.org/abs/2501.16476</link>
<guid>https://arxiv.org/abs/2501.16476</guid>
<content:encoded><![CDATA[
arXiv:2501.16476v4 Announce Type: replace 
Abstract: State-of-the-art backpropagation-free learning methods employ local error feedback to direct iterative optimisation via gradient descent. Here, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. We propose Forward Projection (FP), a randomised closed-form training method requiring only a single forward pass over the dataset without retrograde communication. FP generates target values for pre-activation membrane potentials through randomised nonlinear projections of pre-synaptic inputs and labels. Local loss functions are optimised using closed-form regression without feedback from downstream layers. A key advantage is interpretability: membrane potentials in FP-trained networks encode information interpretable layer-wise as label predictions. Across several biomedical datasets, FP achieves generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, yielding significant training speedup. In few-shot learning tasks, FP produces more generalisable models than backpropagation-optimised alternatives, with local interpretation functions successfully identifying clinically salient diagnostic features.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2502.10239</link>
<guid>https://arxiv.org/abs/2502.10239</guid>
<content:encoded><![CDATA[
arXiv:2502.10239v2 Announce Type: replace 
Abstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose \ac{METHOD} that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $1.6-3\times$ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Guided Diffusion for Multi-Objective Offline Optimization</title>
<link>https://arxiv.org/abs/2503.17299</link>
<guid>https://arxiv.org/abs/2503.17299</guid>
<content:encoded><![CDATA[
arXiv:2503.17299v2 Announce Type: replace 
Abstract: Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/ surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Finetunes Small Subnetworks in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11711</link>
<guid>https://arxiv.org/abs/2505.11711</guid>
<content:encoded><![CDATA[
arXiv:2505.11711v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization</title>
<link>https://arxiv.org/abs/2505.21423</link>
<guid>https://arxiv.org/abs/2505.21423</guid>
<content:encoded><![CDATA[
arXiv:2505.21423v2 Announce Type: replace 
Abstract: A widely believed explanation for the remarkable generalization capacities of overparameterized neural networks is that the optimization algorithms used for training induce an implicit bias towards benign solutions. To grasp this theoretically, recent works examine gradient descent and its variants in simplified training settings, often assuming vanishing learning rates. These studies reveal various forms of implicit regularization, such as $\ell_1$-norm minimizing parameters in regression and max-margin solutions in classification. Concurrently, empirical findings show that moderate to large learning rates exceeding standard stability thresholds lead to faster, albeit oscillatory, convergence in the so-called Edge-of-Stability regime, and induce an implicit bias towards minima of low sharpness (norm of training loss Hessian). In this work, we argue that a comprehensive understanding of the generalization performance of gradient descent requires analyzing the interaction between these various forms of implicit regularization. We empirically demonstrate that the learning rate balances between low parameter norm and low sharpness of the trained model. We furthermore prove for diagonal linear networks trained on a simple regression task that neither implicit bias alone minimizes the generalization error. These findings demonstrate that focusing on a single implicit bias is insufficient to explain good generalization, and they motivate a broader view of implicit regularization that captures the dynamic trade-off between norm and sharpness induced by non-negligible learning rates.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study</title>
<link>https://arxiv.org/abs/2506.03931</link>
<guid>https://arxiv.org/abs/2506.03931</guid>
<content:encoded><![CDATA[
arXiv:2506.03931v2 Announce Type: replace 
Abstract: Conventional wisdom attributes the mysterious generalization abilities of overparameterized neural networks to gradient descent (and its variants). The recent volume hypothesis challenges this view: it posits that these generalization abilities persist even when gradient descent is replaced by Guess & Check (G&amp;C), i.e., by drawing weight settings until one that fits the training data is found. The validity of the volume hypothesis for wide and deep neural networks remains an open question. In this paper, we theoretically investigate this question for matrix factorization (with linear and non-linear activation)--a common testbed in neural network theory. We first prove that generalization under G&amp;C deteriorates with increasing width, establishing what is, to our knowledge, the first case where G&amp;C is provably inferior to gradient descent. Conversely, we prove that generalization under G&amp;C improves with increasing depth, revealing a stark contrast between wide and deep networks, which we further validate empirically. These findings suggest that even in simple settings, there may not be a simple answer to the question of whether neural networks need gradient descent to generalize well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes</title>
<link>https://arxiv.org/abs/2506.23165</link>
<guid>https://arxiv.org/abs/2506.23165</guid>
<content:encoded><![CDATA[
arXiv:2506.23165v4 Announce Type: replace 
Abstract: Safety is an essential requirement for reinforcement learning systems. The newly emerging framework of robust constrained Markov decision processes allows learning policies that satisfy long-term constraints while providing guarantees under epistemic uncertainty. This paper presents mirror descent policy optimisation for robust constrained Markov decision processes, making use of policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian representing a constrained Markov decision process. Our proposed algorithm obtains an $\tilde{\mathcal{O}}\left(1/T^{1/3}\right)$ convergence rate in the sample-based robust constrained Markov decision process setting. The paper also contributes an algorithm for approximate gradient descent in the space of transition kernels, which is of independent interest for designing adversarial environments in general Markov decision processes. Experiments confirm the benefits of mirror descent policy optimisation in constrained and unconstrained optimisation, and significant improvements are observed in robustness tests when compared to baseline policy optimisation algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations</title>
<link>https://arxiv.org/abs/2507.03631</link>
<guid>https://arxiv.org/abs/2507.03631</guid>
<content:encoded><![CDATA[
arXiv:2507.03631v3 Announce Type: replace 
Abstract: Discovering governing equations that describe complex chaotic systems remains a fundamental challenge in physics and neuroscience. Here, we introduce the PEM-UDE method, which combines the prediction-error method with universal differential equations to extract interpretable mathematical expressions from chaotic dynamical systems, even with limited or noisy observations. This approach succeeds where traditional techniques fail by smoothing optimization landscapes and removing the chaotic properties during the fitting process without distorting optimal parameters. We demonstrate its efficacy by recovering hidden states in the Rossler system and reconstructing dynamics from noise-corrupted electrical-circuit data, in which the correct functional form of the dynamics is recovered even when one of the observed time series is corrupted by noise 5x the magnitude of the true signal. We demonstrate that this method can recover the correct dynamics, whereas direct symbolic regression methods, such as STLSQ, fail to do so with the available data and noise. Importantly, when applied to neural populations, our method derives novel governing equations that respect biological constraints such as network sparsity - a constraint necessary for cortical information processing yet not captured in next-generation neural mass models - while preserving microscale neuronal parameters. These equations predict an emergent relationship between connection density and both oscillation frequency and synchrony in neural circuits. We validate these predictions using three intracranial electrode recording datasets from the medial entorhinal cortex, prefrontal cortex, and orbitofrontal cortex. Our work provides a pathway to develop mechanistic, multi-scale brain models that generalize across diverse neural architectures, bridging the gap between single-neuron dynamics and macroscale brain activity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Collective Variables for Enhanced Sampling from BioEmu with Time-Lagged Generation</title>
<link>https://arxiv.org/abs/2507.07390</link>
<guid>https://arxiv.org/abs/2507.07390</guid>
<content:encoded><![CDATA[
arXiv:2507.07390v2 Announce Type: replace 
Abstract: Molecular dynamics is crucial for understanding molecular systems but its applicability is often limited by the vast timescales of rare events like protein folding. Enhanced sampling techniques overcome this by accelerating the simulation along key reaction pathways, which are defined by collective variables (CVs). However, identifying effective CVs that capture the slow, macroscopic dynamics of a system remains a major bottleneck. This work proposes a novel framework coined BioEmu-CV that learns these essential CVs automatically from BioEmu, a recently proposed foundation model for generating protein equilibrium samples. In particular, we re-purpose BioEmu to learn time-lagged generation conditioned on the learned CV, i.e., predict the distribution of molecular states after a certain amount of time. This training process promotes the CV to encode only the slow, long-term information while disregarding fast, random fluctuations. We validate our learned CV on fast-folding proteins with two key applications: (1) estimating free energy differences using on-the-fly probability enhanced sampling and (2) sampling transition paths with steered molecular dynamics. Our empirical study also serves as a new systematic and comprehensive benchmark for MLCVs on fast-folding proteins larger than Alanine Dipeptide.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Continual Graph Learning</title>
<link>https://arxiv.org/abs/2508.03283</link>
<guid>https://arxiv.org/abs/2508.03283</guid>
<content:encoded><![CDATA[
arXiv:2508.03283v2 Announce Type: replace 
Abstract: Continual Learning (CL) aims to incrementally acquire new knowledge while mitigating catastrophic forgetting. Within this setting, Online Continual Learning (OCL) focuses on updating models promptly and incrementally from single or small batches of observations from a data stream. Extending OCL to graph-structured data is crucial, as many real-world networks evolve over time and require timely, online predictions. However, existing continual or streaming graph learning methods typically assume access to entire graph snapshots or multiple passes over tasks, violating the efficiency constraints of the online setting. To address this gap, we introduce the Online Continual Graph Learning (OCGL) setting, which formalizes node-level continual learning on evolving graphs under strict memory and computational budgets. OCGL defines how a model incrementally processes a stream of node-level information while maintaining anytime inference and respecting resource constraints. We further establish a comprehensive benchmark comprising seven datasets and nine CL strategies, suitably adapted to the OCGL setting, enabling a standardized evaluation setup. Finally, we present a minimalistic yet competitive baseline for OCGL, inspired by our benchmarking results, that achieves strong empirical performance with high efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</title>
<link>https://arxiv.org/abs/2508.17361</link>
<guid>https://arxiv.org/abs/2508.17361</guid>
<content:encoded><![CDATA[
arXiv:2508.17361v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated code review and static analysis at scale, supporting tasks such as vulnerability detection, summarization, and refactoring. In this paper, we identify and exploit a critical vulnerability in LLM-based code analysis: an abstraction bias that causes models to overgeneralize familiar programming patterns and overlook small, meaningful bugs. Adversaries can exploit this blind spot to hijack the control flow of the LLM's interpretation with minimal edits and without affecting actual runtime behavior. We refer to this attack as a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects FPAs into target code. Our evaluation shows that FPAs are not only effective against basic and reasoning models, but are also transferable across model families (OpenAI, Anthropic, Google), and universal across programming languages (Python, C, Rust, Go). Moreover, FPAs remain effective even when models are explicitly warned about the attack via robust system prompts. Finally, we explore positive, defensive uses of FPAs and discuss their broader implications for the reliability and safety of code-oriented LLMs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModalSurv: Investigating opportunities and limitations of multimodal deep survival learning in prostate and bladder cancer</title>
<link>https://arxiv.org/abs/2509.05037</link>
<guid>https://arxiv.org/abs/2509.05037</guid>
<content:encoded><![CDATA[
arXiv:2509.05037v5 Announce Type: replace 
Abstract: Accurate survival prediction is essential for personalised cancer treatment. We propose ModalSurv, a multimodal deep survival framework integrating clinical, MRI, histopathology, and RNA-sequencing data via modality-specific projections and cross-attention fusion. On the CHIMERA Grand Challenge datasets, ModalSurv achieved a C-index of 0.7402 (1st) for prostate and 0.5740 (5th) for bladder cancer. Notably, clinical features alone outperformed multimodal models on external tests, highlighting challenges of limited multimodal alignment and potential overfitting. Local validation showed multimodal gains but limited generalisation. ModalSurv provides a systematic evaluation of multimodal survival modelling, underscoring both its promise and current limitations for scalable, generalisable cancer prognosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Failure Modes of Maximum Entropy RLHF</title>
<link>https://arxiv.org/abs/2509.20265</link>
<guid>https://arxiv.org/abs/2509.20265</guid>
<content:encoded><![CDATA[
arXiv:2509.20265v2 Announce Type: replace 
Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</title>
<link>https://arxiv.org/abs/2509.21735</link>
<guid>https://arxiv.org/abs/2509.21735</guid>
<content:encoded><![CDATA[
arXiv:2509.21735v2 Announce Type: replace 
Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease (AD) progression is crucial for timely intervention. However, this task remains challenging due to the complex dysfunctions in the spatio-temporal characteristics of underlying brain networks, which are often overlooked by existing methods. To address these limitations, we develop an interpretable spatio-temporal graph neural network framework to predict future AD progression, leveraging dual Stochastic Differential Equations (SDEs) to model the irregularly-sampled longitudinal functional magnetic resonance imaging (fMRI) data. We validate our approach on two independent cohorts, including the Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework effectively learns sparse regional and connective importance probabilities, enabling the identification of key brain circuit abnormalities associated with disease progression. Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal lobule as salient regions, with significant disruptions in the ventral attention, dorsal attention, and default mode networks. These abnormalities correlate strongly with longitudinal AD-related clinical symptoms. Moreover, our interpretability strategy reveals both established and novel neural systems-level and sex-specific biomarkers, offering new insights into the neurobiological mechanisms underlying AD progression. Our findings highlight the potential of spatio-temporal graph-based learning for early, individualized prediction of AD progression, even in the context of irregularly-sampled longitudinal imaging data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuON: A Linear-time Alternative to Orthogonal Momentum Updates</title>
<link>https://arxiv.org/abs/2509.24320</link>
<guid>https://arxiv.org/abs/2509.24320</guid>
<content:encoded><![CDATA[
arXiv:2509.24320v4 Announce Type: replace 
Abstract: Orthogonal momentum gradient updates have emerged to overcome the limitations of vector-based optimizers like Adam. The vector-based optimizer Adam suffers from high memory costs and ill-conditioned momentum gradient updates. However, traditional Orthogonal momentum approaches, such as SVD/QR decomposition, suffer from high computational and memory costs and underperform compared to well-tuned SGD with momentum. Recent advances, such as Muon, improve efficiency by applying momentum before orthogonalization and approximate orthogonal matrices via Newton-Schulz iterations, which gives better GPU utilization, active high TFLOPS, and reduces memory usage by up to 3x. Nevertheless, Muon(Vanilla) suffers from exploding attention logits and has cubic computation complexity. In this paper, we deep dive into orthogonal momentum gradient updates to find the main properties that help Muon achieve remarkable performance. We propose AuON (Alternative Unit-norm momentum updates by Normalized nonlinear scaling), a linear-time optimizer that achieves strong performance without approximate orthogonal matrices, while preserving structural alignment and reconditioning ill-posed updates. AuON has an automatic "emergency brake" to handle exploding attention logits. We further introduce a hybrid variant, Hybrid-AuON, that applies the linear transformations with Newton-Schulz iterations, which outperforms Muon in the language modeling tasks. Code is available at: https://github.com/ryyzn9/AuON
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved High-probability Convergence Guarantees of Decentralized SGD</title>
<link>https://arxiv.org/abs/2510.06141</link>
<guid>https://arxiv.org/abs/2510.06141</guid>
<content:encoded><![CDATA[
arXiv:2510.06141v2 Announce Type: replace 
Abstract: Convergence in high-probability (HP) has been receiving increasing interest, due to its attractive properties, such as exponentially decaying tail bounds and strong guarantees for each individual run of an algorithm. While HP guarantees are extensively studied in centralized settings, much less is understood in the decentralized, networked setup. Existing HP studies in decentralized settings impose strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise, resulting in a significant gap between assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized settings, where it is known that $\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed to guarantee MSE convergence. Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing uniformly bounded gradients and other restrictive assumptions, while simultaneously achieving order-optimal rates for both non-convex and strongly convex costs. Moreover, our improved analysis yields linear speed-up in the number of users, demonstrating that $\mathtt{DSGD}$ maintains strong performance in the HP sense and matches existing MSE guarantees. Our improved results stem from a careful analysis of the MGF of quantities of interest (norm-squared of gradient or optimality gap) and the MGF of the consensus gap between users' models. To achieve linear speed-up, we provide a novel result on the variance-reduction effect of decentralized methods in the HP sense and more fine-grained bounds on the MGF for strongly convex costs, which are both of independent interest.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAMDP: Uncertainty-Aware Markov Decision Process for Risk-Constrained Reinforcement Learning from Probabilistic Forecasts</title>
<link>https://arxiv.org/abs/2510.08226</link>
<guid>https://arxiv.org/abs/2510.08226</guid>
<content:encoded><![CDATA[
arXiv:2510.08226v2 Announce Type: replace 
Abstract: Sequential decisions in volatile, high-stakes settings require more than maximizing expected return; they require principled uncertainty management. This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a unified framework that couples Bayesian forecasting, posterior-sampling reinforcement learning, and planning under a conditional value-at-risk (CVaR) constraint. In a closed loop, the agent updates its beliefs over latent dynamics, samples plausible futures via Thompson sampling, and optimizes policies subject to preset risk tolerances. We establish regret bounds that converge to the Bayes-optimal benchmark under standard regularity conditions. We evaluate UAMDP in two domains including high-frequency equity trading and retail inventory control, both marked by structural uncertainty and economic volatility. Relative to strong deep learning baselines, UAMDP improves long-horizon forecasting accuracy (RMSE decreases by up to 25% and sMAPE by 32%), and these gains translate into economic performance: the trading Sharpe ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These results show that integrating calibrated probabilistic modeling, exploration aligned with posterior uncertainty, and risk-aware control yields a robust, generalizable approach to safer and more profitable sequential decision-making.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</title>
<link>https://arxiv.org/abs/2510.10446</link>
<guid>https://arxiv.org/abs/2510.10446</guid>
<content:encoded><![CDATA[
arXiv:2510.10446v2 Announce Type: replace 
Abstract: We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN</title>
<link>https://arxiv.org/abs/2511.03634</link>
<guid>https://arxiv.org/abs/2511.03634</guid>
<content:encoded><![CDATA[
arXiv:2511.03634v2 Announce Type: replace 
Abstract: Tabular foundation models such as TabPFN have revolutionized predictive machine learning for tabular data. At the same time, the driving factors of this revolution are hard to understand. Existing open-source tabular foundation models are implemented in complicated pipelines boasting over 10,000 lines of code, lack architecture documentation or code quality. In short, the implementations are hard to understand, not beginner-friendly, and complicated to adapt for new experiments. We introduce nanoTabPFN, a simplified and lightweight implementation of the TabPFN v2 architecture and a corresponding training loop that uses pre-generated training data. nanoTabPFN makes tabular foundation models more accessible to students and researchers alike. For example, restricted to a small data setting it achieves a performance comparable to traditional machine learning baselines within one minute of pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This eliminated requirement of large computational resources makes pre-training tabular foundation models accessible for educational purposes. Our code is available at https://github.com/automl/nanoTabPFN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v4 Announce Type: replace 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting is Everywhere</title>
<link>https://arxiv.org/abs/2511.04666</link>
<guid>https://arxiv.org/abs/2511.04666</guid>
<content:encoded><![CDATA[
arXiv:2511.04666v2 Announce Type: replace 
Abstract: A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget and shows that Bayesian learners are capable of adapting without forgetting. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all deep learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnviSAgE: A Survey of Environment Scaling for Qualitative Agentic Experience Collection</title>
<link>https://arxiv.org/abs/2511.09586</link>
<guid>https://arxiv.org/abs/2511.09586</guid>
<content:encoded><![CDATA[
arXiv:2511.09586v2 Announce Type: replace 
Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v2 Announce Type: replace 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v2 Announce Type: replace 
Abstract: In this paper, we present the first detailed analysis of how training hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the training hyperparameter space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
arXiv:2511.14317v5 Announce Type: replace 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting</title>
<link>https://arxiv.org/abs/2511.18732</link>
<guid>https://arxiv.org/abs/2511.18732</guid>
<content:encoded><![CDATA[
arXiv:2511.18732v2 Announce Type: replace 
Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wrist Photoplethysmography Predicts Dietary Information</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v2 Announce Type: replace 
Abstract: Whether wearable photoplethysmography (PPG) contains dietary information remains unknown. We trained a language model on 1.1M meals to predict meal descriptions from PPG, aligning PPG to text. PPG nontrivially predicts meal content; predictability decreases for PPGs farther from meals. This transfers to dietary tasks: PPG increases AUC by 11% for intake and satiety across held-out and independent cohorts, with gains robust to text degradation. Wearable PPG may enable passive dietary monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</title>
<link>https://arxiv.org/abs/2511.21016</link>
<guid>https://arxiv.org/abs/2511.21016</guid>
<content:encoded><![CDATA[
arXiv:2511.21016v2 Announce Type: replace 
Abstract: As efficient alternatives to softmax Attention, linear State-Space Models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall-oriented tasks. We propose Gated KalmaNet (GKA), a layer that accounts for the full past while maintaining SSM-style efficiency. We ground our approach in the Kalman Filter (KF) framework, which provides a principled solution for optimal inference in dynamical systems. We show that several existing SSM layers (DeltaNet, Gated DeltaNet, and Kimi Delta Attention) are approximations to the KF recurrence that assume identity error covariance, thereby ignoring how past measurements (keys and values) should optimally influence state updates. In contrast, GKA computes the exact Kalman gain by maintaining the full error covariance. Under a steady-state assumption that enables parallelization, this reduces to solving an online ridge regression problem with constant memory and linear compute cost. A critical insight is that standard KF equations are numerically unstable in low-precision environments (like bfloat16) and hard to parallelize on modern hardware. We address this through: (1) adaptive regularization with input-dependent gating to control the condition number of the ridge regression for numerical stability, and (2) Chebyshev Iteration, which we show is more stable than conventional iterative solvers in low-precision settings. We further develop hardware-aware chunk-wise kernels to enable efficient training. Empirically, GKA outperforms existing SSM layers (like Mamba2 and Gated DeltaNet) on short-context tasks and achieves more than 10\% relative improvement on long-context RAG and LongQA tasks up to 128k tokens.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[
arXiv:2511.22862v2 Announce Type: replace 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at https://github.com/Luchicken/BriMPR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion for Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.23021</link>
<guid>https://arxiv.org/abs/2511.23021</guid>
<content:encoded><![CDATA[
arXiv:2511.23021v2 Announce Type: replace 
Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v2 Announce Type: replace 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v2 Announce Type: replace 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCFO: Density-Based Counterfactuals for Outliers - Additional Material</title>
<link>https://arxiv.org/abs/2512.10659</link>
<guid>https://arxiv.org/abs/2512.10659</guid>
<content:encoded><![CDATA[
arXiv:2512.10659v2 Announce Type: replace 
Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mat\'ern Kernels for Tunable Implicit Surface Reconstruction</title>
<link>https://arxiv.org/abs/2409.15466</link>
<guid>https://arxiv.org/abs/2409.15466</guid>
<content:encoded><![CDATA[
arXiv:2409.15466v3 Announce Type: replace-cross 
Abstract: We propose to use the family of Mat\'ern kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Mat\'ern kernels have some appealing properties which make them particularly well suited for surface reconstruction -- outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scalable. Being stationary, we demonstrate that Mat\'ern kernels allow for tunable surface reconstruction in the same way as Fourier feature mappings help coordinate-based MLPs overcome spectral bias. Moreover, we theoretically analyze Mat\'ern kernels' connection to SIREN networks as well as their relation to previously employed arc-cosine kernels. Finally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\'ern kernels and conclude that especially the Laplace kernel (being part of the Mat\'ern family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk: Default Probabilities and Loan Portfolios</title>
<link>https://arxiv.org/abs/2410.02846</link>
<guid>https://arxiv.org/abs/2410.02846</guid>
<content:encoded><![CDATA[
arXiv:2410.02846v3 Announce Type: replace-cross 
Abstract: We introduce a novel machine learning model for credit risk by combining tree-boosting with a latent spatio-temporal Gaussian process model accounting for frailty correlation. This allows for modeling non-linearities and interactions among predictor variables in a flexible data-driven manner and for accounting for spatio-temporal variation that is not explained by observable predictor variables. We also show how estimation and prediction can be done in a computationally efficient manner. In an application to a large U.S. mortgage credit risk data set, we find that both predictive default probabilities for individual loans and predictive loan portfolio loss distributions obtained with our novel approach are more accurate compared to conventional independent linear hazard models and also linear spatio-temporal models. Using interpretability tools for machine learning models, we find that the likely reasons for this outperformance are strong interaction and non-linear effects in the predictor variables and the presence of spatio-temporal frailty effects.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for Microbiology and Microbiome Research</title>
<link>https://arxiv.org/abs/2411.01098</link>
<guid>https://arxiv.org/abs/2411.01098</guid>
<content:encoded><![CDATA[
arXiv:2411.01098v2 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology and microbiome research now experiencing significant breakthroughs through machine learning applications. This review provides a comprehensive overview of AI-driven approaches tailored for microbiology and microbiome studies, emphasizing both technical advancements and biological insights. We begin with an introduction to foundational AI techniques, including primary machine learning paradigms and various deep learning architectures, and offer guidance on choosing between traditional machine learning and sophisticated deep learning methods based on specific research goals. The primary section on application scenarios spans diverse research areas, from taxonomic profiling, functional annotation \& prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision nutrition, clinical microbiology, to prevention \& therapeutics. Finally, we discuss challenges in this field and highlight some recent breakthroughs. Together, this review underscores AI's transformative role in microbiology and microbiome research, paving the way for innovative methodologies and applications that enhance our understanding of microbial life and its impact on our planet and our health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic regularity of a generalised stochastic Halpern scheme</title>
<link>https://arxiv.org/abs/2411.04845</link>
<guid>https://arxiv.org/abs/2411.04845</guid>
<content:encoded><![CDATA[
arXiv:2411.04845v3 Announce Type: replace-cross 
Abstract: We provide abstract, general and highly uniform rates of asymptotic regularity for a generalized stochastic Halpern-style iteration, which incorporates a second mapping in the style of a Krasnoselskii-Mann iteration. This iteration is general in two ways: First, it incorporates stochasticity completely abstractly, rather than fixing a sampling method; second, it includes as special cases stochastic versions of various schemes from the optimization literature, including Halpern's iteration as well as a Krasnoselskii-Mann iteration with Tikhonov regularization terms in the sense of Bo\c{t}, Csetnek and Meier (where this stochastic variant of the latter is considered for the first time in this paper). For these specific cases, we obtain linear rates of asymptotic regularity, matching (or improving) the currently best known rates for these iterations in stochastic optimization, and quadratic rates of asymptotic regularity are obtained in the context of inner product spaces for the general iteration. We conclude by discussing how variance can be managed in practice through sampling methods in the style of minibatching, how our convergence rates can be adapted to provide oracle complexity bounds, and by sketching how the schemes presented here can be instantiated in the context of reinforcement learning to yield novel methods for Q-learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Backdoor Attacks on Neural Networks</title>
<link>https://arxiv.org/abs/2411.14516</link>
<guid>https://arxiv.org/abs/2411.14516</guid>
<content:encoded><![CDATA[
arXiv:2411.14516v2 Announce Type: replace-cross 
Abstract: Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nested subspace learning with flags</title>
<link>https://arxiv.org/abs/2502.06022</link>
<guid>https://arxiv.org/abs/2502.06022</guid>
<content:encoded><![CDATA[
arXiv:2502.06022v2 Announce Type: replace-cross 
Abstract: Many machine learning methods look for low-dimensional representations of the data. The underlying subspace can be estimated by first choosing a dimension $q$ and then optimizing a certain objective function over the space of $q$-dimensional subspaces (the Grassmannian). Trying different $q$ yields in general non-nested subspaces, which raises an important issue of consistency between the data representations. In this paper, we propose a simple and easily implementable principle to enforce nestedness in subspace learning methods. It consists in lifting Grassmannian optimization criteria to flag manifolds (the space of nested subspaces of increasing dimension) via nested projectors. We apply the flag trick to several classical machine learning methods and show that it successfully addresses the nestedness issue.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An interpretation of the Brownian bridge as a physics-informed prior for the Poisson equation</title>
<link>https://arxiv.org/abs/2503.00213</link>
<guid>https://arxiv.org/abs/2503.00213</guid>
<content:encoded><![CDATA[
arXiv:2503.00213v2 Announce Type: replace-cross 
Abstract: Many inverse problems require reconstructing physical fields from limited and noisy data while incorporating known governing equations. A growing body of work within probabilistic numerics formalizes such tasks via Bayesian inference in function spaces by assigning a physically meaningful prior to the latent field. In this work, we demonstrate that Brownian bridge Gaussian processes can be viewed as a softly-enforced physics-constrained prior for the Poisson equation. We first show equivalence between the variational problem associated with the Poisson equation and a kernel ridge regression objective. Then, through the connection between Gaussian process regression and kernel methods, we identify a Gaussian process for which the posterior mean function and the minimizer to the variational problem agree, thereby placing this PDE-based regularization within a fully Bayesian framework. This connection allows us to probe different theoretical questions, such as convergence and behavior of inverse problems. We then develop a finite-dimensional representation in function space and prove convergence of the projected prior and resulting posterior in Wasserstein distance. Finally, we connect the method to the important problem of identifying model-form error in applications, providing a diagnostic for model misspecification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Insights into Protein Architecture: Persistent Homology and Machine Learning Applied to the Flagellar Motor</title>
<link>https://arxiv.org/abs/2504.16941</link>
<guid>https://arxiv.org/abs/2504.16941</guid>
<content:encoded><![CDATA[
arXiv:2504.16941v5 Announce Type: replace-cross 
Abstract: We present a machine learning approach that leverages persistent homology to classify bacterial flagellar motors into two functional states: rotated and stalled. By embedding protein structural data into a topological framework, we extract multiscale features from filtered simplicial complexes constructed over atomic coordinates. These topological invariants, specifically persistence diagrams and barcodes, capture critical geometric and connectivity patterns that correlate with motor function. The extracted features are vectorized and integrated into a machine learning pipeline that includes dimensionality reduction and supervised classification. Applied to a curated dataset of experimentally characterized flagellar motors from diverse bacterial species, our model demonstrates high classification accuracy and robustness to structural variation. This approach highlights the power of topological data analysis in revealing functionally relevant patterns beyond the reach of traditional geometric descriptors, offering a novel computational tool for protein function prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Krylov Subspace Methods for Generalized Mixed Effects Models with Crossed Random Effects</title>
<link>https://arxiv.org/abs/2505.09552</link>
<guid>https://arxiv.org/abs/2505.09552</guid>
<content:encoded><![CDATA[
arXiv:2505.09552v2 Announce Type: replace-cross 
Abstract: Mixed-effects models are widely used to model data with hierarchical grouping structures and high-cardinality categorical predictor variables. However, for high-dimensional crossed random effects, sparse Cholesky decompositions, the current standard approach, can become prohibitively slow. In this work, we present Krylov subspace-based methods that address these computational bottlenecks and analyze them both theoretically and empirically. In particular, we derive new results on the convergence and accuracy of the preconditioned stochastic Lanczos quadrature and conjugate gradient methods for mixed-effects models, and we develop scalable methods for calculating predictive variances. In experiments with simulated and real-world data, the proposed methods yield speedups by factors of up to about 10,000 and are numerically more stable than Cholesky-based computations as implemented in state-of-the-art packages such as lme4 and glmmTMB. Our methodology is available in the open-source C++ software library GPBoost, with accompanying high-level Python and R packages.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Deep Learning for Discrete Choice</title>
<link>https://arxiv.org/abs/2505.18077</link>
<guid>https://arxiv.org/abs/2505.18077</guid>
<content:encoded><![CDATA[
arXiv:2505.18077v2 Announce Type: replace-cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Force Metrics: Pre-Training MLFFs for Stable MD Simulations</title>
<link>https://arxiv.org/abs/2506.14850</link>
<guid>https://arxiv.org/abs/2506.14850</guid>
<content:encoded><![CDATA[
arXiv:2506.14850v2 Announce Type: replace-cross 
Abstract: Machine-learning force fields (MLFFs) have emerged as a promising solution for speeding up ab initio molecular dynamics (MD) simulations, where accurate force predictions are critical but often computationally expensive. In this work, we employ GemNet-T, a graph neural network model, as an MLFF and investigate two training strategies: (1) direct training on MD17 (10K samples) without pre-training, and (2) pre-training on the large-scale OC20 dataset followed by fine-tuning on MD17 (10K). While both approaches achieve low force mean absolute errors (MAEs), reaching 5 meV/A per atom, we find that lower force errors do not necessarily guarantee stable MD simulations. Notably, the pre-trained GemNet-T model yields significantly improved simulation stability, sustaining trajectories up to three times longer than the model trained from scratch. By analyzing local properties of the learned force fields, we find that pre-training produces more structured latent representations, smoother force responses to local geometric changes, and more consistent force differences between nearby configurations, all of which contribute to more stable and reliable MD simulations. These findings underscore the value of pre-training on large, diverse datasets to capture complex molecular interactions and highlight that force MAE alone is not always a sufficient metric of MD simulation stability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
arXiv:2507.21112v3 Announce Type: replace-cross 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations, focusing on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate feature de-biasing, feature compression, and industry classification in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classification techniques. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element of modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v3 Announce Type: replace-cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACE: A unified Irreducible Cartesian Tensor Framework for Atomistic Machine Learning</title>
<link>https://arxiv.org/abs/2509.14961</link>
<guid>https://arxiv.org/abs/2509.14961</guid>
<content:encoded><![CDATA[
arXiv:2509.14961v2 Announce Type: replace-cross 
Abstract: Here, we introduce the Tensor Atomic Cluster Expansion (TACE), a unified framework formulated entirely in Cartesian space, enabling systematic and consistent prediction of arbitrary structure-dependent tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of irreducible Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including computational levels, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic and dispersion effects. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials. This includes in-domain and out-of-domain benchmarks, spectra, Hessian, external-field responses, charged and magnetic systems, multi-fidelity training, heterogeneous catalysis, and even superior performance within the uMLIP benchmark. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-tensor-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Segmentation of Polyps and Visual Explainability Analysis</title>
<link>https://arxiv.org/abs/2509.18159</link>
<guid>https://arxiv.org/abs/2509.18159</guid>
<content:encoded><![CDATA[
arXiv:2509.18159v5 Announce Type: replace-cross 
Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Network Topology of a Linear Reservoir Computer</title>
<link>https://arxiv.org/abs/2509.23391</link>
<guid>https://arxiv.org/abs/2509.23391</guid>
<content:encoded><![CDATA[
arXiv:2509.23391v2 Announce Type: replace-cross 
Abstract: Machine learning has become a fundamental approach for modeling, prediction, and control, enabling systems to learn from data and perform complex tasks. Reservoir computing is a machine learning tool that leverages high-dimensional dynamical systems to efficiently process temporal data for prediction and observation tasks. Traditionally, the connectivity of the network that underlies a reservoir computer (RC) is generated randomly, lacking a principled design. Here, we focus on optimizing the connectivity of a linear RC to improve its performance and interpretability, which we achieve by decoupling the RC dynamics into a number of independent modes. We then proceed to optimize each one of these modes to perform a given task, which corresponds to selecting an optimal RC connectivity in terms of a given set of eigenvalues of the RC adjacency matrix. Simulations on networks of varying sizes show that the optimized RC significantly outperforms randomly constructed reservoirs in both training and testing phases and often surpasses nonlinear reservoirs of comparable size. This approach provides both practical performance advantages and theoretical guidelines for designing efficient, task-specific, and analytically transparent RC architectures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Optimised Spiking Neural Network for Event-Driven Networking</title>
<link>https://arxiv.org/abs/2509.23516</link>
<guid>https://arxiv.org/abs/2509.23516</guid>
<content:encoded><![CDATA[
arXiv:2509.23516v2 Announce Type: replace-cross 
Abstract: Time-critical networking requires low-latency decisions from sparse and bursty telemetry, where fixed-step neural inference waste computation. We introduce Network-Optimised Spiking (NOS), a two-state neuron whose variables correspond to normalised queue occupancy and a recovery resource. NOS combines a saturating excitability nonlinearity for finite buffers, service and damping leaks, graph-local inputs with per-link gates and delays, and differentiable resets compatible with surrogate gradients and neuromorphic deployment. We establish existence and uniqueness of subthreshold equilibria, derive Jacobian-based local stability tests, and obtain a scalar network stability threshold that separates topology from node physics through a Perron-mode spectral condition. A stochastic arrival model aligned with telemetry smoothing links NOS responses to classical queueing behaviour while explaining increased variability near stability margins. Across chain, star, and scale-free graphs, NOS improves early-warning F1 and detection latency over MLP, RNN, GRU, and temporal-GNN baselines under a common residual-based protocol, while providing practical calibration and stability rules suited to resource-constrained networking deployments. Code and Demos: https://mbilal84.github.io/nos-snn-networking/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins</title>
<link>https://arxiv.org/abs/2510.00774</link>
<guid>https://arxiv.org/abs/2510.00774</guid>
<content:encoded><![CDATA[
arXiv:2510.00774v2 Announce Type: replace-cross 
Abstract: While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who is In Charge? Dissecting Role Conflicts in Instruction Following</title>
<link>https://arxiv.org/abs/2510.01228</link>
<guid>https://arxiv.org/abs/2510.01228</guid>
<content:encoded><![CDATA[
arXiv:2510.01228v2 Announce Type: replace-cross 
Abstract: Large language models should follow hierarchical instructions where system prompts override user inputs, yet recent work shows they often ignore this rule while strongly obeying social cues such as authority or consensus. We extend these behavioral findings with mechanistic interpretations on a large-scale dataset. Linear probing shows conflict-decision signals are encoded early, with system-user and social conflicts forming distinct subspaces. Direct Logit Attribution reveals stronger internal conflict detection in system-user cases but consistent resolution only for social cues. Steering experiments show that, despite using social cues, the vectors surprisingly amplify instruction following in a role-agnostic way. Together, these results explain fragile system obedience and underscore the need for lightweight hierarchy-sensitive alignment methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning and Control: Foundations, Advances, and Perspectives</title>
<link>https://arxiv.org/abs/2510.03303</link>
<guid>https://arxiv.org/abs/2510.03303</guid>
<content:encoded><![CDATA[
arXiv:2510.03303v2 Announce Type: replace-cross 
Abstract: Control theory of dynamical systems offers a powerful framework for tackling challenges in deep neural networks and other machine learning architectures. We show that concepts such as simultaneous and ensemble controllability offer new insights into the classification and representation properties of deep neural networks, while the control and optimization of static systems can be employed to better understand the performance of shallow networks. Inspired by the classical concept of turnpike, we also explore the relationship between dynamic and static neural networks, where depth is traded for width, and the role of transformers as mechanisms for accelerating classical neural network tasks. We also exploit the expressive power of neural networks (exemplified, for instance, by the Universal Approximation Theorem) to develop a novel hybrid modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining mechanics and data-driven methods in a game-theoretic setting. Finally, we describe how classical properties of diffusion processes, long established in the context of partial differential equations, contribute to explaining the success of modern generative artificial intelligence (AI). We present an overview of our recent results in these areas, illustrating how control, machine learning, numerical analysis, and partial differential equations come together to motivate a fertile ground for future research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep generative priors for 3D brain analysis</title>
<link>https://arxiv.org/abs/2510.15119</link>
<guid>https://arxiv.org/abs/2510.15119</guid>
<content:encoded><![CDATA[
arXiv:2510.15119v2 Announce Type: replace-cross 
Abstract: Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Optimization with Random Search</title>
<link>https://arxiv.org/abs/2510.15610</link>
<guid>https://arxiv.org/abs/2510.15610</guid>
<content:encoded><![CDATA[
arXiv:2510.15610v2 Announce Type: replace-cross 
Abstract: We revisit random search for stochastic optimization, where only noisy function evaluations are available. We show that the method works under weaker smoothness assumptions than previously considered, and that stronger assumptions enable improved guarantees. In the finite-sum setting, we design a variance-reduced variant that leverages multiple samples to accelerate convergence. Our analysis relies on a simple translation invariance property, which provides a principled way to balance noise and reduce variance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Split-Client Approach to Second-Order Optimization</title>
<link>https://arxiv.org/abs/2510.15714</link>
<guid>https://arxiv.org/abs/2510.15714</guid>
<content:encoded><![CDATA[
arXiv:2510.15714v2 Announce Type: replace-cross 
Abstract: Second-order methods promise faster convergence but are rarely used in practice because Hessian computations and decompositions are far more expensive than gradients. We propose a \emph{split-client} framework where gradients and curvature are computed asynchronously by separate clients. This abstraction captures realistic delays and inexact Hessian updates while avoiding the manual tuning required by Lazy Hessian methods. Focusing on cubic regularization, we show that our approach retains strong convergence guarantees and achieves a provable wall-clock speedup of order $\sqrt{\tau}$, where $\tau$ is the relative time needed to compute and decompose the Hessian compared to a gradient step. Since $\tau$ can be orders of magnitude larger than one in high-dimensional problems, this improvement is practically significant. Experiments on synthetic and real datasets confirm the theory: asynchronous curvature consistently outperforms vanilla and Lazy Hessian baselines, while maintaining second-order accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</title>
<link>https://arxiv.org/abs/2510.26609</link>
<guid>https://arxiv.org/abs/2510.26609</guid>
<content:encoded><![CDATA[
arXiv:2510.26609v2 Announce Type: replace-cross 
Abstract: Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces FARM: Fine-tuning Agricultural Regression Models, a deep learning framework designed for high-resolution, intra-field canola yield prediction. FARM leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level (30 m) yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, FARM achieves a Root Mean Squared Error (RMSE) of 0.44 and an R^2 of 0.81. Using an independent high-resolution yield monitor dataset, we further show that fine-tuning FARM on limited ground-truth labels outperforms training the same architecture from scratch, confirming the benefit of pre-training on large, upsampled county-level data for data-scarce precision agriculture. These results represent improvement over baseline architectures like 3D-CNN and DeepYield, which highlight the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, FARM offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian model selection and misspecification testing in imaging inverse problems only from noisy and partial measurements</title>
<link>https://arxiv.org/abs/2510.27663</link>
<guid>https://arxiv.org/abs/2510.27663</guid>
<content:encoded><![CDATA[
arXiv:2510.27663v2 Announce Type: replace-cross 
Abstract: Modern imaging techniques heavily rely on Bayesian statistical models to address difficult image reconstruction and restoration tasks. This paper addresses the objective evaluation of such models in settings where ground truth is unavailable, with a focus on model selection and misspecification diagnosis. Existing unsupervised model evaluation methods are often unsuitable for computational imaging due to their high computational cost and incompatibility with modern image priors defined implicitly via machine learning models. We herein propose a general methodology for unsupervised model selection and misspecification detection in Bayesian imaging sciences, based on a novel combination of Bayesian cross-validation and data fission, a randomized measurement splitting technique. The approach is compatible with any Bayesian imaging sampler, including diffusion and plug-and-play samplers. We demonstrate the methodology through experiments involving various scoring rules and types of model misspecification, where we achieve excellent selection and detection accuracy with a low computational cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v2 Announce Type: replace-cross 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
arXiv:2512.09566v2 Announce Type: replace-cross 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Fake-News Detection with Node-Level Topological Features</title>
<link>https://arxiv.org/abs/2512.09974</link>
<guid>https://arxiv.org/abs/2512.09974</guid>
<content:encoded><![CDATA[
arXiv:2512.09974v2 Announce Type: replace-cross 
Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, deep neural networks, sparse decision boundaries, influence functions, Eminence framework<br /><br />Summary:<br /><br />1. Deep neural networks are vulnerable to backdoor attacks often relying on heuristic brute-force approaches, but existing work lacks rigorous theoretical analyses to fully understand the mechanisms behind such attacks.<br />2. This paper theoretically examines how sparse decision boundaries in models create ambiguous regions where even a negligible number of relabeled (poisoned) samples cause significant misclassification.<br />3. Using influence function analysis, the study quantifies how margin samples induce major parameter changes with minimal impact on clean data accuracy, explaining why extremely low poison rates are effective.<br />4. Based on these insights, the authors propose Eminence, a black-box backdoor attack framework that generates a universal, visually subtle trigger targeting vulnerable decision boundaries to induce robust misclassification.<br />5. Eminence requires notably low poison rates (< 0.1%, versus >1% in prior methods), achieves more than 90% attack success rate, preserves clean accuracy, demonstrates high transferability across various models and datasets, and is supported by comprehensive experiments validating its theoretical foundations and effectiveness. <div>
arXiv:2512.10402v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
<div> Keywords: Confucius Code Agent, software engineering, scalable agents, long-context reasoning, meta-agent  

<br /><br />Summary:  
The paper introduces the Confucius Code Agent (CCA), a scalable coding agent designed to handle large-scale software engineering tasks involving massive repositories, long-horizon sessions, and complex toolchains. Built on the Confucius SDK, the platform emphasizes three perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX), ensuring extensibility, interpretability, and controllability. The SDK includes a unified orchestrator and hierarchical working memory to support long-context reasoning, a persistent note-taking system for continual learning across sessions, and a modular extension system for reliable tool integration. A novel meta-agent automates the build-test-improve loop by synthesizing, evaluating, and refining agent configurations, allowing rapid adaptation to new tasks and environments. Experiments on the SWE-Bench-Pro benchmark indicate that CCA achieves a Resolve@1 score of 54.3%, surpassing prior research baselines and matching or exceeding commercial systems under identical conditions. Overall, the Confucius SDK and CCA provide a general, extensible, and production-ready foundation for constructing effective, robust coding agents, addressing the gap between research prototypes and practical deployment in large-scale software engineering contexts. <div>
arXiv:2512.10398v4 Announce Type: replace-cross 
Abstract: Real-world software engineering tasks require coding agents that can operate over massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade agents offer transparency but struggle when scaled to real-world workloads, while proprietary systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a scalable software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK integrates a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid adaptation to new tasks, environments, and tool stacks. Instantiated with these mechanisms, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA reaches a Resolve@1 of 54.3%, exceeding prior research baselines and comparing favorably to commercial results, under identical repositories, model backend, and tool access. Together, the Confucius SDK and CCA form a general, extensible, and production-grade foundation for building effective and robust coding agents, bridging the gap between research prototypes and practical large-scale deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Architecture Search, Large Language Models, Image Captioning, BLEU-4, AutoML<br /><br />Summary:  
1. The paper introduces NN-Caption, a neural architecture search (NAS) pipeline guided by large language models (LLMs) to automatically generate runnable image-captioning models.  
2. NN-Caption composes convolutional neural network (CNN) encoders from LEMUR’s classification backbones with sequence decoders such as LSTM, GRU, or Transformer, adhering to a strict Net API for compatibility.  
3. The approach utilizes DeepSeek-R1-0528-Qwen3-8B as the primary LLM generator, and the authors provide prompt templates and examples of generated architectures.  
4. Evaluation is performed on the MS COCO dataset, using the BLEU-4 metric to measure caption quality.  
5. Results demonstrate that the LLM generated dozens of image-captioning models, with over half successfully trained and producing meaningful captions.  
6. An analysis compares using 5 versus 10 input model snippets in prompts, finding a slight decrease in success rate when more components are provided.  
7. Training dynamics, including caption accuracy over epochs and peak BLEU-4 scores, are reported to characterize model performance.  
8. The study highlights the LLM’s additional role in suggesting hyperparameters and training practices alongside architecture design.  
9. Challenges encountered include code hallucinations and API compliance issues, which were mitigated through prompt engineering and iterative code corrections.  
10. The work integrates prompt-based code generation with automatic evaluation and contributes numerous novel captioning models to the public LEMUR dataset, facilitating reproducible benchmarking and advancing AutoML research. <div>
arXiv:2512.14706v1 Announce Type: new 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Source Knowledge Selection in Multi-Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.14710</link>
<guid>https://arxiv.org/abs/2512.14710</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised multi-domain adaptation, transfer learning, source knowledge selection, density-driven selection, pseudo-label enhancement<br /><br />Summary: Unsupervised multi-domain adaptation is a critical area of transfer learning that utilizes knowledge from multiple labeled source domains to improve learning in an unlabeled target domain. The challenge addressed in this paper is the redundancy and irrelevance of information present in numerous source domains, which can negatively impact transfer performance, especially in scenarios with many source domains. To tackle this, the authors propose a novel method called Autonomous Source Knowledge Selection (AutoS), which autonomously selects the most relevant source training samples and models to enhance target task prediction. AutoS employs a density-driven selection strategy to efficiently pick source samples during training and to determine the contribution of various source models toward target predictions. Additionally, the method integrates a pseudo-label enhancement module that leverages a pre-trained multimodal model to reduce label noise in the target domain and boost self-supervision. Experimental results on real-world datasets demonstrate the superiority and effectiveness of AutoS in improving transfer learning performance by selecting more transferable knowledge and filtering out less relevant source information, paving the way for more scalable and efficient multi-domain adaptation. <div>
arXiv:2512.14710v1 Announce Type: new 
Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
<div> Keywords: Sepsis, Deep Fusion, Context-Aware Stacking, Mixture-of-Experts, Antibiotic Selection<br /><br />Summary:<br /><br />Sepsis is a major cause of ICU admissions worldwide, but traditional prediction models struggle to effectively combine diverse data types such as vitals, text, and imaging. This study compares two architectural approaches: End-to-End Deep Fusion and Context-Aware Stacking. Initially, the authors developed SepsisFusionFormer, a Quad-Modal Hierarchical Gated Attention Network designed to handle complex cross-modal interactions. However, it underperformed due to "attention starvation" and overfitting in a limited antibiotic cohort, achieving an AUC of only 0.66 on MIMIC-IV data. Learning from this, they proposed SepsisLateFusion, a simplified Context-Aware Mixture-of-Experts model that considers each modality as an independent expert: "Historian" (Static data), "Monitor" (Temporal data), and "Reader" (NLP data). These are dynamically weighted by a CatBoost meta-learner. This approach significantly improved performance, reaching a state-of-the-art AUC of 0.915 for predicting sepsis 4 hours before clinical onset. Additionally, adjusting the decision threshold reduced missed cases by 48%, allowing a genuine preventive intervention window. For multi-class antibiotic selection—a novel prescriptive task—the Quad-Modal Ensemble also excelled with a 0.72 AUC. The resulting models are packaged in SepsisSuite, a freely available Python clinical decision support framework. <div>
arXiv:2512.14712v1 Announce Type: new 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</title>
<link>https://arxiv.org/abs/2512.14713</link>
<guid>https://arxiv.org/abs/2512.14713</guid>
<content:encoded><![CDATA[
<div> travel decisions, experience formation, heterogeneity, Latent Class Reinforcement Learning, Variational Bayes  

<br /><br />Summary:  
This paper addresses the role of experience formation in travel decisions, emphasizing that individuals learn and adapt their preferences over time. It highlights the significant heterogeneity among travelers, both in their baseline preferences and in how these preferences develop. To model these aspects, the authors propose a Latent Class Reinforcement Learning (LCRL) framework that captures preference evolution and individual differences simultaneously. The model is applied to data from a driving simulator experiment, providing a practical setting for parameter estimation. Variational Bayes is employed as the estimation technique, allowing for efficient inference within the complex model. Through estimation, three distinct traveler classes are identified: (1) individuals with context-dependent preferences who show exploitative behavior tailored to specific contexts, (2) individuals who persistently exploit a fixed strategy regardless of the context, and (3) individuals who adopt an exploratory approach while still maintaining context-sensitive preferences. These findings advance understanding of how different travelers form and adjust preferences and provide a valuable tool for analyzing dynamic decision-making in travel behavior. The study’s methodological contributions and insights have implications for personalized travel interventions and policy design. <div>
arXiv:2512.14713v1 Announce Type: new 
Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.14714</link>
<guid>https://arxiv.org/abs/2512.14714</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater acoustic classification, Gabor convolution, ResNeXt, squeeze-and-excitation, environmental noise<br /><br />Summary:<br /><br />This paper addresses the challenge of remotely detecting and classifying underwater acoustic targets, crucial for environmental monitoring and defense. Underwater noise complexity, including ship-radiated and environmental noise, complicates accurate signal processing. To overcome these challenges, the authors propose GSE ResNeXt, a deep learning model that integrates learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor convolution layers act as adaptive 2D band-pass filters, expanding feature channel representation, while the channel attention modules improve training stability and convergence. The model is tested on three classification tasks with increasing complexity, including the impact of temporal and spatial differences between training and testing data, highlighting that vessel-sensor distance significantly influences performance. Results demonstrate that GSE ResNeXt consistently outperforms popular baseline models such as Xception, ResNet, and MobileNetV2 in classification accuracy. Additionally, incorporating Gabor convolutions in early layers leads to a 28% reduction in training time, boosting convergence speed. The findings emphasize the importance of integrating signal processing techniques in deep learning to enhance robustness and generalization in data-limited underwater acoustic classification scenarios. Future work should aim to further mitigate environmental factors affecting input signals for improved reliability. <div>
arXiv:2512.14714v1 Announce Type: new 
Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title>
<link>https://arxiv.org/abs/2512.14715</link>
<guid>https://arxiv.org/abs/2512.14715</guid>
<content:encoded><![CDATA[
<div> Keywords: bit flips, large language model, image captioning, semantic drift, differentiable fault analysis  

<br /><br />Summary:  
This work explores the impact of hardware bit flips—low-level, bitwise perturbations—in the weights of large language models (LLMs) used for image captioning. Unlike previous fault injection studies that focused primarily on accuracy degradation or system crashes, this research investigates how single flipped bits can subtly influence the semantic meaning of generated captions without disrupting grammar or fluency. The authors hypothesize that these semantic changes are not random but can be predicted using the model’s gradients. They propose BLADE (Bit-level Fault Analysis via Differentiable Estimation), a novel framework that leverages gradient-based sensitivity estimation combined with a semantic-fluency objective at the caption level to identify bits that critically affect meaning. This approach allows a targeted understanding of how semantic information is encoded and distributed at an extremely low level within generative vision-language models. The findings demonstrate that imperceptible bit-level alterations can steer the narrative produced by such models, highlighting vulnerabilities and interpretability aspects. This insight facilitates robustness testing and adversarial defense while contributing to explainable AI by showing how structured low-level faults reshape high-level semantic outputs in generative systems. <div>
arXiv:2512.14715v1 Announce Type: new 
Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox</title>
<link>https://arxiv.org/abs/2512.14717</link>
<guid>https://arxiv.org/abs/2512.14717</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS, financial NLP, model efficiency, large language models, benchmark evaluation  

<br /><br />Summary:  
This paper presents a comprehensive evaluation of the GPT-OSS large language model family in financial natural language processing (NLP) tasks, benchmarking against other contemporary LLMs. The study focuses on two variants of GPT-OSS, a 120 billion parameter model and a smaller 20 billion parameter version, revealing that the smaller GPT-OSS-20B achieves performance nearly equal to its larger counterpart (65.1% vs 66.5% accuracy). It also demonstrates superior computational efficiency, measured by a Token Efficiency Score of 198.4 and a processing speed of 159.80 tokens per second. The evaluation covers ten diverse financial NLP tasks, including sentiment analysis, question answering, and entity recognition, utilizing real-world datasets such as Financial PhraseBank, FiQA-SA, and FLARE FINERORD. The authors introduce new efficiency metrics that effectively balance model accuracy with resource consumption, critical for practical deployment decisions. Additionally, the GPT-OSS models consistently outperform larger models like Qwen3-235B, challenging the assumption that bigger models always yield better task performance. The findings highlight how architectural advancements and training methodologies enable smaller GPT-OSS models to maintain competitive accuracy while significantly reducing computational demands, supporting more sustainable and cost-effective use of LLMs in financial service applications. <div>
arXiv:2512.14717v1 Announce Type: new 
Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.14718</link>
<guid>https://arxiv.org/abs/2512.14718</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series, spectral entropy, dependency modeling, signed graph, temporal positions<br /><br />Summary: This paper introduces SEED, a novel framework designed to improve multivariate time series forecasting by effectively modeling complex spatial-temporal dependencies among variables. The authors identify three main challenges in existing attention- or graph-based methods: disruption of strong temporal self-dependencies by irrelevant variables, the masking and reversal of negative correlations due to softmax normalization, and poor perception of temporal positions by variables. To overcome these, SEED incorporates a Dependency Evaluator that uses spectral entropy to dynamically assess spatial and temporal dependencies per variable, allowing an adaptive balance between channel independence and dependence. It further employs a Spectral Entropy-based Fuser to isolate temporal regularities influenced externally rather than intrinsically. The framework also features a Signed Graph Constructor to maintain negative correlations by enabling signed edge weights, circumventing the softmax limitations. Lastly, SEED introduces a Context Spatial Extractor that uses local contextual windows to help variables capture temporal positions, leading to richer spatial feature representations. Extensive experiments on 12 diverse real-world datasets demonstrate that SEED achieves state-of-the-art forecasting performance, confirming its effectiveness and broad applicability across domains. <div>
arXiv:2512.14718v1 Announce Type: new 
Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Attribution Priors for Explainable and Robust Model Training</title>
<link>https://arxiv.org/abs/2512.14719</link>
<guid>https://arxiv.org/abs/2512.14719</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Attribution Priors, Interpretability, Class-Aware Attribution Prior, Robustness<br /><br />Summary:<br /><br />Small language models (SLMs) are increasingly employed in low-latency tasks such as classification, where interpretability and robustness are critical. Existing explanation-guided learning methods introduce attribution-based supervision but struggle due to the difficulty in deriving reliable and general attribution priors. The paper analyzes popular attribution techniques and reveals that while they effectively highlight tokens relevant to each class, they tend to focus on shared keywords across semantically similar classes. This overlap hampers the model’s ability to distinguish between such classes because the attribution priors lack discriminative power. To address this, the authors propose the Class-Aware Attribution Prior (CAP), a novel method designed to extract attribution priors that emphasize fine-grained class distinctions and generate more discriminative features. Extending this further, CAP Hybrid integrates CAP-derived priors with those from existing methods to provide a more balanced and comprehensive supervisory signal. By training models to align their self-attribution with these enriched priors, the approach fosters the learning of diverse and decision-relevant features. Extensive experiments across full-data, few-shot, and adversarial testing scenarios demonstrate consistent improvements in both interpretability and robustness of the language models. <div>
arXiv:2512.14719v1 Announce Type: new 
Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example</title>
<link>https://arxiv.org/abs/2512.14721</link>
<guid>https://arxiv.org/abs/2512.14721</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, Synthea, rule-based data generation, glioblastoma, privacy-preserving research  

<br /><br />Summary:  
1. The paper addresses the generation of synthetic medical data as a tool for enabling privacy-compliant secondary use of medical records.  
2. It highlights the rule-based Synthea data generator, which creates realistic patient data by simulating the lifetime of synthetic patients using probabilistic rules based on factors like age.  
3. Although Synthea rules contain only statistical information and typically have no specific data protection issues, designing them requires expert knowledge and realistic datasets.  
4. The authors propose an automated method to generate Synthea rules by extracting statistical information from existing tabular medical data specifically related to cancer reports.  
5. As a case study, they developed a Synthea module for glioblastoma using real-world data, then generated synthetic datasets that successfully mirrored known disease courses and largely preserved the statistical properties of the original data.  
6. The study underscores the potential of synthetic patient data for privacy-preserving medical research, enabling hypothesis formulation and prototype development while cautioning that interpretation must consider inherent limitations of synthetic data. <div>
arXiv:2512.14721v1 Announce Type: new 
Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</title>
<link>https://arxiv.org/abs/2512.14722</link>
<guid>https://arxiv.org/abs/2512.14722</guid>
<content:encoded><![CDATA[
<div> transformers, Groebner bases, Hierarchical Attention Transformers, multivariate polynomials, curriculum learning<br /><br />Summary:<br />1. The paper builds on the work presented by Kera et al. at NeurIPS 2024, who introduced the use of transformers to compute Groebner bases, which are essential tools in computer algebra for solving polynomial systems and other applications.<br />2. The authors propose an improved method that leverages Hierarchical Attention Transformers (HATs), a transformer architecture with a tree-structured inductive bias that better captures the hierarchical relationships inherent in the data.<br />3. Compared to traditional flat attention models, the HAT approach provides substantial computational savings by exploiting these hierarchical structures, making it more efficient for Groebner basis computation.<br />4. The approach generalizes to arbitrary depths, allowing it to handle more complex and larger polynomial systems than previously possible.<br />5. By integrating curriculum learning, the method successfully solves larger problem instances than those tackled by Kera et al. (2024), demonstrating enhanced scalability and effectiveness in computing Groebner bases for multivariate polynomial equations. <div>
arXiv:2512.14722v1 Announce Type: new 
Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</title>
<link>https://arxiv.org/abs/2512.14725</link>
<guid>https://arxiv.org/abs/2512.14725</guid>
<content:encoded><![CDATA[
<div> Keywords: urban wind flow, generative diffusion, graph neural network, CFD simulation, uncertainty-aware predictions<br /><br />Summary:<br /><br />1. The article addresses the challenge of modeling and simulating urban wind flow, which is crucial for air quality and sustainable urban planning. <br />2. Traditional low-order models fail to capture complex urban geometries effectively, while high-fidelity CFD simulations are computationally expensive and impractical for multiple scenarios.<br />3. The authors propose a generative diffusion framework that synthesizes steady-state urban wind velocity fields on unstructured meshes based solely on geometry information.<br />4. The framework integrates a hierarchical graph neural network with score-based diffusion modeling, enabling the generation of accurate, diverse velocity fields without the need for temporal rollouts or dense measurement data.<br />5. Trained on datasets spanning multiple mesh slices and wind angles, the model generalizes well to unseen geometries and successfully recovers important flow features like wakes and recirculation zones.<br />6. The model provides uncertainty-aware predictions and robustness to variations in mesh design and inference methods, as demonstrated by ablation studies.<br />7. This approach sets the foundation for building environment models that can assist urban planners in rapidly evaluating design decisions, especially under conditions of urban densification and climate uncertainty. <div>
arXiv:2512.14725v1 Announce Type: new 
Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14726</link>
<guid>https://arxiv.org/abs/2512.14726</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline reinforcement learning, Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, Long-horizon credit assignment<br /><br />Summary:<br /><br />1. The paper introduces the Quantum Decision Transformer (QDT), a novel architecture designed to improve offline reinforcement learning, especially addressing challenges with long-horizon credit assignment and complex state-action dependencies.  
2. QDT incorporates two main quantum-inspired components: Quantum-Inspired Attention that uses entanglement operations to capture non-local feature correlations, and Quantum Feedforward Networks which employ multi-path processing and learnable interference for adaptive computation.  
3. Experimental results on continuous control tasks show that QDT achieves over 2,000% performance improvement compared to standard Decision Transformers, along with superior generalization across different data qualities.  
4. Ablation studies highlight a significant synergy between the quantum-inspired components, where individually neither component achieves competitive results, but together they produce dramatic performance gains exceeding their individual effects.  
5. The architecture’s advantages include enhanced credit assignment through capturing non-local correlations, implicit ensemble-like behavior via parallel processing paths, and adaptive resource allocation enabled by learnable interference, demonstrating a holistic quantum-inspired design principle that could influence broader neural architecture design beyond reinforcement learning. <div>
arXiv:2512.14726v1 Announce Type: new 
Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title>
<link>https://arxiv.org/abs/2512.14727</link>
<guid>https://arxiv.org/abs/2512.14727</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Conformal Prediction, Uncertainty Estimates, Calibration Set Size, Medical Image Classification  

<br /><br />Summary:  
1. Machine learning (ML) is increasingly used in healthcare, but dependable uncertainty estimates are necessary for safe clinical decision-making, which traditional ML models typically lack.  
2. Conformal prediction (CP) is a technique that provides statistically guaranteed uncertainty estimates by converting ML model predictions, combined with a calibration sample, into prediction sets with a user-specified coverage probability.  
3. CP theory claims to hold regardless of calibration set size, implying that meaningful uncertainty quantification can still be achieved with small calibration datasets.  
4. The authors challenge this assumption by demonstrating that while statistical guarantees remain valid for arbitrarily sized calibration sets, the practical effectiveness of these guarantees is strongly dependent on the calibration set size.  
5. This finding is particularly significant for medical applications where data scarcity often restricts the size of calibration sets, potentially limiting the usefulness of CP-based uncertainty estimates.  
6. Their critique is supported by empirical results from a medical image classification task, illustrating the practical limitations of CP with small calibration sets in a real-world healthcare context. <div>
arXiv:2512.14727v1 Announce Type: new 
Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems</title>
<link>https://arxiv.org/abs/2512.14728</link>
<guid>https://arxiv.org/abs/2512.14728</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory inference, urban rail transit, Automatic Fare Collection, data-driven, KL divergence

<br /><br />Summary:  
This paper presents a fully data-driven method for inferring individual travel trajectories within urban rail transit systems by leveraging data from Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems. The approach focuses on identifying key trajectory elements, including the selected train, access and egress times, and transfer times. It begins by establishing sets of alternative trains based on spatio-temporal constraints, followed by adaptive trajectory inference and complete travel trajectory construction. A novel data-driven parameter estimation algorithm, KLEM, which combines KL divergence with the Expectation-Maximization (EM) algorithm, is introduced to eliminate dependency on external or survey datasets for parameter fitting. This advancement enhances the robustness and applicability of the model across different contexts. Unlike previous studies that mostly rely on synthetic data, this paper validates its results using real individual travel trajectory data, addressing a significant gap in verification practices. The experimental findings demonstrate that the proposed approach achieves over 90% accuracy in inferring urban rail transit travel trajectories during peak hours, indicating its potential effectiveness and precision for operational organization and transit management purposes. <div>
arXiv:2512.14728v1 Announce Type: new 
Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Geometry for policy-constrained interpretation</title>
<link>https://arxiv.org/abs/2512.14731</link>
<guid>https://arxiv.org/abs/2512.14731</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric framework, policy constraints, semantic interpretation, hallucination prevention, financial regulation<br /><br />Summary:<br /><br />This paper introduces a novel geometric framework designed to enforce policy constraints on semantic interpretation processes, thereby preventing hallucinated commitments that could lead to errors in critical, high-stakes applications. Semantic meanings are geometrically represented as directions on a unit sphere, while evidence is captured as sets of witness vectors. The framework defines admissible interpretations as spherical convex regions, enabling a clear geometric delineation between possible semantic outcomes. Policy constraints are incorporated as explicit priors on the manifold, distinctly separated from the evidence geometry to maintain clarity and modularity. Interpretation is formulated as a constrained optimization problem over these admissible regions, where refusal to interpret naturally arises from topological conditions under contradictory evidence or policy exclusion. The framework is rigorously connected to established theoretical foundations such as information theory, Bayesian inference, and sheaf-theoretic semantics, achieving provably optimal complexity bounds from an information-theoretic perspective. Empirically, the approach is validated on large-scale, regulated financial datasets, demonstrating zero instances of hallucinated approvals across various policy regimes, marking a significant advance as the first scalable method to guarantee policy-compliant semantic interpretation without erroneous commitments in this domain. <div>
arXiv:2512.14731v1 Announce Type: new 
Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
<div> Keywords: incidental findings, CT scans, large language models, vision-language models, automated detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting and reporting incidental findings in abdominal CT scans, which are commonly benign but clinically important when identified correctly.<br /><br />2. Traditional manual inspection by radiologists is time-consuming and inconsistently reliable, necessitating automated methods for efficiency and precision.<br /><br />3. The authors propose a novel plan-and-execute framework combining large language models (LLMs) with foundational vision-language models (VLMs) to automate detection, classification, and reporting of incidental findings.<br /><br />4. The framework includes a planner, powered by an LLM that generates Python scripts based on medical guidelines and predefined base functions, and an executor that runs these scripts using VLMs, segmentation models, and image processing tools.<br /><br />5. Experimental evaluation on an abdominal CT benchmark targeting three organs demonstrates that this integrated approach achieves better accuracy and efficiency than existing VLM-only methods, enabling fully automatic end-to-end incidental findings management. <div>
arXiv:2512.14732v1 Announce Type: new 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness</title>
<link>https://arxiv.org/abs/2512.14734</link>
<guid>https://arxiv.org/abs/2512.14734</guid>
<content:encoded><![CDATA[
<div> Keywords: recommender systems, intra-day personalization, long-form video streaming, user engagement, model-agnostic approach  

<br /><br />Summary:  
1. Traditional recommender systems for long-form video streaming often rely on batch-trained models with user features updated once daily, creating stale recommendations throughout the day.  
2. The paper proposes a lightweight, model-agnostic method to inject recent watch history at inference time, enabling intra-day personalization without the need for model retraining.  
3. This method selectively overrides outdated user features using recent user actions, allowing the recommendation system to adapt quickly to changing user preferences.  
4. Implementation of this approach reduced the personalization feedback cycle from a daily update to intra-day updates, resulting in a statistically significant increase of 0.47% in key user engagement metrics.  
5. This work is the first to provide published evidence that intra-day personalization can produce meaningful improvements in user engagement within long-form video streaming, offering an efficient alternative to costly full real-time retraining architectures. <div>
arXiv:2512.14734v1 Announce Type: new 
Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NoveltyRank: Estimating Conceptual Novelty of AI Papers</title>
<link>https://arxiv.org/abs/2512.14738</link>
<guid>https://arxiv.org/abs/2512.14738</guid>
<content:encoded><![CDATA[
<div> Keywords: novelty estimation, AI research, semantic similarity, binary classification, pairwise comparison<br /><br />Summary: This paper addresses the challenge of evaluating the conceptual novelty of AI research papers amidst a rapid increase in academic publications. It proposes a model to estimate and rank the novelty of papers by analyzing their titles, abstracts, and semantic similarity to existing literature. The goal is to provide a scalable, data-driven way to identify genuinely innovative work and assist conference reviewers with consistent novelty assessments. Two task formulations are explored: (1) binary classification, which predicts absolute novelty based on patterns learned from prior novel works, and (2) pairwise novelty comparison, which ranks papers relative to others in terms of novelty. The authors fine-tune two language models, Qwen3-4B-Instruct-2507 and SciBERT, under both formulations and benchmark their performance against GPT-5.1. This comparison highlights how different modeling choices impact novelty estimation effectiveness. The implementation of their approach is publicly accessible via GitHub, encouraging further research and practical application in managing the increasing volume of AI research publications. <div>
arXiv:2512.14738v1 Announce Type: new 
Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Discrete Diffusion for Constraint Satisfaction Problems</title>
<link>https://arxiv.org/abs/2512.14765</link>
<guid>https://arxiv.org/abs/2512.14765</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, constraint satisfaction problems, CSPs, Sudoku, unsupervised learning<br /><br />Summary:  
1. The paper introduces a novel method called discrete diffusion guidance tailored for solving constraint satisfaction problems (CSPs).  
2. The approach leverages discrete diffusion processes to effectively navigate the solution space of CSPs, guiding the search towards feasible solutions.  
3. A key application demonstrated is the ability of this method to solve Sudoku puzzles, a classical NP-complete CSP, without any supervised training or labeled data.  
4. By utilizing unsupervised learning mechanisms, the method circumvents the need for handcrafted heuristics or extensive domain knowledge traditionally required in solving CSPs like Sudoku.  
5. Experimental results illustrate that discrete diffusion guidance can efficiently and accurately find valid Sudoku solutions, showcasing its potential as a general strategy for CSPs beyond Sudoku puzzles. <div>
arXiv:2512.14765v1 Announce Type: new 
Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Weather Forecasts from a Decision Maker's Perspective</title>
<link>https://arxiv.org/abs/2512.14779</link>
<guid>https://arxiv.org/abs/2512.14779</guid>
<content:encoded><![CDATA[
<div> Keywords: decision calibration, weather forecast evaluation, machine learning, numerical weather prediction, decision-making

<br /><br />Summary: This paper critiques standard weather forecast evaluations for focusing primarily on the forecaster's perspective and statistical comparisons between forecasts and observations. It argues that since forecasts are ultimately employed to make decisions, evaluating forecasts from the decision-maker's perspective offers greater practical relevance. To this end, the study introduces and applies decision calibration—a framework that assesses forecast performance based on improvement in decision-making outcomes rather than traditional forecast accuracy metrics. The authors conduct experiments comparing machine learning-based models and classical numerical weather prediction models across various weather-dependent decision tasks. Their findings reveal a disconnect between forecast-level performance and utility in downstream decision-making; some differences only surface when viewed through decision calibration. They also observe that model rankings fluctuate depending on the specific decision task considered. The study concludes that conventional forecast evaluations are often insufficient for choosing the best forecasting model for particular decision applications, highlighting the need for decision-level evaluation approaches to better align forecasting with operational needs. <div>
arXiv:2512.14779v1 Announce Type: new 
Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unreliable Uncertainty Estimates with Monte Carlo Dropout</title>
<link>https://arxiv.org/abs/2512.14851</link>
<guid>https://arxiv.org/abs/2512.14851</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty estimation, Monte Carlo dropout, Bayesian inference, Gaussian Processes, Bayesian Neural Networks<br /><br />Summary:<br /><br />Reliable uncertainty estimation is essential for machine learning models, especially in safety-critical applications. Exact Bayesian inference provides a principled method for uncertainty quantification but is often computationally prohibitive for deep neural networks. Monte Carlo dropout (MCD) is proposed as an efficient approximation of Bayesian inference by applying neuron dropout during inference to generate multiple sub-model predictions. This research empirically evaluates MCD's effectiveness in capturing true uncertainty compared to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). The findings reveal that MCD struggles to accurately represent uncertainty and fails notably in reflecting increased uncertainty in extrapolation and interpolation regions, where Bayesian models show expected rise in uncertainty. Consequently, the study suggests that uncertainty estimates produced by MCD are less reliable for capturing both epistemic (model) and aleatoric (data) uncertainty than those obtained from traditional Bayesian approaches. <div>
arXiv:2512.14851v1 Announce Type: new 
Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal</title>
<link>https://arxiv.org/abs/2512.14873</link>
<guid>https://arxiv.org/abs/2512.14873</guid>
<content:encoded><![CDATA[
<div> Keywords: Fourier Analysis Network, sine activation, dying-ReLU problem, Dual-Activation Layer, neural network convergence<br /><br />Summary:<br /><br />1. Fourier Analysis Network (FAN) improves neural network performance by replacing parts of ReLU activations with sine and cosine functions, yet the exact reason behind the gains remained unclear.<br />2. The study finds that only the sine activation positively impacts performance, while cosine activation is often detrimental.<br />3. Improvement stems not from the sine function’s periodic nature but from its local behavior near zero, where the non-zero derivative mitigates the vanishing-gradient problem.<br />4. FAN primarily addresses the dying-ReLU problem, where neurons stuck with negative inputs produce zero gradients and cease learning.<br />5. Although modern ReLU variants like Leaky ReLU, GELU, and Swish reduce zero-gradient regions, they still suffer from diminished gradients that slow optimization.<br />6. FAN introduces a more stable gradient pathway improving training dynamics beyond the spectral interpretation.<br />7. Based on this analysis, the Dual-Activation Layer (DAL) is developed as a more efficient convergence accelerator.<br />8. Experiments on noisy sinusoidal signal classification, MNIST digit recognition, and ECG-based biometric recognition show DAL models converge faster and achieve equal or better validation accuracy than conventional activations. <div>
arXiv:2512.14873v1 Announce Type: new 
Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</title>
<link>https://arxiv.org/abs/2512.14879</link>
<guid>https://arxiv.org/abs/2512.14879</guid>
<content:encoded><![CDATA[
<div> Keywords: self-referential learning, entropy collapse, Bregman projection, entropy reservoir, model stabilization<br /><br />Summary:<br /><br />1. The paper addresses the problem of model collapse in self-referential learning, where models trained on their own generated data deteriorate into low-entropy states such as repetitive text, mode dropping, or policy over-exploitation.<br /><br />2. Existing fixes like mixing real data, entropy bonuses, knowledge distillation, or retrieval-augmented generation work but lack a unifying theoretical explanation.<br /><br />3. The authors introduce the Entropy-Reservoir Bregman Projection (ERBP) framework, which models closed-loop learning as a stochastic Bregman projection sequence in distribution space and explains entropy decay as a shrinking empirical support caused by finite-sample noise.<br /><br />4. The concept of an Entropy Reservoir—mixing a high-entropy distribution at each projection step—inserts entropy flux to stabilize training dynamics, preventing collapse.<br /><br />5. Their theory derives necessary and sufficient conditions for collapse and stabilized entropy floors, along with closed-form rates dependent on sample size and mathematical properties of the Bregman generator.<br /><br />6. Experimental validation covers large language model self-training, Soft Actor-Critic reinforcement learning, and GAN optimization, confirming that many heuristics correspond to specific choices of entropy reservoirs.<br /><br />7. Overall, ERBP unifies disparate stabilization techniques into a single quantitative design principle: actively monitor and manage your entropy flux to prevent model collapse in self-referential training systems. <div>
arXiv:2512.14879v1 Announce Type: new 
Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</title>
<link>https://arxiv.org/abs/2512.14880</link>
<guid>https://arxiv.org/abs/2512.14880</guid>
<content:encoded><![CDATA[
<div> Keywords: task matrix, linear transformation, vision models, language models, finetuning<br /><br />Summary:<br /><br />1. This paper introduces the concept of a task matrix, defined as a linear transformation that maps embeddings from a base pretrained model to their finetuned counterparts. <br />2. The authors demonstrate that for both vision and text models, applying a task matrix enables the base model to achieve performance that exceeds traditional linear probes and sometimes nears that of fully finetuned models. <br />3. Their experiments span ten different datasets, confirming the broad applicability of the task matrix across multiple domains and architectures. <br />4. The work validates the presence of implicit cross-layer linear encodings connecting pretrained and finetuned model states, extending previous interpretability findings from in-context prompting scenarios to general adaptation regimes. <br />5. Additionally, they propose an efficient, data-driven method to approximate these task matrices, which is shown to be generalizable and practical. The authors have released their implementation publicly to facilitate further research and adoption. <div>
arXiv:2512.14880v1 Announce Type: new 
Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</title>
<link>https://arxiv.org/abs/2512.14892</link>
<guid>https://arxiv.org/abs/2512.14892</guid>
<content:encoded><![CDATA[
<div> Keywords: Online learning, Weighted average, Multivariate regression, Concept drift, Convergence analysis<br /><br />Summary:<br /><br />1. The paper introduces OLR-WA (OnLine Regression with Weighted Average), a novel and versatile multivariate online linear regression model designed to update incrementally using new data, reducing storage demands and computational overhead.<br /><br />2. OLR-WA is specifically crafted to handle scenarios involving drift, where data patterns evolve over time, including both time-based (temporal) and confidence-based drift situations.<br /><br />3. A key innovation of OLR-WA is its conservative update mechanism that prioritizes older data points with higher confidence, enabling it to effectively manage challenging confidence-based scenarios, which other models struggle with.<br /><br />4. Through convergence analysis and comparison with existing online regression models, OLR-WA demonstrates rapid convergence and high performance, consistently achieving strong r² values from the first iteration onward, even when initialized with as little as 1% to 10% of the total data.<br /><br />5. Overall, OLR-WA matches or exceeds the performance of state-of-the-art online regression approaches and performs comparably to batch regression, affirming its efficacy and versatility as a robust solution for online linear regression tasks across diverse contexts. <div>
arXiv:2512.14892v1 Announce Type: new 
Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</title>
<link>https://arxiv.org/abs/2512.14895</link>
<guid>https://arxiv.org/abs/2512.14895</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, covariate shift, on-policy expert corrections, multi-turn language models, software engineering tasks<br /><br />Summary:  
This paper addresses the limitations of imitation learning for training multi-turn language model (LM) agents, specifically highlighting the problem of covariate shift where the student policy diverges from the expert behavior and encounters unfamiliar states. To overcome this, the authors propose a novel data generation method inspired by the DAgger algorithm called on-policy expert corrections (OECs). OECs involve generating partially on-policy data by starting rollouts with a student model and switching to an expert model mid-trajectory to provide corrections. The approach is evaluated in the domain of software engineering (SWE) tasks, where LM agents interact with a development environment to fix bugs over multiple turns. Experiments compare OEC data with other on-policy and imitation learning approaches using a unified training method that combines rejection sampling based on environment rewards and supervised fine-tuning. Results show that models trained with OEC trajectories achieve a significant relative improvement of 14% and 13% over traditional imitation learning benchmarks in 7 billion and 32 billion parameter models respectively on the SWE-bench verified dataset. The findings emphasize the importance of integrating expert demonstrations with on-policy data to effectively train multi-turn LM agents in complex interactive environments. <div>
arXiv:2512.14895v1 Announce Type: new 
Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2512.14908</link>
<guid>https://arxiv.org/abs/2512.14908</guid>
<content:encoded><![CDATA[
<div> Keywords: ATLAS, graph neural networks, heterophilic graphs, community detection, scalability<br /><br />Summary:  
1. The paper introduces ATLAS (Adaptive Topology-based Learning at Scale), a novel graph learning algorithm designed to overcome key challenges in graph neural networks (GNNs), particularly in heterophilic graph scenarios and scalability issues.  
2. Traditional GNNs experience reduced accuracy on heterophilic graphs, where node neighbors tend to have different labels, and their iterative feature aggregation mechanisms often struggle to scale to large graphs.  
3. ATLAS addresses these challenges by extracting topological information through multi-level community detection, concatenating these community assignments to node feature vectors, and then processing them using multilayer perceptrons (MLPs).  
4. This approach provides rich topological context without relying on iterative neighborhood aggregation, enabling scalability to large graphs without the need for sampling, as MLPs are typically more scalable than GNNs.  
5. Experimentally, ATLAS matches or exceeds baseline methods, achieving improvements up to 20 percentage points over Graph Convolutional Networks (GCN) for heterophilic graphs and 11 percentage points over MLP for homophilic graphs.  
6. By leveraging multi-resolution community features, ATLAS not only improves accuracy but also offers a pathway to more explainable graph learning models that perform effectively across different graph structures. <div>
arXiv:2512.14908v1 Announce Type: new 
Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective</title>
<link>https://arxiv.org/abs/2512.14932</link>
<guid>https://arxiv.org/abs/2512.14932</guid>
<content:encoded><![CDATA[
<div> regularization parameter, low-rank MMSE filters, Kronecker-product representation, rank selection, simulations<br /><br />Summary:<br /><br />1. The article introduces a novel method to efficiently determine the regularization parameter in low-rank MMSE (Minimum Mean Square Error) filters by utilizing a Kronecker-product representation.<br /><br />2. The authors reveal a surprising connection between the regularization parameter and the problem of rank selection, emphasizing that correct choice of this parameter is vital for achieving optimal performance in low-rank filtering scenarios.<br /><br />3. The proposed approach is designed to enhance the effectiveness of low-rank MMSE filters by carefully tuning the regularization parameter according to the underlying rank structure.<br /><br />4. Extensive simulations are conducted to validate the method, demonstrating its advantages and significant performance improvements over traditional, commonly used techniques for regularization parameter selection.<br /><br />5. Overall, the study contributes a practical and theoretically insightful solution that addresses one of the key challenges in low-rank MMSE filtering, potentially impacting a range of signal processing applications. <div>
arXiv:2512.14932v1 Announce Type: new 
Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</title>
<link>https://arxiv.org/abs/2512.14967</link>
<guid>https://arxiv.org/abs/2512.14967</guid>
<content:encoded><![CDATA[
<div> Keywords: McKean-Vlasov equations, elicitability, deep learning, mean-field games, systemic risk<br /><br />Summary:<br /><br />1. The paper introduces a new numerical method to solve McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise by integrating Picard iterations, the concept of elicitability, and deep learning techniques. <br />2. A significant advancement is the use of elicitability to formulate a path-wise loss function that enables efficient neural network training for approximating the backward process and the conditional expectations associated with common noise, avoiding costly nested Monte Carlo simulations. <br />3. The mean-field interaction term is modeled with a recurrent neural network optimized by minimizing an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. <br />4. The methodology is validated on a systemic risk inter-bank borrowing and lending model where analytic solutions exist, showing high accuracy in solution recovery. The framework is further extended to handle quantile-mediated interactions, demonstrating flexibility beyond conditional means or moments. <br />5. Finally, the approach is applied to a non-stationary Aiyagari–Bewley–Huggett economic growth model featuring endogenous interest rates, highlighting the method’s applicability to complex mean-field games lacking closed-form solutions. <div>
arXiv:2512.14967v1 Announce Type: new 
Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softly Constrained Denoisers for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.14980</link>
<guid>https://arxiv.org/abs/2512.14980</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, constraints, denoiser adjustment, scientific data, inductive bias  

<br /><br />Summary:  
This paper addresses the challenge diffusion models face in generating samples that adhere to specific constraints, which is particularly important in scientific applications. 1) Existing methods incorporate constraints by adding regularization terms to the loss function or using guidance strategies during sampling; however, these approaches can lead to biased generative models that deviate from the true data distribution. 2) This bias is especially problematic when constraints are misspecified, a frequent issue in real-world scientific data scenarios. 3) Instead of modifying the loss or the sampling process, the authors propose embedding a guidance-inspired adjustment directly within the denoiser component of diffusion models. 4) This modification acts as a soft inductive bias that encourages the generation of constraint-compliant samples without rigidly enforcing the constraints. 5) Experiments demonstrate that the softly constrained denoisers better exploit prior constraint knowledge to improve adherence compared to standard denoisers while retaining sufficient flexibility to accommodate discrepancies caused by misspecified constraints in observed data. This approach balances constraint compliance with model adaptability, enhancing the practical applicability of diffusion models in scientific domains. <div>
arXiv:2512.14980v1 Announce Type: new 
Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Repetition Improves Non-Reasoning LLMs</title>
<link>https://arxiv.org/abs/2512.14982</link>
<guid>https://arxiv.org/abs/2512.14982</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt repetition, performance improvement, reasoning, language models, inference efficiency  

<br /><br />Summary:  
1. The article investigates the effect of repeating the input prompt on the performance of popular language models such as Gemini, GPT, Claude, and Deepseek.  
2. It demonstrates that repeating the prompt enhances these models’ performance even when reasoning processes are not employed.  
3. Importantly, this performance boost occurs without an increase in the number of generated tokens.  
4. Moreover, repeating the input does not lead to higher latency, meaning the inference speed remains unaffected.  
5. These findings suggest a simple and efficient method to improve model output quality during inference without additional computational cost or delay. <div>
arXiv:2512.14982v1 Announce Type: new 
Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes</title>
<link>https://arxiv.org/abs/2512.14991</link>
<guid>https://arxiv.org/abs/2512.14991</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, controlled diffusion processes, adaptive partitioning, regret bounds, high-dimensional finance<br /><br />Summary:<br /><br />1. The article addresses reinforcement learning in settings with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards, commonly encountered in finance, economics, and operations research.<br /><br />2. A novel model-based algorithm is proposed that adaptively partitions the joint state-action space to manage complexity in continuous and high-dimensional domains.<br /><br />3. This algorithm keeps estimators for drift, volatility, and rewards in each partition and refines partitions when the bias surpasses statistical confidence, effectively balancing exploration and approximation.<br /><br />4. Theoretical contributions include regret bounds dependent on the horizon, state dimension, reward growth, and a newly introduced zooming dimension concept adapted for unbounded diffusion processes.<br /><br />5. The results extend known bounds from bounded settings to a wider range of diffusion problems, and numerical experiments demonstrate the algorithm’s efficacy, including its application to challenging tasks like multi-asset mean-variance portfolio selection. <div>
arXiv:2512.14991v1 Announce Type: new 
Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</title>
<link>https://arxiv.org/abs/2512.15000</link>
<guid>https://arxiv.org/abs/2512.15000</guid>
<content:encoded><![CDATA[
<div> Process Reward Models, Large Language Models, Code Generation, Meta-learning, Chain-of-Function  

<br /><br />Summary:  
This paper addresses the limitations of Process Reward Models (PRMs) in coding tasks, where step decompositions are less meaningful and labels generated via Monte-Carlo methods tend to be noisy. The authors introduce DreamPRM-Code, a specialized PRM for coding that conceptualizes functions as reasoning steps by employing a Chain-of-Function prompting strategy. This approach encourages modular code generation, enabling the PRM framework to be applied similarly to mathematical reasoning problems. To tackle noisy intermediate labels, DreamPRM-Code incorporates a meta-learning-based correction mechanism. This mechanism uses clean final unit-test results as reliable labels to perform bi-level optimization, which refines and improves the quality of noisy intermediate labels. The proposed model is applied in test-time scaling scenarios to effectively enhance Large Language Model coding performance. Experiments on LiveCodeBench demonstrate that DreamPRM-Code achieves a state-of-the-art pass@1 rate of 80.9%, outperforming the OpenAI o4-mini baseline. This shows the viability and effectiveness of treating coding functions as reasoning steps combined with a meta-learning strategy for label noise correction. The work provides a significant advancement in improving coding ability of LLMs via PRMs with modular, step-wise training and test-time scaling. <div>
arXiv:2512.15000v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets</title>
<link>https://arxiv.org/abs/2512.15008</link>
<guid>https://arxiv.org/abs/2512.15008</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock Pattern Assistant, price structure, deterministic framework, event alignment, explainability  

<br /><br />Summary:  
1. The article introduces the Stock Pattern Assistant (SPA), a deterministic framework designed to analyze price movements in financial markets by extracting monotonic price runs and linking them with relevant public events.  
2. SPA uses daily OHLCV (Open, High, Low, Close, Volume) data alongside a normalized event stream, ensuring the methodology is easy to audit, reproduce, and transparent.  
3. Unlike traditional technical indicators or predictive models, SPA focuses on interpretability and factual explanation rather than forecasting or generating trading signals, filling the gap in tools that lack transparency and auditability.  
4. The framework applies a symmetric correlation window to attach public events to price movements, providing clear, historically grounded narratives that help explain price structure over time.  
5. The study evaluates SPA on four equities (AAPL, NVDA, SCHW, and PGR) representing diverse volatility profiles and sectors, demonstrating consistent structural decomposition and contextual storytelling.  
6. Ablation experiments highlight that deterministic segmentation, event alignment, and constrained explanation mechanisms are crucial for SPA’s interpretability.  
7. SPA is intended to complement analyst workflows, risk assessments, and explainable AI efforts rather than serve as a prediction or trading system, promoting transparency in understanding historical price behavior. <div>
arXiv:2512.15008v1 Announce Type: new 
Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic diversity across language models mitigates knowledge collapse</title>
<link>https://arxiv.org/abs/2512.15011</link>
<guid>https://arxiv.org/abs/2512.15011</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, knowledge collapse, epistemic diversity, AI ecosystem, model performance<br /><br />Summary: The article investigates the phenomenon of knowledge collapse in artificial intelligence (AI), where repeated training on AI-generated outputs results in a loss of information diversity, leading to performance degradation. Building on prior studies of single-model collapse, the authors explore how diversity across multiple AI models—referred to as an AI ecosystem—affects this decline. They segment training data among various language models and evaluate these ecosystems through ten iterations of self-training on their collective outputs. Their findings reveal that increasing epistemic diversity mitigates knowledge collapse but only up to an optimal point. Ecosystems with too few diverse models fail to capture the full complexity of the true data distribution, causing rapid performance decay over time. Conversely, distributing data across too many models diminishes each model's capacity to approximate the true distribution effectively, resulting in poor initial performance. The study highlights the importance of balancing diversity within AI ecosystems to sustain model performance and knowledge richness. Finally, the authors emphasize the risk of AI monoculture and advocate for monitoring diversity among AI systems, recommending policies that encourage the development of more domain- and community-specific AI models to maintain a healthy and robust AI ecosystem. <div>
arXiv:2512.15011v1 Announce Type: new 
Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Representation-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15036</link>
<guid>https://arxiv.org/abs/2512.15036</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, spectral representations, transition operator, policy optimization, DeepMind Control Suite<br /><br />Summary:  
This paper addresses challenges in reinforcement learning (RL) encountered in environments with large state and action spaces by introducing spectral representations derived from the spectral decomposition of the transition operator. These representations offer a theoretically grounded abstraction of system dynamics, which can improve policy optimization. The authors develop methods to construct spectral representations tailored to transition operators exhibiting latent variable or energy-based structures, highlighting distinct learning techniques for extracting these representations from data. Each method corresponds to an effective RL algorithm within the proposed spectral framework. Importantly, the framework is extended with provable guarantees to handle partially observable Markov decision processes (POMDPs), broadening its applicability. The effectiveness of these spectral RL algorithms is empirically validated on over 20 challenging tasks from the DeepMind Control Suite, where they demonstrate performance that is comparable to or better than existing model-free and model-based state-of-the-art baselines. This work not only offers a novel theoretical perspective to better understand and address instability and exploration issues in RL but also presents practical algorithms that leverage spectral properties for improved learning and generalization. <div>
arXiv:2512.15036v1 Announce Type: new 
Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.15067</link>
<guid>https://arxiv.org/abs/2512.15067</guid>
<content:encoded><![CDATA[
<div> Keywords: EMFusion, electromagnetic field forecasting, multivariate diffusion, uncertainty quantification, residual U-Net<br /><br />Summary:  
1. The paper addresses the growing need for accurate estimation and forecasting of electromagnetic field (EMF) levels due to the expansion of wireless infrastructure.  
2. Current studies mainly employ univariate forecasting on aggregate EMF data, but capturing frequency-selective and inter-operator variations requires a multivariate approach.  
3. EMFusion, the proposed framework, utilizes a conditional multivariate diffusion-based probabilistic forecasting model that incorporates contextual factors like time of day, season, and holidays.  
4. The architecture is built on a residual U-Net enhanced with a cross-attention mechanism, allowing dynamic integration of external conditions to guide predictions.  
5. An imputation-based sampling strategy is used to treat the forecasting task as structural inpainting, maintaining temporal coherence even with irregular data.  
6. Unlike standard single-point forecasts, EMFusion produces calibrated probabilistic intervals from the learned conditional distribution, enabling explicit uncertainty quantification.  
7. Experiments on frequency-selective EMF datasets demonstrate that EMFusion, especially when incorporating working hours as contextual information, outperforms baseline models.  
8. The model improves continuous ranked probability score (CRPS) by 23.85%, normalized root mean square error by 13.93%, and reduces CRPS prediction error by 22.47%, showing superior accuracy and reliability for network planning and health assessment purposes. <div>
arXiv:2512.15067v1 Announce Type: new 
Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title>
<link>https://arxiv.org/abs/2512.15068</link>
<guid>https://arxiv.org/abs/2512.15068</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Hallucination Detection, Conformal Prediction, Embedding-based Methods, Large Language Models<br /><br />Summary: This paper investigates the challenge of detecting hallucinations in Retrieval-Augmented Generation (RAG) systems, which often produce factually incorrect outputs despite being grounded in retrieved evidence. Current detection techniques commonly rely on semantic similarity measures and natural language inference (NLI), but these methods have not been rigorously evaluated for their fundamental limitations. The authors introduce conformal prediction to hallucination detection, offering finite-sample coverage guarantees that enable precise assessment of detection performance. With calibration sets of around 600 examples, they demonstrate 94% coverage and 0% false positive rate (FPR) on synthetic hallucinations using the Natural Questions dataset. However, when tested on three real-world hallucination benchmarks involving multiple prominent large language models (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based detection methods, including top-tier models like OpenAI text-embedding-3-large and cross-encoder architectures, show very high false positive rates—100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. In contrast, GPT-4 used as a reasoning judge achieves a much lower FPR of 7% (95% CI: [3.4%, 13.7%]) on the same datasets. The authors identify a "semantic illusion," whereby hallucinated texts remain semantically close to source documents yet contain factual errors undetectable by embeddings. This phenomenon, consistent across embedding designs, LLM generators, and task types, suggests that embedding-based detection methods are insufficient for reliable deployment of RAG systems in production. <div>
arXiv:2512.15068v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</title>
<link>https://arxiv.org/abs/2512.15082</link>
<guid>https://arxiv.org/abs/2512.15082</guid>
<content:encoded><![CDATA[
<div> Feature Engineering, Multi-Label Learning, Large Language Models, Code Generation, Feedback Mechanism<br /><br />Summary:<br /><br />This paper addresses the limitations of existing feature engineering methods based on large language models (LLMs) in the context of multi-label learning, where complex label dependencies are common and task characteristics demand specialized approaches. The authors propose FEAML (Feature Engineering Automation for Multi-Label Learning), a novel automated feature engineering framework specifically designed for multi-label classification tasks. FEAML leverages the code generation capabilities of LLMs to produce high-quality features by incorporating metadata and label co-occurrence matrices, enabling the model to better understand relationships between data features and task objectives. The effectiveness of the generated features is evaluated using model accuracy, while Pearson correlation coefficients are employed to detect feature redundancy. A key innovation of FEAML is its feedback mechanism, which uses evaluation results to iteratively optimize the LLM’s code generation process, fostering continuous self-improvement. This integration of LLMs with a feedback-driven loop facilitates an efficient and interpretable feature engineering pipeline tailored for multi-label problems. Empirical evaluations on diverse multi-label datasets demonstrate that FEAML outperforms existing feature engineering methods, validating its effectiveness and efficiency for complex multi-label learning scenarios. <div>
arXiv:2512.15082v1 Announce Type: new 
Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Modular Physics for Elastic Simulation</title>
<link>https://arxiv.org/abs/2512.15083</link>
<guid>https://arxiv.org/abs/2512.15083</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Modular Physics, elastic simulation, physical interpretability, modular neural networks, generalization

<br /><br />Summary:  
This paper introduces Neural Modular Physics (NMP), a novel approach for elastic simulation that integrates the learning capability of neural networks with the reliability and interpretability of classical physics simulators. Unlike traditional end-to-end neural models that treat dynamics as a monolithic problem, NMP decomposes elastic dynamics into distinct neural modules corresponding to physically meaningful intermediate quantities. This modular design allows for direct supervision at intermediate stages and the enforcement of physical constraints, leading to improved physical consistency. The authors design a specialized architecture and training method that effectively transform the numerical computation process into a modular neural simulation framework. Experimentally, NMP shows superior generalization abilities when tested on unseen initial conditions and different spatial resolutions, addressing limitations seen in other neural simulators. It also achieves long-term stable simulations and better preservation of core physical properties. Moreover, NMP demonstrates greater flexibility and practicality in scenarios where the underlying physical dynamics are unknown, an area where conventional simulators often struggle. Overall, NMP represents a significant step forward in combining neural approximation with physical fidelity for improved elastic dynamics simulation. <div>
arXiv:2512.15083v1 Announce Type: new 
Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network</title>
<link>https://arxiv.org/abs/2512.15086</link>
<guid>https://arxiv.org/abs/2512.15086</guid>
<content:encoded><![CDATA[
<div> Keywords: Operator learning, DeepONet, Partition-of-unity, PDEs, Physics-informed regularization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of improving operator learning methods, which are used for efficiently predicting solutions of parameterized partial differential equations (PDEs) by learning mappings from inputs like initial conditions or forcing functions to full spatiotemporal fields. 2. Existing state-of-the-art architectures such as DeepONet and Fourier Neural Operator (FNO) demonstrate strong empirical results but often demand large training datasets, lack explicit incorporation of physical structure, and may experience instability issues in trunk-network features, notably mode imbalance or collapse, which degrade operator approximation accuracy. 3. Inspired by the stability and locality properties of classical partition-of-unity (PoU) methods, the authors propose PoU-based regularization techniques to enhance operator learning frameworks. 4. They develop an improved version of the PoU-PI-DeepONet framework, named Physics-informed Partition Penalty Deep Operator Network (PIP² Net), which introduces a simplified, more principled partition penalty. This penalty encourages better coordination among trunk outputs, increasing expressiveness without losing DeepONet’s flexibility. 5. The performance of PIP² Net is demonstrated on three nonlinear PDEs—the viscous Burgers equation, the Allen–Cahn equation, and a diffusion–reaction system—where it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in both prediction accuracy and robustness, highlighting its practical benefits for stable, efficient operator learning in complex PDE scenarios. <div>
arXiv:2512.15086v1 Announce Type: new 
Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</title>
<link>https://arxiv.org/abs/2512.15088</link>
<guid>https://arxiv.org/abs/2512.15088</guid>
<content:encoded><![CDATA[
<div> Keywords: Stochastic Differential Equations, fractional Brownian motion, Signature Multi-head Attention, parameter estimation, deep learning  

<br /><br />Summary:  
This paper addresses the challenge of parameter inference in stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm), which exhibit rough dynamics and long-range dependence. Classical estimation methods struggle due to the non-Markovian and non-semimartingale nature of these processes. The authors propose SigMA (Signature Multi-head Attention), a novel neural network architecture that integrates path signatures with multi-head self-attention. SigMA also uses convolutional preprocessing and a multilayer perceptron for effective feature extraction. The model is trained on synthetic data generated from fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, focusing on estimating the Hurst parameter and handling joint multi-parameter inference. SigMA shows strong generalization to unseen data. Extensive experiments on synthetic and two real-world datasets—equity-index realized volatility and Li-ion battery degradation—demonstrate SigMA's superiority over CNNs, LSTMs, vanilla Transformers, and Deep Signature models in terms of accuracy, robustness, and compactness. The results highlight that combining signature transforms with attention-based architectures forms an effective, scalable framework for parameter estimation in complex stochastic systems with persistent or rough temporal dependencies. <div>
arXiv:2512.15088v1 Announce Type: new 
Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</title>
<link>https://arxiv.org/abs/2512.15112</link>
<guid>https://arxiv.org/abs/2512.15112</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised node representation learning, graph convolution, homophily, adaptive convolution usage, clustering proxies  

<br /><br />Summary:  
Unsupervised node representation learning aims to generate meaningful node embeddings without the use of node labels, typically utilizing graph convolution to aggregate information from neighboring nodes and encode features and topology. However, relying heavily on graph convolution can be detrimental, particularly in non-homophilic graphs, as it may cause embeddings of nodes with different features or topology to become overly similar. While adaptive adjustment of graph convolution has been studied in supervised settings, it remains underexplored in unsupervised learning. The authors propose FUEL, a method that adaptively learns the optimal degree of graph convolution by enhancing intra-class similarity and inter-class separability in the embedding space. Since true class labels are unavailable, FUEL identifies node clusters based on features and uses these clusters as proxies for classes. Extensive experiments comparing FUEL against 15 baseline methods across 14 benchmark datasets show that FUEL consistently improves performance in downstream tasks. Furthermore, FUEL achieves state-of-the-art results on graphs exhibiting various levels of homophily, demonstrating its robustness and wide applicability in unsupervised node representation learning scenarios. <div>
arXiv:2512.15112v1 Announce Type: new 
Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</title>
<link>https://arxiv.org/abs/2512.15115</link>
<guid>https://arxiv.org/abs/2512.15115</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence modeling, interaction operator, factorized framework, state space models, gradient propagation  

<br /><br />Summary:  
This paper introduces a unified theoretical framework for understanding sequence modeling architectures, including recurrent neural networks, Transformers, and state space models (SSMs). The framework represents sequence maps through an input-dependent interaction operator \( W_{ij}(X) \), highlighting two key construction patterns: (i) the Unified Factorized Framework (explicit attention-style mixing) where \( W_{ij}(X) \) changes via scalar coefficients on shared value maps, and (ii) Structured Dynamics (implicit state-space recurrences) where \( W_{ij} \) is generated by latent dynamical systems. Three main theoretical contributions are derived: first, the Interaction Rank Gap theorem which shows that models in the factorized class (like single-head attention) have a limited low-dimensional operator span and thus cannot express certain structured dynamical maps; second, the Equivalence (Head-Count) Theorem demonstrating that representing a linear SSM with lag operators spanning a \( k \)-dimensional subspace over sequences of length \( n \) requires exactly \( H = k \) attention heads; third, the Gradient Highway Result proving that attention mechanisms support gradient pathways that do not diminish with distance, unlike stable linear dynamics where gradient signals attenuate over time. Collectively, these results illuminate a fundamental trade-off between algebraic expressivity and trainability (gradient propagation), providing a rigorous foundation for the design and understanding of modern sequence models. <div>
arXiv:2512.15115v1 Announce Type: new 
Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</title>
<link>https://arxiv.org/abs/2512.15116</link>
<guid>https://arxiv.org/abs/2512.15116</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series imputation, diffusion model, frequency bias, Fourier Bias Projection, temporal modeling  

<br /><br />Summary:  
Multivariate time series imputation is crucial in fields like healthcare, traffic forecasting, and biological modeling, where data often suffers from missing values due to sensor failures and irregular sampling. Existing methods, particularly Transformer- and diffusion-based models, struggle with explicit inductive biases and frequency awareness, which limits their ability to generalize under structured missing data and distribution shifts. To address this, the authors propose FADTI, a novel diffusion-based imputation framework that incorporates frequency-informed feature modulation using a learnable Fourier Bias Projection (FBP) module. The FBP module supports multiple spectral bases, allowing adaptive encoding of both stationary and non-stationary time series patterns, thus introducing frequency-domain inductive bias into the imputation process. FADTI also integrates temporal modeling mechanisms, leveraging self-attention and gated convolution for enhanced performance. Experiments are conducted across multiple benchmark datasets, including a newly introduced biological time series dataset, demonstrating that FADTI consistently outperforms current state-of-the-art approaches. Its advantage is especially notable under high missing data rates, showcasing robustness in challenging scenarios. The authors have made their code publicly available, enabling reproducibility and further research on frequency-aware diffusion models for time series imputation. <div>
arXiv:2512.15116v1 Announce Type: new 
Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Multi-Objective Human Heuristics</title>
<link>https://arxiv.org/abs/2512.15120</link>
<guid>https://arxiv.org/abs/2512.15120</guid>
<content:encoded><![CDATA[
<div> Multi-Objective, Reward Shaping, Exploration, Bi-level Optimization, Reinforcement Learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of designing effective reward functions in reinforcement learning, particularly in environments with multiple objectives. 2. It introduces MORSE (Multi-Objective Reward Shaping with Exploration), a novel framework that automatically integrates several human-designed heuristic rewards into a single unified reward function. 3. MORSE approaches reward shaping as a bi-level optimization problem where the inner loop focuses on training a policy to maximize the current shaped reward, and the outer loop updates the reward function to enhance overall task performance. 4. To promote exploration and avoid getting stuck in suboptimal local minima, MORSE incorporates stochasticity by injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. 5. Experimental evaluations conducted in MuJoCo and Isaac Sim environments demonstrate that MORSE successfully balances competing objectives across diverse robotic tasks, delivering task performance comparable to that achieved by manually tuned reward functions. <div>
arXiv:2512.15120v1 Announce Type: new 
Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</title>
<link>https://arxiv.org/abs/2512.15123</link>
<guid>https://arxiv.org/abs/2512.15123</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, adversarial training, privacy-preserving, proxy dataset, edge devices  

<br /><br />Summary:  
1. The paper addresses the challenge of enhancing adversarial robustness for deep learning models deployed on edge devices in Federated Learning (FL) environments.  
2. Traditional adversarial training methods are difficult to implement in FL due to strict client data privacy and limited computational resources on edge devices.  
3. The authors propose TrajSyn, a novel framework that synthesizes a proxy dataset on the server side using trajectories of client model updates, enabling adversarial training without direct access to raw client data.  
4. TrajSyn is privacy-preserving, as it does not expose any sensitive client data and requires no additional computational burden on the clients.  
5. Experimental results demonstrate that TrajSyn consistently improves adversarial robustness on standard image classification benchmarks while maintaining the privacy and efficiency constraints critical in FL settings. <div>
arXiv:2512.15123v1 Announce Type: new 
Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</title>
<link>https://arxiv.org/abs/2512.15134</link>
<guid>https://arxiv.org/abs/2512.15134</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretability, disentanglement, sparse autoencoders, concept representations, compositional evaluation

<br /><br />Summary:  
This study addresses the challenge of recovering representations of causally relevant concepts from neural network activations, with a focus on interpretability. It critiques common concept representation evaluations as relying on isolation and independence assumptions that often do not hold in practical settings involving correlated concepts. To address this, the authors propose a multi-concept evaluation framework where correlations between textual concepts like sentiment, domain, and tense are controlled and progressively increased. They evaluate how different featurization methods, including sparse autoencoders (SAEs) and sparse probes, perform under varying correlation strengths with regard to learning disentangled concept representations. Results reveal a one-to-many relationship: each feature tends to correspond to a single concept, but individual concepts are distributed across multiple features. Steering experiments show that SAE features, even when trained on uniformly distributed concepts, generally influence multiple concepts upon manipulation, indicating limited selectivity and independence, though the affected features tend to occupy disjoint subspaces. The findings highlight that commonly used correlational metrics do not reliably indicate true independence or concept selectivity when features are steered. Overall, the work stresses the necessity of compositional and multi-concept evaluations to better understand and improve interpretability methods for neural network concept representations. <div>
arXiv:2512.15134v1 Announce Type: new 
Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</title>
<link>https://arxiv.org/abs/2512.15140</link>
<guid>https://arxiv.org/abs/2512.15140</guid>
<content:encoded><![CDATA[
<div> Keywords: generalization, crop yield prediction, machine learning, interpretability, temporal validation  

<br /><br />Summary:  
This study investigates the generalization ability and interpretability of various machine learning models for predicting crop yield and anomalies across Germany's NUTS-3 regions. Leveraging a robust, long-term dataset, the research compares ensemble tree-based models such as XGBoost and Random Forest with deep learning techniques including LSTM and TCN. While all models demonstrate strong performance on spatially split, conventional test sets, their accuracy significantly declines when validated on temporally independent years, highlighting persistent challenges in temporal generalization. Interestingly, models showing high accuracy on conventional tests but poor temporal validation still generate plausible SHAP feature importance explanations, revealing a critical flaw in post hoc interpretability methods—explanations can seem reliable even when model predictions fail to generalize. This emphasizes the importance of validation-aware interpretation in agricultural and environmental machine learning applications. The study argues against accepting feature importance at face value unless the underlying models prove capable of generalizing across both unseen temporal and spatial conditions. Ultimately, it calls for incorporating domain knowledge into validation protocols, exploring hybrid modeling approaches, and applying stricter evaluation of explainability methods to improve trustworthiness in data-driven agricultural decision-making. The findings confront a broader challenge in environmental data science concerning the robustness of generalization assessment needed to confidently rely on model explanations. <div>
arXiv:2512.15140v1 Announce Type: new 
Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Gradient-Based Inference Attack for Federated Learning</title>
<link>https://arxiv.org/abs/2512.15143</link>
<guid>https://arxiv.org/abs/2512.15143</guid>
<content:encoded><![CDATA[
arXiv:2512.15143v1 Announce Type: new 
Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding NTK Variance in Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2512.15169</link>
<guid>https://arxiv.org/abs/2512.15169</guid>
<content:encoded><![CDATA[
arXiv:2512.15169v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEER: Draft with Diffusion, Verify with Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.15176</link>
<guid>https://arxiv.org/abs/2512.15176</guid>
<content:encoded><![CDATA[
arXiv:2512.15176v1 Announce Type: new 
Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT</title>
<link>https://arxiv.org/abs/2512.15206</link>
<guid>https://arxiv.org/abs/2512.15206</guid>
<content:encoded><![CDATA[
arXiv:2512.15206v1 Announce Type: new 
Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</title>
<link>https://arxiv.org/abs/2512.15228</link>
<guid>https://arxiv.org/abs/2512.15228</guid>
<content:encoded><![CDATA[
arXiv:2512.15228v1 Announce Type: new 
Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \text{\AA} on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization</title>
<link>https://arxiv.org/abs/2512.15229</link>
<guid>https://arxiv.org/abs/2512.15229</guid>
<content:encoded><![CDATA[
arXiv:2512.15229v1 Announce Type: new 
Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</title>
<link>https://arxiv.org/abs/2512.15250</link>
<guid>https://arxiv.org/abs/2512.15250</guid>
<content:encoded><![CDATA[
arXiv:2512.15250v1 Announce Type: new 
Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</title>
<link>https://arxiv.org/abs/2512.15267</link>
<guid>https://arxiv.org/abs/2512.15267</guid>
<content:encoded><![CDATA[
arXiv:2512.15267v1 Announce Type: new 
Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topological Metric for Unsupervised Embedding Quality Evaluation</title>
<link>https://arxiv.org/abs/2512.15285</link>
<guid>https://arxiv.org/abs/2512.15285</guid>
<content:encoded><![CDATA[
arXiv:2512.15285v1 Announce Type: new 
Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</title>
<link>https://arxiv.org/abs/2512.15286</link>
<guid>https://arxiv.org/abs/2512.15286</guid>
<content:encoded><![CDATA[
arXiv:2512.15286v1 Announce Type: new 
Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference</title>
<link>https://arxiv.org/abs/2512.15335</link>
<guid>https://arxiv.org/abs/2512.15335</guid>
<content:encoded><![CDATA[
arXiv:2512.15335v1 Announce Type: new 
Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</title>
<link>https://arxiv.org/abs/2512.15344</link>
<guid>https://arxiv.org/abs/2512.15344</guid>
<content:encoded><![CDATA[
arXiv:2512.15344v1 Announce Type: new 
Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Regime-Aware Fusion Framework for Time Series Classification</title>
<link>https://arxiv.org/abs/2512.15378</link>
<guid>https://arxiv.org/abs/2512.15378</guid>
<content:encoded><![CDATA[
arXiv:2512.15378v1 Announce Type: new 
Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</title>
<link>https://arxiv.org/abs/2512.15385</link>
<guid>https://arxiv.org/abs/2512.15385</guid>
<content:encoded><![CDATA[
arXiv:2512.15385v1 Announce Type: new 
Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15405</link>
<guid>https://arxiv.org/abs/2512.15405</guid>
<content:encoded><![CDATA[
arXiv:2512.15405v1 Announce Type: new 
Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows</title>
<link>https://arxiv.org/abs/2512.15420</link>
<guid>https://arxiv.org/abs/2512.15420</guid>
<content:encoded><![CDATA[
arXiv:2512.15420v1 Announce Type: new 
Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistics of Min-max Normalized Eigenvalues in Random Matrices</title>
<link>https://arxiv.org/abs/2512.15427</link>
<guid>https://arxiv.org/abs/2512.15427</guid>
<content:encoded><![CDATA[
arXiv:2512.15427v1 Announce Type: new 
Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.15430</link>
<guid>https://arxiv.org/abs/2512.15430</guid>
<content:encoded><![CDATA[
arXiv:2512.15430v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Double Horizon Model-Based Policy Optimization</title>
<link>https://arxiv.org/abs/2512.15439</link>
<guid>https://arxiv.org/abs/2512.15439</guid>
<content:encoded><![CDATA[
arXiv:2512.15439v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</title>
<link>https://arxiv.org/abs/2512.15442</link>
<guid>https://arxiv.org/abs/2512.15442</guid>
<content:encoded><![CDATA[
arXiv:2512.15442v1 Announce Type: new 
Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2512.15460</link>
<guid>https://arxiv.org/abs/2512.15460</guid>
<content:encoded><![CDATA[
arXiv:2512.15460v1 Announce Type: new 
Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance</title>
<link>https://arxiv.org/abs/2512.15469</link>
<guid>https://arxiv.org/abs/2512.15469</guid>
<content:encoded><![CDATA[
arXiv:2512.15469v1 Announce Type: new 
Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs</title>
<link>https://arxiv.org/abs/2512.15483</link>
<guid>https://arxiv.org/abs/2512.15483</guid>
<content:encoded><![CDATA[
arXiv:2512.15483v1 Announce Type: new 
Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier</title>
<link>https://arxiv.org/abs/2512.15492</link>
<guid>https://arxiv.org/abs/2512.15492</guid>
<content:encoded><![CDATA[
arXiv:2512.15492v1 Announce Type: new 
Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Geometric Inductive Bias for Object Centric Dynamics</title>
<link>https://arxiv.org/abs/2512.15493</link>
<guid>https://arxiv.org/abs/2512.15493</guid>
<content:encoded><![CDATA[
arXiv:2512.15493v1 Announce Type: new 
Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking Temporal Dynamics of Vector Sets with Gaussian Process</title>
<link>https://arxiv.org/abs/2512.15538</link>
<guid>https://arxiv.org/abs/2512.15538</guid>
<content:encoded><![CDATA[
arXiv:2512.15538v1 Announce Type: new 
Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation</title>
<link>https://arxiv.org/abs/2512.15574</link>
<guid>https://arxiv.org/abs/2512.15574</guid>
<content:encoded><![CDATA[
arXiv:2512.15574v1 Announce Type: new 
Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Corrective Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.15596</link>
<guid>https://arxiv.org/abs/2512.15596</guid>
<content:encoded><![CDATA[
arXiv:2512.15596v1 Announce Type: new 
Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Smoothing is N-simplicial Attention?</title>
<link>https://arxiv.org/abs/2512.15600</link>
<guid>https://arxiv.org/abs/2512.15600</guid>
<content:encoded><![CDATA[
arXiv:2512.15600v1 Announce Type: new 
Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</title>
<link>https://arxiv.org/abs/2512.15605</link>
<guid>https://arxiv.org/abs/2512.15605</guid>
<content:encoded><![CDATA[
arXiv:2512.15605v1 Announce Type: new 
Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary</title>
<link>https://arxiv.org/abs/2512.15614</link>
<guid>https://arxiv.org/abs/2512.15614</guid>
<content:encoded><![CDATA[
arXiv:2512.15614v1 Announce Type: new 
Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoFlow: Solution Flow Models for One-Step Generative Modeling</title>
<link>https://arxiv.org/abs/2512.15657</link>
<guid>https://arxiv.org/abs/2512.15657</guid>
<content:encoded><![CDATA[
arXiv:2512.15657v1 Announce Type: new 
Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks</title>
<link>https://arxiv.org/abs/2512.15685</link>
<guid>https://arxiv.org/abs/2512.15685</guid>
<content:encoded><![CDATA[
arXiv:2512.15685v1 Announce Type: new 
Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15687</link>
<guid>https://arxiv.org/abs/2512.15687</guid>
<content:encoded><![CDATA[
arXiv:2512.15687v1 Announce Type: new 
Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Semantic Communication</title>
<link>https://arxiv.org/abs/2512.15691</link>
<guid>https://arxiv.org/abs/2512.15691</guid>
<content:encoded><![CDATA[
arXiv:2512.15691v1 Announce Type: new 
Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FrontierCS: Evolving Challenges for Evolving Intelligence</title>
<link>https://arxiv.org/abs/2512.15699</link>
<guid>https://arxiv.org/abs/2512.15699</guid>
<content:encoded><![CDATA[
arXiv:2512.15699v1 Announce Type: new 
Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data</title>
<link>https://arxiv.org/abs/2512.15706</link>
<guid>https://arxiv.org/abs/2512.15706</guid>
<content:encoded><![CDATA[
arXiv:2512.15706v1 Announce Type: new 
Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Graph-Embedded Hazard Model (GEHM): Stochastic Network Survival Dynamics on Economic Graphs</title>
<link>https://arxiv.org/abs/2512.14705</link>
<guid>https://arxiv.org/abs/2512.14705</guid>
<content:encoded><![CDATA[
arXiv:2512.14705v1 Announce Type: cross 
Abstract: This paper develops a nonlinear evolution framework for modelling survival dynamics on weighted economic networks by coupling a graph-based $p$-Laplacian diffusion operator with a stochastic structural drift. The resulting finite-dimensional PDE--SDE system captures how node-level survival reacts to nonlinear diffusion pressures while an aggregate complexity factor evolves according to an It\^o{} process. Using accretive operator theory, nonlinear semigroup methods, and stochastic analysis, we establish existence and uniqueness of mild solutions, derive topology-dependent energy dissipation inequalities, and characterise the stability threshold separating dissipative, critical, amplifying, and explosive regimes. Numerical experiments on Barab\'asi--Albert networks confirm that hub dominance magnifies nonlinear gradients and compresses stability margins, producing heavy-tailed survival distributions and occasional explosive behaviour.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis</title>
<link>https://arxiv.org/abs/2512.14708</link>
<guid>https://arxiv.org/abs/2512.14708</guid>
<content:encoded><![CDATA[
arXiv:2512.14708v1 Announce Type: cross 
Abstract: Current deep learning approaches for physiological signal monitoring suffer from static topologies and constant energy consumption. We introduce SGEMAS (Self-Growing Ephemeral Multi-Agent System), a bio-inspired architecture that treats intelligence as a dynamic thermodynamic process. By coupling a structural plasticity mechanism (agent birth death) to a variational free energy objective, the system naturally evolves to minimize prediction error with extreme sparsity. An ablation study on the MIT-BIH Arrhythmia Database reveals that adding a multi-scale instability index to the agent dynamics significantly improves performance. In a challenging inter-patient, zero-shot setting, the final SGEMAS v3.3 model achieves a mean AUC of 0.570 +- 0.070, outperforming both its simpler variants and a standard autoencoder baseline. This result validates that a physics-based, energy-constrained model can achieve robust unsupervised anomaly detection, offering a promising direction for efficient biomedical AI.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</title>
<link>https://arxiv.org/abs/2512.14709</link>
<guid>https://arxiv.org/abs/2512.14709</guid>
<content:encoded><![CDATA[
arXiv:2512.14709v1 Announce Type: cross 
Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems</title>
<link>https://arxiv.org/abs/2512.14733</link>
<guid>https://arxiv.org/abs/2512.14733</guid>
<content:encoded><![CDATA[
arXiv:2512.14733v1 Announce Type: cross 
Abstract: Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the "Something Completely Different" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title>
<link>https://arxiv.org/abs/2512.14742</link>
<guid>https://arxiv.org/abs/2512.14742</guid>
<content:encoded><![CDATA[
arXiv:2512.14742v1 Announce Type: cross 
Abstract: Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compute the edge p-Laplacian centrality for air traffic network</title>
<link>https://arxiv.org/abs/2512.14749</link>
<guid>https://arxiv.org/abs/2512.14749</guid>
<content:encoded><![CDATA[
arXiv:2512.14749v1 Announce Type: cross 
Abstract: The problem that we would like to solve in this paper is to compute the edge p-Laplacian centrality for the air traffic network. In this problem, instead of computing the edge p-Laplacian centrality directly which is the very hard problem, we convert the air traffic network to the line graph. Finally, we will compute the node p-Laplacian centrality of the line graph which is equivalent to the edge p-Laplacian of the air traffic network. In this paper, the novel un-normalized graph (p-) Laplacian based ranking method will be developed based on the un-normalized graph p-Laplacian operator definitions such as the curvature operator of graph (i.e. the un-normalized graph 1-Laplacian operator) and will be used to compute the node p-Laplacian centrality of the line graph. The results from the experiments show that the un-normalized graph p-Laplacian ranking methods can be implemented successfully.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: Capability Achievement via Policy Execution</title>
<link>https://arxiv.org/abs/2512.14761</link>
<guid>https://arxiv.org/abs/2512.14761</guid>
<content:encoded><![CDATA[
arXiv:2512.14761v1 Announce Type: cross 
Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</title>
<link>https://arxiv.org/abs/2512.14766</link>
<guid>https://arxiv.org/abs/2512.14766</guid>
<content:encoded><![CDATA[
arXiv:2512.14766v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[
arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title>
<link>https://arxiv.org/abs/2512.14801</link>
<guid>https://arxiv.org/abs/2512.14801</guid>
<content:encoded><![CDATA[
arXiv:2512.14801v1 Announce Type: cross 
Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction</title>
<link>https://arxiv.org/abs/2512.14865</link>
<guid>https://arxiv.org/abs/2512.14865</guid>
<content:encoded><![CDATA[
arXiv:2512.14865v1 Announce Type: cross 
Abstract: End-to-end (E2E) spoken dialogue systems are increasingly replacing cascaded pipelines for voice-based human-AI interaction, processing raw audio directly without intermediate transcription. Existing benchmarks primarily evaluate these models on synthetic speech and single-turn tasks, leaving realistic multi-turn conversational ability underexplored. We introduce Audio MultiChallenge, an open-source benchmark to evaluate E2E spoken dialogue systems under natural multi-turn interaction patterns. Building on the text-based MultiChallenge framework, which evaluates Inference Memory, Instruction Retention, and Self Coherence, we introduce a new axis Voice Editing that tests robustness to mid-utterance speech repairs and backtracking. We further augment each axis to the audio modality, such as introducing Audio-Cue challenges for Inference Memory that require recalling ambient sounds and paralinguistic signals beyond semantic content. We curate 452 conversations from 47 speakers with 1,712 instance-specific rubrics through a hybrid audio-native agentic and human-in-the-loop pipeline that exposes model failures at scale while preserving natural disfluencies found in unscripted human speech. Our evaluation of proprietary and open-source models reveals that even frontier models struggle on our benchmark, with Gemini 3 Pro Preview (Thinking), our highest-performing model achieving a 54.65% pass rate. Error analysis shows that models fail most often on our new axes and that Self Coherence degrades with longer audio context. These failures reflect difficulty of tracking edits, audio cues, and long-range context in natural spoken dialogue. Audio MultiChallenge provides a reproducible testbed to quantify them and drive improvements in audio-native multi-turn interaction capability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</title>
<link>https://arxiv.org/abs/2512.14926</link>
<guid>https://arxiv.org/abs/2512.14926</guid>
<content:encoded><![CDATA[
arXiv:2512.14926v1 Announce Type: cross 
Abstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</title>
<link>https://arxiv.org/abs/2512.14929</link>
<guid>https://arxiv.org/abs/2512.14929</guid>
<content:encoded><![CDATA[
arXiv:2512.14929v1 Announce Type: cross 
Abstract: Purpose: Magnetic Resonance Spectroscopic Imaging (MRSI) maps endogenous brain metabolism while suppressing the overwhelming water signal. Water-unsuppressed MRSI (wu-MRSI) allows simultaneous imaging of water and metabolites, but large water sidebands cause challenges for metabolic fitting. We developed an end-to-end deep-learning pipeline to overcome these challenges at ultra-high field. Methods:Fast high-resolution wu-MRSI was acquired at 7T with non-cartesian ECCENTRIC sampling and ultra-short echo time. A water and lipid removal network (WALINET+) was developed to remove lipids, water signal, and sidebands. MRSI reconstruction was performed by DeepER and a physics-informed network for metabolite fitting. Water signal was used for absolute metabolite quantification, quantitative susceptibility mapping (QSM), and myelin water fraction imaging (MWF). Results: WALINET+ provided the lowest NRMSE (< 2%) in simulations and in vivo the smallest bias (< 20%) and limits-of-agreement (+-63%) between wu-MRSI and ws-MRSI scans. Several metabolites such as creatine and glutamate showed higher SNR in wu-MRSI. QSM and MWF obtained from wu-MRSI and GRE showed good agreement with 0 ppm/5.5% bias and +-0.05 ppm/ +- 12.75% limits-of-agreement. Conclusion: High-quality metabolic, QSM, and MWF mapping of the human brain can be obtained simultaneously by ECCENTRIC wu-MRSI at 7T with 2 mm isotropic resolution in 12 min. WALINET+ robustly removes water sidebands while preserving metabolite signal, eliminating the need for water suppression and separate water acquisitions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection</title>
<link>https://arxiv.org/abs/2512.14935</link>
<guid>https://arxiv.org/abs/2512.14935</guid>
<content:encoded><![CDATA[
arXiv:2512.14935v1 Announce Type: cross 
Abstract: Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\_CONFIDENCE\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary condition enforcement with PINNs: a comparative study and verification on 3D geometries</title>
<link>https://arxiv.org/abs/2512.14941</link>
<guid>https://arxiv.org/abs/2512.14941</guid>
<content:encoded><![CDATA[
arXiv:2512.14941v1 Announce Type: cross 
Abstract: Since their advent nearly a decade ago, physics-informed neural networks (PINNs) have been studied extensively as a novel technique for solving forward and inverse problems in physics and engineering. The neural network discretization of the solution field is naturally adaptive and avoids meshing the computational domain, which can both improve the accuracy of the numerical solution and streamline implementation. However, there have been limited studies of PINNs on complex three-dimensional geometries, as the lack of mesh and the reliance on the strong form of the partial differential equation (PDE) make boundary condition (BC) enforcement challenging. Techniques to enforce BCs with PINNs have proliferated in the literature, but a comprehensive side-by-side comparison of these techniques and a study of their efficacy on geometrically complex three-dimensional test problems are lacking. In this work, we i) systematically compare BC enforcement techniques for PINNs, ii) propose a general solution framework for arbitrary three-dimensional geometries, and iii) verify the methodology on three-dimensional, linear and nonlinear test problems with combinations of Dirichlet, Neumann, and Robin boundaries. Our approach is agnostic to the underlying PDE, the geometry of the computational domain, and the nature of the BCs, while requiring minimal hyperparameter tuning. This work represents a step in the direction of establishing PINNs as a mature numerical method, capable of competing head-to-head with incumbents such as the finite element method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2512.14946</link>
<guid>https://arxiv.org/abs/2512.14946</guid>
<content:encoded><![CDATA[
arXiv:2512.14946v1 Announce Type: cross 
Abstract: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.
  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation</title>
<link>https://arxiv.org/abs/2512.14954</link>
<guid>https://arxiv.org/abs/2512.14954</guid>
<content:encoded><![CDATA[
arXiv:2512.14954v1 Announce Type: cross 
Abstract: Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only O(1) model evaluations per token. When used for distillation, this yields up to a 12% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrusion Detection in Internet of Vehicles Using Machine Learning</title>
<link>https://arxiv.org/abs/2512.14958</link>
<guid>https://arxiv.org/abs/2512.14958</guid>
<content:encoded><![CDATA[
arXiv:2512.14958v1 Announce Type: cross 
Abstract: The Internet of Vehicles (IoV) has evolved modern transportation through enhanced connectivity and intelligent systems. However, this increased connectivity introduces critical vulnerabilities, making vehicles susceptible to cyber-attacks such Denial-ofService (DoS) and message spoofing. This project aims to develop a machine learning-based intrusion detection system to classify malicious Controller Area network (CAN) bus traffic using the CiCIoV2024 benchmark dataset. We analyzed various attack patterns including DoS and spoofing attacks targeting critical vehicle parameters such as Spoofing-GAS - gas pedal position, Spoofing-RPM, Spoofing-Speed, and Spoofing-Steering\_Wheel. Our initial findings confirm a multi-class classification problem with a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</title>
<link>https://arxiv.org/abs/2512.14990</link>
<guid>https://arxiv.org/abs/2512.14990</guid>
<content:encoded><![CDATA[
arXiv:2512.14990v1 Announce Type: cross 
Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Nudged Elastic Band Method using Neural Network Bayesian Algorithm Execution</title>
<link>https://arxiv.org/abs/2512.14993</link>
<guid>https://arxiv.org/abs/2512.14993</guid>
<content:encoded><![CDATA[
arXiv:2512.14993v1 Announce Type: cross 
Abstract: The discovery of a minimum energy pathway (MEP) between metastable states is crucial for scientific tasks including catalyst and biomolecular design. However, the standard nudged elastic band (NEB) algorithm requires hundreds to tens of thousands of compute-intensive simulations, making applications to complex systems prohibitively expensive. We introduce Neural Network Bayesian Algorithm Execution (NN-BAX), a framework that jointly learns the energy landscape and the MEP. NN-BAX sequentially fine-tunes a foundation model by actively selecting samples targeted at improving the MEP. Tested on Lennard-Jones and Embedded Atom Method systems, our approach achieves a one to two order of magnitude reduction in energy and force evaluations with negligible loss in MEP accuracy and demonstrates scalability to >100-dimensional systems. This work is therefore a promising step towards removing the computational barrier for MEP discovery in scientifically relevant systems, suggesting that weeks-long calculations may be achieved in hours or days with minimal loss in accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports</title>
<link>https://arxiv.org/abs/2512.15003</link>
<guid>https://arxiv.org/abs/2512.15003</guid>
<content:encoded><![CDATA[
arXiv:2512.15003v1 Announce Type: cross 
Abstract: Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title>
<link>https://arxiv.org/abs/2512.15053</link>
<guid>https://arxiv.org/abs/2512.15053</guid>
<content:encoded><![CDATA[
arXiv:2512.15053v1 Announce Type: cross 
Abstract: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based "prompt engineering," fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for "Observable Software Engineering" in the era of probabilistic computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions</title>
<link>https://arxiv.org/abs/2512.15113</link>
<guid>https://arxiv.org/abs/2512.15113</guid>
<content:encoded><![CDATA[
arXiv:2512.15113v1 Announce Type: cross 
Abstract: Long-term price forecasting remains a formidable challenge due to the inherent uncertainty over the long term, despite some success in short-term predictions. Nonetheless, accurate long-term forecasts are essential for high-net-worth individuals, institutional investors, and traders. The proposed improved genetic algorithm-optimized support vector regression (IGA-SVR) model is specifically designed for long-term price prediction of global indices. The performance of the IGA-SVR model is rigorously evaluated and compared against the state-of-the-art baseline models, the Long Short-Term Memory (LSTM), and the forward-validating genetic algorithm optimized support vector regression (OGA-SVR). Extensive testing was conducted on the five global indices, namely Nifty, Dow Jones Industrial Average (DJI), DAX Performance Index (DAX), Nikkei 225 (N225), and Shanghai Stock Exchange Composite Index (SSE) from 2021 to 2024 of daily price prediction up to a year. Overall, the proposed IGA-SVR model achieved a reduction in MAPE by 19.87% compared to LSTM and 50.03% compared to OGA-SVR, demonstrating its superior performance in long-term daily price forecasting of global indices. Further, the execution time for LSTM was approximately 20 times higher than that of IGA-SVR, highlighting the high accuracy and computational efficiency of the proposed model. The genetic algorithm selects the optimal hyperparameters of SVR by minimizing the arithmetic mean of the Mean Absolute Percentage Error (MAPE) calculated over the full training dataset and the most recent five years of training data. This purposefully designed training methodology adjusts for recent trends while retaining long-term trend information, thereby offering enhanced generalization compared to the LSTM and rolling-forward validation approach employed by OGA-SVR, which forgets long-term trends and suffers from recency bias.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEAT2AASIST model with layer fusion for ESDD 2026 Challenge</title>
<link>https://arxiv.org/abs/2512.15180</link>
<guid>https://arxiv.org/abs/2512.15180</guid>
<content:encoded><![CDATA[
arXiv:2512.15180v1 Announce Type: cross 
Abstract: Recent advances in audio generation have increased the risk of realistic environmental sound manipulation, motivating the ESDD 2026 Challenge as the first large-scale benchmark for Environmental Sound Deepfake Detection (ESDD). We propose BEAT2AASIST which extends BEATs-AASIST by splitting BEATs-derived representations along frequency or channel dimension and processing them with dual AASIST branches. To enrich feature representations, we incorporate top-k transformer layer fusion using concatenation, CNN-gated, and SE-gated strategies. In addition, vocoder-based data augmentation is applied to improve robustness against unseen spoofing methods. Experimental results on the official test sets demonstrate that the proposed approach achieves competitive performance across the challenge tracks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-consistent clustering for evolving data</title>
<link>https://arxiv.org/abs/2512.15210</link>
<guid>https://arxiv.org/abs/2512.15210</guid>
<content:encoded><![CDATA[
arXiv:2512.15210v1 Announce Type: cross 
Abstract: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.
  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColliderML: The First Release of an OpenDataDetector High-Luminosity Physics Benchmark Dataset</title>
<link>https://arxiv.org/abs/2512.15230</link>
<guid>https://arxiv.org/abs/2512.15230</guid>
<content:encoded><![CDATA[
arXiv:2512.15230v1 Announce Type: cross 
Abstract: We introduce ColliderML - a large, open, experiment-agnostic dataset of fully simulated and digitised proton-proton collisions in High-Luminosity Large Hadron Collider conditions ($\sqrt{s}=14$ TeV, mean pile-up $\mu = 200$). ColliderML provides one million events across ten Standard Model and Beyond Standard Model processes, plus extensive single-particle samples, all produced with modern next-to-leading order matrix element calculation and showering, realistic per-event pile-up overlay, a validated OpenDataDetector geometry, and standard reconstructions. The release fills a major gap for machine learning (ML) research on detector-level data, provided on the ML-friendly Hugging Face platform. We present physics coverage and the generation, simulation, digitisation and reconstruction pipeline, describe format and access, and initial collider physics benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15254</link>
<guid>https://arxiv.org/abs/2512.15254</guid>
<content:encoded><![CDATA[
arXiv:2512.15254v1 Announce Type: cross 
Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model inference for ranking from pairwise comparisons</title>
<link>https://arxiv.org/abs/2512.15269</link>
<guid>https://arxiv.org/abs/2512.15269</guid>
<content:encoded><![CDATA[
arXiv:2512.15269v1 Announce Type: cross 
Abstract: We consider the problem of ranking objects from noisy pairwise comparisons, for example, ranking tennis players from the outcomes of matches. We follow a standard approach to this problem and assume that each object has an unobserved strength and that the outcome of each comparison depends probabilistically on the strengths of the comparands. However, we do not assume to know a priori how skills affect outcomes. Instead, we present an efficient algorithm for simultaneously inferring both the unobserved strengths and the function that maps strengths to probabilities. Despite this problem being under-constrained, we present experimental evidence that the conclusions of our Bayesian approach are robust to different model specifications. We include several case studies to exemplify the method on real-world data sets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</title>
<link>https://arxiv.org/abs/2512.15306</link>
<guid>https://arxiv.org/abs/2512.15306</guid>
<content:encoded><![CDATA[
arXiv:2512.15306v1 Announce Type: cross 
Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</title>
<link>https://arxiv.org/abs/2512.15313</link>
<guid>https://arxiv.org/abs/2512.15313</guid>
<content:encoded><![CDATA[
arXiv:2512.15313v1 Announce Type: cross 
Abstract: Deep learning has become a standard approach for the modeling of audio effects, yet strictly black-box modeling remains problematic for time-varying systems. Unlike time-invariant effects, training models on devices with internal modulation typically requires the recording or extraction of control signals to ensure the time-alignment required by standard loss functions. This paper introduces a Generative Adversarial Network (GAN) framework to model such effects using only input-output audio recordings, removing the need for modulation signal extraction. We propose a convolutional-recurrent architecture trained via a two-stage strategy: an initial adversarial phase allows the model to learn the distribution of the modulation behavior without strict phase constraints, followed by a supervised fine-tuning phase where a State Prediction Network (SPN) estimates the initial internal states required to synchronize the model with the target. Additionally, a new objective metric based on chirp-train signals is developed to quantify modulation accuracy. Experiments modeling a vintage hardware phaser demonstrate the method's ability to capture time-varying dynamics in a fully black-box context.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</title>
<link>https://arxiv.org/abs/2512.15347</link>
<guid>https://arxiv.org/abs/2512.15347</guid>
<content:encoded><![CDATA[
arXiv:2512.15347v1 Announce Type: cross 
Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[
arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remotely Detectable Robot Policy Watermarking</title>
<link>https://arxiv.org/abs/2512.15379</link>
<guid>https://arxiv.org/abs/2512.15379</guid>
<content:encoded><![CDATA[
arXiv:2512.15379v1 Announce Type: cross 
Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[
arXiv:2512.15396v1 Announce Type: cross 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Partitioned Local Depth for semi-supervised applications</title>
<link>https://arxiv.org/abs/2512.15436</link>
<guid>https://arxiv.org/abs/2512.15436</guid>
<content:encoded><![CDATA[
arXiv:2512.15436v1 Announce Type: cross 
Abstract: We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title>
<link>https://arxiv.org/abs/2512.15503</link>
<guid>https://arxiv.org/abs/2512.15503</guid>
<content:encoded><![CDATA[
arXiv:2512.15503v1 Announce Type: cross 
Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</title>
<link>https://arxiv.org/abs/2512.15521</link>
<guid>https://arxiv.org/abs/2512.15521</guid>
<content:encoded><![CDATA[
arXiv:2512.15521v1 Announce Type: cross 
Abstract: The development of nuclear fusion requires materials that can withstand extreme conditions. The IFMIF-DONES facility, a high-power particle accelerator, is being designed to qualify these materials. A critical testbed for its development is the MuVacAS prototype, which replicates the final segment of the accelerator beamline. Precise regulation of argon gas pressure within its ultra-high vacuum chamber is vital for this task. This work presents a fully data-driven approach for autonomous pressure control. A Deep Learning Surrogate Model, trained on real operational data, emulates the dynamics of the argon injection system. This high-fidelity digital twin then serves as a fast-simulation environment to train a Deep Reinforcement Learning agent. The results demonstrate that the agent successfully learns a control policy that maintains gas pressure within strict operational limits despite dynamic disturbances. This approach marks a significant step toward the intelligent, autonomous control systems required for the demanding next-generation particle accelerator facilities.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditioned UNet for Music Source Separation</title>
<link>https://arxiv.org/abs/2512.15532</link>
<guid>https://arxiv.org/abs/2512.15532</guid>
<content:encoded><![CDATA[
arXiv:2512.15532v1 Announce Type: cross 
Abstract: In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photonics-Enhanced Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2512.15549</link>
<guid>https://arxiv.org/abs/2512.15549</guid>
<content:encoded><![CDATA[
arXiv:2512.15549v1 Announce Type: cross 
Abstract: Photonics can offer a hardware-native route for machine learning (ML). However, efficient deployment of photonics-enhanced ML requires hybrid workflows that integrate optical processing with conventional CPU/GPU based neural network architectures. Here, we propose such a workflow that combines photonic positional embeddings (PEs) with advanced graph ML models. We introduce a photonics-based method that augments graph convolutional networks (GCNs) with PEs derived from light propagation on synthetic frequency lattices whose couplings match the input graph. We simulate propagation and readout to obtain internode intensity correlation matrices, which are used as PEs in GCNs to provide global structural information. Evaluated on Long Range Graph Benchmark molecular datasets, the method outperforms baseline GCNs with Laplacian based PEs, achieving $6.3\%$ lower mean absolute error for regression and $2.3\%$ higher average precision for classification tasks using a two-layer GCN as a baseline. When implemented in high repetition rate photonic hardware, correlation measurements can enable fast feature generation by bypassing digital simulation of PEs. Our results show that photonic PEs improve GCN performance and support optical acceleration of graph ML.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2512.15567</link>
<guid>https://arxiv.org/abs/2512.15567</guid>
<content:encoded><![CDATA[
arXiv:2512.15567v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title>
<link>https://arxiv.org/abs/2512.15581</link>
<guid>https://arxiv.org/abs/2512.15581</guid>
<content:encoded><![CDATA[
arXiv:2512.15581v1 Announce Type: cross 
Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point</title>
<link>https://arxiv.org/abs/2512.15606</link>
<guid>https://arxiv.org/abs/2512.15606</guid>
<content:encoded><![CDATA[
arXiv:2512.15606v1 Announce Type: cross 
Abstract: Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning continuous SOC-dependent thermal decomposition kinetics for Li-ion cathodes using KA-CRNNs</title>
<link>https://arxiv.org/abs/2512.15628</link>
<guid>https://arxiv.org/abs/2512.15628</guid>
<content:encoded><![CDATA[
arXiv:2512.15628v1 Announce Type: cross 
Abstract: Thermal runaway in lithium-ion batteries is strongly influenced by the state of charge (SOC). Existing predictive models typically infer scalar kinetic parameters at a full SOC or a few discrete SOC levels, preventing them from capturing the continuous SOC dependence that governs exothermic behavior during abuse conditions. To address this, we apply the Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN) framework to learn continuous and realistic SOC-dependent exothermic cathode-electrolyte interactions. We apply a physics-encoded KA-CRNN to learn SOC-dependent kinetic parameters for cathode-electrolyte decomposition directly from differential scanning calorimetry (DSC) data. A mechanistically informed reaction pathway is embedded into the network architecture, enabling the activation energies, pre-exponential factors, enthalpies, and related parameters to be represented as continuous and fully interpretable functions of the SOC. The framework is demonstrated for NCA, NM, and NMA cathodes, yielding models that reproduce DSC heat-release features across all SOCs and provide interpretable insight into SOC-dependent oxygen-release and phase-transformation mechanisms. This approach establishes a foundation for extending kinetic parameter dependencies to additional environmental and electrochemical variables, supporting more accurate and interpretable thermal-runaway prediction and monitoring.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</title>
<link>https://arxiv.org/abs/2512.15634</link>
<guid>https://arxiv.org/abs/2512.15634</guid>
<content:encoded><![CDATA[
arXiv:2512.15634v1 Announce Type: cross 
Abstract: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&amp;A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</title>
<link>https://arxiv.org/abs/2512.15658</link>
<guid>https://arxiv.org/abs/2512.15658</guid>
<content:encoded><![CDATA[
arXiv:2512.15658v1 Announce Type: cross 
Abstract: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prospects for quantum advantage in machine learning from the representability of functions</title>
<link>https://arxiv.org/abs/2512.15661</link>
<guid>https://arxiv.org/abs/2512.15661</guid>
<content:encoded><![CDATA[
arXiv:2512.15661v1 Announce Type: cross 
Abstract: Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation. We argue that this analysis uncovers common pathways to dequantization that underlie many existing simulation methods. More importantly, it reveals critical distinctions between models that are fully simulatable, those whose function space is classically tractable, and those that remain robustly quantum. This perspective provides a conceptual map of this landscape, clarifying how different models relate to classical simulability and pointing to where opportunities for quantum advantage may lie.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title>
<link>https://arxiv.org/abs/2512.15674</link>
<guid>https://arxiv.org/abs/2512.15674</guid>
<content:encoded><![CDATA[
arXiv:2512.15674v1 Announce Type: cross 
Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Synthetic Augmentation further improves Corruption Robustness</title>
<link>https://arxiv.org/abs/2512.15675</link>
<guid>https://arxiv.org/abs/2512.15675</guid>
<content:encoded><![CDATA[
arXiv:2512.15675v1 Announce Type: cross 
Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</title>
<link>https://arxiv.org/abs/2512.15684</link>
<guid>https://arxiv.org/abs/2512.15684</guid>
<content:encoded><![CDATA[
arXiv:2512.15684v1 Announce Type: cross 
Abstract: Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[
arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Rebatching for Efficient Early-Exit Inference with DREX</title>
<link>https://arxiv.org/abs/2512.15705</link>
<guid>https://arxiv.org/abs/2512.15705</guid>
<content:encoded><![CDATA[
arXiv:2512.15705v1 Announce Type: cross 
Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title>
<link>https://arxiv.org/abs/2512.15712</link>
<guid>https://arxiv.org/abs/2512.15712</guid>
<content:encoded><![CDATA[
arXiv:2512.15712v1 Announce Type: cross 
Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension</title>
<link>https://arxiv.org/abs/2302.13849</link>
<guid>https://arxiv.org/abs/2302.13849</guid>
<content:encoded><![CDATA[
arXiv:2302.13849v4 Announce Type: replace 
Abstract: A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + \Theta(d) + O(\sqrt{k d})$, thus resolving an open question which was studied by Auer and Long ['99].
  As an application of our theory, we revisit the classical problem of prediction using expert advice: about 30 years ago Cesa-Bianchi, Freund, Haussler, Helmbold, Schapire and Warmuth studied prediction using expert advice, provided that the best among the $n$ experts makes at most $k$ mistakes, and asked what are the optimal mistake bounds. Cesa-Bianchi, Freund, Helmbold, and Warmuth ['93, '96] provided a nearly optimal bound for deterministic learners, and left the randomized case as an open problem. We resolve this question by providing an optimal learning rule in the randomized case, and showing that its expected mistake bound equals half of the deterministic bound of Cesa-Bianchi et al. ['93,'96], up to negligible additive terms. In contrast with previous works by Abernethy, Langford, and Warmuth ['06], and by Br\^anzei and Peres ['19], our result applies to all pairs $n,k$.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchOGD: Memory-Efficient Continual Learning</title>
<link>https://arxiv.org/abs/2305.16424</link>
<guid>https://arxiv.org/abs/2305.16424</guid>
<content:encoded><![CDATA[
arXiv:2305.16424v3 Announce Type: replace 
Abstract: When machine learning models are trained continually on a sequence of tasks, they are often liable to forget what they learned on previous tasks--a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper develops a memory-efficient solution to catastrophic forgetting using the idea of matrix sketching, in the context of a simple continual learning algorithm known as orthogonal gradient descent (OGD). OGD finds weight updates that aim to preserve performance on prior datapoints, using gradients of the model on those datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over long time horizons. To address this problem, we propose SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fixed, user-determined size. In contrast to existing memory-efficient variants of OGD, SketchOGD runs online without the need for advance knowledge of the total number of tasks, is simple to implement, and is more amenable to analysis. We provide theoretical guarantees on the approximation error of the relevant sketches under a novel metric suited to the downstream task of OGD. Experimentally, we find that SketchOGD tends to outperform current state-of-the-art variants of OGD given a fixed memory budget.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Continual Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2402.08182</link>
<guid>https://arxiv.org/abs/2402.08182</guid>
<content:encoded><![CDATA[
arXiv:2402.08182v2 Announce Type: replace 
Abstract: Continual Test-Time Adaptation (CTTA) task investigates effective domain adaptation under the scenario of continuous domain shifts during testing time. Due to the utilization of solely unlabeled samples, there exists significant uncertainty in model updates, leading CTTA to encounter severe error accumulation issues. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pretrained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2403.13522</link>
<guid>https://arxiv.org/abs/2403.13522</guid>
<content:encoded><![CDATA[
arXiv:2403.13522v3 Announce Type: replace 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis</title>
<link>https://arxiv.org/abs/2406.10090</link>
<guid>https://arxiv.org/abs/2406.10090</guid>
<content:encoded><![CDATA[
arXiv:2406.10090v2 Announce Type: replace 
Abstract: Thanks to their extensive capacity, over-parameterized neural networks exhibit superior predictive capabilities and generalization. However, having a large parameter space is considered one of the main suspects of the neural networks' vulnerability to adversarial example -- input samples crafted ad-hoc to induce a desired misclassification. Relevant literature has claimed contradictory remarks in support of and against the robustness of over-parameterized networks. These contradictory findings might be due to the failure of the attack employed to evaluate the networks' robustness. Previous research has demonstrated that depending on the considered model, the algorithm employed to generate adversarial examples may not function properly, leading to overestimating the model's robustness. In this work, we empirically study the robustness of over-parameterized networks against adversarial examples. However, unlike the previous works, we also evaluate the considered attack's reliability to support the results' veracity. Our results show that over-parameterized networks are robust against adversarial attacks as opposed to their under-parameterized counterparts.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imbalances in Neurosymbolic Learning: Characterization and Mitigating Strategies</title>
<link>https://arxiv.org/abs/2407.10000</link>
<guid>https://arxiv.org/abs/2407.10000</guid>
<content:encoded><![CDATA[
arXiv:2407.10000v4 Announce Type: replace 
Abstract: We study one of the most popular problems in **neurosymbolic learning** (NSL), that of learning neural classifiers given only the result of applying a symbolic component $\sigma$ to the gold labels of the elements of a vector $\mathbf x$. The gold labels of the elements in $\mathbf x$ are unknown to the learner. We make multiple contributions, theoretical and practical, to address a problem that has not been studied so far in this context, that of characterizing and mitigating *learning imbalances*, i.e., major differences in the errors that occur when classifying instances of different classes (aka **class-specific risks**). Our theoretical analysis reveals a unique phenomenon: that $\sigma$ can greatly impact learning imbalances. This result sharply contrasts with previous research on supervised and weakly supervised learning, which only studies learning imbalances under data imbalances. On the practical side, we introduce a technique for estimating the marginal of the hidden gold labels using weakly supervised data. Then, we introduce algorithms that mitigate imbalances at training and testing time by treating the marginal of the hidden labels as a constraint. We demonstrate the effectiveness of our techniques using strong baselines from NSL and long-tailed learning, suggesting performance improvements of up to 14%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title>
<link>https://arxiv.org/abs/2411.17645</link>
<guid>https://arxiv.org/abs/2411.17645</guid>
<content:encoded><![CDATA[
arXiv:2411.17645v4 Announce Type: replace 
Abstract: The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs). We implemented a data pre-processing and curation pipeline that transforms the raw EHR data into a structured format suitable for developing predictive models focused on data fairness, accountability and transparency. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Pairwise XGBoost models are trained using this framework to differentiate UTI risk categories with explainable AI techniques applied to identify key predictors and support interpretability. Our findings reveal differences in clinical and demographic predictors across risk groups. While this study highlights the potential of AI-driven insights to support UTI clinical decision-making, further investigation of patient sub-strata and extensive validation are needed to ensure robustness and applicability in clinical practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveGNN: Integrating Graph Neural Networks and Transformers for Decay-Aware Classification of Irregular Clinical Time-Series</title>
<link>https://arxiv.org/abs/2412.10621</link>
<guid>https://arxiv.org/abs/2412.10621</guid>
<content:encoded><![CDATA[
arXiv:2412.10621v2 Announce Type: replace 
Abstract: Clinical time series are often irregularly sampled, with varying sensor frequencies, missing observations, and misaligned timestamps. Prior approaches typically address these irregularities by interpolating data into regular sequences, thereby introducing bias, or by generating inconsistent and uninterpretable relationships across sensor measurements, complicating the accurate learning of both intra-series and inter-series dependencies. We introduce WaveGNN, a model that operates directly on irregular multivariate time series without interpolation or conversion to a regular representation. WaveGNN combines a decay-aware Transformer to capture intra-series dynamics with a sample-specific graph neural network that models both short-term and long-term inter-sensor relationships. Therefore, it generates a single, sparse, and interpretable graph per sample. Across multiple benchmark datasets (P12, P19, MIMIC-III, and PAM), WaveGNN delivers consistently strong performance, whereas other state-of-the-art baselines tend to perform well on some datasets or tasks but poorly on others. While WaveGNN does not necessarily surpass every method in every case, its consistency and robustness across diverse settings set it apart. Moreover, the learned graphs align well with known physiological structures, enhancing interpretability and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data</title>
<link>https://arxiv.org/abs/2412.11800</link>
<guid>https://arxiv.org/abs/2412.11800</guid>
<content:encoded><![CDATA[
arXiv:2412.11800v2 Announce Type: replace 
Abstract: Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (\textsc{AnomalyCD}), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The \textsc{AnomalyCD} presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of of the approach on two datasets: monitoring sensor data of the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public data set for information technology monitoring. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly data sets. Source code: https://github.com/muleina/AnomalyCD .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Bayesian Optimization via Focalized Sparse Gaussian Processes</title>
<link>https://arxiv.org/abs/2412.20375</link>
<guid>https://arxiv.org/abs/2412.20375</guid>
<content:encoded><![CDATA[
arXiv:2412.20375v2 Announce Type: replace 
Abstract: Bayesian optimization is an effective technique for black-box optimization, but its applicability is typically limited to low-dimensional and small-budget problems due to the cubic complexity of computing the Gaussian process (GP) surrogate. While various approximate GP models have been employed to scale Bayesian optimization to larger sample sizes, most suffer from overly-smooth estimation and focus primarily on problems that allow for large online samples. In this work, we argue that Bayesian optimization algorithms with sparse GPs can more efficiently allocate their representational power to relevant regions of the search space. To achieve this, we propose focalized GP, which leverages a novel variational loss function to achieve stronger local prediction, as well as FocalBO, which hierarchically optimizes the focalized GP acquisition function over progressively smaller search spaces. Experimental results demonstrate that FocalBO can efficiently leverage large amounts of offline and online data to achieve state-of-the-art performance on robot morphology design and to control a 585-dimensional musculoskeletal system.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning applications in archaeological practices: a review</title>
<link>https://arxiv.org/abs/2501.03840</link>
<guid>https://arxiv.org/abs/2501.03840</guid>
<content:encoded><![CDATA[
arXiv:2501.03840v5 Announce Type: replace 
Abstract: Artificial intelligence and machine learning applications in archaeology have increased significantly in recent years, and these now span all subfields, geographical regions, and time periods. The prevalence and success of these applications have remained largely unexamined, as recent reviews on the use of machine learning in archaeology have only focused only on specific subfields of archaeology. Our review examined an exhaustive corpus of 135 articles published between 1997 and 2022. We observed a significant increase in the number of publications from 2019 onwards. Automatic structure detection and artefact classification were the most represented tasks in the articles reviewed, followed by taphonomy, and archaeological predictive modelling. From the review, clustering and unsupervised methods were underrepresented compared to supervised models. Artificial neural networks and ensemble learning account for two thirds of the total number of models used. However, if machine learning models are gaining in popularity they remain subject to misunderstanding. We observed, in some cases, poorly defined requirements and caveats of the machine learning methods used. Furthermore, the goals and the needs of machine learning applications for archaeological purposes are in some cases unclear or poorly expressed. To address this, we proposed a workflow guide for archaeologists to develop coherent and consistent methodologies adapted to their research questions, project scale and data. As in many other areas, machine learning is rapidly becoming an important tool in archaeological research and practice, useful for the analyses of large and multivariate data, although not without limitations. This review highlights the importance of well-defined and well-reported structured methodologies and collaborative practices to maximise the potential of applications of machine learning methods in archaeology.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry and Optimization of Shallow Polynomial Networks</title>
<link>https://arxiv.org/abs/2501.06074</link>
<guid>https://arxiv.org/abs/2501.06074</guid>
<content:encoded><![CDATA[
arXiv:2501.06074v2 Announce Type: replace 
Abstract: We study shallow neural networks with monomial activations and output dimension one. The function space for these models can be identified with a set of symmetric tensors with bounded rank. We describe general features of these networks, focusing on the relationship between width and optimization. We then consider teacher-student problems, which can be viewed as problems of low-rank tensor approximation with respect to non-standard inner products that are induced by the data distribution. In this setting, we introduce a teacher-metric data discriminant which encodes the qualitative behavior of the optimization as a function of the training data distribution. Finally, we focus on networks with quadratic activations, presenting an in-depth analysis of the optimization landscape. In particular, we present a variation of the Eckart-Young Theorem characterizing all critical points and their Hessian signatures for teacher-student problems with quadratic networks and Gaussian training data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing</title>
<link>https://arxiv.org/abs/2502.00882</link>
<guid>https://arxiv.org/abs/2502.00882</guid>
<content:encoded><![CDATA[
arXiv:2502.00882v3 Announce Type: replace 
Abstract: Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time. In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps. To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a $\textit{weighted}$ least-squares solution. Unfortunately, for general problems the bias of the weighted least-squares solution and the variance of the iterates can become arbitrarily large. We show that these quantities can be rigorously controlled by incorporating regularization into the RBK iterations, yielding the regularized algorithm ReBlocK. Numerical experiments including examples arising from natural gradient optimization demonstrate that ReBlocK can outperform both RBK and minibatch stochastic gradient descent for inconsistent problems with rapidly decaying singular values.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Quantum Optimization with Continuous Bandits</title>
<link>https://arxiv.org/abs/2502.04021</link>
<guid>https://arxiv.org/abs/2502.04021</guid>
<content:encoded><![CDATA[
arXiv:2502.04021v2 Announce Type: replace 
Abstract: We introduce a novel approach to variational Quantum algorithms (VQA) via continuous bandits. VQA are a class of hybrid Quantum-classical algorithms where the parameters of Quantum circuits are optimized by classical algorithms. Previous work has used zero and first order gradient based methods, however such algorithms suffer from the barren plateau (BP) problem where gradients and loss differences are exponentially small. We introduce an approach using bandits methods which combine global exploration with local exploitation. We show how VQA can be formulated as a best arm identification problem in a continuous space of arms with Lipschitz smoothness. While regret minimization has been addressed in this setting, existing methods for pure exploration only cover discrete spaces. We give the first results for pure exploration in a continuous setting and derive a fixed-confidence, information-theoretic, instance specific lower bound. Under certain assumptions on the expected payoff, we derive a simple algorithm, which is near-optimal with respect to our lower bound. Finally, we apply our continuous bandit algorithm to two VQA schemes: a PQC and a QAOA quantum circuit, showing that we significantly outperform the previously known state of the art methods (which used gradient based methods).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Optimization over Generative Priors via Coarse Learnability</title>
<link>https://arxiv.org/abs/2503.06917</link>
<guid>https://arxiv.org/abs/2503.06917</guid>
<content:encoded><![CDATA[
arXiv:2503.06917v3 Announce Type: replace 
Abstract: In zeroth-order optimization, we seek to minimize a function $d(\cdot)$, which may encode combinatorial feasibility, using only function evaluations. We focus on the setting where solutions must also satisfy qualitative constraints or conform to a complex prior distribution. To address this, we introduce a new framework in which such constraints are represented by an initial generative prior $\L(\cdot)$, for example, a Large Language Model (LLM). The objective is to find solutions $s$ that minimize $d(s)$ while having high probability under $\L(s)$, effectively sampling from a target distribution proportional to $\L(s) \cdot e^{-T \cdot d(s)}$ for a temperature parameter $T$.
  While this framework aligns with classical Model-Based Optimization (e.g., the Cross-Entropy method), existing theory is ill-suited for deriving sample complexity bounds in black-box deep generative models. We therefore propose a novel learning assumption, which we term \emph{coarse learnability}, where an agent with access to a polynomial number of samples can learn a model whose point-wise density approximates the target within a polynomial factor. Leveraging this assumption, we design an iterative algorithm that employs a Metropolis-Hastings correction to provably approximate the target distribution using a polynomial number of samples. To the best of our knowledge, this is one of the first works to establish such sample-complexity guarantees for model-based optimization with deep generative priors.
  We provide two lines of evidence supporting the coarse learnability assumption. Theoretically, we show that maximum likelihood estimation naturally induces the required coverage properties, holding for both standard exponential families and for misspecified models. Empirically, we demonstrate that LLMs can adapt their learned distributions to zeroth-order feedback to solve combinatorial optimization problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch</title>
<link>https://arxiv.org/abs/2503.19779</link>
<guid>https://arxiv.org/abs/2503.19779</guid>
<content:encoded><![CDATA[
arXiv:2503.19779v2 Announce Type: replace 
Abstract: Machine learning (ML) workloads launch hundreds to thousands of short-running GPU kernels per iteration. With GPU compute throughput growing rapidly, CPU-side launch latency of kernels is emerging as a bottleneck. CUDA Graphs promise to address this by replaying a set of kernels with a single dispatch of the graph, removing per-kernel launch costs. However, CUDA Graphs remain surprisingly difficult to deploy correctly and efficiently.
  We present PyGraph - a compiler framework to maximize the coverage and benefits of CUDA Graphs for ML workloads. It introduces three novel optimizations: it applies automatic code transformations to make ML applications amenable to CUDA Graphs; it eliminates the parameter copy overheads for kernels executing in CUDA Graphs, and it selectively deploys CUDA Graphs guided by a cost-benefit analysis. For 25 ML workloads from TorchBench, HuggingFace, and TIMM, PyGraph more than doubles the benefit from deploying CUDA Graph compared to the most popular and widely used ML compiler, PyTorch2. PyGraph is built atop PyTorch2's compilation framework and requires no programmer intervention.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity</title>
<link>https://arxiv.org/abs/2504.17913</link>
<guid>https://arxiv.org/abs/2504.17913</guid>
<content:encoded><![CDATA[
arXiv:2504.17913v2 Announce Type: replace 
Abstract: Long-term time series forecasting plays a pivotal role in various real-world applications. Despite recent advancements and the success of different architectures, forecasting is often challenging due to non-stationary nature of the real-world data, which frequently exhibit distribution shifts and temporal changes in statistical properties like mean and variance over time. Previous studies suggest that this inherent variability complicates forecasting, limiting the performance of many models by leading to loss of non-stationarity and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To address this challenge, we introduce a novel architecture, ChoronoAdaptive Network (CANet), inspired by style-transfer techniques. The core of CANet is the Non-stationary Adaptive Normalization module, seamlessly integrating the Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and Belongie, 2017). The Style Blending Gate preserves and reintegrates non-stationary characteristics, such as mean and standard deviation, by blending internal and external statistics, preventing over-stationarization while maintaining essential temporal dependencies. Coupled with AdaIN, which dynamically adapts the model to statistical changes, this approach enhances predictive accuracy under non-stationary conditions. CANet also employs multi-resolution patching to handle short-term fluctuations and long-term trends, along with Fourier analysis-based adaptive thresholding to reduce noise. A Stacked Kronecker Product Layer further optimizes the model's efficiency while maintaining high performance. Extensive experiments on real-world datasets validate CANet's superiority over state-of-the-art methods, achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is publicly available at https://github.com/mertsonmezer/CANet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume</title>
<link>https://arxiv.org/abs/2505.06292</link>
<guid>https://arxiv.org/abs/2505.06292</guid>
<content:encoded><![CDATA[
arXiv:2505.06292v2 Announce Type: replace 
Abstract: Graph Neural Networks have shown strong performance in traffic volume forecasting, particularly on highways and major arterial networks. Applying them to urban settings, however, presents unique challenges: urban networks exhibit greater structural diversity, traffic volumes are highly overdispersed with many zeros, the best way to account for spatial dependencies remains unclear, and sensor coverage is often very sparse. We introduce the Graph Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume estimation approach. GNNUI employs a masking algorithm to learn interpolation, integrates node features to capture functional roles, and uses a loss function tailored to zero-inflated traffic distributions. In addition to the model, we introduce two new open, large-scale urban traffic volume benchmarks, covering different transportation modes: Strava cycling data from Berlin and New York City taxi data. GNNUI outperforms recent, some graph-based, interpolation methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence) and remains robust from 90% to 1% sensor coverage. For example, on the Strava dataset, the MAE increases only from 7.1 to 10.5, and on the Taxi dataset, from 23.0 to 40.4. These results demonstrate that GNNUI maintains strong performance despite extreme data scarcity, a common condition in real-world urban settings. We also examine how graph connectivity choices influence model accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[
arXiv:2505.16896v2 Announce Type: replace 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but often lack the structural knowledge essential for some biological applications. To address this, we introduce a method to enrich pLMs with structural knowledge by leveraging pre-trained protein graph neural networks (pGNNs). First, a latent-level contrastive learning task aligns residue representations from pLMs with those from pGNNs across multiple proteins, injecting inter-protein structural information. Additionally, a physical-level task integrates intra-protein information by training pLMs to predict structure tokens. Together, the proposed dual-task framework effectively incorporates both inter- and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module that uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method as a simple, lightweight post-training step to the state-of-the-art ESM2 and AMPLIFY yields notable performance gains. These improvements are consistent across a wide range of tasks, including substantial gains in deep mutational scanning (DMS) fitness prediction and a 59% increase in P@L for ESM2 650M contact prediction on CASP16. Furthermore, we demonstrate that these performance gains are robust, scaling with model sizes from 8M to 650M and extending to different downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</title>
<link>https://arxiv.org/abs/2505.23049</link>
<guid>https://arxiv.org/abs/2505.23049</guid>
<content:encoded><![CDATA[
arXiv:2505.23049v2 Announce Type: replace 
Abstract: Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation - especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices. Our method can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are available at https://github.com/Axel-gu/DenoiseRotator.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[
arXiv:2505.23415v2 Announce Type: replace 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration</title>
<link>https://arxiv.org/abs/2506.18916</link>
<guid>https://arxiv.org/abs/2506.18916</guid>
<content:encoded><![CDATA[
arXiv:2506.18916v2 Announce Type: replace 
Abstract: Text-to-SQL generation bridges the gap between natural language and databases, enabling users to query data without requiring SQL expertise. While large language models (LLMs) have significantly advanced the field, challenges remain in handling complex queries that involve multi-table joins, nested conditions, and intricate operations. Existing methods often rely on multi-step pipelines that incur high computational costs, increase latency, and are prone to error propagation. To address these limitations, we propose HI-SQL, a pipeline that incorporates a novel hint generation mechanism utilizing historical query logs to guide SQL generation. By analyzing prior queries, our method generates contextual hints that focus on handling the complexities of multi-table and nested operations. These hints are seamlessly integrated into the SQL generation process, eliminating the need for costly multi-step approaches and reducing reliance on human-crafted prompts. Experimental evaluations on multiple benchmark datasets demonstrate that our approach significantly improves query accuracy of LLM-generated queries while ensuring efficiency in terms of LLM calls and latency, offering a robust and practical solution for enhancing Text-to-SQL systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
arXiv:2507.10532v3 Announce Type: replace 
Abstract: Reasoning in large language models has long been a central research focus, and recent studies employing reinforcement learning (RL) have introduced diverse methods that yield substantial performance gains with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance performance. However, these breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama, which warrants a more in-depth investigation. In this work, our empirical analysis reveals that pre-training on massive web-scale corpora leaves Qwen2.5 susceptible to data contamination in widely used benchmarks. Consequently, conclusions derived from contaminated benchmarks on Qwen2.5 series may be unreliable. To obtain trustworthy evaluation results, we introduce a generator that creates fully clean arithmetic problems of arbitrary length and difficulty, dubbed RandomCalculation. Using this leakage-free dataset, we show that only accurate reward signals yield steady improvements that surpass the base model's performance boundary in mathematical reasoning, whereas random or incorrect rewards do not. Moreover, we conduct more fine-grained analyses to elucidate the factors underlying the different performance observed on the MATH-500 and RandomCalculation benchmarks. Consequently, we recommend that future studies evaluate models on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Latency and Bandwidth: A Theoretical Framework and Adaptive Algorithm for Communication-Constrained Training</title>
<link>https://arxiv.org/abs/2507.17346</link>
<guid>https://arxiv.org/abs/2507.17346</guid>
<content:encoded><![CDATA[
arXiv:2507.17346v2 Announce Type: replace 
Abstract: Regional energy caps limit the growth of any single data center used for large-scale model training. This single-center training paradigm works when model size remains manageable, but exponential growth in the model size and computational demand challenges it. A natural alternative is to distribute training across multiple data centers over wide-area networks. This pools distributed resources, but suffers from high latency and low, time-varying bandwidth, sharply reducing throughout. Employing jointly gradient compression and delayed aggregation can alleviate communication problems, but introduces a complex three-way trade-off among compression ratio, staleness (delayed synchronization steps), and convergence rate. Existing work lacks theoretical guidance and can only propose fixed strategies, insensitive to computation and communication conditions. We address this with a new theoretical tool, decomposing the joint optimization problem into a traditional process plus multiple analyzable noise terms. Our analysis yields the first convergence rate for this setting and shows that increasing staleness exponentially amplifies the detrimental effect of compression. Leveraging these insights, we propose DeCo-SGD, which dynamically selects the compression ratio and staleness based on the real-time communication and computation conditions. DeCo-SGD achieves up to $5.07\times$ and $1.37\times$ speed-ups over distributed SGD and static strategy in high-latency and low, varying bandwidth networks, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory</title>
<link>https://arxiv.org/abs/2507.18183</link>
<guid>https://arxiv.org/abs/2507.18183</guid>
<content:encoded><![CDATA[
arXiv:2507.18183v2 Announce Type: replace 
Abstract: Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[
arXiv:2507.23600v4 Announce Type: replace 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed signals into components (base patterns) and their concentrations (intensity), playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified number of components, usually unknown in real data. Once data or component number increases, the scalability of these MCR approaches face significant challenges. This study reformulates MCR as a data generative process (gMCR), and introduces an Energy-Based solver, EB-gMCR, that automatically discovers the smallest component set and their concentrations for reconstructing the mixed signals faithfully. On synthetic benchmarks with up to 256 components, EB-gMCR attains high reconstruction fidelity and recovers the component count within 5% at 20dB noise and near-exact at 30dB. On two public spectral datasets, it identifies the correct component count and improves component separation over MF-based MCR approaches (NMF variants, ICA, MCR-ALS). EB-gMCR is a general solver for fixed-pattern signal unmixing (components remain invariant across mixtures). Domain priors (non-negativity, nonlinear mixing) enter as plug-in modules, enabling adaptation to new instruments or domains without altering the core selection learning step. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics</title>
<link>https://arxiv.org/abs/2508.03965</link>
<guid>https://arxiv.org/abs/2508.03965</guid>
<content:encoded><![CDATA[
arXiv:2508.03965v3 Announce Type: replace 
Abstract: In this work, we employ physics-informed neural operators to map pressure profiles from an input function space to the corresponding bubble radius responses. Our approach employs a two-step DeepONet architecture. To address the intrinsic spectral bias of deep learning models, our model incorporates the Rowdy adaptive activation function, enhancing the representation of high-frequency features. Moreover, we introduce the Kolmogorov-Arnold network (KAN) based two-step DeepOKAN model, which enhances interpretability (often lacking in conventional multilayer perceptron architectures) while efficiently capturing high-frequency bubble dynamics without explicit utilization of activation functions in any form. We particularly investigate the use of spline basis functions in combination with radial basis functions (RBF) within our architecture, as they demonstrate superior performance in constructing a universal basis for approximating high-frequency bubble dynamics compared to alternative formulations. Furthermore, we emphasize on the performance bottleneck of RBF while learning the high frequency bubble dynamics and showcase the advantage of using spline basis function for the trunk network in overcoming this inherent spectral bias. The model is systematically evaluated across three representative scenarios: (1) bubble dynamics governed by the Rayleigh-Plesset equation with a single initial radius, (2) bubble dynamics governed by the Keller-Miksis equation with a single initial radius, and (3) Keller-Miksis dynamics with multiple initial radii. We also compare our results with state-of-the-art neural operators, including Fourier Neural Operators, Wavelet Neural Operators, OFormer, and Convolutional Neural Operators. Our findings demonstrate that the two-step DeepOKAN accurately captures both low- and high-frequency behaviors, and offers a promising alternative to conventional numerical solvers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference</title>
<link>https://arxiv.org/abs/2508.05190</link>
<guid>https://arxiv.org/abs/2508.05190</guid>
<content:encoded><![CDATA[
arXiv:2508.05190v2 Announce Type: replace 
Abstract: Accurately modeling and inferring solutions to time-dependent partial differential equations (PDEs) over extended horizons remains a core challenge in scientific machine learning. Traditional full rollout (FR) methods, which predict entire trajectories in one pass, often fail to capture the causal dependencies and generalize poorly outside the training time horizon. Autoregressive (AR) approaches, evolving the system step by step, suffer from error accumulation, limiting long-term accuracy. These shortcomings limit the long-term accuracy and reliability of both strategies. To address these issues, we introduce the Physics-Informed Time-Integrated Deep Operator Network (PITI-DeepONet), a dual-output architecture trained via fully physics-informed or hybrid physics- and data-driven objectives to ensure stable, accurate long-term evolution well beyond the training horizon. Instead of forecasting future states, the network learns the time-derivative operator from the current state, integrating it using classical time-stepping schemes to advance the solution in time. Additionally, the framework can leverage residual monitoring during inference to estimate prediction quality and detect when the system transitions outside the training domain. Applied to benchmark problems, PITI-DeepONet shows improved accuracy over extended inference time horizons when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation; by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation. By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for more reliable, long-term integration of complex, time-dependent PDEs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
<link>https://arxiv.org/abs/2508.09320</link>
<guid>https://arxiv.org/abs/2508.09320</guid>
<content:encoded><![CDATA[
arXiv:2508.09320v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile exact verifier for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on real-world fraud datasets (Amazon and Yelp) and biochemical datasets (MUTAG and ENZYMES) demonstrates its usability and effectiveness, as well as superior performance for node classification and competitiveness on graph classification compared to existing exact verification tools on sum-aggregated GNNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Tensor Decompositions for the Theory of Neural Networks</title>
<link>https://arxiv.org/abs/2508.18408</link>
<guid>https://arxiv.org/abs/2508.18408</guid>
<content:encoded><![CDATA[
arXiv:2508.18408v2 Announce Type: replace 
Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge of interest in providing a mathematical basis to deep learning theory. Low-rank tensor decompositions are specially befitting for this task due to their close connection to NNs and their rich theoretical results. Different tensor decompositions have strong uniqueness guarantees, which allow for a direct interpretation of their factors, and polynomial time algorithms have been proposed to compute them. Through the connections between tensors and NNs, such results supported many important advances in the theory of NNs. In this review, we show how low-rank tensor methods--which have been a core tool in the signal processing and machine learning communities--play a fundamental role in theoretically explaining different aspects of the performance of deep NNs, including their expressivity, algorithmic learnability and computational hardness, generalization, and identifiability. Our goal is to give an accessible overview of existing approaches (developed by different communities, ranging from computer science to mathematics) in a coherent and unified way, and to open a broader perspective on the use of low-rank tensor decompositions for the theory of deep NNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.25300</link>
<guid>https://arxiv.org/abs/2509.25300</guid>
<content:encoded><![CDATA[
arXiv:2509.25300v2 Announce Type: replace 
Abstract: While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1.Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2.The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3.Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4.In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Foundation Models for Early Disease Detection</title>
<link>https://arxiv.org/abs/2510.01899</link>
<guid>https://arxiv.org/abs/2510.01899</guid>
<content:encoded><![CDATA[
arXiv:2510.01899v2 Announce Type: replace 
Abstract: Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v2 Announce Type: replace 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</title>
<link>https://arxiv.org/abs/2510.11062</link>
<guid>https://arxiv.org/abs/2510.11062</guid>
<content:encoded><![CDATA[
arXiv:2510.11062v4 Announce Type: replace 
Abstract: Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.
  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Reinforcement Learning After Next-Token Prediction Facilitates Learning</title>
<link>https://arxiv.org/abs/2510.11495</link>
<guid>https://arxiv.org/abs/2510.11495</guid>
<content:encoded><![CDATA[
arXiv:2510.11495v2 Announce Type: replace 
Abstract: Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms. We introduce a framework to study the success of this paradigm, and we theoretically expose the optimization mechanisms by which reinforcement learning improves over next-token prediction in this setting. We study learning from mixture distributions of short and long ``chain-of-thought'' sequences encoding a single task. In particular, when the task consists of predicting the parity of $d$ bits and long sequences are rare, we show how reinforcement learning after next-token prediction enables autoregressive transformers to generalize, whereas mere next-token prediction requires extreme statistical or computational resources to do so. We further explain how reinforcement learning leverages increased test-time computation, manifested in longer responses, to facilitate this learning process. In a simplified setting, we theoretically prove that autoregressive linear models following this training recipe can efficiently learn to predict the parity of $d$ bits as long as the proportion of long demonstrations in the data mix is not exponentially small in the input dimension $d$. Finally, we demonstrate these same phenomena in other settings, including the post-training of Llama-series models on mixture variations of common mathematical reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QLENS: Towards A Quantum Perspective of Language Transformers</title>
<link>https://arxiv.org/abs/2510.11963</link>
<guid>https://arxiv.org/abs/2510.11963</guid>
<content:encoded><![CDATA[
arXiv:2510.11963v2 Announce Type: replace 
Abstract: In natural language processing, current methods for understanding Transformers are successful at identifying intermediate predictions during a model's inference. However, these approaches function as limited diagnostic checkpoints, lacking a mathematical framework for mechanistically modeling how each layer facilitates transitions between these evolving states. This interpretability gap and past successes of interdisciplinary outlooks inspire us to turn to physics in search of a descriptive mathematical framework for Transformers. We observe that language models are intrinsically probabilistic, an attribute that is echoed in the core postulates of quantum mechanics. This parallel inspires us to translate insights from this discipline to that of natural language processing. Towards this objective, we propose QLENS a novel attempt to develop a physics-based perspective on the Transformer generation process. Under QLENS, a Transformer is studied by converting its latent activations into a state vector in a Hilbert space derived from the model's output units. This state subsequently evolves through hidden layers - reformulated as unitary operators and analogously defined Hamiltonians - during inference. The model's final probability distribution is obtained by applying the Born rule to the end state using a specific measurement operator. To demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy Transformer to investigate the influence of individual layers in a model's prediction trajectory. We present our work as a foundation for cross-domain insights to be leveraged towards a broader understanding of Transformers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[
arXiv:2510.12691v2 Announce Type: replace 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Active Generation of Pareto Sets</title>
<link>https://arxiv.org/abs/2510.21052</link>
<guid>https://arxiv.org/abs/2510.21052</guid>
<content:encoded><![CDATA[
arXiv:2510.21052v3 Announce Type: replace 
Abstract: We introduce active generation of Pareto sets (A-GPS), a new framework for online discrete black-box multi-objective optimization (MOO). A-GPS learns a generative model of the Pareto set that supports a-posteriori conditioning on user preferences. The method employs a class probability estimator (CPE) to predict non-dominance relations and to condition the generative model toward high-performing regions of the search space. We also show that this non-dominance CPE implicitly estimates the probability of hypervolume improvement (PHVI). To incorporate subjective trade-offs, A-GPS introduces preference direction vectors that encode user-specified preferences in objective space. At each iteration, the model is updated using both Pareto membership and alignment with these preference directions, producing an amortized generative model capable of sampling across the Pareto front without retraining. The result is a simple yet powerful approach that achieves high-quality Pareto set approximations, avoids explicit hypervolume computation, and flexibly captures user preferences. Empirical results on synthetic benchmarks and protein design tasks demonstrate strong sample efficiency and effective preference incorporation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tab-PET: Graph-Based Positional Encodings for Tabular Transformers</title>
<link>https://arxiv.org/abs/2511.13338</link>
<guid>https://arxiv.org/abs/2511.13338</guid>
<content:encoded><![CDATA[
arXiv:2511.13338v2 Announce Type: replace 
Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems</title>
<link>https://arxiv.org/abs/2511.17435</link>
<guid>https://arxiv.org/abs/2511.17435</guid>
<content:encoded><![CDATA[
arXiv:2511.17435v2 Announce Type: replace 
Abstract: This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</title>
<link>https://arxiv.org/abs/2512.02826</link>
<guid>https://arxiv.org/abs/2512.02826</guid>
<content:encoded><![CDATA[
arXiv:2512.02826v2 Announce Type: replace 
Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEA: Spectral Edge Attack</title>
<link>https://arxiv.org/abs/2512.08964</link>
<guid>https://arxiv.org/abs/2512.08964</guid>
<content:encoded><![CDATA[
arXiv:2512.08964v2 Announce Type: replace 
Abstract: Graph based machine learning algorithms occupy an important position in today AI landscape. The ability of graph topology to represent complex data structures is both the key strength of graph algorithms and a source of their vulnerability. In other words, attacking or perturbing a graph can severely degrade the performance of graph-based methods. For the attack methods, the greatest challenge is achieving strong attack effectiveness while remaining undetected. To address this problem, this paper proposes a new attack model that employs spectral adversarial robustness evaluation to quantitatively analyze the vulnerability of each edge in a graph under attack. By precisely targeting the weakest links, the proposed approach achieves the maximum attack impact with minimal perturbation. Experimental results demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Tensor Principal Component Analysis: Exact Recovery via Deterministic Model</title>
<link>https://arxiv.org/abs/2008.02211</link>
<guid>https://arxiv.org/abs/2008.02211</guid>
<content:encoded><![CDATA[
arXiv:2008.02211v2 Announce Type: replace-cross 
Abstract: Tensor, also known as multi-dimensional array, arises from many applications in signal processing, manufacturing processes, healthcare, among others. As one of the most popular methods in tensor literature, Robust tensor principal component analysis (RTPCA) is a very effective tool to extract the low rank and sparse components in tensors. In this paper, a new method to analyze RTPCA is proposed based on the recently developed tensor-tensor product and tensor singular value decomposition (t-SVD). Specifically, it aims to solve a convex optimization problem whose objective function is a weighted combination of the tensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact recovery is built on the tensor incoherence conditions and the assumption of a uniform model on the sparse support. Unlike this conventional way, in this paper, without any assumption of randomness, the exact recovery can be achieved in a completely deterministic fashion by characterizing the tensor rank-sparsity incoherence, which is an uncertainty principle between the low-rank tensor spaces and the pattern of sparse tensor.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>tensorflow-riemopt: A Library for Optimization on Riemannian Manifolds</title>
<link>https://arxiv.org/abs/2105.13921</link>
<guid>https://arxiv.org/abs/2105.13921</guid>
<content:encoded><![CDATA[
arXiv:2105.13921v4 Announce Type: replace-cross 
Abstract: This paper presents tensorflow-riemopt, a Python library for geometric machine learning in TensorFlow. The library provides efficient implementations of neural network layers with manifold-constrained parameters, geometric operations on Riemannian manifolds, and stochastic optimization algorithms for non-Euclidean spaces. Designed for integration with TensorFlow Extended, it supports both research prototyping and production deployment of machine learning pipelines. The code and documentation are distributed under the MIT license and available at https://github.com/master/tensorflow-riemopt
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Task Dynamic Pricing in Credit Market with Contextual Information</title>
<link>https://arxiv.org/abs/2410.14839</link>
<guid>https://arxiv.org/abs/2410.14839</guid>
<content:encoded><![CDATA[
arXiv:2410.14839v4 Announce Type: replace-cross 
Abstract: We study the dynamic pricing problem faced by a broker seeking to learn prices for a large number of credit market securities, such as corporate bonds, government bonds, loans, and other credit-related securities. A major challenge in pricing these securities stems from their infrequent trading and the lack of transparency in over-the-counter (OTC) markets, which leads to insufficient data for individual pricing. Nevertheless, many securities share structural similarities that can be exploited. Moreover, brokers often place small "probing" orders to infer competitors' pricing behavior. Leveraging these insights, we propose a multi-task dynamic pricing framework that leverages the shared structure across securities to enhance pricing accuracy.
  In the OTC market, a broker wins a quote by offering a more competitive price than rivals. The broker's goal is to learn winning prices while minimizing expected regret against a clairvoyant benchmark. We model each security using a $d$-dimensional feature vector and assume a linear contextual model for the competitor's pricing of the yield, with parameters unknown a priori. We propose the Two-Stage Multi-Task (TSMT) algorithm: first, an unregularized MLE over pooled data to obtain a coarse parameter estimate; second, a regularized MLE on individual securities to refine the parameters. We show that the TSMT achieves a regret bounded by $\tilde{O} ( \delta_{\max} \sqrt{T M d} + M d ) $, outperforming both fully individual and fully pooled baselines, where $M$ is the number of securities and $\delta_{\max}$ quantifies their heterogeneity.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Variational Annealing for Multimodal Optimization</title>
<link>https://arxiv.org/abs/2501.04667</link>
<guid>https://arxiv.org/abs/2501.04667</guid>
<content:encoded><![CDATA[
arXiv:2501.04667v3 Announce Type: replace-cross 
Abstract: We introduce a new multimodal optimization approach called Natural Variational Annealing (NVA) that combines the strengths of three foundational concepts to simultaneously search for multiple global and local modes of black-box nonconvex objectives. First, it implements a simultaneous search by using variational posteriors, such as, mixtures of Gaussians. Second, it applies annealing to gradually trade off exploration for exploitation. Finally, it learns the variational search distribution using natural-gradient learning where updates resemble well-known and easy-to-implement algorithms. The three concepts come together in NVA giving rise to new algorithms and also allowing us to incorporate "fitness shaping", a core concept from evolutionary algorithms. We assess the quality of search on simulations and compare them to methods using gradient descent and evolution strategies. We also provide an application to a real-world inverse problem in planetary science.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[
arXiv:2501.13439v2 Announce Type: replace-cross 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Models Are Miscalibrated, But Some Less So: Comparing Calibration with Conditional Mean Operators</title>
<link>https://arxiv.org/abs/2502.11465</link>
<guid>https://arxiv.org/abs/2502.11465</guid>
<content:encoded><![CDATA[
arXiv:2502.11465v2 Announce Type: replace-cross 
Abstract: When working in a high-risk setting, having well calibrated probabilistic predictive models is a crucial requirement. However, estimators for calibration error are not always able to correctly distinguish which model is better calibrated. We propose the \emph{conditional kernel calibration error} (CKCE) which is based on the Hilbert-Schmidt norm of the difference between conditional mean operators. By working directly with the definition of strong calibration as the distance between conditional distributions, which we represent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is less sensitive to the marginal distribution of predictive models. This makes it more effective for relative comparisons than previously proposed calibration metrics. Our experiments, using both synthetic and real data, show that CKCE provides a more consistent ranking of models by their calibration error and is more robust against distribution shift.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Primer C-VAE: An interpretable deep learning primer design method to detect emerging virus variants</title>
<link>https://arxiv.org/abs/2503.01459</link>
<guid>https://arxiv.org/abs/2503.01459</guid>
<content:encoded><![CDATA[
arXiv:2503.01459v2 Announce Type: replace-cross 
Abstract: Motivation: PCR is more economical and quicker than Next Generation Sequencing for detecting target organisms, with primer design being a critical step. In epidemiology with rapidly mutating viruses, designing effective primers is challenging. Traditional methods require substantial manual intervention and struggle to ensure effective primer design across different strains. For organisms with large, similar genomes like Escherichia coli and Shigella flexneri, differentiating between species is also difficult but crucial.
  Results: We developed Primer C-VAE, a model based on a Variational Auto-Encoder framework with Convolutional Neural Networks to identify variants and generate specific primers. Using SARS-CoV-2, our model classified variants (alpha, beta, gamma, delta, omicron) with 98% accuracy and generated variant-specific primers. These primers appeared with >95% frequency in target variants and <5% in others, showing good performance in in-silico PCR tests. For Alpha, Delta, and Omicron, our primer pairs produced fragments <200 bp, suitable for qPCR detection. The model also generated effective primers for organisms with longer gene sequences like E. coli and S. flexneri.
  Conclusion: Primer C-VAE is an interpretable deep learning approach for developing specific primer pairs for target organisms. This flexible, semi-automated and reliable tool works regardless of sequence completeness and length, allowing for qPCR applications and can be applied to organisms with large and highly similar genomes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Global Optimization of the Acquisition Function in Bayesian Optimization: To Do or Not To Do?</title>
<link>https://arxiv.org/abs/2503.03625</link>
<guid>https://arxiv.org/abs/2503.03625</guid>
<content:encoded><![CDATA[
arXiv:2503.03625v2 Announce Type: replace-cross 
Abstract: Bayesian Optimization (BO) with Gaussian Processes relies on optimizing an acquisition function to determine sampling. We investigate the advantages and disadvantages of using a deterministic global solver (MAiNGO) compared to conventional local and stochastic global solvers (L-BFGS-B and multi-start, respectively) for the optimization of the acquisition function. For CPU efficiency, we set a time limit for MAiNGO, taking the best point as optimal. We perform repeated numerical experiments, initially using the Muller-Brown potential as a benchmark function, utilizing the lower confidence bound acquisition function; we further validate our findings with three alternative benchmark functions. Statistical analysis reveals that when the acquisition function is more exploitative (as opposed to exploratory), BO with MAiNGO converges in fewer iterations than with the local solvers. However, when the dataset lacks diversity, or when the acquisition function is overly exploitative, BO with MAiNGO, compared to the local solvers, is more likely to converge to a local rather than a global ly near-optimal solution of the black-box function. L-BFGS-B and multi-start mitigate this risk in BO by introducing stochasticity in the selection of the next sampling point, which enhances the exploration of uncharted regions in the search space and reduces dependence on acquisition function hyperparameters. Ultimately, suboptimal optimization of poorly chosen acquisition functions may be preferable to their optimal solution. When the acquisition function is more exploratory, BO with MAiNGO, multi-start, and L-BFGS-B achieve comparable probabilities of convergence to a globally near-optimal solution (although BO with MAiNGO may require more iterations to converge under these conditions).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning</title>
<link>https://arxiv.org/abs/2504.02191</link>
<guid>https://arxiv.org/abs/2504.02191</guid>
<content:encoded><![CDATA[
arXiv:2504.02191v3 Announce Type: replace-cross 
Abstract: We introduce MHNpath, a machine learning-driven retrosynthetic tool designed for computer-aided synthesis planning. Leveraging modern Hopfield networks and novel comparative metrics, MHNpath efficiently prioritizes reaction templates, improving the scalability and accuracy of retrosynthetic predictions. The tool incorporates a tunable scoring system that allows users to prioritize pathways based on cost, reaction temperature, and toxicity, thereby facilitating the design of greener and cost-effective reaction routes. We demonstrate its effectiveness through case studies involving complex molecules from ChemByDesign, showcasing its ability to predict novel synthetic and enzymatic pathways. Furthermore, we benchmark MHNpath against existing frameworks using the PaRoutes dataset, achieving a solution rate of 85.4% and replicating 69.2% of experimentally validated "gold-standard" pathways. Our case studies reveal that the tool can generate shorter, cheaper, moderate-temperature routes employing green solvents, as exemplified by compounds such as dronabinol, arformoterol, and lupinine.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dexterous Manipulation through Imitation Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.03515</link>
<guid>https://arxiv.org/abs/2504.03515</guid>
<content:encoded><![CDATA[
arXiv:2504.03515v5 Announce Type: replace-cross 
Abstract: Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning, details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curved representational Bregman divergences and their applications</title>
<link>https://arxiv.org/abs/2504.05654</link>
<guid>https://arxiv.org/abs/2504.05654</guid>
<content:encoded><![CDATA[
arXiv:2504.05654v3 Announce Type: replace-cross 
Abstract: By analogy to the terminology of curved exponential families in statistics, we define curved Bregman divergences as Bregman divergences restricted to nonlinear parameter subspaces and sub-dimensional Bregman divergences when the restrictions are linear. A common example of curved Bregman divergence is the cosine dissimilarity between normalized vectors. We show that the barycenter of a finite weighted set of parameters under a curved Bregman divergence amounts to the right Bregman projection onto the nonlinear subspace of the barycenter with respect to the full Bregman divergence. We demonstrate the significance of curved Bregman divergences with two examples: (1) symmetrized Bregman divergences, (2) pointwise symmetrized Bregman divergences, and (3) the Kullback-Leibler divergence between circular complex normal distributions. We explain how to reparameterize sub-dimensional Bregman divergences on simplicial sub-dimensional domains. We then consider monotonic embeddings to define representational curved Bregman divergences and show that the $\alpha$-divergences are representational curved Bregman divergences with respect to $\alpha$-embeddings of the probability simplex into the positive measure cone. As an application, we report an efficient method to calculate the intersection of a finite set of $\alpha$-divergence spheres.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformalized Decision Risk Assessment</title>
<link>https://arxiv.org/abs/2505.13243</link>
<guid>https://arxiv.org/abs/2505.13243</guid>
<content:encoded><![CDATA[
arXiv:2505.13243v3 Announce Type: replace-cross 
Abstract: In many operational settings, decision-makers must commit to actions before uncertainty resolves, but existing optimization tools rarely quantify how consistently a chosen decision remains optimal across plausible scenarios. This paper introduces CREDO -- Conformalized Risk Estimation for Decision Optimization, a distribution-free framework that quantifies the probability that a prescribed decision remains (near-)optimal across realizations of uncertainty. CREDO reformulates decision risk through the inverse feasible region -- the set of outcomes under which a decision is optimal -- and estimates its probability using inner approximations constructed from conformal prediction balls generated by a conditional generative model. This approach yields finite-sample, distribution-free lower bounds on the probability of decision optimality. The framework is model-agnostic and broadly applicable across a wide range of optimization problems. Extensive numerical experiments demonstrate that CREDO provides accurate, efficient, and reliable evaluations of decision optimality across various optimization settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-invariant Attention</title>
<link>https://arxiv.org/abs/2505.17083</link>
<guid>https://arxiv.org/abs/2505.17083</guid>
<content:encoded><![CDATA[
arXiv:2505.17083v2 Announce Type: replace-cross 
Abstract: One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title>
<link>https://arxiv.org/abs/2505.18148</link>
<guid>https://arxiv.org/abs/2505.18148</guid>
<content:encoded><![CDATA[
arXiv:2505.18148v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) face significant challenges with needle-in-ahaystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size, the length of the answer-containing document, has received little attention. We present the first systematic study of gold context size in long-context question answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning models), and more than 150K controlled runs. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This effect persists under rigorous confounder analysis: even after controlling for gold context position, answer token repetition, gold-to-distractor ratio, distractor volume, and domain specificity, gold context size remains a decisive, independent predictor of success. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v2 Announce Type: replace-cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations</title>
<link>https://arxiv.org/abs/2506.11262</link>
<guid>https://arxiv.org/abs/2506.11262</guid>
<content:encoded><![CDATA[
arXiv:2506.11262v2 Announce Type: replace-cross 
Abstract: Learning from Demonstration (LfD) is a popular approach for robots to acquire new skills, but most LfD methods suffer from imperfections in human demonstrations. Prior work typically treats these suboptimalities as random noise. In this paper we study non-optimal behaviors in non-expert demonstrations and show that they are systematic, forming what we call demonstration sidetracks. Using a public space study with 40 participants performing a long-horizon robot task, we recreated the setup in simulation and annotated all demonstrations. We identify four types of sidetracks (Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension control). Sidetracks appear frequently across participants, and their temporal and spatial distribution is tied to task context. We also find that users' control patterns depend on the control interface. These insights point to the need for better models of suboptimal demonstrations to improve LfD algorithms and bridge the gap between lab training and real-world deployment. All demonstrations, infrastructure, and annotations are available at https://github.com/AABL-Lab/Human-Demonstration-Sidetracks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable representation learning of quantum data enabled by probabilistic variational autoencoders</title>
<link>https://arxiv.org/abs/2506.11982</link>
<guid>https://arxiv.org/abs/2506.11982</guid>
<content:encoded><![CDATA[
arXiv:2506.11982v3 Announce Type: replace-cross 
Abstract: Interpretable machine learning is rapidly becoming a crucial tool for scientific discovery. Among existing approaches, variational autoencoders (VAEs) have shown promise in extracting the hidden physical features of some input data, with no supervision nor prior knowledge of the system at study. Yet, the ability of VAEs to create meaningful, interpretable representations relies on their accurate approximation of the underlying probability distribution of their input. When dealing with quantum data, VAEs must hence account for its intrinsic randomness and complex correlations. While VAEs have been previously applied to quantum data, they have often neglected its probabilistic nature, hindering the extraction of meaningful physical descriptors. Here, we demonstrate that two key modifications enable VAEs to learn physically meaningful latent representations: a decoder capable of faithfully reproduce quantum states and a probabilistic loss tailored to this task. Using benchmark quantum spin models, we identify regimes where standard methods fail while the representations learned by our approach remain meaningful and interpretable. Applied to experimental data from Rydberg atom arrays, the model autonomously uncovers the phase structure without access to prior labels, Hamiltonian details, or knowledge of relevant order parameters, highlighting its potential as an unsupervised and interpretable tool for the study of quantum systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI</title>
<link>https://arxiv.org/abs/2507.06092</link>
<guid>https://arxiv.org/abs/2507.06092</guid>
<content:encoded><![CDATA[
arXiv:2507.06092v2 Announce Type: replace-cross 
Abstract: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical stability for dense patterns in discrete attractor neural networks</title>
<link>https://arxiv.org/abs/2507.10383</link>
<guid>https://arxiv.org/abs/2507.10383</guid>
<content:encoded><![CDATA[
arXiv:2507.10383v3 Announce Type: replace-cross 
Abstract: Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and the outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>