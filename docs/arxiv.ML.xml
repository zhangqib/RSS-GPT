<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement</title>
<link>https://arxiv.org/abs/2506.00030</link>
<guid>https://arxiv.org/abs/2506.00030</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal fusion, Shapley-guided alternating training, modality imbalance, memory module, equilibrium deviation metric (EDM) <br>
Summary: 
The article proposes a Shapley-guided alternating training framework to address modality imbalance in multimodal fusion, prioritizing minor modalities to enhance fusion balance. The method uses Shapley Value-based scheduling for adaptive training sequencing and introduces a memory module for refining modality-specific representations. By adopting conventional and LLM-based backbones in the encoder module, the approach achieves state-of-the-art results across four multimodal benchmark datasets. The proposed equilibrium deviation metric (EDM) evaluates performance in balance and accuracy, demonstrating robustness under missing modalities. The findings highlight the potential of modality prioritization in balancing multimodal learning dynamics, offering a new paradigm for optimizing multimodal training. <br><br>Summary: <div>
arXiv:2506.00030v1 Announce Type: new 
Abstract: Multimodal fusion is susceptible to modality imbalance, where dominant modalities overshadow weak ones, easily leading to biased learning and suboptimal fusion, especially for incomplete modality conditions. To address this problem, we propose a Shapley-guided alternating training framework that adaptively prioritizes minor modalities to balance and thus enhance the fusion. Our method leverages Shapley Value-based scheduling to improve the training sequence adaptively, ensuring that under-optimized modalities receive sufficient learning. Additionally, we introduce the memory module to refine and inherit modality-specific representations with a cross-modal mapping mechanism to align features at both the feature and sample levels. To further validate the adaptability of the proposed approach, the encoder module empirically adopts both conventional and LLM-based backbones. With building up a novel multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we evaluate the performance in both balance and accuracy across four multimodal benchmark datasets, where our method achieves state-of-the-art (SOTA) results. Meanwhile, robustness analysis under missing modalities highlights its strong generalization capabilities. Accordingly, our findings reveal the untapped potential of alternating training, demonstrating that strategic modality prioritization fundamentally balances and promotes multimodal learning, offering a new paradigm for optimizing multimodal training dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing</title>
<link>https://arxiv.org/abs/2506.00039</link>
<guid>https://arxiv.org/abs/2506.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, fNIRS, auditory processing, AbsoluteNet, classification

Summary: 
AbsoluteNet, a novel deep learning architecture, has been developed for classifying auditory event-related responses captured by fNIRS. It utilizes spatio-temporal convolution and custom activation functions to achieve superior performance in comparison to existing models such as fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. AbsoluteNet demonstrated an accuracy of 87.0%, sensitivity of 84.8%, and specificity of 89.2% in binary classification tasks, outperforming fNIRSNET by 3.8% in accuracy. The success of AbsoluteNet highlights the significance of spatio-temporal feature aggregation and customized activation functions in decoding hemodynamic responses linked to auditory processing using fNIRS technology. This research showcases the potential of deep learning approaches in BCI applications and sheds light on optimizing neural network architectures for analyzing fNIRS data. 

Summary:<br><br>Keywords: deep learning, fNIRS, auditory processing, AbsoluteNet, classification<br><br>AbsoluteNet, a novel deep learning architecture, outperforms existing models in classifying auditory event-related responses recorded with fNIRS. It achieves 87.0% accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification, surpassing fNIRSNET by 3.8% in accuracy. The effectiveness of AbsoluteNet emphasizes the importance of spatio-temporal feature aggregation and customized activation functions in decoding hemodynamic responses related to auditory processing with fNIRS. This research showcases the potential of deep learning models for BCI applications and underscores the need for tailored neural network architectures for analyzing fNIRS data. <div>
arXiv:2506.00039v1 Announce Type: new 
Abstract: In recent years, deep learning (DL) approaches have demonstrated promising results in decoding hemodynamic responses captured by functional near-infrared spectroscopy (fNIRS), particularly in the context of brain-computer interface (BCI) applications. This work introduces AbsoluteNet, a novel deep learning architecture designed to classify auditory event-related responses recorded using fNIRS. The proposed network is built upon principles of spatio-temporal convolution and customized activation functions. Our model was compared against several models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The results showed that AbsoluteNet outperforms existing models, reaching 87.0% accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification, surpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings underscore the effectiveness of our proposed deep learning model in decoding hemodynamic responses related to auditory processing and highlight the importance of spatio-temporal feature aggregation and customized activation functions to better fit fNIRS dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Offline Reinforcement Learning with Online Delays</title>
<link>https://arxiv.org/abs/2506.00131</link>
<guid>https://arxiv.org/abs/2506.00131</guid>
<content:encoded><![CDATA[
<div> Offline-to-online deployment, reinforcement learning (RL), sim-to-real gap, interaction gap, delay-robust actions <br>
<br>
Summary:
Offline-to-online deployment of RL agents faces challenges in bridging the sim-to-real and interaction gaps. DT-CORL, a framework introduced in this study, addresses these challenges by producing delay-robust actions using a transformer-based belief predictor and improving sample efficiency compared to history-augmentation baselines. The framework demonstrates superior performance on D4RL benchmarks with varying delay settings, outperforming both history-augmentation and vanilla belief-based methods. By narrowing the sim-to-real latency gap and maintaining data efficiency, DT-CORL shows promise in addressing the challenges associated with deploying RL agents in real-world environments with delayed dynamics. <div>
arXiv:2506.00131v1 Announce Type: new 
Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than na\"ive history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning</title>
<link>https://arxiv.org/abs/2506.00135</link>
<guid>https://arxiv.org/abs/2506.00135</guid>
<content:encoded><![CDATA[
<div> Online learning, transductive learning, Empirical Risk Minimization (ERM), weak consistency oracles, concept class<br>
<br>Summary: <br>This study examines online and transductive online learning with interaction through ERM or weak consistency oracles on subsets of a concept class. Tight lower bounds are proven in standard online settings with ERM access. Existing online learning results with ERM access are shown to carry over to the weak consistency setting with an additional cost in oracle calls. For the transductive online model, optimal mistake bounds can be achieved using weak consistency oracle calls for general Littlestone classes. Limiting the learner to a certain number of queries is necessary for efficient transductive online learning. Randomized algorithms are employed to reduce oracle calls while maintaining mistake bounds for specific concept classes such as Thresholds and $k$-Intervals. In particular, a logarithmic number of ERM queries are sufficient for Thresholds, while a polynomial number of weak consistency queries are required for $k$-Intervals. <div>
arXiv:2506.00135v1 Announce Type: new 
Abstract: We study online and transductive online learning when the learner interacts with the concept class only via Empirical Risk Minimization (ERM) or weak consistency oracles on arbitrary instance subsets. This contrasts with standard online models, where the learner knows the entire class. The ERM oracle returns a hypothesis minimizing loss on a given subset, while the weak consistency oracle returns a binary signal indicating whether the subset is realizable by some concept. The learner is evaluated by the number of mistakes and oracle calls. In the standard online setting with ERM access, we prove tight lower bounds in both realizable and agnostic cases: $\Omega(2^{d_{VC}})$ mistakes and $\Omega(\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and $d_{LD}$ is the Littlestone dimension. We further show that existing online learning results with ERM access carry over to the weak consistency setting, incurring an additional $O(T)$ in oracle calls. We then consider the transductive online model, where the instance sequence is known but labels are revealed sequentially. For general Littlestone classes, we show that optimal realizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$ weak consistency oracle calls. On the negative side, we show that limiting the learner to $\Omega(T)$ weak consistency queries is necessary for transductive online learnability, and that restricting the learner to $\Omega(T)$ ERM queries is necessary to avoid exponential dependence on the Littlestone dimension. Finally, for certain concept classes, we reduce oracle calls via randomized algorithms while maintaining similar mistake bounds. In particular, for Thresholds on an unknown ordering, $O(\log T)$ ERM queries suffice; for $k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning</title>
<link>https://arxiv.org/abs/2506.00136</link>
<guid>https://arxiv.org/abs/2506.00136</guid>
<content:encoded><![CDATA[
<div> Diffusion autoencoders, latent variable, generative models, downstream tasks, domain transfer
<br>
Diffusion autoencoders (DAs) are models that use an input-dependent latent variable to capture representations in addition to the diffusion process. Another class of diffusion models focuses on learning the forward process. By combining design decisions from both classes, a new model termed DMZ is proposed. The DMZ model achieves effective representations for downstream tasks and domain transfer while also improving generative performance by reducing denoising steps compared to standard models. This innovative approach enhances the efficiency of generative modelling while maintaining the quality of learned representations.
<br><br>Summary: <div>
arXiv:2506.00136v1 Announce Type: new 
Abstract: Diffusion autoencoders (DAs) are variants of diffusion generative models that use an input-dependent latent variable to capture representations alongside the diffusion process. These representations, to varying extents, can be used for tasks such as downstream classification, controllable generation, and interpolation. However, the generative performance of DAs relies heavily on how well the latent variables can be modelled and subsequently sampled from. Better generative modelling is also the primary goal of another class of diffusion models -- those that learn their forward (noising) process. While effective at adjusting the noise process in an input-dependent manner, they must satisfy additional constraints derived from the terminal conditions of the diffusion process. Here, we draw a connection between these two classes of models and show that certain design decisions (latent variable choice, conditioning method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us to obtain the best of both worlds: effective representations as evaluated on downstream tasks, including domain transfer, as well as more efficient modelling and generation with fewer denoising steps compared to standard DMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective</title>
<link>https://arxiv.org/abs/2506.00152</link>
<guid>https://arxiv.org/abs/2506.00152</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, observational data, confounders, DeconfoundLM

Summary: 

Large language models (LLMs) are commonly used in various industries for content generation to improve key performance metrics. Fine-tuning these models with good-quality labeled data is crucial for aligning with human preferences and business objectives. However, conducting controlled experiments like A/B tests for gathering such data can be expensive and challenging. This study explores the potential of using observational data, which is often underutilized, for fine-tuning LLMs. While observational outcomes can serve as useful guidance, directly fine-tuning models on such data may lead to learning spurious correlations. The DeconfoundLM method proposed in this work aims to address this issue by removing the impact of known confounding variables from reward signals. Empirical evidence from real-world datasets showcases the importance of causal corrections in utilizing observational data effectively for LLM alignment. By mitigating the risks associated with confounding variables, DeconfoundLM enhances the recovery of causal relationships and improves the performance of fine-tuned models. <div>
arXiv:2506.00152v1 Announce Type: new 
Abstract: Large language models are being widely used across industries to generate content that contributes directly to key performance metrics, such as conversion rates. Pretrained models, however, often fall short when it comes to aligning with human preferences or optimizing for business objectives. As a result, fine-tuning with good-quality labeled data is essential to guide models to generate content that achieves better results. Controlled experiments, like A/B tests, can provide such data, but they are often expensive and come with significant engineering and logistical challenges. Meanwhile, companies have access to a vast amount of historical (observational) data that remains underutilized. In this work, we study the challenges and opportunities of fine-tuning LLMs using observational data. We show that while observational outcomes can provide valuable supervision, directly fine-tuning models on such data can lead them to learn spurious correlations. We present empirical evidence of this issue using various real-world datasets and propose DeconfoundLM, a method that explicitly removes the effect of known confounders from reward signals. Using simulation experiments, we demonstrate that DeconfoundLM improves the recovery of causal relationships and mitigates failure modes found in fine-tuning methods that ignore or naively incorporate confounding variables. Our findings highlight that while observational data presents risks, with the right causal corrections, it can be a powerful source of signal for LLM alignment. Please refer to the project page for code and related resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States</title>
<link>https://arxiv.org/abs/2506.00158</link>
<guid>https://arxiv.org/abs/2506.00158</guid>
<content:encoded><![CDATA[
<div> emerging, zeroth-order optimization, differential privacy, memory constraints, smooth loss functions
<br>
Zeroth-order optimization has become a popular method for fine-tuning large language models with domain-specific data while ensuring differential privacy and working within memory constraints. This approach is promising but lacks comprehensive analysis and design for privacy protection. The question of hidden-state differential privacy analysis in zeroth-order optimization has been unanswered, hindering further development in this area. This work addresses this gap by proving a convergent differential privacy bound for zeroth-order optimization, adapting the privacy amplification-by-iteration framework to smooth loss functions. This novel analysis leads to improved algorithmic designs for zeroth-order optimization that were previously unknown in the literature.
<br><br>Summary: <div>
arXiv:2506.00158v1 Announce Type: new 
Abstract: Zeroth-order optimization has emerged as a promising approach for fine-tuning large language models on domain-specific data, particularly under differential privacy (DP) and memory constraints. While first-order methods have been extensively studied from a privacy perspective, the privacy analysis and algorithmic design for zeroth-order methods remain significantly underexplored. A critical open question concerns hidden-state DP analysis: although convergent privacy bounds are known for first-order methods, it has remained unclear whether similar guarantees can be established for zeroth-order methods. In this work, we provide an affirmative answer by proving a convergent DP bound for zeroth-order optimization. Our analysis generalizes the celebrated privacy amplification-by-iteration framework to the setting of smooth loss functions in zeroth-order optimization. Furthermore, it induces better DP zeroth-order algorithmic designs that are previously unknown to the literature.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2506.00166</link>
<guid>https://arxiv.org/abs/2506.00166</guid>
<content:encoded><![CDATA[
<div> guardrail models, alignment training, Disentangled Safety Adapters, efficiency, flexibility

Summary:
Disentangled Safety Adapters (DSA) is introduced as a novel framework addressing challenges in AI safety. DSA decouples safety-specific computations from a task-optimized base model, utilizing lightweight adapters to enable diverse safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails outperform standalone models in hallucination detection, hate speech classification, and identifying unsafe inputs and responses. DSA-based safety alignment allows dynamic adjustment of alignment strength at inference time, offering a fine-grained trade-off between instruction following performance and model safety. Combining DSA safety guardrail with DSA safety alignment enhances context-dependent alignment strength, significantly boosting safety on StrongReject while maintaining high performance on MTBench. Overall, DSA presents a promising approach towards modular, efficient, and adaptable AI safety and alignment.<br><br>Summary: <div>
arXiv:2506.00166v1 Announce Type: new 
Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents</title>
<link>https://arxiv.org/abs/2506.00172</link>
<guid>https://arxiv.org/abs/2506.00172</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, benchmarking methodology, code-repair tasks, task difficulty, automatic generation

Summary:
The article introduces Breakpoint, a benchmarking methodology designed to assess the capabilities of large language models (LLMs) in solving complex code-repair tasks in real-world software repositories. Breakpoint automatically generates tasks by corrupting functions in these repositories, controlling task difficulty based on local and system-level reasoning dimensions. Through more than 900 generated tasks, Breakpoint demonstrates scalable difficulty levels, with success rates of state-of-the-art models ranging from 55% on easier tasks to 0% on the most challenging ones. This novel approach allows for the evaluation of LLMs in tasks that require swift comprehension and manipulation of intricate structures, crucial for fields like software engineering and scientific research. <div>
arXiv:2506.00172v1 Announce Type: new 
Abstract: Benchmarks for large language models (LLMs) have predominantly assessed short-horizon, localized reasoning. Existing long-horizon suites (e.g. SWE-bench) rely on manually curated issues, so expanding or tuning difficulty demands expensive human effort and evaluations quickly saturate. However, many real-world tasks, such as software engineering or scientific research, require agents to rapidly comprehend and manipulate novel, complex structures dynamically; evaluating these capabilities requires the ability to construct large and varied sets of problems for agents to solve. We introduce Breakpoint, a benchmarking methodology that automatically generates code-repair tasks by adversarially corrupting functions within real-world software repositories. Breakpoint systematically controls task difficulty along two clear dimensions: local reasoning (characterized by code complexity metrics such as cyclomatic complexity) and system-level reasoning (characterized by call-graph centrality and the number of simultaneously corrupted interdependent functions). In experiments across more than 900 generated tasks we demonstrate that our methodology can scale to arbitrary difficulty, with state-of-the-art models' success rates ranging from 55% on the easiest tasks down to 0% on the hardest.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountability Attribution: Tracing Model Behavior to Training Processes</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
<div> accountability, AI development pipelines, model behavior, training stages, estimators
Summary: 
The article addresses the issue of accountability attribution in modern AI development pipelines, where multiple stages such as pretraining and fine-tuning rounds play a role in the model's success or failure. The proposed framework aims to trace model behavior back to specific training stages by answering counterfactual questions about the effects of each stage. Estimators based on first-order approximations are introduced to efficiently quantify the effects of training stages without the need for retraining. These estimators consider elements such as learning rate schedules, momentum, and weight decay in optimization dynamics. Empirical demonstrations show that the approach can identify the training stages responsible for specific model behaviors, providing a practical tool for model analysis and promoting more accountable AI development. 
<br><br>Summary: <div>
arXiv:2506.00175v1 Announce Type: new 
Abstract: Modern AI development pipelines often involve multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment-with numerous model update steps within each stage. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the problem of accountability attribution, which aims to trace model behavior back to specific stages of the training process. To address this, we propose a general framework that answers counterfactual questions about stage effects: how would the model behavior have changed if the updates from a training stage had not been executed?. Within this framework, we introduce estimators based on first-order approximations that efficiently quantify the stage effects without retraining. Our estimators account for both the training data and key aspects of optimization dynamics, including learning rate schedules, momentum, and weight decay. Empirically, we demonstrate that our approach identifies training stages accountable for specific behaviors, offering a practical tool for model analysis and a step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach</title>
<link>https://arxiv.org/abs/2506.00181</link>
<guid>https://arxiv.org/abs/2506.00181</guid>
<content:encoded><![CDATA[
<div> stochastic differential equation, Distributed SGD, Distributed Compressed SGD, Distributed SignSGD, batch noise <br>
Summary: <br>
This article investigates the dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed SignSGD using stochastic differential equation approximations. The study considers smoothness and noise assumptions and explores the interactions between batch noise, stochastic gradient compression, and adaptivity. The analysis reveals that adaptive methods like Distributed SignSGD can converge successfully with standard learning rate scheduler assumptions, even with heavy-tailed noise. Conversely, Distributed (Compressed) SGD with pre-scheduled decaying learning rate struggles to converge unless the schedule incorporates an inverse dependency on the gradient norm, essentially behaving like an adaptive method in practice. <div>
arXiv:2506.00181v1 Announce Type: new 
Abstract: Using stochastic differential equation (SDE) approximations, we study the dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed SignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our analysis provides insights -- which we validate through simulation -- into the intricate interactions between batch noise, stochastic gradient compression, and adaptivity in this modern theoretical setup. For instance, we show that \textit{adaptive} methods such as Distributed SignSGD can successfully converge under standard assumptions on the learning rate scheduler, even under heavy-tailed noise. On the contrary, Distributed (Compressed) SGD with pre-scheduled decaying learning rate fails to achieve convergence, unless such a schedule also accounts for an inverse dependency on the gradient norm -- de facto falling back into an adaptive method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series</title>
<link>https://arxiv.org/abs/2506.00188</link>
<guid>https://arxiv.org/abs/2506.00188</guid>
<content:encoded><![CDATA[
<div> MLP-based mixer models, anomaly detection, multivariate time series, causal mechanism, cluster-aware, temporal dependencies<br>
<br>
Summary:<br>
Anomaly detection in time series data is crucial, and the lack of causality in MLP-based mixer models can hinder accurate detection. To address this, a novel cluster-aware causal mixer model is proposed, grouping channels into clusters based on correlations. Each cluster has a dedicated embedding layer, enhancing the model's ability to capture complex inter-channel relationships. The introduction of a causal mixer ensures that temporal dependencies are preserved while mixing information. Furthermore, an anomaly detection framework accumulates evidence over time to prevent false positives. The model operates online, making it suitable for real-time tasks. Experimental results on benchmark datasets show the model consistently achieves superior F1 scores, highlighting its effectiveness in multivariate time series anomaly detection tasks. <br><br> <div>
arXiv:2506.00188v1 Announce Type: new 
Abstract: Early and accurate detection of anomalies in time series data is critical, given the significant risks associated with false or missed detections. While MLP-based mixer models have shown promise in time series analysis, they lack a causality mechanism to preserve temporal dependencies inherent in the system. Moreover, real-world multivariate time series often contain numerous channels with diverse inter-channel correlations. A single embedding mechanism for all channels does not effectively capture these complex relationships. To address these challenges, we propose a novel cluster-aware causal mixer to effectively detect anomalies in multivariate time series. Our model groups channels into clusters based on their correlations, with each cluster processed through a dedicated embedding layer. In addition, we introduce a causal mixer in our model, which mixes the information while maintaining causality. Furthermore, we present an anomaly detection framework that accumulates the anomaly evidence over time to prevent false positives due to nominal outliers. Our proposed model operates in an online fashion, making it suitable for real-time time-series anomaly detection tasks. Experimental evaluations across six public benchmark datasets demonstrate that our model consistently achieves superior F1 scores.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models</title>
<link>https://arxiv.org/abs/2506.00198</link>
<guid>https://arxiv.org/abs/2506.00198</guid>
<content:encoded><![CDATA[
<div> MOFs, transformer-based framework, reinforcement learning, de novo design, generative modeling<br>
<br>
Summary: 
The article introduces a novel approach for the design of Metal-Organic Frameworks (MOFs) using a reinforcement learning-enhanced, transformer-based framework. The method involves a generative model trained on MOFid sequences, a transformer-based property predictor, and a reinforcement learning module to optimize MOF candidates based on desired properties. By integrating property feedback into the sequence generation process, the approach aims to accelerate the inverse design of MOFs with specific functional attributes. This innovative use of large language models coupled with reinforcement learning demonstrates the potential for advancing computational MOF discovery and expanding possibilities in materials chemistry. <div>
arXiv:2506.00198v1 Announce Type: new 
Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2506.00205</link>
<guid>https://arxiv.org/abs/2506.00205</guid>
<content:encoded><![CDATA[
<div> Keywords: rehearsal-based methods, continual learning, forgetting, generalization error, overparameterized linear models

Summary: 
Rehearsal-based methods have been effective in combating catastrophic forgetting in continual learning. This study investigates if sequential rehearsal, like human learning processes, can offer advantages over standard concurrent rehearsal. By analyzing overparameterized linear models, the researchers compare the two strategies and find that sequential rehearsal performs better in scenarios where tasks are less similar. Motivated by these findings, a Hybrid Rehearsal method is proposed, combining concurrent training for similar tasks and sequential revisiting for dissimilar tasks. The theoretical analysis demonstrates the benefits of this approach in mitigating forgetting and improving generalization performance. Experimental results with deep neural networks further validate the superiority of the hybrid method over standard concurrent rehearsal. This study contributes a comprehensive theoretical analysis of rehearsal-based continual learning strategies. 

<br><br>Summary: <div>
arXiv:2506.00205v1 Announce Type: new 
Abstract: Rehearsal-based methods have shown superior performance in addressing catastrophic forgetting in continual learning (CL) by storing and training on a subset of past data alongside new data in current task. While such a concurrent rehearsal strategy is widely used, it remains unclear if this approach is always optimal. Inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting, we explore whether sequential rehearsal can offer greater benefits for CL compared to standard concurrent rehearsal. To address this question, we conduct a theoretical analysis of rehearsal-based CL in overparameterized linear models, comparing two strategies: 1) Concurrent Rehearsal, where past and new data are trained together, and 2) Sequential Rehearsal, where new data is trained first, followed by revisiting past data sequentially. By explicitly characterizing forgetting and generalization error, we show that sequential rehearsal performs better when tasks are less similar. These insights further motivate a novel Hybrid Rehearsal method, which trains similar tasks concurrently and revisits dissimilar tasks sequentially. We characterize its forgetting and generalization performance, and our experiments with deep neural networks further confirm that the hybrid approach outperforms standard concurrent rehearsal. This work provides the first comprehensive theoretical analysis of rehearsal-based CL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models</title>
<link>https://arxiv.org/abs/2506.00209</link>
<guid>https://arxiv.org/abs/2506.00209</guid>
<content:encoded><![CDATA[
<div> Keywords: Cancer screening, CATCH-FM, healthcare records, risk prediction, EHR foundation models <br>
<br>
Summary: 
The article introduces CATCH-FM, a cancer pre-screening methodology that uses historical medical records to identify high-risk patients for further screening. By pretraining and fine-tuning foundation models on electronic healthcare records, CATCH-FM achieves strong efficacy with low risk, outperforming other models in cancer risk prediction. The methodology demonstrates robustness across different patient distributions, excels in capturing non-trivial cancer risk factors, and achieves state-of-the-art results in pancreatic cancer risk prediction. By operating in the ICD code space, CATCH-FM proves its ability to leverage healthcare data efficiently. The open-sourcing of the code will further contribute to advancing cancer screening methods and potentially saving lives. <div>
arXiv:2506.00209v1 Announce Type: new 
Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately, existing screening techniques require expensive and intrusive medical procedures, not globally available, resulting in too many lost would-be-saved lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation Models, a cancer pre-screening methodology that identifies high-risk patients for further screening solely based on their historical medical records. With millions of electronic healthcare records (EHR), we establish the scaling law of EHR foundation models pretrained on medical code sequences, pretrain compute-optimal foundation models of up to 2.4 billion parameters, and finetune them on clinician-curated cancer risk prediction cohorts. In our retrospective evaluation comprising of thirty thousand patients, CATCH-FM achieved strong efficacy (60% sensitivity) with low risk (99% specificity and Negative Predictive Value), outperforming feature-based tree models as well as general and medical large language models by large margins. Despite significant demographic, healthcare system, and EHR coding differences, CATCH-FM achieves state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot leaderboard, outperforming EHR foundation models pretrained using on-site patient data. Our analysis demonstrates the robustness of CATCH-FM in various patient distributions, the benefits of operating in the ICD code space, and its ability to capture non-trivial cancer risk factors. Our code will be open-sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00236</link>
<guid>https://arxiv.org/abs/2506.00236</guid>
<content:encoded><![CDATA[
<div> Parameter-efficient fine-tuning, LoRA, low-rank updates, pretrained weights, Localized LoRA <br>
<br>Summary:
Parameter-efficient fine-tuning methods like LoRA offer compact alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. Existing approaches often overlook spatial patterns by relying on global low-rank structures. This study introduces Localized LoRA, a framework modeling weight updates as low-rank matrices applied to structured blocks of the weight matrix. This enables dense, localized updates without increasing trainable parameters. Formal comparisons between global, diagonal-local, and fully localized low-rank approximations show that Localized LoRA consistently achieves lower approximation errors under matched parameter budgets. Experiments in synthetic and practical settings demonstrate that Localized LoRA provides a more expressive and adaptable alternative, facilitating efficient fine-tuning with improved performance. <div>
arXiv:2506.00236v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact and effective alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. However, most existing approaches rely on global low-rank structures, which can overlook spatial patterns spread across the parameter space. In this work, we propose Localized LoRA, a generalized framework that models weight updates as a composition of low-rank matrices applied to structured blocks of the weight matrix. This formulation enables dense, localized updates throughout the parameter space-without increasing the total number of trainable parameters. We provide a formal comparison between global, diagonal-local, and fully localized low-rank approximations, and show that our method consistently achieves lower approximation error under matched parameter budgets. Experiments on both synthetic and practical settings demonstrate that Localized LoRA offers a more expressive and adaptable alternative to existing methods, enabling efficient fine-tuning with improved performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeGLIF for Label Noise Robust Node Classification using GNNs</title>
<link>https://arxiv.org/abs/2506.00244</link>
<guid>https://arxiv.org/abs/2506.00244</guid>
<content:encoded><![CDATA[
<div> Keywords: denoising, graph data, label noise, node-level prediction, influence function

Summary:
DeGLIF is a denoising technique that uses clean data and the leave-one-out influence function to improve node-level prediction on graph data. It approximates the change in model parameters when a training point is removed and extends this to estimate the change in validation loss for GNNs. DeGLIF does not require knowledge of the noise model or level in the dataset, and two variants are proposed to identify noisy nodes. Computational experiments demonstrate DeGLIF's effectiveness, outperforming baseline algorithms in accuracy. One variant is proven to detect noisy points that increase risk. Overall, DeGLIF offers a robust method for denoising graph data and improving prediction accuracy. 

<br><br>Summary: <div>
arXiv:2506.00244v1 Announce Type: new 
Abstract: Noisy labelled datasets are generally inexpensive compared to clean labelled datasets, and the same is true for graph data. In this paper, we propose a denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence Function. DeGLIF uses a small set of clean data and the leave-one- out influence function to make label noise robust node-level prediction on graph data. Leave-one-out influence function approximates the change in the model parameters if a training point is removed from the training dataset. Recent advances propose a way to calculate the leave-one-out influence function for Graph Neural Networks (GNNs). We extend that recent work to estimate the change in validation loss, if a training node is removed from the training dataset. We use this estimate and a new theoretically motivated relabelling function to denoise the training dataset. We propose two DeGLIF variants to identify noisy nodes. Both these variants do not require any information about the noise model or the noise level in the dataset; DeGLIF also does not estimate these quantities. For one of these variants, we prove that the noisy points detected can indeed increase risk. We carry out detailed computational experiments on different datasets to show the effectiveness of DeGLIF. It achieves better accuracy than other baseline algorithms
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity</title>
<link>https://arxiv.org/abs/2506.00245</link>
<guid>https://arxiv.org/abs/2506.00245</guid>
<content:encoded><![CDATA[
<div> semantic entropy, uncertainty quantification, nearest neighbor, text generation tasks, language models

Summary:
Nearest neighbor-inspired black-box method is proposed for detecting hallucination in large language models by quantifying uncertainty effectively. Intra-cluster and inter-cluster similarities are taken into account, enhancing traditional entropy estimation. The method, extendable to white-box settings, outperforms semantic entropy in text generation tasks like question answering, text summarization, and machine translation across Phi3 and Llama3 models. Theoretical results support the method's generalization of semantic entropy. The code repository for this approach is available on GitHub for further exploration and implementation. <br><br>Summary: <div>
arXiv:2506.00245v1 Announce Type: new 
Abstract: Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at https://github.com/BigML-CS-UCLA/SNNE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming</title>
<link>https://arxiv.org/abs/2506.00247</link>
<guid>https://arxiv.org/abs/2506.00247</guid>
<content:encoded><![CDATA[
<div> Keywords: Convolutional Neural Networks, Quantum Computing, Optimization, Stochastic Gradient Descent, MNIST dataset

Summary: 
Convolutional Neural Networks (CNNs) are widely used in computer vision and Big Data analytics but require significant computational resources for training. Traditional training methods using back-propagation may be sub-optimal and time-consuming. Quantum computing offers a promising alternative to efficiently search complex optimization landscapes. This study proposes a hybrid optimization method combining Unconstrained Binary Quadratic Programming and Stochastic Gradient Descent to accelerate CNN training. Tested on the MNIST dataset, the approach achieves a 10-15% accuracy improvement over standard methods while maintaining similar execution times. These results demonstrate the potential of hybrid quantum-classical techniques in High-Performance Computing environments for Big Data and Deep Learning. However, successfully harnessing these benefits requires careful alignment of algorithmic structures with underlying quantum mechanisms. 

<br><br>Summary: <div>
arXiv:2506.00247v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big Data analytics but demand significant computational resources when trained on large-scale datasets. Conventional training via back-propagation (BP) with losses like Mean Squared Error or Cross-Entropy often requires extensive iterations and may converge sub-optimally. Quantum computing offers a promising alternative by leveraging superposition, tunneling, and entanglement to search complex optimization landscapes more efficiently. In this work, we propose a hybrid optimization method that combines an Unconstrained Binary Quadratic Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to accelerate CNN training. Evaluated on the MNIST dataset, our approach achieves a 10--15\% accuracy improvement over a standard BP-CNN baseline while maintaining similar execution times. These results illustrate the potential of hybrid quantum-classical techniques in High-Performance Computing (HPC) environments for Big Data and Deep Learning. Fully realizing these benefits, however, requires a careful alignment of algorithmic structures with underlying quantum mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction</title>
<link>https://arxiv.org/abs/2506.00259</link>
<guid>https://arxiv.org/abs/2506.00259</guid>
<content:encoded><![CDATA[
<div> Keywords: Remaining Useful Life Prediction, Convolutional Neural Networks, Vision Transformer, Permutation-based Approach, PHM Applications

Summary:
The article introduces the PerFormer, a permutation-based vision transformer approach designed to improve the accuracy of Remaining Useful Life (RUL) prediction for degradation systems. This approach addresses the challenge of applying Vision Transformer (ViT) to multivariate sensor data for RUL prediction by permuting time series data to mimic spatial characteristics of image data. A novel permutation loss function is utilized to guide the convergence of matrices towards a permutation matrix. Experimental results on NASA's C-MAPSS dataset demonstrate the superior performance of the PerFormer in RUL prediction compared to state-of-the-art methods using CNNs, RNNs, and other Transformer models. The study highlights the effectiveness and potential of the PerFormer in Prognostic and Health Management (PHM) applications. 

Summary: <br>
1. Introduction of PerFormer, a permutation-based vision transformer for RUL prediction. <br>
2. Addressing challenges in applying ViT to multivariate sensor data. <br>
3. Novel permutation loss function to guide matrix convergence. <br>
4. Superior performance of PerFormer in RUL prediction compared to CNNs, RNNs, and other Transformer models. <br>
5. Demonstration of PerFormer's effectiveness and potential in PHM applications. <div>
arXiv:2506.00259v1 Announce Type: new 
Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</title>
<link>https://arxiv.org/abs/2506.00286</link>
<guid>https://arxiv.org/abs/2506.00286</guid>
<content:encoded><![CDATA[
<div> risk-preferences, Markov decision process, model-based approach, PAC-bounds, policy-learning

Summary:<br>
The paper investigates the sample complexities of learning the optimal state-action value function and an optimal policy in a discounted Markov decision process with recursive entropic risk-preferences. The model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) approach is introduced to provide PAC-bounds on the difference between the estimated value functions and the optimal ones. The bounds exhibit exponential dependence on the effective horizon and the learner's risk-sensitivity. Lower bounds are derived to show the tightness of the PAC-bounds in both $\varepsilon$ and $\delta$, as well as in terms of the number of actions involved. The analysis highlights the complexity of learning in risk-sensitive settings and reveals the trade-offs between risk-preferences and the efficiency of learning optimal policies. <div>
arXiv:2506.00286v1 Announce Type: new 
Abstract: In this paper we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and $\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the strength of this dependence grows with the learners risk-sensitivity $|\beta|$. We also provide two lower bounds which shows that exponential dependence on $|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and that the PAC-bound on policy-learning is nearly tight in $A$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Protein Sequence Design through Designability Preference Optimization</title>
<link>https://arxiv.org/abs/2506.00297</link>
<guid>https://arxiv.org/abs/2506.00297</guid>
<content:encoded><![CDATA[
<div> Direct Preference Optimization, AlphaFold, Residue-level Designability Preference Optimization, LigandMPNN, EnhancedMPNN

Summary:
Direct Preference Optimization and Residue-level Designability Preference Optimization were used to improve protein sequence design methods. By integrating AlphaFold pLDDT scores and applying residue-level structural rewards, the training objective was redefined to increase designability. This approach significantly increased the success rate of in silico protein design, particularly in challenging enzyme design benchmarks. The fine-tuned LigandMPNN with ResiDPO, called EnhancedMPNN, demonstrated a nearly 3-fold improvement in design success rate, from 6.56% to 17.57%. This method focuses on generating sequences with high designability, ensuring that the designed sequences have a higher likelihood of folding into the desired structure. By optimizing at a residue-level granularity and decoupling optimization across residues, the ResiDPO technique allows for direct improvement in designability while preserving well-performing regions. Overall, this approach enhances the effectiveness and reliability of protein sequence design for de novo protein design projects. 

<br><br>Summary: <div>
arXiv:2506.00297v1 Announce Type: new 
Abstract: Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms</title>
<link>https://arxiv.org/abs/2506.00299</link>
<guid>https://arxiv.org/abs/2506.00299</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, alignment, evolutionary algorithms, black-box optimization, generative models

Summary: 
Evolutionary algorithms are introduced as an inference-time alignment framework for diffusion models, treating them as black-boxes to maximize alignment objectives in their latent space. The method allows for efficient alignment for differentiable and non-differentiable objectives, outperforming gradient-based methods on benchmark tests like DrawBench and Open Image Preferences. It also shows significant reductions in memory consumption, requiring 55% to 76% less GPU memory, and faster running times, being 72% to 80% quicker than gradient-based approaches. The evolutionary algorithms achieve higher alignment scores on the Open Image Preferences benchmark over 50 optimization steps compared to both gradient-based and gradient-free methods. <div>
arXiv:2506.00299v1 Announce Type: new 
Abstract: Diffusion models are state-of-the-art generative models in various domains, yet their samples often fail to satisfy downstream objectives such as safety constraints or domain-specific validity. Existing techniques for alignment require gradients, internal model access, or large computational budgets. We introduce an inference-time alignment framework based on evolutionary algorithms. We treat diffusion models as black-boxes and search their latent space to maximize alignment objectives. Our method enables efficient inference-time alignment for both differentiable and non-differentiable alignment objectives across a range of diffusion models. On the DrawBench and Open Image Preferences benchmark, our EA methods outperform state-of-the-art gradient-based and gradient-free inference-time methods. In terms of memory consumption, we require 55% to 76% lower GPU memory than gradient-based methods. In terms of running-time, we are 72% to 80% faster than gradient-based methods. We achieve higher alignment scores over 50 optimization steps on Open Image Preferences than gradient-based and gradient-free methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework</title>
<link>https://arxiv.org/abs/2506.00302</link>
<guid>https://arxiv.org/abs/2506.00302</guid>
<content:encoded><![CDATA[
<div> Dataset, Materials science, Multimodal learning, Machine learning, Annotation quality

Summary:
The article introduces MultiCrystalSpectrumSet (MCS-Set), a framework that enhances materials science datasets by incorporating atomic structures, 2D projections, and structured textual annotations. This expansion allows for multimodal property and summary prediction and constrained crystal generation with partial cluster supervision. The MCS-Set utilizes a human-in-the-loop pipeline to combine domain expertise with standardized descriptors for high-quality annotation. Evaluations using advanced language and vision-language models demonstrate significant performance differences between modalities, underscoring the importance of annotation quality for generalization. The MCS-Set serves as a basis for benchmarking multimodal models, improving annotation practices, and creating accessible, versatile materials science datasets. The dataset and implementations are accessible on GitHub at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet. 

<br><br>Summary: <div>
arXiv:2506.00302v1 Announce Type: new 
Abstract: Most materials science datasets are limited to atomic geometries (e.g., XYZ files), restricting their utility for multimodal learning and comprehensive data-centric analysis. These constraints have historically impeded the adoption of advanced machine learning techniques in the field. This work introduces MultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials datasets by integrating atomic structures with 2D projections and structured textual annotations, including lattice parameters and coordination metrics. MCS-Set enables two key tasks: (1) multimodal property and summary prediction, and (2) constrained crystal generation with partial cluster supervision. Leveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with standardized descriptors for high-quality annotation. Evaluations using state-of-the-art language and vision-language models reveal substantial modality-specific performance gaps and highlight the importance of annotation quality for generalization. MCS-Set offers a foundation for benchmarking multimodal models, advancing annotation practices, and promoting accessible, versatile materials science datasets. The dataset and implementations are available at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning via Regression Beyond Realizability</title>
<link>https://arxiv.org/abs/2506.00316</link>
<guid>https://arxiv.org/abs/2506.00316</guid>
<content:encoded><![CDATA[
<div> active learning, multiclass classification, surrogate risk minimization, realizability assumption, convex models

Summary:
In the context of active learning for multiclass classification, a new framework based on surrogate risk minimization is introduced. Unlike existing methods that rely on the realizability assumption, this framework operates beyond this constraint, allowing for applicability in practical, misspecified settings. The algorithm proposed in this work demonstrates that label and sample complexity can be achieved with weaker assumptions than realizability, as long as the model class is convex. The novel epoch-based approach involves fitting a model from the entire class to the queried data in each epoch and producing an improper classifier by aggregating these models. This departure from previous methods ensures comparable rates of performance, even in non-realizable settings where conventional approaches may fail to yield accurate results. <div>
arXiv:2506.00316v1 Announce Type: new 
Abstract: We present a new active learning framework for multiclass classification based on surrogate risk minimization that operates beyond the standard realizability assumption. Existing surrogate-based active learning algorithms crucially rely on realizability$\unicode{x2014}$the assumption that the optimal surrogate predictor lies within the model class$\unicode{x2014}$limiting their applicability in practical, misspecified settings. In this work we show that under conditions significantly weaker than realizability, as long as the class of models considered is convex, one can still obtain a label and sample complexity comparable to prior work. Despite achieving similar rates, the algorithmic approaches from prior works can be shown to fail in non-realizable settings where our assumption is satisfied. Our epoch-based active learning algorithm departs from prior methods by fitting a model from the full class to the queried data in each epoch and returning an improper classifier obtained by aggregating these models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2506.00329</link>
<guid>https://arxiv.org/abs/2506.00329</guid>
<content:encoded><![CDATA[
<div> transformers, video generation, adaptive layer-reuse, computational efficiency, Foresight

Summary:
Foresight introduces an adaptive layer-reuse technique to enhance the computational efficiency of Diffusion Transformers (DiTs) in video generation tasks. The proposed method dynamically identifies and reuses DiT block outputs across denoising steps, optimizing efficiency based on generation parameters like resolution and denoising schedules. By applying Foresight to OpenSora, Latte, and CogVideoX models, up to a 1.63x speedup in end-to-end processing is achieved while maintaining video quality. This approach addresses the computational cost challenges of spatial-temporal attention in DiTs by reducing redundancy without compromising performance. Foresight's source code is publicly available for further exploration and implementation. <div>
arXiv:2506.00329v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \texttt{https://github.com/STAR-Laboratory/foresight}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification</title>
<link>https://arxiv.org/abs/2506.00337</link>
<guid>https://arxiv.org/abs/2506.00337</guid>
<content:encoded><![CDATA[
<div> Method, Channel Imposed Fusion, Classification, EEG, ECG

Summary:
Channel Imposed Fusion (CIF) is introduced for transparent medical time series signal classification, enhancing signal-to-noise ratio and reducing redundancy. The method integrates with Temporal Convolutional Network (TCN) for efficiency and explicitness. Experimental results on EEG and ECG datasets show CIF outperforms existing state-of-the-art approaches in classification metrics, improving transparency in the process. This novel approach offers a new perspective for medical time series classification.<br><br>Summary: <div>
arXiv:2506.00337v1 Announce Type: new 
Abstract: The automatic classification of medical time series signals, such as electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in clinical decision support and early detection of diseases. Although Transformer based models have achieved notable performance by implicitly modeling temporal dependencies through self-attention mechanisms, their inherently complex architectures and opaque reasoning processes undermine their trustworthiness in high stakes clinical settings. In response to these limitations, this study shifts focus toward a modeling paradigm that emphasizes structural transparency, aligning more closely with the intrinsic characteristics of medical data. We propose a novel method, Channel Imposed Fusion (CIF), which enhances the signal-to-noise ratio through cross-channel information fusion, effectively reduces redundancy, and improves classification performance. Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN), known for its structural simplicity and controllable receptive field, to construct an efficient and explicit classification framework. Experimental results on multiple publicly available EEG and ECG datasets demonstrate that the proposed method not only outperforms existing state-of-the-art (SOTA) approaches in terms of various classification metrics, but also significantly enhances the transparency of the classification process, offering a novel perspective for medical time series classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Performance of Perforated Backpropagation through Further Experiments</title>
<link>https://arxiv.org/abs/2506.00356</link>
<guid>https://arxiv.org/abs/2506.00356</guid>
<content:encoded><![CDATA[
<div> Perforated Backpropagation, neural network optimization, dendrites, biological neurons, hackathon
Summary:<br><br>Perforated Backpropagation is a neural network optimization technique inspired by the computational significance of dendrites in biological neurons. This study presents additional experiments conducted during a hackathon at Carnegie Mellon Swartz Center in February 2025. Participants explored the algorithm's impact on their projects and found that it could achieve up to 90% model compression without compromising accuracy. Additionally, some projects experienced a 16% increase in accuracy compared to their original models. The collaboration between students and ML practitioners in Pittsburgh showcased the potential of Perforated Backpropagation in enhancing machine learning projects. <div>
arXiv:2506.00356v1 Announce Type: new 
Abstract: Perforated Backpropagation is a neural network optimization technique based on modern understanding of the computational importance of dendrites within biological neurons. This paper explores further experiments from the original publication, generated from a hackathon held at the Carnegie Mellon Swartz Center in February 2025. Students and local Pittsburgh ML practitioners were brought together to experiment with the Perforated Backpropagation algorithm on the datasets and models which they were using for their projects. Results showed that the system could enhance their projects, with up to 90% model compression without negative impact on accuracy, or up to 16% increased accuracy of their original models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees</title>
<link>https://arxiv.org/abs/2506.00362</link>
<guid>https://arxiv.org/abs/2506.00362</guid>
<content:encoded><![CDATA[
<div> Machine learning, constrained optimization, feasibility-seeking, neural network, real-time<br>
Summary:<br>
The study proposes FSNet, a novel neural network aimed at efficiently solving constrained optimization problems while ensuring feasibility. By integrating a feasibility-seeking step into the solution procedure, FSNet minimizes constraint violations in a differentiable manner, allowing for end-to-end training. Experimental results across various optimization problems show that FSNet can provide feasible solutions at faster speeds with comparable or superior solution quality to traditional solvers. The model proves effective in handling smooth/nonsmooth, convex/nonconvex problems, demonstrating its versatility and efficiency in real-world applications. <div>
arXiv:2506.00362v1 Announce Type: new 
Abstract: Efficiently solving constrained optimization problems is crucial for numerous real-world applications, yet traditional solvers are often computationally prohibitive for real-time use. Machine learning-based approaches have emerged as a promising alternative to provide approximate solutions at faster speeds, but they struggle to strictly enforce constraints, leading to infeasible solutions in practice. To address this, we propose the Feasibility-Seeking-Integrated Neural Network (FSNet), which integrates a feasibility-seeking step directly into its solution procedure to ensure constraint satisfaction. This feasibility-seeking step solves an unconstrained optimization problem that minimizes constraint violations in a differentiable manner, enabling end-to-end training and providing guarantees on feasibility and convergence. Our experiments across a range of different optimization problems, including both smooth/nonsmooth and convex/nonconvex problems, demonstrate that FSNet can provide feasible solutions with solution quality comparable to (or in some cases better than) traditional solvers, at significantly faster speeds.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Insights into Data-Oblivious Critical Layers in Large Language Models</title>
<link>https://arxiv.org/abs/2506.00382</link>
<guid>https://arxiv.org/abs/2506.00382</guid>
<content:encoded><![CDATA[
<div> CKA, layer analysis, pre-fine-tuned LLMs, spectral analysis, domain adaptation <br>
Summary: 
The study investigates the evolution of feature representations across layers in large language models (LLMs) using a data-oblivious approach. Critical layers in pre-fine-tuned LLMs are identified by analyzing representation dynamics through Centered Kernel Alignment (CKA). It is observed that layers experiencing significant shifts in representation space are most affected during fine-tuning, consistently across tasks for a given model. Spectral analysis reveals that these shifts are influenced by changes in the top principal components, representing semantic transitions. In practical applications, fine-tuning critical layers proves more effective for efficient domain adaptation, leading to greater loss reduction compared to non-critical layers. Additionally, freezing critical layers assists in backdoor defense, reducing attack success rates by up to 40%. <div>
arXiv:2506.00382v1 Announce Type: new 
Abstract: Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a data-oblivious approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment(CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning--a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions. We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-Learning-Driven Prefetching for Far Memory</title>
<link>https://arxiv.org/abs/2506.00384</link>
<guid>https://arxiv.org/abs/2506.00384</guid>
<content:encoded><![CDATA[
<div> Keywords: far-memory, deep learning, data prefetching, runtime performance, software optimization <br>
<br>
Summary: <br>
The article introduces FarSight, a far-memory system that utilizes deep learning for efficient data prefetching in modern software systems facing performance demands. FarSight separates application semantics from memory layout, enabling offline-trained deep learning models to predict access patterns using a compact vocabulary. Through asynchronous inference, lookahead prediction, and cache-resident models, FarSight achieves high prediction accuracy with low runtime overhead. Evaluation on four data-intensive workloads shows FarSight outperforms existing far-memory systems by up to 3.6 times. This highlights the potential of applying modern machine learning techniques to address complex, performance-critical software runtime challenges. <div>
arXiv:2506.00384v1 Announce Type: new 
Abstract: Modern software systems face increasing runtime performance demands, particularly in emerging architectures like far memory, where local-memory misses incur significant latency. While machine learning (ML) has proven effective in offline systems optimization, its application to high-frequency, runtime-level problems remains limited due to strict performance, generalization, and integration constraints. We present FarSight, a Linux-based far-memory system that leverages deep learning (DL) to efficiently perform accurate data prefetching. FarSight separates application semantics from runtime memory layout, allowing offline-trained DL models to predict access patterns using a compact vocabulary of ordinal possibilities, resolved at runtime through lightweight mapping structures. By combining asynchronous inference, lookahead prediction, and a cache-resident DL model, FarSight achieves high prediction accuracy with low runtime overhead. Our evaluation of FarSight on four data-intensive workloads shows that it outperforms the state-of-the-art far-memory system by up to 3.6 times. Overall, this work demonstrates the feasibility and advantages of applying modern ML techniques to complex, performance-critical software runtime problems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries</title>
<link>https://arxiv.org/abs/2506.00388</link>
<guid>https://arxiv.org/abs/2506.00388</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference-based reinforcement learning, human preferences, trajectory embedding space, unambiguous queries, CLARIFY<br>
Summary:<br>
Preference-based reinforcement learning (PbRL) aims to infer reward functions from human preference comparisons, eliminating the need for explicit reward engineering. However, labeling clear preferences between similar segments is challenging for humans, leading to reduced label efficiency. In response to this, the offline PbRL method CLARIFY is introduced, which focuses on resolving ambiguous feedback by learning a trajectory embedding space that incorporates preference information. By ensuring clearly distinguished segments are spaced apart, CLARIFY facilitates the selection of more unambiguous queries. Experimental results show that CLARIFY outperforms baseline methods in scenarios with non-ideal teachers and real human feedback. This approach not only improves query selection but also results in meaningful trajectory embeddings. Overall, CLARIFY addresses the challenge of ambiguous feedback in PbRL, enhancing its practical applicability in real-world settings. <br><br>Summary: <div>
arXiv:2506.00388v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL's real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias as a Virtue: Rethinking Generalization under Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.00407</link>
<guid>https://arxiv.org/abs/2506.00407</guid>
<content:encoded><![CDATA[
<div> Framework, machine learning, generalization, bias, validation
<br>
Summary: 
The article introduces the Adaptive Distribution Bridge (ADB) framework, which challenges traditional validation paradigms in machine learning. It demonstrates that higher in-distribution (ID) bias can actually lead to better out-of-distribution (OOD) generalization. By introducing controlled statistical diversity during training, ADB allows models to develop bias profiles that improve generalization across distributions. Empirical evaluations on various datasets show that ADB significantly enhances OOD generalization, with mean error reductions of up to 26.8% compared to traditional cross-validation. The framework also consistently identifies high-performing training strategies, with percentile ranks often exceeding 74.4%. This work provides a practical approach for improving generalization in machine learning models and offers a theoretical framework for reevaluating the role of bias in achieving robustness. 
<br> <div>
arXiv:2506.00407v1 Announce Type: new 
Abstract: Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering</title>
<link>https://arxiv.org/abs/2506.00410</link>
<guid>https://arxiv.org/abs/2506.00410</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, clustering, contrastive learning, shrinkage estimator, hierarchical Bayesian estimation

Summary:
JojoSCL is introduced as a self-supervised contrastive learning framework for clustering single-cell RNA sequencing data. It incorporates a shrinkage estimator based on hierarchical Bayesian estimation to adjust gene expression estimates and reduce intra-cluster dispersion. The framework is optimized using Stein's Unbiased Risk Estimate (SURE) to refine both instance-level and cluster-level contrastive learning. Experimental results on ten scRNA-seq datasets demonstrate that JojoSCL outperforms existing clustering methods consistently. The practicality of JojoSCL is validated through robustness analysis and ablation studies. The code for JojoSCL is open-source and available on GitHub. <div>
arXiv:2506.00410v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular processes by enabling gene expression analysis at the individual cell level. Clustering allows for the identification of cell types and the further discovery of intrinsic patterns in single-cell data. However, the high dimensionality and sparsity of scRNA-seq data continue to challenge existing clustering models. In this paper, we introduce JojoSCL, a novel self-supervised contrastive learning framework for scRNA-seq clustering. By incorporating a shrinkage estimator based on hierarchical Bayesian estimation, which adjusts gene expression estimates towards more reliable cluster centroids to reduce intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate (SURE), JojoSCL refines both instance-level and cluster-level contrastive learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL consistently outperforms prevalent clustering methods, with further validation of its practicality through robustness analysis and ablation studies. JojoSCL's code is available at: https://github.com/ziwenwang28/JojoSCL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title>
<link>https://arxiv.org/abs/2506.00416</link>
<guid>https://arxiv.org/abs/2506.00416</guid>
<content:encoded><![CDATA[
<div> Optimized Second-Order FL, Personalized Healthcare Systems, Federated Edge Learning, Blockchain, Privacy and Security <br>
Summary: <br>
The study introduces a new framework called BFEL (blockchain-enhanced federated edge learning) for personalized healthcare systems. It utilizes an optimized second-order FL approach, FedCurv, which maintains stability and consistency in non-iid datasets while improving personalized model training. The framework minimizes communication rounds required for convergence and manages personalized training on heterogeneous data. Incorporating Ethereum-based model aggregation ensures trust, verifiability, and auditability, while public key encryption enhances privacy and security. Experimental results using federated CNNs and MLPs on various datasets illustrate the high efficiency and scalability of the proposed framework. <div>
arXiv:2506.00416v1 Announce Type: new 
Abstract: Federated learning (FL) has attracted increasing attention to mitigate security and privacy challenges in traditional cloud-centric machine learning models specifically in healthcare ecosystems. FL methodologies enable the training of global models through localized policies, allowing independent operations at the edge clients' level. Conventional first-order FL approaches face several challenges in personalized model training due to heterogeneous non-independent and identically distributed (non-iid) data of each edge client. Recently, second-order FL approaches maintain the stability and consistency of non-iid datasets while improving personalized model training. This study proposes and develops a verifiable and auditable optimized second-order FL framework BFEL (blockchain-enhanced federated edge learning) based on optimized FedCurv for personalized healthcare systems. FedCurv incorporates information about the importance of each parameter to each client's task (through Fisher Information Matrix) which helps to preserve client-specific knowledge and reduce model drift during aggregation. Moreover, it minimizes communication rounds required to achieve a target precision convergence for each edge client while effectively managing personalized training on non-iid and heterogeneous data. The incorporation of Ethereum-based model aggregation ensures trust, verifiability, and auditability while public key encryption enhances privacy and security. Experimental results of federated CNNs and MLPs utilizing Mnist, Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2506.00420</link>
<guid>https://arxiv.org/abs/2506.00420</guid>
<content:encoded><![CDATA[
<div> model architecture, spatiotemporal correlation, two-stage training, anomaly detection, WSNs

Summary:
The proposed MTAD-RD anomaly detection model addresses challenges in WSN anomaly detection by integrating spatiotemporal correlation features and implementing a two-stage training strategy. The model architecture includes a RetNet enhanced by a CR module, multigranular feature fusion, and a graph attention network module to extract internode correlation information for improved anomaly detection. A two-stage training approach involves a contrastive learning proxy task for unlabeled data and a sample sampler for addressing sample imbalance. In experiments on real datasets, the MTAD-RD method achieved a high F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods. The serialized inference characteristic of the model also helps reduce inference overhead, making it a reliable and efficient solution for detecting anomalies in WSN data. 

<br><br>Summary: <div>
arXiv:2506.00420v1 Announce Type: new 
Abstract: Detecting anomalies in the data collected by WSNs can provide crucial evidence for assessing the reliability and stability of WSNs. Existing methods for WSN anomaly detection often face challenges such as the limited extraction of spatiotemporal correlation features, the absence of sample labels, few anomaly samples, and an imbalanced sample distribution. To address these issues, a spatiotemporal correlation detection model (MTAD-RD) considering both model architecture and a two-stage training strategy perspective is proposed. In terms of model structure design, the proposed MTAD-RD backbone network includes a retentive network (RetNet) enhanced by a cross-retention (CR) module, a multigranular feature fusion module, and a graph attention network module to extract internode correlation information. This proposed model can integrate the intermodal correlation features and spatial features of WSN neighbor nodes while extracting global information from time series data. Moreover, its serialized inference characteristic can remarkably reduce inference overhead. For model training, a two-stage training approach was designed. First, a contrastive learning proxy task was designed for time series data with graph structure information in WSNs, enabling the backbone network to learn transferable features from unlabeled data using unsupervised contrastive learning methods, thereby addressing the issue of missing sample labels in the dataset. Then, a caching-based sample sampler was designed to divide samples into few-shot and contrastive learning data. A specific joint loss function was developed to jointly train the dual-graph discriminator network to address the problem of sample imbalance effectively. In experiments carried out on real public datasets, the designed MTAD-RD anomaly detection method achieved an F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</title>
<link>https://arxiv.org/abs/2506.00424</link>
<guid>https://arxiv.org/abs/2506.00424</guid>
<content:encoded><![CDATA[
<div> Sparse tensor programs, deep learning, graph analytics, specialized hardware accelerators, COGNATE.

Summary:
COGNATE is a framework designed to optimize sparse tensor programs for specialized hardware accelerators. It addresses challenges such as sensitivity to input variations and reliance on expensive simulators. The framework leverages data samples from general-purpose hardware for cost model training, followed by fine-tuning on emerging hardware. By exploiting input feature homogeneity and mitigating heterogeneity, COGNATE achieves comparable performance to accelerator-specific models with just 5% of the data samples. Extensive experiments demonstrate that COGNATE outperforms existing techniques, with average speedups of 1.47x for SpMM and 1.39x for SDDMM. <div>
arXiv:2506.00424v1 Announce Type: new 
Abstract: Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer</title>
<link>https://arxiv.org/abs/2506.00431</link>
<guid>https://arxiv.org/abs/2506.00431</guid>
<content:encoded><![CDATA[
<div> Proposed method, SAM interpretability, Temporal dynamics, Interactive dynamics, Efficiency <br>
<br>
Summary: TIDFormer is introduced as a dynamic graph TransFormer that efficiently captures temporal and interactive dynamics. The self-attention mechanism (SAM) used is interpretable and effectively models changes in historical interaction patterns. Time partitioning information and interaction embeddings are utilized for bipartite and non-bipartite graphs, extracted from first-order neighbors. TIDFormer excels in performance compared to existing models on various dynamic graph datasets, demonstrating its effectiveness. Additionally, TIDFormer exhibits efficiency advantages over previous Transformer-based methods, making it a promising approach for dynamic graph modeling. <div>
arXiv:2506.00431v1 Announce Type: new 
Abstract: Due to the proficiency of self-attention mechanisms (SAMs) in capturing dependencies in sequence modeling, several existing dynamic graph neural networks (DGNNs) utilize Transformer architectures with various encoding designs to capture sequential evolutions of dynamic graphs. However, the effectiveness and efficiency of these Transformer-based DGNNs vary significantly, highlighting the importance of properly defining the SAM on dynamic graphs and comprehensively encoding temporal and interactive dynamics without extra complex modules. In this work, we propose TIDFormer, a dynamic graph TransFormer that fully exploits Temporal and Interactive Dynamics in an efficient manner. We clarify and verify the interpretability of our proposed SAM, addressing the open problem of its uninterpretable definitions on dynamic graphs in previous works. To model the temporal and interactive dynamics, respectively, we utilize the calendar-based time partitioning information and extract informative interaction embeddings for both bipartite and non-bipartite graphs using merely the sampled first-order neighbors. In addition, we jointly model temporal and interactive features by capturing potential changes in historical interaction patterns through a simple decomposition. We conduct extensive experiments on several dynamic graph datasets to verify the effectiveness and efficiency of TIDFormer. The experimental results demonstrate that TIDFormer excels, outperforming state-of-the-art models across most datasets and experimental settings. Furthermore, TIDFormer exhibits significant efficiency advantages compared to previous Transformer-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel Normalization for Time Series Channel Identification</title>
<link>https://arxiv.org/abs/2506.00432</link>
<guid>https://arxiv.org/abs/2506.00432</guid>
<content:encoded><![CDATA[
<div> Channel identifiability (CID), Channel Normalization (CN), Adaptive CN (ACN), Prototypical CN (PCN), time series modeling <br>
Summary: 
Channel identifiability (CID) in time series (TS) modeling is crucial to distinguish between individual channels. A new normalization strategy called Channel Normalization (CN) enhances CID by assigning distinct parameters to each channel. Adaptive CN (ACN) adjusts parameters based on input TS, improving adaptability. Prototypical CN (PCN) uses learnable prototypes instead of per-channel parameters, making it applicable to datasets with unknown or varying channels. Applying CN and its variants to various TS models results in significant performance gains for both non-CID and CID models. The success of the approach is analyzed from an information theory perspective. To implement this approach, the code is available at https://github.com/seunghan96/CN. <br> <div>
arXiv:2506.00432v1 Announce Type: new 
Abstract: Channel identifiability (CID) refers to the ability to distinguish between individual channels in time series (TS) modeling. The absence of CID often results in producing identical outputs for identical inputs, disregarding channel-specific characteristics. In this paper, we highlight the importance of CID and propose Channel Normalization (CN), a simple yet effective normalization strategy that enhances CID by assigning distinct affine transformation parameters to each channel. We further extend CN in two ways: 1) Adaptive CN (ACN) dynamically adjusts parameters based on the input TS, improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a set of learnable prototypes instead of per-channel parameters, enabling applicability to datasets with unknown or varying number of channels and facilitating use in TS foundation models. We demonstrate the effectiveness of CN and its variants by applying them to various TS models, achieving significant performance gains for both non-CID and CID models. In addition, we analyze the success of our approach from an information theory perspective. Code is available at https://github.com/seunghan96/CN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
<link>https://arxiv.org/abs/2506.00436</link>
<guid>https://arxiv.org/abs/2506.00436</guid>
<content:encoded><![CDATA[
<div> method, identifying, potential customers, targeted marketing, learning

Summary:
- The study proposes a method for targeted marketing using learning from positive and unlabeled data (PU learning).
- It addresses a scenario where a company can only observe customers who have made purchases and aims to market effectively to customers who lack loyalty.
- By focusing on individuals interested in a product but lacking loyalty, marketing efforts can be more efficient.
- The proposed algorithm, called double PU learning, combines two losses derived from standard PU learning settings to identify potential customers without loyalty.
- Numerical experiments confirm the effectiveness of the algorithm for the targeted marketing problem. 

<br><br>Summary: <div>
arXiv:2506.00436v1 Announce Type: new 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00437</link>
<guid>https://arxiv.org/abs/2506.00437</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, explanation methods, reliability, confidence scoring, GIB-CC <br>
Summary: 
Explaining Graph Neural Networks (GNNs) is crucial for interpretability. Existing post-hoc explanation methods for GNNs lack reliability, especially when dealing with out-of-distribution data. To address this, a new framework called ConfExplainer is introduced, incorporating a confidence scoring module based on the theoretical concept of Generalized Graph Information Bottleneck with Confidence Constraint (GIB-CC). This framework enhances the trustworthiness and robustness of GNN explanations by quantifying the reliability of generated explanations. Experimental results demonstrate the effectiveness of the confidence score in improving the reliability of explanations provided by GNNs. ConfExplainer offers a promising solution to the challenge of interpreting GNN predictions, particularly in scenarios involving unknown test datasets.<br><br>Summary: <div>
arXiv:2506.00437v1 Announce Type: new 
Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention due to the need for interpretability, enabling users to understand the behavior of these black-box models better and extract valuable insights from their predictions. While numerous post-hoc instance-level explanation methods have been proposed to interpret GNN predictions, the reliability of these explanations remains uncertain, particularly in the out-of-distribution or unknown test datasets. In this paper, we address this challenge by introducing an explainer framework with the confidence scoring module ( ConfExplainer), grounded in theoretical principle, which is generalized graph information bottleneck with confidence constraint (GIB-CC), that quantifies the reliability of generated explanations. Experimental results demonstrate the superiority of our approach, highlighting the effectiveness of the confidence score in enhancing the trustworthiness and robustness of GNN explanations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge</title>
<link>https://arxiv.org/abs/2506.00438</link>
<guid>https://arxiv.org/abs/2506.00438</guid>
<content:encoded><![CDATA[
<div> PointODE, ResNet-like architecture, point cloud feature extraction, Neural ODE, lightweight version <br>
Summary: <br>
PointODE introduces a parameter-efficient ResNet-like architecture for point cloud feature extraction, utilizing Neural ODE to compress the model and handle non-uniform point distributions. The lightweight PointODE-Elite version with 0.58M parameters includes a dedicated FPGA accelerator that speeds up inference by 3.7x compared to a CPU, with 3.5x better energy efficiency. Despite its simplicity, PointODE-Elite achieves competitive accuracy on classification datasets, improving the trade-off between accuracy and inference cost. <div>
arXiv:2506.00438v1 Announce Type: new 
Abstract: Embedded edge devices are often used as a computing platform to run real-world point cloud applications, but recent deep learning-based methods may not fit on such devices due to limited resources. In this paper, we aim to fill this gap by introducing PointODE, a parameter-efficient ResNet-like architecture for point cloud feature extraction based on a stack of MLP blocks with residual connections. We leverage Neural ODE (Ordinary Differential Equation), a continuous-depth version of ResNet originally developed for modeling the dynamics of continuous-time systems, to compress PointODE by reusing the same parameters across MLP blocks. The point-wise normalization is proposed for PointODE to handle the non-uniform distribution of feature points. We introduce PointODE-Elite as a lightweight version with 0.58M trainable parameters and design its dedicated accelerator for embedded FPGAs. The accelerator consists of a four-stage pipeline to parallelize the feature extraction for multiple points and stores the entire parameters on-chip to eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53 CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature extraction by 4.9x, leading to 3.7x faster inference and 3.5x better energy-efficiency. Despite the simple architecture, PointODE-Elite shows competitive accuracy to the state-of-the-art models on both synthetic and real-world classification datasets, greatly improving the trade-off between accuracy and inference cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</title>
<link>https://arxiv.org/abs/2506.00439</link>
<guid>https://arxiv.org/abs/2506.00439</guid>
<content:encoded><![CDATA[
<div> Ensembling, Large Language Models, Reinforcement Learning, Markov Decision Process, RLAE Ensemble<br>
Summary:<br>
Ensembling large language models (LLMs) with fixed weighting strategies is limiting. This work proposes RLAE, a framework that uses Reinforcement Learning to dynamically adjust ensemble weights based on input context and intermediate generation states. RLAE outperforms conventional methods by up to 3.3% accuracy points on various tasks. The RL agent in RLAE is trained using rewards tied to output quality. Implementing RLAE with single-agent and multi-agent reinforcement learning algorithms shows significant improvements. RLAE showcases better generalization abilities across tasks, without needing retraining. It also achieves lower time latency, making it an effective framework for LLM ensembling.<br><br>Summary: <div>
arXiv:2506.00439v1 Announce Type: new 
Abstract: Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2506.00440</link>
<guid>https://arxiv.org/abs/2506.00440</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Personalized Federated Learning, Population Stability Index, Data heterogeneity, Non-IIDness <br>
<br>
Summary: <br>
The article introduces PSI-PFL, a client selection framework for Personalized Federated Learning that uses the Population Stability Index (PSI) to address data heterogeneity challenges in non-independent and identically distributed (non-IID) data scenarios. The approach aims to mitigate label skew, a key factor affecting Federated Learning (FL) performance. Experimental results across various data types show that PSI-PFL significantly improves global model accuracy, surpassing existing baselines by up to 10% in non-IID settings. By selecting more homogeneous clients based on PSI, the framework enhances FL performance while ensuring fairer local model performance. Overall, PSI-PFL offers practical benefits in scenarios where data privacy and heterogeneity are crucial considerations. <br> <div>
arXiv:2506.00440v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized machine learning (ML) model training while preserving data privacy by keeping data localized across clients. However, non-independent and identically distributed (non-IID) data across clients poses a significant challenge, leading to skewed model updates and performance degradation. Addressing this, we propose PSI-PFL, a novel client selection framework for Personalized Federated Learning (PFL) that leverages the Population Stability Index (PSI) to quantify and mitigate data heterogeneity (so-called non-IIDness). Our approach selects more homogeneous clients based on PSI, reducing the impact of label skew, one of the most detrimental factors in FL performance. Experimental results over multiple data modalities (tabular, image, text) demonstrate that PSI-PFL significantly improves global model accuracy, outperforming state-of-the-art baselines by up to 10\% under non-IID scenarios while ensuring fairer local performance. PSI-PFL enhances FL performance and offers practical benefits in applications where data privacy and heterogeneity are critical.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction</title>
<link>https://arxiv.org/abs/2506.00453</link>
<guid>https://arxiv.org/abs/2506.00453</guid>
<content:encoded><![CDATA[
<div> Dynamic graphs, Dynamic graph neural network models, Dowker Zigzag Persistence (DZP), TMetaNet, Meta-learning<br>
<br>
Dynamic graphs pose challenges for traditional graph learning methods due to their changing structures. To address this, the authors propose a new method called Dowker Zigzag Persistence (DZP) that captures high-order features of dynamic graphs. Based on this, they introduce TMetaNet, a meta-learning parameter update model that incorporates dynamic topological features to enable more effective adaptation across snapshots. Experimental results on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, showcasing its potential for meta-learning and dynamic graph analysis.<br><br>Summary: <div>
arXiv:2506.00453v1 Announce Type: new 
Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional graph learning due to their changing structures and temporal dependencies. Recent advancements have shown potential in addressing these challenges by developing suitable meta-learning-based dynamic graph neural network models. However, most meta-learning approaches for dynamic graphs rely on fixed weight update parameters, neglecting the essential intrinsic complex high-order topological information of dynamically evolving graphs. We have designed Dowker Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent homology representation method based on Dowker complex and zigzag persistence, to capture the high-order features of dynamic graphs. Armed with the DZP ideas, we propose TMetaNet, a new meta-learning parameter update model based on dynamic topological features. By utilizing the distances between high-order topological features, TMetaNet enables more effective adaptation across snapshots. Experiments on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, illustrating its high potential for meta-learning and dynamic graph analysis. Our code is available at https://github.com/Lihaogx/TMetaNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models</title>
<link>https://arxiv.org/abs/2506.00457</link>
<guid>https://arxiv.org/abs/2506.00457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, zero-shot forecasting, state-of-the-art models, sensitivity to noise, fine-tuning<br>
Summary:<br>
Large Language Models (LLMs) have shown great performance in various tasks but struggle as zero-shot forecasters due to sensitivity to noise. Recent studies indicate LLMs may not be as effective in forecasting compared to domain-specific models. The study evaluates LLMs as zero-shot forecasters and finds they often underperform even simple domain-specific models. Solutions to reduce sensitivity to noise in LLMs have been explored but remain challenging. It is suggested that instead of focusing on zero-shot forecasting, fine-tuning LLMs for better processing of numerical sequences shows more promise. The experimental code is available for further exploration. The findings highlight the need for rigorous validation when assessing the effectiveness of LLMs in forecasting tasks. <div>
arXiv:2506.00457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting through prompting alone, recent studies suggest that LLMs lack inherent effectiveness in forecasting. Given these conflicting findings, a rigorous validation is essential for drawing reliable conclusions. In this paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared to state-of-the-art domain-specific models. Our experiments show that LLM-based zero-shot forecasters often struggle to achieve high accuracy due to their sensitivity to noise, underperforming even simple domain-specific models. We have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot setting, but improving their robustness remains a significant challenge. Our findings suggest that rather than emphasizing zero-shot forecasting, a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences. Our experimental code is available at https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Hanabi</title>
<link>https://arxiv.org/abs/2506.00458</link>
<guid>https://arxiv.org/abs/2506.00458</guid>
<content:encoded><![CDATA[
<div> Keywords: Hanabi, reinforcement learning, tabular agents, deep reinforcement learning, TD algorithms

Summary:
Hanabi, a cooperative card game challenging for reinforcement learning (RL) agents due to incomplete knowledge, was studied using tabular and deep RL algorithms. Different agents excelled against specific opponents or through adaptation. Conditions for each algorithm's advantage were identified, with interactions between agents of different types noted. Temporal difference (TD) algorithms, particularly tabular Expected SARSA and deep Q-Learning, exhibited superior overall performance and play type balancing. <div>
arXiv:2506.00458v1 Announce Type: new 
Abstract: Hanabi has become a popular game for research when it comes to reinforcement learning (RL) as it is one of the few cooperative card games where you have incomplete knowledge of the entire environment, thus presenting a challenge for a RL agent. We explored different tabular and deep reinforcement learning algorithms to see which had the best performance both against an agent of the same type and also against other types of agents. We establish that certain agents played their highest scoring games against specific agents while others exhibited higher scores on average by adapting to the opposing agent's behavior. We attempted to quantify the conditions under which each algorithm provides the best advantage and identified the most interesting interactions between agents of different types. In the end, we found that temporal difference (TD) algorithms had better overall performance and balancing of play types compared to tabular agents. Specifically, tabular Expected SARSA and deep Q-Learning agents showed the best performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control</title>
<link>https://arxiv.org/abs/2506.00459</link>
<guid>https://arxiv.org/abs/2506.00459</guid>
<content:encoded><![CDATA[
<div> Keywords: energy storage management, traditional methods, reinforcement learning, micro-grid model, optimization challenges

Summary: 
This study compares traditional methods and reinforcement learning (RL) approaches for energy storage management in a simplified micro-grid model. Three use cases are examined: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. The study provides detailed formulations of each use case and optimization challenges. Comparisons are made between the performance of traditional methods and RL methods, discussing the scenarios in which each method is beneficial. The aim is to promote the use of RL-based methods in challenging domains like energy storage management and suggest future research directions. <div>
arXiv:2506.00459v1 Announce Type: new 
Abstract: We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2506.00467</link>
<guid>https://arxiv.org/abs/2506.00467</guid>
<content:encoded><![CDATA[
<div> Self-training, Self-adaptive Thresholding, Semi-supervised learning, Neural networks, SSL <br>
Summary: <br>
The paper introduces a novel framework called Self-training with Self-adaptive Thresholding (SST) for semi-supervised learning (SSL) utilizing a Self-Adaptive Thresholding (SAT) mechanism to adjust class-specific thresholds based on the model's learning progress. This innovative approach aims to select high-quality pseudo-labeled data efficiently and mitigate risks of inaccurate labels and confirmation bias. Experimental results show that SST outperforms existing methods, achieving state-of-the-art performance with high efficiency, generalization, and scalability across various architectures and datasets. In particular, Semi-SST-ViT-Huge achieves impressive results on ImageNet-1K SSL benchmarks, achieving 80.7% / 84.9% Top-1 accuracy with only 1% / 10% labeled data, surpassing the performance of fully-supervised methods with significantly less labeled data. The proposed framework showcases superior performance and demonstrates the potential of adaptive thresholding techniques in SSL. <br> <div>
arXiv:2506.00467v1 Announce Type: new 
Abstract: Neural networks have demonstrated exceptional performance in supervised learning, benefiting from abundant high-quality annotated data. However, obtaining such data in real-world scenarios is costly and labor-intensive. Semi-supervised learning (SSL) offers a solution to this problem. Recent studies, such as Semi-ViT and Noisy Student, which employ consistency regularization or pseudo-labeling, have demonstrated significant achievements. However, they still face challenges, particularly in accurately selecting sufficient high-quality pseudo-labels due to their reliance on fixed thresholds. Recent methods such as FlexMatch and FreeMatch have introduced flexible or self-adaptive thresholding techniques, greatly advancing SSL research. Nonetheless, their process of updating thresholds at each iteration is deemed time-consuming, computationally intensive, and potentially unnecessary. To address these issues, we propose Self-training with Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL framework. SST introduces an innovative Self-Adaptive Thresholding (SAT) mechanism that adaptively adjusts class-specific thresholds based on the model's learning progress. SAT ensures the selection of high-quality pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and confirmation bias. Extensive experiments demonstrate that SST achieves state-of-the-art performance with remarkable efficiency, generalization, and scalability across various architectures and datasets. Semi-SST-ViT-Huge achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7% / 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using 100% labeled data, our method demonstrates superior performance using only 10% labeled data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet - A ResNet-based Model Classification Dataset</title>
<link>https://arxiv.org/abs/2506.00476</link>
<guid>https://arxiv.org/abs/2506.00476</guid>
<content:encoded><![CDATA[
<div> Dataset, Federated Learning, Privacy, Image Classification, ModelNet

Summary: 
The paper introduces ModelNet, a novel image classification dataset created from embeddings of a pre-trained ResNet50 model. The dataset includes three client-specific variants with different domain heterogeneities (homogeneous, heterogeneous, random) to simulate realistic Federated Learning (FL) settings. The paper proposes a new FL algorithm that accesses anonymized model parameters to enhance local privacy protection. This dataset aims to address the lack of domain heterogeneity and client-specific segregation in FL benchmarks. ModelNet-S, ModelNet-D, and ModelNet-R cater to homogeneous, heterogeneous, and random data settings, respectively, making it the first cross-environment client-specific FL dataset. The paper conducts extensive experiments on domain shifts and aggregation strategies, showcasing the effectiveness of the dataset for both classical and graph-based FL research. The dataset and code are openly available for further exploration and development. 

<br><br>Summary: <div>
arXiv:2506.00476v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training machine learning models across distributed data sources while preserving data locality. However, the privacy of local data is always a pivotal concern and has received a lot of attention in recent research on the FL regime. Moreover, the lack of domain heterogeneity and client-specific segregation in the benchmarks remains a critical bottleneck for rigorous evaluation. In this paper, we introduce ModelNet, a novel image classification dataset constructed from the embeddings extracted from a pre-trained ResNet50 model. First, we modify the CIFAR100 dataset into three client-specific variants, considering three domain heterogeneities (homogeneous, heterogeneous, and random). Subsequently, we train each client-specific subset of all three variants on the pre-trained ResNet50 model to save model parameters. In addition to multi-domain image data, we propose a new hypothesis to define the FL algorithm that can access the anonymized model parameters to preserve the local privacy in a more effective manner compared to existing ones. ModelNet is designed to simulate realistic FL settings by incorporating non-IID data distributions and client diversity design principles in the mainframe for both conventional and futuristic graph-driven FL algorithms. The three variants are ModelNet-S, ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and random data settings, respectively. To the best of our knowledge, we are the first to propose a cross-environment client-specific FL dataset along with the graph-based variant. Extensive experiments based on domain shifts and aggregation strategies show the effectiveness of the above variants, making it a practical benchmark for classical and graph-based FL research. The dataset and related code are available online.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flashbacks to Harmonize Stability and Plasticity in Continual Learning</title>
<link>https://arxiv.org/abs/2506.00477</link>
<guid>https://arxiv.org/abs/2506.00477</guid>
<content:encoded><![CDATA[

arXiv:2506.00477v1 Announce Type: new 
Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize the stability and plasticity of models in Continual Learning (CL). Unlike prior approaches that primarily focus on regularizing model updates to preserve old information while learning new concepts, FL explicitly balances this trade-off through a bidirectional form of regularization. This approach effectively guides the model to swiftly incorporate new knowledge while actively retaining its old knowledge. FL operates through a two-phase training process and can be seamlessly integrated into various CL methods, including replay, parameter regularization, distillation, and dynamic architecture techniques. In designing FL, we use two distinct knowledge bases: one to enhance plasticity and another to improve stability. FL ensures a more balanced model by utilizing both knowledge bases to regularize model updates. Theoretically, we analyze how the FL mechanism enhances the stability-plasticity balance. Empirically, FL demonstrates tangible improvements over baseline methods within the same training budget. By integrating FL into at least one representative baseline from each CL category, we observed an average accuracy improvement of up to 4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard image classification benchmarks. Additionally, measurements of the stability-to-plasticity ratio confirm that FL effectively enhances this balance. FL also outperforms state-of-the-art CL methods on more challenging datasets like ImageNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF</title>
<link>https://arxiv.org/abs/2506.00478</link>
<guid>https://arxiv.org/abs/2506.00478</guid>
<content:encoded><![CDATA[

arXiv:2506.00478v1 Announce Type: new 
Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator power outputs by utilizing the non-linear relationships between voltage magnitudes and phase angles in a power system. However, current AC-OPF solvers struggle to effectively represent the complex relationship between variable distributions in the constraint space and their corresponding optimal solutions. This limitation in constraint modeling restricts the system's ability to develop diverse knowledge representations. Additionally, modeling the power grid solely based on spatial topology further limits the integration of additional prior knowledge, such as temporal information. To overcome these challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven Physics-Informed Graph Convolutional Network), a new method designed to address constraint-related issues and build a graph-based learning framework that incorporates spatiotemporal features. DDA-PIGCN improves consistency optimization for features with varying long-range dependencies by applying multi-layer, hard physics-informed constraints. It also uses a dynamic domain adaptation learning mechanism that iteratively updates and refines key state variables under predefined constraints, enabling precise constraint verification. Moreover, it captures spatiotemporal dependencies between generators and loads by leveraging the physical structure of the power grid, allowing for deep integration of topological information across time and space. Extensive comparative and ablation studies show that DDA-PIGCN delivers strong performance across several IEEE standard test cases (such as case9, case30, and case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and constraint satisfaction rates between 99.6% and 100%, establishing it as a reliable and efficient AC-OPF solver.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.00482</link>
<guid>https://arxiv.org/abs/2506.00482</guid>
<content:encoded><![CDATA[

arXiv:2506.00482v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs</title>
<link>https://arxiv.org/abs/2506.00486</link>
<guid>https://arxiv.org/abs/2506.00486</guid>
<content:encoded><![CDATA[

arXiv:2506.00486v1 Announce Type: new 
Abstract: Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts</title>
<link>https://arxiv.org/abs/2506.00495</link>
<guid>https://arxiv.org/abs/2506.00495</guid>
<content:encoded><![CDATA[

arXiv:2506.00495v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely adopted strategy for adapting pre-trained Large Language Models (LLMs) to downstream tasks, significantly reducing memory and computational costs. However, most existing PEFT techniques uniformly deploy LoRA adapters across all layers, disregarding the intrinsic heterogeneity of layer contributions and task-specific rank requirements. This uniform paradigm leads to redundant parameter allocation and suboptimal adaptation efficiency. To address these limitations, we propose FLoE, a novel PEFT framework that introduces two key innovations: (i) a Fisher information-guided importance scoring mechanism to dynamically identify task-critical transformer layers for MoE-based low-rank adaptation, enabling sparse adapter deployment; and (ii) a Bayesian optimization-driven rank allocator that automatically determines optimal LoRA ranks on specific datasets without exhaustive grid search. Extensive experiments across diverse LLMs and benchmarks reveal that FLoE achieves impressive efficiency-accuracy trade-offs, making FLoE particularly advantageous in resource-constrained environments that necessitate rapid adaptation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study</title>
<link>https://arxiv.org/abs/2506.00499</link>
<guid>https://arxiv.org/abs/2506.00499</guid>
<content:encoded><![CDATA[

arXiv:2506.00499v1 Announce Type: new 
Abstract: Complex systems such as aircraft engines are continuously monitored by sensors. In predictive aircraft maintenance, the collected sensor measurements are used to estimate the health condition and the Remaining Useful Life (RUL) of such systems. However, a major challenge when developing prognostics is the limited number of run-to-failure data samples. This challenge could be overcome if multiple airlines would share their run-to-failure data samples such that sufficient learning can be achieved. Due to privacy concerns, however, airlines are reluctant to share their data in a centralized setting. In this paper, a collaborative federated learning framework is therefore developed instead. Here, several airlines cooperate to train a collective RUL prognostic machine learning model, without the need to centrally share their data. For this, a decentralized validation procedure is proposed to validate the prognostics model without sharing any data. Moreover, sensor data is often noisy and of low quality. This paper therefore proposes four novel methods to aggregate the parameters of the global prognostic model. These methods enhance the robustness of the FL framework against noisy data. The proposed framework is illustrated for training a collaborative RUL prognostic model for aircraft engines, using the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in the FL framework to train a collective RUL prognostic model for their aircraft's engines. When comparing the proposed FL framework with the case where each airline independently develops their own prognostic model, the results show that FL leads to more accurate RUL prognostics for five out of the six airlines. Moreover, the novel robust aggregation methods render the FL framework robust to noisy data samples.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending</title>
<link>https://arxiv.org/abs/2506.00505</link>
<guid>https://arxiv.org/abs/2506.00505</guid>
<content:encoded><![CDATA[

arXiv:2506.00505v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) lending enables permissionless borrowing via smart contracts. However, it faces challenges in optimizing interest rates, mitigating bad debt, and improving capital efficiency. Rule-based interest-rate models struggle to adapt to dynamic market conditions, leading to inefficiencies. This work applies Offline Reinforcement Learning (RL) to optimize interest rate adjustments in DeFi lending protocols. Using historical data from Aave protocol, we evaluate three RL approaches: Conservative Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning (TD3-BC). TD3-BC demonstrates superior performance in balancing utilization, capital stability, and risk, outperforming existing models. It adapts effectively to historical stress events like the May 2021 crash and the March 2023 USDC depeg, showcasing potential for automated, real-time governance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings</title>
<link>https://arxiv.org/abs/2506.00528</link>
<guid>https://arxiv.org/abs/2506.00528</guid>
<content:encoded><![CDATA[

arXiv:2506.00528v1 Announce Type: new 
Abstract: Many modern search domains comprise high-dimensional vectors of floating point numbers derived from neural networks, in the form of embeddings. Typical embeddings range in size from hundreds to thousands of dimensions, making the size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values with a smaller representation, for example a short integer. This gives an approximation of the embedding space in return for a smaller data representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of arbitrary-precision floating point values can be replaced by vectors whose elements are drawn from the set {-1,0,1}. This yields very significant savings in space and metric evaluation cost, while maintaining a strong correlation for similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the high-dimensional space. In this article we give an outline description of these objects, and show how they can be used for the basis of such radical quantisation while maintaining a surprising degree of accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model</title>
<link>https://arxiv.org/abs/2506.00531</link>
<guid>https://arxiv.org/abs/2506.00531</guid>
<content:encoded><![CDATA[

arXiv:2506.00531v1 Announce Type: new 
Abstract: The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems</title>
<link>https://arxiv.org/abs/2506.00533</link>
<guid>https://arxiv.org/abs/2506.00533</guid>
<content:encoded><![CDATA[

arXiv:2506.00533v1 Announce Type: new 
Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges: poor generalization for scalable TSPs and high training costs. To address these challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN). Focusing on the scale-dependent features (i.e., features varied with problem scales) related to nodes and edges that influence the sensitivity of GCNs to the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization capability by (1) rescaling adjacent nodes to construct a subgraph with a uniform number of adjacent nodes for each node across various scales of TSPs, which stabilizes the graph message aggregation; (2) rescaling subgraph edges to adjust the lengths of subgraph edges to the same magnitude, which maintains numerical consistency. In addition, an efficient training strategy with a mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit the heatmaps generated by RsGCN, we design an efficient post-search algorithm termed Re2Opt, in which a reconstruction process based on adaptive weight is incorporated to help avoid local optima. Based on a combined architecture of RsGCN and Re2Opt, our solver achieves remarkable generalization and low training cost: with only 3 epochs of training on the mixed-scale dataset containing instances with up to 100 nodes, it can be generalized successfully to 10K-node instances without any fine-tuning. Extensive experiments demonstrate our state-of-the-art performance across uniform distribution instances of 9 different scales from 20 to 10K nodes and 78 real-world instances from TSPLIB, while requiring the fewest learnable parameters and training epochs among neural competitors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach</title>
<link>https://arxiv.org/abs/2506.00545</link>
<guid>https://arxiv.org/abs/2506.00545</guid>
<content:encoded><![CDATA[

arXiv:2506.00545v1 Announce Type: new 
Abstract: Missing data is a relevant issue in time series, especially in biomedical sequences such as those corresponding to smooth pursuit eye movements, which often contain gaps due to eye blinks and track losses, complicating the analysis and extraction of meaningful biomarkers. In this paper, a novel imputation framework is proposed using Self-Attention-based Imputation networks for time series, which leverages the power of deep learning and self-attention mechanisms to impute missing data. We further refine the imputed data using a custom made autoencoder, tailored to represent smooth pursuit eye movement sequences. The proposed approach was implemented using 5,504 sequences from 172 Parkinsonian patients and healthy controls. Results show a significant improvement in the accuracy of reconstructed eye movement sequences with respect to other state of the art techniques, substantially reducing the values for common time domain error metrics such as the mean absolute error, mean relative error, and root mean square error, while also preserving the signal's frequency domain characteristics. Moreover, it demonstrates robustness when large intervals of data are missing. This method offers an alternative solution for robustly handling missing data in time series, enhancing the reliability of smooth pursuit analysis for the screening and monitoring of neurodegenerative disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[

arXiv:2506.00555v1 Announce Type: new 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 18.4% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2506.00563</link>
<guid>https://arxiv.org/abs/2506.00563</guid>
<content:encoded><![CDATA[

arXiv:2506.00563v1 Announce Type: new 
Abstract: A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs</title>
<link>https://arxiv.org/abs/2506.00569</link>
<guid>https://arxiv.org/abs/2506.00569</guid>
<content:encoded><![CDATA[

arXiv:2506.00569v1 Announce Type: new 
Abstract: When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach -- which simply optimizes the total loss across all tasks -- and also outperforms model merging methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Estimation for Scaling Entropic Multimarginal Optimal Transport</title>
<link>https://arxiv.org/abs/2506.00573</link>
<guid>https://arxiv.org/abs/2506.00573</guid>
<content:encoded><![CDATA[

arXiv:2506.00573v1 Announce Type: new 
Abstract: Multimarginal optimal transport (MOT) is a powerful framework for modeling interactions between multiple distributions, yet its applicability is bottlenecked by a high computational overhead. Entropic regularization provides computational speedups via the multimarginal Sinkhorn algorithm, whose time complexity, for a dataset size $n$ and $k$ marginals, generally scales as $O(n^k)$. However, this dependence on the dataset size $n$ is computationally prohibitive for many machine learning problems. In this work, we propose a new computational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT), that enjoys significantly improved scalability. NEMOT employs neural networks trained using mini-batches, which transfers the computational complexity from the dataset size to the size of the mini-batch, leading to substantial gains. We provide formal guarantees on the accuracy of NEMOT via non-asymptotic error bounds. We supplement these with numerical results that demonstrate the performance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to neural computation of multimarginal entropic Gromov-Wasserstein alignment. In particular, orders-of-magnitude speedups are observed relative to the state-of-the-art, with a notable increase in the feasible number of samples and marginals. NEMOT seamlessly integrates as a module in large-scale machine learning pipelines, and can serve to expand the practical applicability of entropic MOT for tasks involving multimarginal data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00574</link>
<guid>https://arxiv.org/abs/2506.00574</guid>
<content:encoded><![CDATA[

arXiv:2506.00574v1 Announce Type: new 
Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00576</link>
<guid>https://arxiv.org/abs/2506.00576</guid>
<content:encoded><![CDATA[

arXiv:2506.00576v1 Announce Type: new 
Abstract: Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow Feature Analysis as Variational Inference Objective</title>
<link>https://arxiv.org/abs/2506.00580</link>
<guid>https://arxiv.org/abs/2506.00580</guid>
<content:encoded><![CDATA[

arXiv:2506.00580v1 Announce Type: new 
Abstract: This work presents a novel probabilistic interpretation of Slow Feature Analysis (SFA) through the lens of variational inference. Unlike prior formulations that recover linear SFA from Gaussian state-space models with linear emissions, this approach relaxes the key constraint of linearity. While it does not lead to full equivalence to non-linear SFA, it recasts the classical slowness objective in a variational framework. Specifically, it allows the slowness objective to be interpreted as a regularizer to a reconstruction loss. Furthermore, we provide arguments, why -- from the perspective of slowness optimization -- the reconstruction loss takes on the role of the constraints that ensure informativeness in SFA. We conclude with a discussion of potential new research directions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding the Stressed Brain with Geometric Machine Learning</title>
<link>https://arxiv.org/abs/2506.00587</link>
<guid>https://arxiv.org/abs/2506.00587</guid>
<content:encoded><![CDATA[

arXiv:2506.00587v1 Announce Type: new 
Abstract: Stress significantly contributes to both mental and physical disorders, yet traditional self-reported questionnaires are inherently subjective. In this study, we introduce a novel framework that employs geometric machine learning to detect stress from raw EEG recordings. Our approach constructs graphs by integrating structural connectivity (derived from electrode spatial arrangement) with functional connectivity from pairwise signal correlations. A spatio-temporal graph convolutional network (ST-GCN) processes these graphs to capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show that the ST-GCN outperforms standard machine learning models on all key classification metrics and enhances interpretability, explored through ablation analyses of key channels and brain regions. These results pave the way for more objective and accurate stress detection methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Chunking Enhances Recognition of Implicit Sequential Patterns</title>
<link>https://arxiv.org/abs/2506.00588</link>
<guid>https://arxiv.org/abs/2506.00588</guid>
<content:encoded><![CDATA[

arXiv:2506.00588v1 Announce Type: new 
Abstract: In this pilot study, we propose a neuro-inspired approach that compresses temporal sequences into context-tagged chunks, where each tag represents a recurring structural unit or``community'' in the sequence. These tags are generated during an offline sleep phase and serve as compact references to past experience, allowing the learner to incorporate information beyond its immediate input range. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. Our results, while preliminary, suggest that temporal chunking can significantly enhance learning efficiency under resource constrained settings. A small-scale human pilot study using a Serial Reaction Time Task further motivates the idea of structural abstraction. Although limited to synthetic tasks, this work serves as an early proof-of-concept, with initial evidence that learned context tags can transfer across related task, offering potential for future applications in transfer learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn</title>
<link>https://arxiv.org/abs/2506.00592</link>
<guid>https://arxiv.org/abs/2506.00592</guid>
<content:encoded><![CDATA[

arXiv:2506.00592v1 Announce Type: new 
Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Evidential Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.00594</link>
<guid>https://arxiv.org/abs/2506.00594</guid>
<content:encoded><![CDATA[

arXiv:2506.00594v1 Announce Type: new 
Abstract: Graph anomaly detection faces significant challenges due to the scarcity of reliable anomaly-labeled datasets, driving the development of unsupervised methods. Graph autoencoders (GAEs) have emerged as a dominant approach by reconstructing graph structures and node features while deriving anomaly scores from reconstruction errors. However, relying solely on reconstruction error for anomaly detection has limitations, as it increases the sensitivity to noise and overfitting. To address these issues, we propose Graph Evidential Learning (GEL), a probabilistic framework that redefines the reconstruction process through evidential learning. By modeling node features and graph topology using evidential distributions, GEL quantifies two types of uncertainty: graph uncertainty and reconstruction uncertainty, incorporating them into the anomaly scoring mechanism. Extensive experiments demonstrate that GEL achieves state-of-the-art performance while maintaining high robustness against noise and structural perturbations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data</title>
<link>https://arxiv.org/abs/2506.00614</link>
<guid>https://arxiv.org/abs/2506.00614</guid>
<content:encoded><![CDATA[

arXiv:2506.00614v1 Announce Type: new 
Abstract: Real-world multichannel time series prediction faces growing demands for efficiency across edge and cloud environments, making channel compression a timely and essential problem. Motivated by success of Multiple-Input Multiple-Output (MIMO) methods, we propose a predictability-aware compression-decompression framework to reduce runtime, lower communication cost, and maintain prediction accuracy across diverse predictors. The core idea involves using a circular periodicity key matrix with orthogonality to capture underlying time series predictability during compression and to mitigate reconstruction errors during decompression by relaxing oversimplified data assumptions. Theoretical and empirical analyses show that the proposed framework is both time-efficient and scalable under a large number of channels. Extensive experiments on six datasets across various predictors demonstrate that the proposed method achieves superior overall performance by jointly considering prediction accuracy and runtime, while maintaining strong compatibility with diverse predictors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Reprogramming Demystified: A Neural Tangent Kernel Perspective</title>
<link>https://arxiv.org/abs/2506.00620</link>
<guid>https://arxiv.org/abs/2506.00620</guid>
<content:encoded><![CDATA[

arXiv:2506.00620v1 Announce Type: new 
Abstract: Model Reprogramming (MR) is a resource-efficient framework that adapts large pre-trained models to new tasks with minimal additional parameters and data, offering a promising solution to the challenges of training large models for diverse tasks. Despite its empirical success across various domains such as computer vision and time-series forecasting, the theoretical foundations of MR remain underexplored. In this paper, we present a comprehensive theoretical analysis of MR through the lens of the Neural Tangent Kernel (NTK) framework. We demonstrate that the success of MR is governed by the eigenvalue spectrum of the NTK matrix on the target dataset and establish the critical role of the source model's effectiveness in determining reprogramming outcomes. Our contributions include a novel theoretical framework for MR, insights into the relationship between source and target models, and extensive experiments validating our findings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models</title>
<link>https://arxiv.org/abs/2506.00630</link>
<guid>https://arxiv.org/abs/2506.00630</guid>
<content:encoded><![CDATA[

arXiv:2506.00630v1 Announce Type: new 
Abstract: Decision-making in building energy systems critically depends on the predictive accuracy of relevant time-series models. In scenarios lacking extensive data from a target building, foundation models (FMs) represent a promising technology that can leverage prior knowledge from vast and diverse pre-training datasets to construct accurate probabilistic predictors for use in decision-making tools. This paper investigates the applicability and fine-tuning strategies of time-series foundation models (TSFMs) in building energy forecasting. We analyze both full fine-tuning and parameter-efficient fine-tuning approaches, particularly low-rank adaptation (LoRA), by using real-world data from a commercial net-zero energy building to capture signals such as room occupancy, carbon emissions, plug loads, and HVAC energy consumption. Our analysis reveals that the zero-shot predictive performance of TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that employing either full fine-tuning or parameter-efficient fine-tuning significantly enhances forecasting accuracy, even with limited historical data. Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs consistently outperform state-of-the-art deep forecasting models (e.g., temporal fusion transformers) in accuracy, robustness, and generalization across varying building zones and seasonal conditions. These results underline the efficacy of TSFMs for practical, data-constrained building energy management systems, enabling improved decision-making in pursuit of energy efficiency and sustainability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2506.00635</link>
<guid>https://arxiv.org/abs/2506.00635</guid>
<content:encoded><![CDATA[

arXiv:2506.00635v1 Announce Type: new 
Abstract: Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Neural-based Matrix Inversion: Why can't, and Where can</title>
<link>https://arxiv.org/abs/2506.00642</link>
<guid>https://arxiv.org/abs/2506.00642</guid>
<content:encoded><![CDATA[

arXiv:2506.00642v1 Announce Type: new 
Abstract: Deep neural networks have achieved substantial success across various scientific computing tasks. A pivotal challenge within this domain is the rapid and parallel approximation of matrix inverses, critical for numerous applications. Despite significant progress, there currently exists no universal neural-based method for approximating matrix inversion. This paper presents a theoretical analysis demonstrating the fundamental limitations of neural networks in developing a general matrix inversion model. We expand the class of Lipschitz functions to encompass a wider array of neural network models, thereby refining our theoretical approach. Moreover, we delineate specific conditions under which neural networks can effectively approximate matrix inverses. Our theoretical results are supported by experimental results from diverse matrix datasets, exploring the efficacy of neural networks in addressing the matrix inversion challenge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</title>
<link>https://arxiv.org/abs/2506.00653</link>
<guid>https://arxiv.org/abs/2506.00653</guid>
<content:encoded><![CDATA[

arXiv:2506.00653v1 Announce Type: new 
Abstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings</title>
<link>https://arxiv.org/abs/2506.00656</link>
<guid>https://arxiv.org/abs/2506.00656</guid>
<content:encoded><![CDATA[

arXiv:2506.00656v1 Announce Type: new 
Abstract: We propose a permutation-invariant neural architecture for indoor localization using RSSI scans from Wi-Fi access points. Each scan is modeled as an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned embeddings and concatenated with signal strength. These are processed by a Set Transformer, enabling the model to handle variable-length, sparse inputs while learning attention- based representations over access point relationships. We evaluate the model on a dataset collected across a campus environment consisting of six buildings. Results show that the model accurately recovers fine-grained spatial structure and maintains performance across physically distinct domains. In our experiments, a simple LSTM consistently outperformed all other models, achieving the lowest mean localization error across three tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer performed competitively, ranking second in every experiment and outperforming the MLP, RNN, and basic attention models, particularly in scenarios involving multiple buildings (E2) and multiple floors (E3). Performance degraded most in E2, where signal conditions varied substantially across buildings, highlighting the importance of architectural robustness to domain diversity. This work demonstrates that set-based neural models are a natural fit for signal-based localization, offering a principled approach to handling sparse, unordered inputs in real-world positioning tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy for Deep Learning in Medicine</title>
<link>https://arxiv.org/abs/2506.00660</link>
<guid>https://arxiv.org/abs/2506.00660</guid>
<content:encoded><![CDATA[

arXiv:2506.00660v1 Announce Type: new 
Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00676</link>
<guid>https://arxiv.org/abs/2506.00676</guid>
<content:encoded><![CDATA[

arXiv:2506.00676v1 Announce Type: new 
Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Existing Large Language Model Unlearning Evaluations Are Inconclusive</title>
<link>https://arxiv.org/abs/2506.00688</link>
<guid>https://arxiv.org/abs/2506.00688</guid>
<content:encoded><![CDATA[

arXiv:2506.00688v1 Announce Type: new 
Abstract: Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00691</link>
<guid>https://arxiv.org/abs/2506.00691</guid>
<content:encoded><![CDATA[

arXiv:2506.00691v1 Announce Type: new 
Abstract: Training reinforcement learning (RL) agents often requires significant computational resources and extended training times. To address this, we build upon the foundation laid by Google Brain's Sensory Neuron, which introduced a novel neural architecture for reinforcement learning tasks that maintained permutation in-variance in the sensory neuron system. While the baseline model demonstrated significant performance improvements over traditional approaches, we identified opportunities to enhance the efficiency of the learning process further. We propose a modified attention mechanism incorporating a non-linear transformation of the key vectors (K) using a mapping function, resulting in a new set of key vectors (K'). This non-linear mapping enhances the representational capacity of the attention mechanism, allowing the model to encode more complex feature interactions and accelerating convergence without compromising performance. Our enhanced model demonstrates significant improvements in learning efficiency, showcasing the potential for non-linear attention mechanisms in advancing reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Path Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2506.00700</link>
<guid>https://arxiv.org/abs/2506.00700</guid>
<content:encoded><![CDATA[

arXiv:2506.00700v1 Announce Type: new 
Abstract: In constrained Markov decision processes, enforcing constraints during training is often thought of as decreasing the final return. Recently, it was shown that constraints can be incorporated directly in the policy geometry, yielding an optimization trajectory close to the central path of a barrier method, which does not compromise final return. Building on this idea, we introduce Central Path Proximal Policy Optimization (C3PO), a simple modification of PPO that produces policy iterates, which stay close to the central path of the constrained optimization problem. Compared to existing on-policy methods, C3PO delivers improved performance with tighter constraint enforcement, suggesting that central path-guided updates offer a promising direction for constrained policy optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Inference of Training Dataset Membership</title>
<link>https://arxiv.org/abs/2506.00701</link>
<guid>https://arxiv.org/abs/2506.00701</guid>
<content:encoded><![CDATA[

arXiv:2506.00701v1 Announce Type: new 
Abstract: Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, a challenge often addressed through membership inference attacks (MIAs). Traditional MIAs typically require access to model internals or rely on computationally intensive shadow models. This paper proposes an efficient, interpretable and principled Bayesian inference method for membership inference. By analyzing post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model, our approach computes posterior probabilities of membership without requiring extensive model training. Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. Beyond membership inference, this method can also detect distribution shifts, offering a practical and interpretable alternative to existing approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00710</link>
<guid>https://arxiv.org/abs/2506.00710</guid>
<content:encoded><![CDATA[

arXiv:2506.00710v1 Announce Type: new 
Abstract: Real-world databases are predominantly relational, comprising multiple interlinked tables that contain complex structural and statistical dependencies. Learning generative models on relational data has shown great promise in generating synthetic data and imputing missing values. However, existing methods often struggle to capture this complexity, typically reducing relational data to conditionally generated flat tables and imposing limiting structural assumptions. To address these limitations, we introduce RelDiff, a novel diffusion generative model that synthesizes complete relational databases by explicitly modeling their foreign key graph structure. RelDiff combines a joint graph-conditioned diffusion process across all tables for attribute synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model for structure generation. The decomposition of graph structure and relational attributes ensures both high fidelity and referential integrity, both of which are crucial aspects of synthetic relational database generation. Experiments on 11 benchmark datasets demonstrate that RelDiff consistently outperforms prior methods in producing realistic and coherent synthetic relational databases. Code is available at https://github.com/ValterH/RelDiff.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</title>
<link>https://arxiv.org/abs/2506.00711</link>
<guid>https://arxiv.org/abs/2506.00711</guid>
<content:encoded><![CDATA[

arXiv:2506.00711v1 Announce Type: new 
Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls in Evaluating Language Model Forecasters</title>
<link>https://arxiv.org/abs/2506.00723</link>
<guid>https://arxiv.org/abs/2506.00723</guid>
<content:encoded><![CDATA[

arXiv:2506.00723v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A condensing approach to multiple shooting neural ordinary differential equation</title>
<link>https://arxiv.org/abs/2506.00724</link>
<guid>https://arxiv.org/abs/2506.00724</guid>
<content:encoded><![CDATA[

arXiv:2506.00724v1 Announce Type: new 
Abstract: Multiple-shooting is a parameter estimation approach for ordinary differential equations. In this approach, the trajectory is broken into small intervals, each of which can be integrated independently. Equality constraints are then applied to eliminate the shooting gap between the end of the previous trajectory and the start of the next trajectory. Unlike single-shooting, multiple-shooting is more stable, especially for highly oscillatory and long trajectories. In the context of neural ordinary differential equations, multiple-shooting is not widely used due to the challenge of incorporating general equality constraints. In this work, we propose a condensing-based approach to incorporate these shooting equality constraints while training a multiple-shooting neural ordinary differential equation (MS-NODE) using first-order optimization methods such as Adam.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00727</link>
<guid>https://arxiv.org/abs/2506.00727</guid>
<content:encoded><![CDATA[

arXiv:2506.00727v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in plane reformatting tasks. In these methods, an agent sequentially adjusts the position and orientation of an initial plane towards an objective location. This process allows accurate plane reformatting, without the need for detailed landmarks, which makes it suitable for images with limited contrast and resolution, such as 4D flow MRI. However, current DRL methods require the test dataset to be in the same position and orientation as the training dataset. In this paper, we present a novel technique that utilizes a flexible coordinate system based on the current state, enabling navigation in volumes at any position or orientation. We adopted the Asynchronous Advantage Actor Critic (A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN). Experimental results in 4D flow MRI demonstrate improved accuracy in plane reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75 mm), as well as statistically equivalent flow measurements determined by a plane reformatting process done by an expert (p=0.21). The method's flexibility and adaptability make it a promising candidate for other medical imaging applications beyond 4D flow MRI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[

arXiv:2506.00731v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms</title>
<link>https://arxiv.org/abs/2506.00732</link>
<guid>https://arxiv.org/abs/2506.00732</guid>
<content:encoded><![CDATA[

arXiv:2506.00732v1 Announce Type: new 
Abstract: We propose a novel discriminative model for sequence labeling called Bregman conditional random fields (BCRF). Contrary to standard linear-chain conditional random fields, BCRF allows fast parallelizable inference algorithms based on iterative Bregman projections. We show how such models can be learned using Fenchel-Young losses, including extension for learning from partial labels. Experimentally, our approach delivers comparable results to CRF while being faster, and achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers</title>
<link>https://arxiv.org/abs/2506.00744</link>
<guid>https://arxiv.org/abs/2506.00744</guid>
<content:encoded><![CDATA[

arXiv:2506.00744v1 Announce Type: new 
Abstract: We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with dynamic synaptic memory through fast-weight programming (FW-memory) -- the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift</title>
<link>https://arxiv.org/abs/2506.00756</link>
<guid>https://arxiv.org/abs/2506.00756</guid>
<content:encoded><![CDATA[

arXiv:2506.00756v1 Announce Type: new 
Abstract: Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing targeted corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how average performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a Subgroup-scanning Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first asks "Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we explain this using more detailed variable(subset)-specific shifts?" (How?). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Juntas under Markov Random Fields</title>
<link>https://arxiv.org/abs/2506.00764</link>
<guid>https://arxiv.org/abs/2506.00764</guid>
<content:encoded><![CDATA[

arXiv:2506.00764v1 Announce Type: new 
Abstract: We give an algorithm for learning $O(\log n)$ juntas in polynomial-time with respect to Markov Random Fields (MRFs) in a smoothed analysis framework where only the external field has been randomly perturbed. This is a broad generalization of the work of Kalai and Teng, who gave an algorithm that succeeded with respect to smoothed product distributions (i.e., MRFs whose dependency graph has no edges). Our algorithm has two phases: (1) an unsupervised structure learning phase and (2) a greedy supervised learning algorithm. This is the first example where algorithms for learning the structure of an undirected graphical model lead to provably efficient algorithms for supervised learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies</title>
<link>https://arxiv.org/abs/2506.00770</link>
<guid>https://arxiv.org/abs/2506.00770</guid>
<content:encoded><![CDATA[

arXiv:2506.00770v1 Announce Type: new 
Abstract: Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space</title>
<link>https://arxiv.org/abs/2506.00771</link>
<guid>https://arxiv.org/abs/2506.00771</guid>
<content:encoded><![CDATA[

arXiv:2506.00771v1 Announce Type: new 
Abstract: Medicinal chemists often optimize drugs considering their 3D structures and designing structurally distinct molecules that retain key features, such as shapes, pharmacophores, or chemical properties. Previous deep learning approaches address this through supervised tasks like molecule inpainting or property-guided optimization. In this work, we propose a flexible zero-shot molecule manipulation method by navigating in a shared latent space of 3D molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space independent of atom counts. MolFLAE encodes 3D molecules using an SE(3)-equivariant neural network into fixed number of latent nodes, distinguished by learned embeddings. The latent space is regularized, and molecular structures are reconstructed via a Bayesian Flow Network (BFN) conditioned on the encoder's latent output. MolFLAE achieves competitive performance on standard unconditional 3D molecule generation benchmarks. Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation, including atom number editing, structure reconstruction, and coordinated latent interpolation for both structure and properties. We further demonstrate our approach on a drug optimization task for the human glucocorticoid receptor, generating molecules with improved hydrophilicity while preserving key interactions, under computational evaluations. These results highlight the flexibility, robustness, and real-world utility of our method, opening new avenues for molecule editing and optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00772</link>
<guid>https://arxiv.org/abs/2506.00772</guid>
<content:encoded><![CDATA[

arXiv:2506.00772v1 Announce Type: new 
Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization</title>
<link>https://arxiv.org/abs/2506.00795</link>
<guid>https://arxiv.org/abs/2506.00795</guid>
<content:encoded><![CDATA[

arXiv:2506.00795v1 Announce Type: new 
Abstract: Recently, supervised learning (SL) methodology has emerged as an effective approach for offline reinforcement learning (RL) due to their simplicity, stability, and efficiency. However, recent studies show that SL methods lack the trajectory stitching capability, typically associated with temporal difference (TD)-based approaches. A question naturally surfaces: How can we endow SL methods with stitching capability and bridge its performance gap with TD learning? To answer this question, we introduce $Q$-conditioned maximization supervised learning for offline goal-conditioned RL, which enhances SL with the stitching capability through $Q$-conditioned policy and $Q$-conditioned maximization. Concretely, we propose Goal-Conditioned Reinforced Supervised Learning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE from the offline dataset and (2) finding the maximum $Q$-value within the data support by integrating $Q$-function maximization with Expectile Regression. In inference time, our policy chooses optimal actions based on such a maximum $Q$-value. Experimental results from stitching evaluations on offline RL datasets demonstrate that our method outperforms prior SL approaches with stitching capabilities and goal data augmentation techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00797</link>
<guid>https://arxiv.org/abs/2506.00797</guid>
<content:encoded><![CDATA[

arXiv:2506.00797v1 Announce Type: new 
Abstract: Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.00798</link>
<guid>https://arxiv.org/abs/2506.00798</guid>
<content:encoded><![CDATA[

arXiv:2506.00798v1 Announce Type: new 
Abstract: Spatio-temporal time series (STTS) have been widely used in many applications. However, accurately forecasting STTS is challenging due to complex dynamic correlations in both time and space dimensions. Existing graph neural networks struggle to balance effectiveness and efficiency in modeling dynamic spatio-temporal relations. To address this problem, we propose the Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently process STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix in SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded as a filtered graph spectral convolution. We also propose the Linear Dynamic Graph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn the SGFT matrix from the dynamic graph and significantly reduce the computational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that efficiently captures complex spatio-temporal correlations. Extensive experiments on seven spatio-temporal datasets show that DST-SGNN outperforms state-of-the-art methods while maintaining relatively low computational costs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-LoRA: One Vector is All You Need</title>
<link>https://arxiv.org/abs/2506.00799</link>
<guid>https://arxiv.org/abs/2506.00799</guid>
<content:encoded><![CDATA[

arXiv:2506.00799v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space $R^D$, can be reconstructed through a projection from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM - making Uni-LoRA both a unified framework and a "one-vector-only" solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Inversion Attacks for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00808</link>
<guid>https://arxiv.org/abs/2506.00808</guid>
<content:encoded><![CDATA[

arXiv:2506.00808v1 Announce Type: new 
Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery</title>
<link>https://arxiv.org/abs/2506.00844</link>
<guid>https://arxiv.org/abs/2506.00844</guid>
<content:encoded><![CDATA[

arXiv:2506.00844v1 Announce Type: new 
Abstract: This paper critically re-evaluates LLMs' role in causal discovery and argues against their direct involvement in determining causal relationships. We demonstrate that LLMs' autoregressive, correlation-driven modeling inherently lacks the theoretical grounding for causal reasoning and introduces unreliability when used as priors in causal discovery algorithms. Through empirical studies, we expose the limitations of existing LLM-based methods and reveal that deliberate prompt engineering (e.g., injecting ground-truth knowledge) could overstate their performance, helping to explain the consistently favorable results reported in much of the current literature. Based on these findings, we strictly confined LLMs' role to a non-decisional auxiliary capacity: LLMs should not participate in determining the existence or directionality of causal relationships, but can assist the search process for causal graphs (e.g., LLM-based heuristic search). Experiments across various settings confirm that, by strictly isolating LLMs from causal decision-making, LLM-guided heuristic search can accelerate the convergence and outperform both traditional and LLM-based methods in causal structure learning. We conclude with a call for the community to shift focus from naively applying LLMs to developing specialized models and training method that respect the core principles of causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00845</link>
<guid>https://arxiv.org/abs/2506.00845</guid>
<content:encoded><![CDATA[

arXiv:2506.00845v1 Announce Type: new 
Abstract: Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shortest path: we need generalization from synthetic graph data to real-world tasks with implicit graph structures. In this work, we propose to unlock generalizable learning of graph synthetic data with reinforcement learning. We first design solution-based and process-based rewards for synthetic graph problems: instead of rigid memorizing response patterns in direct fine-tuning, we posit that RL would help LLMs grasp the essentials underlying graph reasoning and alleviate overfitting. We employ RL algorithms such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph data. We then compare them against existing settings on both in-domain synthetic tasks and out-of-domain real-world tasks with implicit graph structures such as multi-hop QA, structured planning, and more. Extensive experiments demonstrate that our RL recipe leads to statistically significant improvement on 5 datasets, with an average gain of 12.9\% over baseline settings. Further analysis reveals that process-based rewards consistently outperform solution-based rewards, mixing synthetic and real-world task data yields potential gains, while compositionality and explainable intermediate steps remains a critical challenge even after RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs</title>
<link>https://arxiv.org/abs/2506.00846</link>
<guid>https://arxiv.org/abs/2506.00846</guid>
<content:encoded><![CDATA[

arXiv:2506.00846v1 Announce Type: new 
Abstract: In modern theoretical analyses of neural networks, the infinite-width limit is often invoked to justify Gaussian approximations of neuron preactivations (e.g., via neural network Gaussian processes or Tensor Programs). However, these Gaussian-based asymptotic theories have so far been unable to capture the behavior of attention layers, except under special regimes such as infinitely many heads or tailored scaling schemes. In this paper, leveraging the Tensor Programs framework, we rigorously identify the infinite-width limit distribution of variables within a single attention layer under realistic architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$ dimensionality. We derive the exact form of this limit law without resorting to infinite-head approximations or tailored scalings, demonstrating that it departs fundamentally from Gaussianity. This limiting distribution exhibits non-Gaussianity from a hierarchical structure, being Gaussian conditional on the random similarity scores. Numerical experiments validate our theoretical predictions, confirming the effectiveness of our theory at finite width and accurate description of finite-head attentions. Beyond characterizing a standalone attention layer, our findings lay the groundwork for developing a unified theory of deep Transformer architectures in the infinite-width regime.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Unlearning</title>
<link>https://arxiv.org/abs/2506.00848</link>
<guid>https://arxiv.org/abs/2506.00848</guid>
<content:encoded><![CDATA[

arXiv:2506.00848v1 Announce Type: new 
Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2506.00849</link>
<guid>https://arxiv.org/abs/2506.00849</guid>
<content:encoded><![CDATA[

arXiv:2506.00849v1 Announce Type: new 
Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that provides guarantees for the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing computable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling</title>
<link>https://arxiv.org/abs/2506.00862</link>
<guid>https://arxiv.org/abs/2506.00862</guid>
<content:encoded><![CDATA[

arXiv:2506.00862v1 Announce Type: new 
Abstract: Modeling complex fluid systems, especially turbulence governed by partial differential equations (PDEs), remains a fundamental challenge in science and engineering. Recently, diffusion-based generative models have gained attention as a powerful approach for these tasks, owing to their capacity to capture long-range dependencies and recover hierarchical structures. However, we present both empirical and theoretical evidence showing that generative models struggle with significant spectral bias and common-mode noise when generating high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative turbulence modeling framework that enhances the frequency-aware learning by both implicitly and explicitly mitigating spectral bias and common-mode noise. FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch backbone architecture, consisting of a salient flow attention branch with local-global awareness to focus on sensitive turbulence areas. Secondly, we introduce a frequency-guided Fourier mixing branch, which is integrated via an adaptive fusion strategy to explicitly mitigate spectral bias in the generative model. Thirdly, we leverage the high-frequency modeling capabilities of the masked auto-encoder pre-training and implicitly align the features of the generative model toward high-frequency components. We validate the effectiveness of FourierFlow on three canonical turbulent flow scenarios, demonstrating superior performance compared to state-of-the-art methods. Furthermore, we show that our model exhibits strong generalization capabilities in challenging settings such as out-of-distribution domains, long-term temporal extrapolation, and robustness to noisy inputs. The code can be found at https://github.com/AI4Science-WestlakeU/FourierFlow.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning</title>
<link>https://arxiv.org/abs/2506.00867</link>
<guid>https://arxiv.org/abs/2506.00867</guid>
<content:encoded><![CDATA[

arXiv:2506.00867v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose Local Manifold Approximation and Projection (LoMAP), a training-free method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2506.00880</link>
<guid>https://arxiv.org/abs/2506.00880</guid>
<content:encoded><![CDATA[

arXiv:2506.00880v1 Announce Type: new 
Abstract: Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-Covering Trajectory Stitching for Diffusion Planners</title>
<link>https://arxiv.org/abs/2506.00895</link>
<guid>https://arxiv.org/abs/2506.00895</guid>
<content:encoded><![CDATA[

arXiv:2506.00895v1 Announce Type: new 
Abstract: Diffusion-based generative models are emerging as powerful tools for long-horizon planning in reinforcement learning (RL), particularly with offline datasets. However, their performance is fundamentally limited by the quality and diversity of training data. This often restricts their generalization to tasks outside their training distribution or longer planning horizons. To overcome this challenge, we propose State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory augmentation method that incrementally stitches together short trajectory segments, systematically generating diverse and extended trajectories. SCoTS first learns a temporal distance-preserving latent representation that captures the underlying temporal structure of the environment, then iteratively stitches trajectory segments guided by directional exploration and novelty to effectively cover and expand this latent space. We demonstrate that SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Furthermore, augmented trajectories generated by SCoTS significantly improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00910</link>
<guid>https://arxiv.org/abs/2506.00910</guid>
<content:encoded><![CDATA[

arXiv:2506.00910v1 Announce Type: new 
Abstract: Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs--i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-learning with Posterior Sampling</title>
<link>https://arxiv.org/abs/2506.00917</link>
<guid>https://arxiv.org/abs/2506.00917</guid>
<content:encoded><![CDATA[

arXiv:2506.00917v1 Announce Type: new 
Abstract: Bayesian posterior sampling techniques have demonstrated superior empirical performance in many exploration-exploitation settings. However, their theoretical analysis remains a challenge, especially in complex settings like reinforcement learning. In this paper, we introduce Q-Learning with Posterior Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian posteriors on Q-values for exploration, akin to the popular Thompson Sampling algorithm in the multi-armed bandit setting. We show that in the tabular episodic MDP setting, PSQL achieves a regret bound of $\tilde O(H^2\sqrt{SAT})$, closely matching the known lower bound of $\Omega(H\sqrt{SAT})$. Here, S, A denote the number of states and actions in the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the number of episodes and $H$ being the planning horizon. Our work provides several new technical insights into the core challenges in combining posterior sampling with dynamic programming and TD-learning-based RL algorithms, along with novel ideas for resolving those difficulties. We hope this will form a starting point for analyzing this efficient and important algorithmic technique in even more complex RL settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks</title>
<link>https://arxiv.org/abs/2506.00918</link>
<guid>https://arxiv.org/abs/2506.00918</guid>
<content:encoded><![CDATA[

arXiv:2506.00918v1 Announce Type: new 
Abstract: Uncertainty quantification is critical in safety-sensitive applications but is often omitted from off-the-shelf neural networks due to adverse effects on predictive performance. Retrofitting uncertainty estimates post-hoc typically requires access to model parameters or gradients, limiting feasibility in practice. We propose a theoretically grounded framework for post-hoc uncertainty estimation in regression tasks by fitting an auxiliary model to both original inputs and frozen model outputs. Drawing from principles of maximum likelihood estimation and sequential parameter fitting, we formalize an exact post-hoc optimization objective that recovers the canonical MLE of Gaussian parameters, without requiring sampling or approximation at inference. While prior work has used model outputs to estimate uncertainty, we explicitly characterize the conditions under which this is valid and demonstrate the extent to which structured outputs can support quasi-epistemic inference. We find that using diverse auxiliary data, such as augmented subsets of the original training data, significantly enhances OOD detection and metric performance. Our hypothesis that frozen model outputs contain generalizable latent information about model error and predictive uncertainty is tested and confirmed. Finally, we ensure that our method maintains proper estimation of input-dependent uncertainty without relying exclusively on base model forecasts. These findings are demonstrated in toy problems and adapted to both UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation</title>
<link>https://arxiv.org/abs/2506.00920</link>
<guid>https://arxiv.org/abs/2506.00920</guid>
<content:encoded><![CDATA[

arXiv:2506.00920v1 Announce Type: new 
Abstract: Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity</title>
<link>https://arxiv.org/abs/2506.00932</link>
<guid>https://arxiv.org/abs/2506.00932</guid>
<content:encoded><![CDATA[

arXiv:2506.00932v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy, leveraging aggregated updates to build robust global models. However, this training paradigm faces significant challenges due to data heterogeneity and limited local datasets, which often impede effective collaboration. In such scenarios, we identify the Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model undergo minimal updates after early communication rounds, ultimately limiting the effectiveness of global aggregation. We demonstrate the presence of this phenomenon across a wide range of federated settings, spanning diverse datasets and architectures. To address this issue, we propose LIPS (Layer-wise Inertia Phenomenon with Sparsity), a simple yet effective method that periodically introduces transient sparsity to stimulate meaningful updates and empower global aggregation. Experiments demonstrate that LIPS effectively mitigates layer-wise inertia, enhances aggregation effectiveness, and improves overall performance in various FL scenarios. This work not only deepens the understanding of layer-wise learning dynamics in FL but also paves the way for more effective collaboration strategies in resource-constrained environments. Our code is publicly available at: https://github.com/QiaoXiao7282/LIPS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.00936</link>
<guid>https://arxiv.org/abs/2506.00936</guid>
<content:encoded><![CDATA[

arXiv:2506.00936v1 Announce Type: new 
Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for drug research and development but remains challenging due to the complex interplay of molecular interactions. Despite recent advances in graph neural networks (GNNs) for MS prediction, current approaches face two critical limitations: (1) incomplete molecular modeling due to atom-centric message-passing mechanisms that disregard bond-level topological features, and (2) prediction frameworks that lack reliable uncertainty quantification. To address these challenges, we propose TrustworthyMS, a novel contrastive learning framework designed for uncertainty-aware metabolic stability prediction. First, a molecular graph topology remapping mechanism synchronizes atom-bond interactions through edge-induced feature propagation, capturing both localized electronic effects and global conformational constraints. Second, contrastive topology-bond alignment enforces consistency between molecular topology views and bond patterns via feature alignment, enhancing representation robustness. Third, uncertainty modeling through Beta-Binomial uncertainty quantification enables simultaneous prediction and confidence calibration under epistemic uncertainty. Through extensive experiments, our results demonstrate that TrustworthyMS outperforms current state-of-the-art methods in terms of predictive performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation</title>
<link>https://arxiv.org/abs/2506.00959</link>
<guid>https://arxiv.org/abs/2506.00959</guid>
<content:encoded><![CDATA[

arXiv:2506.00959v1 Announce Type: new 
Abstract: Marketing optimization, commonly formulated as an online budget allocation problem, has emerged as a pivotal factor in driving user growth. Most existing research addresses this problem by following the principle of 'first predict then optimize' for each individual, which presents challenges related to large-scale counterfactual prediction and solving complexity trade-offs. Note that the practical data quality is uncontrollable, and the solving scale tends to be tens of millions. Therefore, the existing approaches make the robust budget allocation non-trivial, especially in industrial scenarios with considerable data noise. To this end, this paper proposes a novel approach that solves the problem from the cluster perspective. Specifically, we propose a multi-task representation network to learn the inherent attributes of individuals and project the original features into high-dimension hidden representations through the first two layers of the trained network. Then, we divide these hidden representations into $K$ groups through partitioning-based clustering, thus reformulating the problem as an integer stochastic programming problem under different total budgets. Finally, we distill the representation module and clustering model into a multi-category model to facilitate online deployment. Offline experiments validate the effectiveness and superiority of our approach compared to six state-of-the-art marketing optimization algorithms. Online A/B tests on the Meituan platform indicate that the approach outperforms the online algorithm by 0.53% and 0.65%, considering order volume (OV) and gross merchandise volume (GMV), respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Parallelism in Decentralized Stochastic Convex Optimization</title>
<link>https://arxiv.org/abs/2506.00961</link>
<guid>https://arxiv.org/abs/2506.00961</guid>
<content:encoded><![CDATA[

arXiv:2506.00961v1 Announce Type: new 
Abstract: Decentralized learning has emerged as a powerful approach for handling large datasets across multiple machines in a communication-efficient manner. However, such methods often face scalability limitations, as increasing the number of machines beyond a certain point negatively impacts convergence rates. In this work, we propose Decentralized Anytime SGD, a novel decentralized learning algorithm that significantly extends the critical parallelism threshold, enabling the effective use of more machines without compromising performance. Within the stochastic convex optimization (SCO) framework, we establish a theoretical upper bound on parallelism that surpasses the current state-of-the-art, allowing larger networks to achieve favorable statistical guarantees and closing the gap with centralized learning in highly connected topologies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Random Time Horizons</title>
<link>https://arxiv.org/abs/2506.00962</link>
<guid>https://arxiv.org/abs/2506.00962</guid>
<content:encoded><![CDATA[

arXiv:2506.00962v1 Announce Type: new 
Abstract: We extend the standard reinforcement learning framework to random time horizons. While the classical setting typically assumes finite and deterministic or infinite runtimes of trajectories, we argue that multiple real-world applications naturally exhibit random (potentially trajectory-dependent) stopping times. Since those stopping times typically depend on the policy, their randomness has an effect on policy gradient formulas, which we (mostly for the first time) derive rigorously in this work both for stochastic and deterministic policies. We present two complementary perspectives, trajectory or state-space based, and establish connections to optimal control theory. Our numerical experiments demonstrate that using the proposed formulas can significantly improve optimization convergence compared to traditional approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO</title>
<link>https://arxiv.org/abs/2506.00967</link>
<guid>https://arxiv.org/abs/2506.00967</guid>
<content:encoded><![CDATA[

arXiv:2506.00967v1 Announce Type: new 
Abstract: Optimization-based power control algorithms are predominantly iterative with high computational complexity, making them impractical for real-time applications in cell-free massive multiple-input multiple-output (CFmMIMO) systems. Learning-based methods have emerged as a promising alternative, and among them, graph neural networks (GNNs) have demonstrated their excellent performance in solving power control problems. However, all existing GNN-based approaches assume ideal orthogonality among pilot sequences for user equipments (UEs), which is unrealistic given that the number of UEs exceeds the available orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based methods assume a fixed number of UEs, whereas the number of active UEs varies over time in practice. Additionally, supervised training necessitates costly computational resources for computing the target power control solutions for a large volume of training samples. To address these issues, we propose a graph attention network for downlink power control in CFmMIMO systems that operates in a self-supervised manner while effectively handling pilot contamination and adapting to a dynamic number of UEs. Experimental results show its effectiveness, even in comparison to the optimal accelerated projected gradient method as a baseline.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Heterogeneity Modeling for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2506.00969</link>
<guid>https://arxiv.org/abs/2506.00969</guid>
<content:encoded><![CDATA[

arXiv:2506.00969v1 Announce Type: new 
Abstract: Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization-based Bounds on the Wasserstein Metric</title>
<link>https://arxiv.org/abs/2506.00976</link>
<guid>https://arxiv.org/abs/2506.00976</guid>
<content:encoded><![CDATA[

arXiv:2506.00976v1 Announce Type: new 
Abstract: The Wasserstein metric has become increasingly important in many machine learning applications such as generative modeling, image retrieval and domain adaptation. Despite its appeal, it is often too costly to compute. This has motivated approximation methods like entropy-regularized optimal transport, downsampling, and subsampling, which trade accuracy for computational efficiency. In this paper, we consider the challenge of computing efficient approximations to the Wasserstein metric that also serve as strict upper or lower bounds. Focusing on discrete measures on regular grids, our approach involves formulating and exactly solving a Kantorovich problem on a coarse grid using a quantized measure and specially designed cost matrix, followed by an upscaling and correction stage. This is done either in the primal or dual space to obtain valid upper and lower bounds on the Wasserstein metric of the full-resolution inputs. We evaluate our methods on the DOTmark optimal transport images benchmark, demonstrating a 10x-100x speedup compared to entropy-regularized OT while keeping the approximation error below 2%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers</title>
<link>https://arxiv.org/abs/2506.00998</link>
<guid>https://arxiv.org/abs/2506.00998</guid>
<content:encoded><![CDATA[

arXiv:2506.00998v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) improves performance on domain-specific tasks but can lead to overfitting, making them unreliable on out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD detection monitors to the LoRA layer using boxed abstraction to filter questions beyond the model's competence. Feature vectors from the fine-tuning data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a question is flagged as OoD if its feature vector falls outside all boxes. To improve interpretability and robustness, we introduce a regularization loss during fine-tuning that encourages paraphrased questions to stay close in the feature space, and the enlargement of the decision boundary is based on the feature variance within a cluster. Our method complements existing defenses by providing lightweight and interpretable OoD detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts</title>
<link>https://arxiv.org/abs/2506.01000</link>
<guid>https://arxiv.org/abs/2506.01000</guid>
<content:encoded><![CDATA[

arXiv:2506.01000v1 Announce Type: new 
Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying only the input and output spaces. Visual reprogramming (VR) is one instance for vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to input images to facilitate downstream classification. The existing VR approaches for CLIP train a single visual prompt using all descriptions of different downstream classes. However, the limited learning capacity may result in (1) a failure to capture diverse aspects of the descriptions (e.g., shape, color, and texture), and (2) a possible bias toward less informative attributes that do not help distinguish between classes. In this paper, we introduce a decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are optimized using descriptions grouped by explicit causes (DVP-cse) or unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual prompts with a probabilistic reweighting matrix (PRM) that measures their contributions to each downstream class. Theoretically, DVP lowers the empirical risk bound. Experimentally, DVP outperforms baselines on average across 11 downstream datasets. Notably, the DVP-PRM integration enables insights into how individual visual prompts influence classification decisions, providing a probabilistic framework for understanding reprogramming. Our code is available at https://github.com/tmlr-group/DecoupledVP.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic critics can empower small actors</title>
<link>https://arxiv.org/abs/2506.01016</link>
<guid>https://arxiv.org/abs/2506.01016</guid>
<content:encoded><![CDATA[

arXiv:2506.01016v1 Announce Type: new 
Abstract: Actor-critic methods have been central to many of the recent advances in deep reinforcement learning. The most common approach is to use symmetric architectures, whereby both actor and critic have the same network topology and number of parameters. However, recent works have argued for the advantages of asymmetric setups, specifically with the use of smaller actors. We perform broad empirical investigations and analyses to better understand the implications of this and find that, in general, smaller actors result in performance degradation and overfit critics. Our analyses suggest poor data collection, due to value underestimation, as one of the main causes for this behavior, and further highlight the crucial role the critic can play in alleviating this pathology. We explore techniques to mitigate the observed value underestimation, which enables further research in asymmetric actor-critic methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming LLMs by Scaling Learning Rates with Gradient Grouping</title>
<link>https://arxiv.org/abs/2506.01049</link>
<guid>https://arxiv.org/abs/2506.01049</guid>
<content:encoded><![CDATA[

arXiv:2506.01049v1 Announce Type: new 
Abstract: Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity</title>
<link>https://arxiv.org/abs/2506.01052</link>
<guid>https://arxiv.org/abs/2506.01052</guid>
<content:encoded><![CDATA[

arXiv:2506.01052v1 Announce Type: new 
Abstract: We investigate the finite-time convergence properties of Temporal Difference (TD) learning with linear function approximation, a cornerstone algorithm in reinforcement learning. While prior work has established convergence guarantees, these results typically rely on the assumption that each iterate is projected onto a bounded set or that the learning rate is set according to the unknown strong convexity constant -- conditions that are both artificial and do not match the current practice.
  In this paper, we challenge the necessity of such assumptions and present a refined analysis of TD learning. We show that the simple projection-free variant converges with a rate of $\tilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence of Markovian noise. Our analysis reveals a novel self-bounding property of the TD updates and exploits it to guarantee bounded iterates.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</title>
<link>https://arxiv.org/abs/2506.01054</link>
<guid>https://arxiv.org/abs/2506.01054</guid>
<content:encoded><![CDATA[

arXiv:2506.01054v1 Announce Type: new 
Abstract: The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI-Units: Benchmarking Explainability Methods with Unit Tests</title>
<link>https://arxiv.org/abs/2506.01059</link>
<guid>https://arxiv.org/abs/2506.01059</guid>
<content:encoded><![CDATA[

arXiv:2506.01059v1 Announce Type: new 
Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering LLM Uncertainty Estimation Methods in the Wild</title>
<link>https://arxiv.org/abs/2506.01114</link>
<guid>https://arxiv.org/abs/2506.01114</guid>
<content:encoded><![CDATA[

arXiv:2506.01114v1 Announce Type: new 
Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer</title>
<link>https://arxiv.org/abs/2506.01115</link>
<guid>https://arxiv.org/abs/2506.01115</guid>
<content:encoded><![CDATA[

arXiv:2506.01115v1 Announce Type: new 
Abstract: The Transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of algorithmic tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based training on next-token prediction. While the core component of a Transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard Transformers to variants in which either the multi-layer perceptron (MLP) layers or the attention projectors (queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce MixiT -- the Mixing Transformer -- a simplified, principled model in which the attention coefficients are entirely random and fixed at initialization, eliminating any input-dependent computation or learning in attention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on various algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization. For retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently beneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits such as induction heads -- a specific circuit known to be crucial for learning and exploiting repeating patterns in input sequences. Even more interestingly, we find that attention with frozen key and query projectors is not only able to form induction heads, but can also perform competitively on language modeling. Our results underscore the importance of architectural heterogeneity, where distinct components contribute complementary inductive biases crucial for solving different classes of tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</title>
<link>https://arxiv.org/abs/2506.01121</link>
<guid>https://arxiv.org/abs/2506.01121</guid>
<content:encoded><![CDATA[

arXiv:2506.01121v1 Announce Type: new 
Abstract: Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow Feature Analysis on Markov Chains from Goal-Directed Behavior</title>
<link>https://arxiv.org/abs/2506.01145</link>
<guid>https://arxiv.org/abs/2506.01145</guid>
<content:encoded><![CDATA[

arXiv:2506.01145v1 Announce Type: new 
Abstract: Slow Feature Analysis is a unsupervised representation learning method that extracts slowly varying features from temporal data and can be used as a basis for subsequent reinforcement learning. Often, the behavior that generates the data on which the representation is learned is assumed to be a uniform random walk. Less research has focused on using samples generated by goal-directed behavior, as commonly the case in a reinforcement learning setting, to learn a representation. In a spatial setting, goal-directed behavior typically leads to significant differences in state occupancy between states that are close to a reward location and far from a reward location.
  Through the perspective of optimal slow features on ergodic Markov chains, this work investigates the effects of these differences on value-function approximation in an idealized setting. Furthermore, three correction routes, which can potentially alleviate detrimental scaling effects, are evaluated and discussed. In addition, the special case of goal-averse behavior is considered.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earley-Driven Dynamic Pruning for Efficient Structured Decoding</title>
<link>https://arxiv.org/abs/2506.01151</link>
<guid>https://arxiv.org/abs/2506.01151</guid>
<content:encoded><![CDATA[

arXiv:2506.01151v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight-Space Linear Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2506.01153</link>
<guid>https://arxiv.org/abs/2506.01153</guid>
<content:encoded><![CDATA[

arXiv:2506.01153v1 Announce Type: new 
Abstract: We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet powerful framework that unifies weight-space learning with linear recurrence to redefine sequence modeling. Unlike conventional recurrent neural networks (RNNs) which collapse temporal dynamics into fixed-dimensional hidden states, WARP explicitly parametrizes the hidden state as the weights of a distinct root neural network. This formulation promotes higher-resolution memory, gradient-free adaptation at test-time, and seamless integration of domain-specific physical priors. Empirical validation shows that WARP matches or surpasses state-of-the-art baselines on diverse classification tasks, spanning synthetic benchmarks to real-world datasets. Furthermore, extensive experiments across sequential image completion, dynamical system reconstruction, and multivariate time series forecasting demonstrate its expressiveness and generalization capabilities. Critically, WARP's weight trajectories offer valuable insights into the model's inner workings. Ablation studies confirm the architectural necessity of key components, solidifying weight-space linear RNNs as a transformative paradigm for adaptive machine intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FORT: Forward-Only Regression Training of Normalizing Flows</title>
<link>https://arxiv.org/abs/2506.01158</link>
<guid>https://arxiv.org/abs/2506.01158</guid>
<content:encoded><![CDATA[

arXiv:2506.01158v1 Announce Type: new 
Abstract: Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to neural dynamical systems that encompass modern large-scale diffusion and flow matching models. Despite the scalability of training, the generation of high-quality samples and their corresponding likelihood under the model requires expensive numerical simulation -- inhibiting adoption in numerous scientific applications such as equilibrium sampling of molecular systems. In this paper, we revisit classical normalizing flows as one-step generative models with exact likelihoods and propose a novel, scalable training objective that does not require computing the expensive change of variable formula used in conventional maximum likelihood training. We propose Forward-Only Regression Training (FORT), a simple $\ell_2$-regression objective that maps prior samples under our flow to specifically chosen targets. We demonstrate that FORT supports a wide class of targets, such as optimal transport targets and targets from pre-trained continuous-time normalizing flows (CNF). We further demonstrate that by using CNF targets, our one-step flows allow for larger-scale training that exceeds the performance and stability of maximum likelihood training, while unlocking a broader class of architectures that were previously challenging to train. Empirically, we elucidate that our trained flows can perform equilibrium conformation sampling in Cartesian coordinates of alanine dipeptide, alanine tripeptide, and alanine tetrapeptide.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Learning with Linear Temporal Logic using Differentiable Simulation</title>
<link>https://arxiv.org/abs/2506.01167</link>
<guid>https://arxiv.org/abs/2506.01167</guid>
<content:encoded><![CDATA[

arXiv:2506.01167v1 Announce Type: new 
Abstract: To ensure learned controllers comply with safety and reliability requirements for reinforcement learning in real-world settings remains challenging. Traditional safety assurance approaches, such as state avoidance and constrained Markov decision processes, often inadequately capture trajectory requirements or may result in overly conservative behaviors. To address these limitations, recent studies advocate the use of formal specification languages such as linear temporal logic (LTL), enabling the derivation of correct-by-construction learning objectives from the specified requirements. However, the sparse rewards associated with LTL specifications make learning extremely difficult, whereas dense heuristic-based rewards risk compromising correctness. In this work, we propose the first method, to our knowledge, that integrates LTL with differentiable simulators, facilitating efficient gradient-based learning directly from LTL specifications by coupling with differentiable paradigms. Our approach introduces soft labeling to achieve differentiable rewards and states, effectively mitigating the sparse-reward issue intrinsic to LTL without compromising objective correctness. We validate the efficacy of our method through experiments, demonstrating significant improvements in both reward attainment and training time compared to the discrete methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation</title>
<link>https://arxiv.org/abs/2506.01177</link>
<guid>https://arxiv.org/abs/2506.01177</guid>
<content:encoded><![CDATA[

arXiv:2506.01177v1 Announce Type: new 
Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture for generative adversarial networks (GANs) in molecular discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, using over 60% fewer parameters. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01183</link>
<guid>https://arxiv.org/abs/2506.01183</guid>
<content:encoded><![CDATA[

arXiv:2506.01183v1 Announce Type: new 
Abstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA</title>
<link>https://arxiv.org/abs/2506.01194</link>
<guid>https://arxiv.org/abs/2506.01194</guid>
<content:encoded><![CDATA[

arXiv:2506.01194v1 Announce Type: new 
Abstract: LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiresolution Analysis and Statistical Thresholding on Dynamic Networks</title>
<link>https://arxiv.org/abs/2506.01208</link>
<guid>https://arxiv.org/abs/2506.01208</guid>
<content:encoded><![CDATA[

arXiv:2506.01208v1 Announce Type: new 
Abstract: Detecting structural change in dynamic network data has wide-ranging applications. Existing approaches typically divide the data into time bins, extract network features within each bin, and then compare these features over time. This introduces an inherent tradeoff between temporal resolution and the statistical stability of the extracted features. Despite this tradeoff, reminiscent of time-frequency tradeoffs in signal processing, most methods rely on a fixed temporal resolution. Choosing an appropriate resolution parameter is typically difficult and can be especially problematic in domains like cybersecurity, where anomalous behavior may emerge at multiple time scales. We address this challenge by proposing ANIE (Adaptive Network Intensity Estimation), a multi-resolution framework designed to automatically identify the time scales at which network structure evolves, enabling the joint detection of both rapid and gradual changes. Modeling interactions as Poisson processes, our method proceeds in two steps: (1) estimating a low-dimensional subspace of node behavior, and (2) deriving a set of novel empirical affinity coefficients that quantify change in interaction intensity between latent factors and support statistical testing for structural change across time scales. We provide theoretical guarantees for subspace estimation and the asymptotic behavior of the affinity coefficients, enabling model-based change detection. Experiments on synthetic networks show that ANIE adapts to the appropriate time resolution and is able to capture sharp structural changes while remaining robust to noise. Furthermore, applications to real-world data showcase the practical benefits of ANIE's multiresolution approach to detecting structural change over fixed resolution methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Modes as Time Representation for Spatiotemporal Forecasting</title>
<link>https://arxiv.org/abs/2506.01212</link>
<guid>https://arxiv.org/abs/2506.01212</guid>
<content:encoded><![CDATA[

arXiv:2506.01212v1 Announce Type: new 
Abstract: This paper introduces a data-driven time embedding method for modeling long-range seasonal dependencies in spatiotemporal forecasting tasks. The proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal modes directly from observed data, eliminating the need for explicit timestamps or hand-crafted time features. These temporal modes serve as time representations that can be seamlessly integrated into deep spatiotemporal forecasting models. Unlike conventional embeddings such as time-of-day indicators or sinusoidal functions, our method captures complex multi-scale periodicity through spectral analysis of spatiotemporal data. Extensive experiments on urban mobility, highway traffic, and climate datasets demonstrate that the DMD-based embedding consistently improves long-horizon forecasting accuracy, reduces residual correlation, and enhances temporal generalization. The method is lightweight, model-agnostic, and compatible with any architecture that incorporates time covariates.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective</title>
<link>https://arxiv.org/abs/2506.01213</link>
<guid>https://arxiv.org/abs/2506.01213</guid>
<content:encoded><![CDATA[

arXiv:2506.01213v1 Announce Type: new 
Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Refining Training for Amortized Density Functional Theory</title>
<link>https://arxiv.org/abs/2506.01225</link>
<guid>https://arxiv.org/abs/2506.01225</guid>
<content:encoded><![CDATA[

arXiv:2506.01225v1 Announce Type: new 
Abstract: Density Functional Theory (DFT) allows for predicting all the chemical and physical properties of molecular systems from first principles by finding an approximate solution to the many-body Schr\"odinger equation. However, the cost of these predictions becomes infeasible when increasing the scale of the energy evaluations, e.g., when calculating the ground-state energy for simulating molecular dynamics. Recent works have demonstrated that, for substantially large datasets of molecular conformations, Deep Learning-based models can predict the outputs of the classical DFT solvers by amortizing the corresponding optimization problems. In this paper, we propose a novel method that reduces the dependency of amortized DFT solvers on large pre-collected datasets by introducing a self-refining training strategy. Namely, we propose an efficient method that simultaneously trains a deep-learning model to predict the DFT outputs and samples molecular conformations that are used as training data for the model. We derive our method as a minimization of the variational upper bound on the KL-divergence measuring the discrepancy between the generated samples and the target Boltzmann distribution defined by the ground state energy. To demonstrate the utility of the proposed scheme, we perform an extensive empirical study comparing it with the models trained on the pre-collected datasets. Finally, we open-source our implementation of the proposed algorithm, optimized with asynchronous training and sampling stages, which enables simultaneous sampling and training. Code is available at https://github.com/majhas/self-refining-dft.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing ML Pipelines with Adversarial Data Corruption</title>
<link>https://arxiv.org/abs/2506.01230</link>
<guid>https://arxiv.org/abs/2506.01230</guid>
<content:encoded><![CDATA[

arXiv:2506.01230v1 Announce Type: new 
Abstract: Structured data-quality issues, such as missing values correlated with demographics, culturally biased labels, or systemic selection biases, routinely degrade the reliability of machine-learning pipelines. Regulators now increasingly demand evidence that high-stakes systems can withstand these realistic, interdependent errors, yet current robustness evaluations typically use random or overly simplistic corruptions, leaving worst-case scenarios unexplored. We introduce SAVAGE, a causally inspired framework that (i) formally models realistic data-quality issues through dependency graphs and flexible corruption templates, and (ii) systematically discovers corruption patterns that maximally degrade a target performance metric. SAVAGE employs a bi-level optimization approach to efficiently identify vulnerable data subpopulations and fine-tune corruption severity, treating the full ML pipeline, including preprocessing and potentially non-differentiable models, as a black box. Extensive experiments across multiple datasets and ML tasks (data cleaning, fairness-aware learning, uncertainty quantification) demonstrate that even a small fraction (around 5 %) of structured corruptions identified by SAVAGE severely impacts model performance, far exceeding random or manually crafted errors, and invalidating core assumptions of existing techniques. Thus, SAVAGE provides a practical tool for rigorous pipeline stress-testing, a benchmark for evaluating robustness methods, and actionable guidance for designing more resilient data workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution</title>
<link>https://arxiv.org/abs/2506.01231</link>
<guid>https://arxiv.org/abs/2506.01231</guid>
<content:encoded><![CDATA[

arXiv:2506.01231v1 Announce Type: new 
Abstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration</title>
<link>https://arxiv.org/abs/2506.01250</link>
<guid>https://arxiv.org/abs/2506.01250</guid>
<content:encoded><![CDATA[

arXiv:2506.01250v1 Announce Type: new 
Abstract: In this paper, we address the contextual dueling bandit problem by proposing variance-aware algorithms that leverage neural networks to approximate nonlinear utility functions. Our approach employs a \textit{variance-aware exploration strategy}, which adaptively accounts for uncertainty in pairwise comparisons while relying only on the gradients with respect to the learnable parameters of the last layer. This design effectively balances the exploration--exploitation tradeoff under both the Upper Confidence Bound (UCB) and Thompson Sampling (TS) frameworks. As a result, under standard assumptions, we establish theoretical guarantees showing that our algorithms achieve sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T \sigma_t^2} + \sqrt{dT}\rt),$ for sufficiently wide neural networks, where $ d $ is the contextual dimension, $ \sigma_t^2 $ the variance of comparisons at round $ t $, and $ T $ the total number of rounds. We also empirically validate that our approach offers reasonable computational efficiency and achieves sublinear regret on both synthetic tasks with nonlinear utilities and real-world tasks, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism</title>
<link>https://arxiv.org/abs/2506.01260</link>
<guid>https://arxiv.org/abs/2506.01260</guid>
<content:encoded><![CDATA[

arXiv:2506.01260v1 Announce Type: new 
Abstract: Scaling models has led to significant advancements in deep learning, but training these models in decentralized settings remains challenging due to communication bottlenecks. While existing compression techniques are effective in data-parallel, they do not extend to model parallelism. Unlike data-parallel training, where weight gradients are exchanged, model-parallel requires compressing activations and activation gradients as they propagate through layers, accumulating compression errors. We propose a novel compression algorithm that compresses both forward and backward passes, enabling up to 99% compression with no convergence degradation with negligible memory/compute overhead. By leveraging a recursive structure in transformer networks, we predefine a low-dimensional subspace to confine the activations and gradients, allowing full reconstruction in subsequent layers. Our method achieves up to 100x improvement in communication efficiency and enables training billion-parameter-scale models over low-end GPUs connected via consumer-grade internet speeds as low as 80Mbps, matching the convergence of centralized datacenter systems with 100Gbps connections with model parallel.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01261</link>
<guid>https://arxiv.org/abs/2506.01261</guid>
<content:encoded><![CDATA[

arXiv:2506.01261v1 Announce Type: new 
Abstract: In the context of Federated Reinforcement Learning (FRL), applying Proximal Policy Optimization (PPO) faces challenges related to the update order of its actor and critic due to the aggregation step occurring between successive iterations. In particular, when local actors are updated based on local critic estimations, the algorithm becomes vulnerable to data heterogeneity. As a result, the conventional update order in PPO (critic first, then actor) may cause heterogeneous gradient directions among clients, hindering convergence to a globally optimal policy. To address this issue, we propose FedRAC, which reverses the update order (actor first, then critic) to eliminate the divergence of critics from different clients. Theoretical analysis shows that the convergence bound of FedRAC is immune to data heterogeneity under mild conditions, i.e., bounded level of heterogeneity and accurate policy evaluation. Empirical results indicate that the proposed algorithm obtains higher cumulative rewards and converges more rapidly in five experiments, including three classical RL environments and a highly heterogeneous autonomous driving scenario using the SUMO traffic simulator.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment</title>
<link>https://arxiv.org/abs/2506.01290</link>
<guid>https://arxiv.org/abs/2506.01290</guid>
<content:encoded><![CDATA[

arXiv:2506.01290v1 Announce Type: new 
Abstract: High-quality time series (TS) data are essential for ensuring TS model performance, rendering research on rating TS data quality indispensable. Existing methods have shown promising rating accuracy within individual domains, primarily by extending data quality rating techniques such as influence functions and Shapley values to account for temporal characteristics. However, they neglect the fact that real-world TS data can span vastly different domains and exhibit distinct properties, hampering the accurate and efficient rating of diverse TS data. In this paper, we propose TSRating, a novel and unified framework for rating the quality of time series data crawled from diverse domains. TSRating is built on the assumption that LLMs inherit ample knowledge, acquired during their extensive pretraining, enabling them to comprehend and discern quality differences in diverse TS data. We verify this assumption by devising a series of prompts to elicit quality comparisons from LLMs for pairs of TS samples. We then fit a dedicated rating model, termed TSRater, to convert the LLMs' judgments into efficient quality predictions via TSRater's inference on future TS samples. To ensure cross-domain adaptability, we develop a meta-learning scheme to train TSRater on quality comparisons collected from nine distinct domains. To improve training efficiency, we employ signSGD for inner-loop updates, thus circumventing the demanding computation of hypergradients. Extensive experimental results on eleven benchmark datasets across three time series tasks, each using both conventional TS models and TS foundation models, demonstrate that TSRating outperforms baselines in terms of estimation accuracy, efficiency, and domain adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Developments in GNNs for Drug Discovery</title>
<link>https://arxiv.org/abs/2506.01302</link>
<guid>https://arxiv.org/abs/2506.01302</guid>
<content:encoded><![CDATA[

arXiv:2506.01302v1 Announce Type: new 
Abstract: In this paper, we review recent developments and the role of Graph Neural Networks (GNNs) in computational drug discovery, including molecule generation, molecular property prediction, and drug-drug interaction prediction. By summarizing the most recent developments in this area, we underscore the capabilities of GNNs to comprehend intricate molecular patterns, while exploring both their current and prospective applications. We initiate our discussion by examining various molecular representations, followed by detailed discussions and categorization of existing GNN models based on their input types and downstream application tasks. We also collect a list of commonly used benchmark datasets for a variety of applications. We conclude the paper with brief discussions and summarize common trends in this important research area.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Structured Hopfield Network for Semantic Association and Retrieval</title>
<link>https://arxiv.org/abs/2506.01303</link>
<guid>https://arxiv.org/abs/2506.01303</guid>
<content:encoded><![CDATA[

arXiv:2506.01303v1 Announce Type: new 
Abstract: Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored. Inspired by hippocampal CA3 dynamics and its role in associative memory, we propose the Latent Structured Hopfield Network (LSHN), a biologically inspired framework that integrates continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input. Unlike traditional Hopfield networks, our model is trained end-to-end with gradient descent, achieving scalable and robust memory retrieval. Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models. Our work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Considerations for Large Pretrained Neural Networks</title>
<link>https://arxiv.org/abs/2506.01311</link>
<guid>https://arxiv.org/abs/2506.01311</guid>
<content:encoded><![CDATA[

arXiv:2506.01311v1 Announce Type: new 
Abstract: Increasingly complex neural network architectures have achieved phenomenal performance. However, these complex models require massive computational resources that consume substantial amounts of electricity, which highlights the potential environmental impact of such models. Previous studies have demonstrated that substantial redundancies exist in large pre-trained models. However, previous work has primarily focused on compressing models while retaining comparable model performance, and the direct impact on electricity consumption appears to have received relatively little attention. By quantifying the energy usage associated with both uncompressed and compressed models, we investigate compression as a means of reducing electricity consumption. We consider nine different pre-trained models, ranging in size from 8M parameters to 138M parameters. To establish a baseline, we first train each model without compression and record the electricity usage and time required during training, along with other relevant statistics. We then apply three compression techniques: Steganographic capacity reduction, pruning, and low-rank factorization. In each of the resulting cases, we again measure the electricity usage, training time, model accuracy, and so on. We find that pruning and low-rank factorization offer no significant improvements with respect to energy usage or other related statistics, while steganographic capacity reduction provides major benefits in almost every case. We discuss the significance of these findings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2506.01317</link>
<guid>https://arxiv.org/abs/2506.01317</guid>
<content:encoded><![CDATA[

arXiv:2506.01317v1 Announce Type: new 
Abstract: Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples using 40 minutes on a single GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack</title>
<link>https://arxiv.org/abs/2506.01318</link>
<guid>https://arxiv.org/abs/2506.01318</guid>
<content:encoded><![CDATA[

arXiv:2506.01318v1 Announce Type: new 
Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: "over-unlearning" that deteriorates retained data near the forget set, and post-hoc "relearning" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@{\epsilon}, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Psi$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title>
<link>https://arxiv.org/abs/2506.01320</link>
<guid>https://arxiv.org/abs/2506.01320</guid>
<content:encoded><![CDATA[

arXiv:2506.01320v1 Announce Type: new 
Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation</title>
<link>https://arxiv.org/abs/2506.01327</link>
<guid>https://arxiv.org/abs/2506.01327</guid>
<content:encoded><![CDATA[

arXiv:2506.01327v1 Announce Type: new 
Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models</title>
<link>https://arxiv.org/abs/2506.01337</link>
<guid>https://arxiv.org/abs/2506.01337</guid>
<content:encoded><![CDATA[

arXiv:2506.01337v1 Announce Type: new 
Abstract: Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks. In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization. Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes and Reinforcement Learning. Our code will be available at https://github.com/HKUST-SAIL/NoiseAR/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01339</link>
<guid>https://arxiv.org/abs/2506.01339</guid>
<content:encoded><![CDATA[

arXiv:2506.01339v1 Announce Type: new 
Abstract: Machine unlearning offers a promising solution to privacy and safety concerns in large language models (LLMs) by selectively removing targeted knowledge while preserving utility. However, current methods are highly sensitive to downstream fine-tuning, which can quickly recover forgotten information-even from unrelated tasks. To address this, we introduce invariance into unlearning for the first time, inspired by invariant risk minimization (IRM). Building on this principle, we propose invariant LLM unlearning (ILU), a regularization-based framework that enhances robustness. Notably, ILU generalizes well to diverse fine-tuning tasks, even when trained using a single dataset. A task vector analysis is also provided to further elucidate the rationale behind ILU's effectiveness. Extensive experiments on the WMDP and MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art unlearning methods, including negative preference optimization (NPO) and representation misdirection for unlearning (RMU). Notably, ILU achieves superior unlearning robustness across diverse downstream fine-tuning scenarios (e.g., math, paraphrase detection, and sentiment analysis) while preserving the fine-tuning performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Learning in Survival Analysis</title>
<link>https://arxiv.org/abs/2506.01348</link>
<guid>https://arxiv.org/abs/2506.01348</guid>
<content:encoded><![CDATA[

arXiv:2506.01348v1 Announce Type: new 
Abstract: We introduce an innovative approach that incorporates a Distributionally Robust Learning (DRL) approach into Cox regression to enhance the robustness and accuracy of survival predictions. By formulating a DRL framework with a Wasserstein distance-based ambiguity set, we develop a variant Cox model that is less sensitive to assumptions about the underlying data distribution and more resilient to model misspecification and data perturbations. By leveraging Wasserstein duality, we reformulate the original min-max DRL problem into a tractable regularized empirical risk minimization problem, which can be computed by exponential conic programming. We provide guarantees on the finite sample behavior of our DRL-Cox model. Moreover, through extensive simulations and real world case studies, we demonstrate that our regression model achieves superior performance in terms of prediction accuracy and robustness compared with traditional methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2506.01350</link>
<guid>https://arxiv.org/abs/2506.01350</guid>
<content:encoded><![CDATA[

arXiv:2506.01350v1 Announce Type: new 
Abstract: This paper proposes a novel stable learning theory for recurrent neural networks (RNNs), so-called variational adaptive noise and dropout (VAND). As stabilizing factors for RNNs, noise and dropout on the internal state of RNNs have been separately confirmed in previous studies. We reinterpret the optimization problem of RNNs as variational inference, showing that noise and dropout can be derived simultaneously by transforming the explicit regularization term arising in the optimization problem into implicit regularization. Their scale and ratio can also be adjusted appropriately to optimize the main objective of RNNs, respectively. In an imitation learning scenario with a mobile manipulator, only VAND is able to imitate sequential and periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network</title>
<link>https://arxiv.org/abs/2506.01352</link>
<guid>https://arxiv.org/abs/2506.01352</guid>
<content:encoded><![CDATA[

arXiv:2506.01352v1 Announce Type: new 
Abstract: Decentralized training of large language models offers the opportunity to pool computational resources across geographically distributed participants but faces significant network communication bottlenecks, particularly in pipeline-parallel settings. While pipeline parallelism partitions model layers across devices to handle large-scale models, it necessitates frequent communication of intermediate activations, creating challenges when network bandwidth is limited. Existing activation compression methods, such as AQ-SGD, mitigate quantization-induced errors through error compensation but impose prohibitive memory overhead by requiring storage of previous activations. To address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard Quantization), a novel activation quantization framework designed specifically for pipeline parallelism. Our approach integrates fine-grained tile-wise quantization for precise control, entropy-guided token-level adaptive bit allocation for optimal bit usage, and a Hadamard-based transform with pivot element swapping to effectively suppress quantization outliers. We further provide a theoretical analysis, proving that pipeline parallel training equipped with TAH-Quant maintains a convergence rate of $\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant achieves aggressive activation quantization (3-4 bits) ratio, which provides up to 4.3$\times$ end-to-end speedup without compromising training convergence, matches state-of-the-art methods, incurs no extra memory overhead, and generalizes well across different training scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion</title>
<link>https://arxiv.org/abs/2506.01356</link>
<guid>https://arxiv.org/abs/2506.01356</guid>
<content:encoded><![CDATA[

arXiv:2506.01356v1 Announce Type: new 
Abstract: Learning-based neural network (NN) control policies have shown impressive empirical performance. However, obtaining stability guarantees and estimations of the region of attraction of these learned neural controllers is challenging due to the lack of stable and scalable training and verification algorithms. Although previous works in this area have achieved great success, much conservatism remains in their framework. In this work, we propose a novel two-stage training framework to jointly synthesize the controller and Lyapunov function for continuous-time systems. By leveraging a Zubov-inspired region of attraction characterization to directly estimate stability boundaries, we propose a novel training data sampling strategy and a domain updating mechanism that significantly reduces the conservatism in training. Moreover, unlike existing works on continuous-time systems that rely on an SMT solver to formally verify the Lyapunov condition, we extend state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN with the capability of performing automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme that avoids expensive bisection. To demonstrate the effectiveness of our approach, we conduct numerical experiments by synthesizing and verifying controllers on several challenging nonlinear systems across multiple dimensions. We show that our training can yield region of attractions with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and our verification on continuous systems can be up to $40-10000$ times faster compared to the traditional SMT solver dReal. Our code is available at https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases</title>
<link>https://arxiv.org/abs/2506.01360</link>
<guid>https://arxiv.org/abs/2506.01360</guid>
<content:encoded><![CDATA[

arXiv:2506.01360v1 Announce Type: new 
Abstract: Relational databases (RDBs) are composed of interconnected tables, where relationships between them are defined through foreign keys. Recent research on applying machine learning to RDBs has explored graph-based representations of RDBs, where rows of tables are modeled as nodes, and foreign key relationships are modeled as edges. RDB-to-graph modeling helps capture cross-table dependencies, ultimately leading to enhanced performance across diverse tasks. However, there are numerous ways to model RDBs as graphs, and performance varies significantly depending on the chosen graph model. In our analysis, applying a common heuristic rule for graph modeling leads to up to a 10% drop in performance compared to the best-performing graph model, which remains non-trivial to identify. To foster research on intelligent RDB-to-graph modeling, we introduce RDB2G-Bench, the first benchmark framework for evaluating such methods. We construct extensive datasets covering 5 real-world RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs for efficient and reproducible evaluations. Thanks to our precomputed datasets, we were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12 tasks over 600x faster than on-the-fly evaluation, which requires repeated model training. Our analysis of the datasets and benchmark results reveals key structural patterns affecting graph model effectiveness, along with practical implications for effective graph modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery</title>
<link>https://arxiv.org/abs/2506.01361</link>
<guid>https://arxiv.org/abs/2506.01361</guid>
<content:encoded><![CDATA[

arXiv:2506.01361v1 Announce Type: new 
Abstract: Robust causal discovery in time series datasets depends on reliable benchmark datasets with known ground-truth causal relationships. However, such datasets remain scarce, and existing synthetic alternatives often overlook critical temporal properties inherent in real-world data, including nonstationarity driven by trends and seasonality, irregular sampling intervals, and the presence of unobserved confounders. To address these challenges, we introduce TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets that systematically incorporates both linear and nonlinear dependencies while modeling key temporal characteristics such as trends, seasonal effects, and heterogeneous noise patterns. Each dataset is accompanied by a fully specified causal graph featuring varying densities and diverse noise distributions and is provided in two versions: one including unobserved confounders and one without, thereby offering extensive coverage of real-world complexity while preserving methodological neutrality. We further demonstrate the utility of TimeGraph through systematic evaluations of state-of-the-art causal discovery algorithms including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and metrics. Our experiments reveal significant variations in algorithmic performance under realistic temporal conditions, underscoring the need for robust synthetic benchmarks in the fair and transparent assessment of causal discovery methods. The complete TimeGraph suite, including dataset generation scripts, evaluation metrics, and recommended experimental protocols, is freely available to facilitate reproducible research and foster community-driven advancements in time-series causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.01364</link>
<guid>https://arxiv.org/abs/2506.01364</guid>
<content:encoded><![CDATA[

arXiv:2506.01364v1 Announce Type: new 
Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such data to support tasks like prediction. However, previous deep learning models designed for specific tasks typically require separate training for each use case, leading to increased computational and storage costs. To address this issue, spatio-temporal foundation models have emerged, offering a unified framework capable of solving multiple spatio-temporal tasks. These foundation models achieve remarkable success by learning general knowledge with spatio-temporal data or transferring the general capabilities of pre-trained language models. While previous surveys have explored spatio-temporal data and methodologies separately, they have ignored a comprehensive examination of how foundation models are designed, selected, pre-trained, and adapted. As a result, the overall pipeline for spatio-temporal foundation models remains unclear. To bridge this gap, we innovatively provide an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline begins with an introduction to different types of spatio-temporal data, followed by details of data preprocessing and embedding techniques. The pipeline then presents a novel data property taxonomy to divide existing methods according to data sources and dependencies, providing efficient and effective model design and selection for researchers. On this basis, we further illustrate the training objectives of primitive models, as well as the adaptation techniques of transferred models. Overall, our survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly. Additionally, we introduce emerging opportunities such as multi-objective training in the field of spatio-temporal foundation models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing LLMs to Self-Verify Their Answers</title>
<link>https://arxiv.org/abs/2506.01369</link>
<guid>https://arxiv.org/abs/2506.01369</guid>
<content:encoded><![CDATA[

arXiv:2506.01369v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance during inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compiler Optimization via LLM Reasoning for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[

arXiv:2506.01374v1 Announce Type: new 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training</title>
<link>https://arxiv.org/abs/2506.01376</link>
<guid>https://arxiv.org/abs/2506.01376</guid>
<content:encoded><![CDATA[

arXiv:2506.01376v1 Announce Type: new 
Abstract: Understanding the various properties of glycans with machine learning has shown some preliminary promise. However, previous methods mainly focused on modeling the backbone structure of glycans as graphs of monosaccharides (i.e., sugar units), while they neglected the atomic structures underlying each monosaccharide, which are actually important indicators of glycan properties. We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide nodes representing its global backbone structure and atom nodes representing its local atomic-level structures. Based on such a graph, GlycanAA performs hierarchical message passing to capture from local atomic-level interactions to global monosaccharide-level interactions. To further enhance model capability, we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow the model about different levels of dependencies in a glycan. Extensive benchmark results show the superiority of GlycanAA over existing glycan encoders and verify the further improvements achieved by PreGlycanAA. We maintain all resources at https://github.com/kasawa1234/GlycanAA
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.01386</link>
<guid>https://arxiv.org/abs/2506.01386</guid>
<content:encoded><![CDATA[

arXiv:2506.01386v1 Announce Type: new 
Abstract: Model editing has become an important tool for addressing privacy, bias, and misinformation in large language models (LLMs) by enabling updates to knowledge without the need for retraining from scratch. However, existing editing techniques often target isolated facts, ignoring ripple effects on related knowledge, allowing edited facts to remain deducible and compromising broader contextual integrity. For example, changing Harry Potter's school from Hogwarts to Ilvermorny requires reassigning his house from Gryffindor to a suitable alternative while preserving Gryffindor's relationship with Hogwarts. In this work, we present a new model-editing setting, deep editing, to show: (1) how editing techniques fail to handle connected facts, evaluating how original knowledge sneaks through unchanged causal links, and (2) their impact on broader contextual knowledge. We introduce ThinkEval, a framework to systematically evaluate model- editing techniques by building model-specific knowledge graphs to analyze pre- and post-edit effects on fact persistence and catastrophic forgetting. We present KnowGIC, a benchmark created with ThinkEval, consisting of sequentially linked queries to measure these effects. We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE across multiple LLMs. We find that these techniques struggle to balance indirect fact suppression with the preservation of related knowledge. Our dataset is available at: https://anonymous.4open.science/r/KnowGIC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Part Deployment of Neural Network</title>
<link>https://arxiv.org/abs/2506.01387</link>
<guid>https://arxiv.org/abs/2506.01387</guid>
<content:encoded><![CDATA[

arXiv:2506.01387v1 Announce Type: new 
Abstract: The increasing scale of modern neural networks, exemplified by architectures from IBM (530 billion neurons) and Google (500 billion parameters), presents significant challenges in terms of computational cost and infrastructure requirements. As deep neural networks continue to grow, traditional training paradigms relying on monolithic GPU clusters become increasingly unsustainable. This paper proposes a distributed system architecture that partitions a neural network across multiple servers, each responsible for a subset of neurons. Neurons are classified as local or remote, with inter-server connections managed via a metadata-driven lookup mechanism. A Multi-Part Neural Network Execution Engine facilitates seamless execution and training across distributed partitions by dynamically resolving and invoking remote neurons using stored metadata. All servers share a unified model through a network file system (NFS), ensuring consistency during parallel updates. A Neuron Distributor module enables flexible partitioning strategies based on neuron count, percentage, identifiers, or network layers. This architecture enables cost-effective, scalable deployment of deep learning models on cloud infrastructure, reducing dependency on high-performance centralized compute resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization</title>
<link>https://arxiv.org/abs/2506.01393</link>
<guid>https://arxiv.org/abs/2506.01393</guid>
<content:encoded><![CDATA[

arXiv:2506.01393v1 Announce Type: new 
Abstract: This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\sqrt{T \ln^4 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound for GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling a more refined analysis of the GP's information gain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping</title>
<link>https://arxiv.org/abs/2506.01396</link>
<guid>https://arxiv.org/abs/2506.01396</guid>
<content:encoded><![CDATA[

arXiv:2506.01396v1 Announce Type: new 
Abstract: Differential privacy (DP) has become an essential framework for privacy-preserving machine learning. Existing DP learning methods, however, often have disparate impacts on model predictions, e.g., for minority groups. Gradient clipping, which is often used in DP learning, can suppress larger gradients from challenging samples. We show that this problem is amplified by adaptive clipping, which will often shrink the clipping bound to tiny values to match a well-fitting majority, while significantly reducing the accuracy for others. We propose bounded adaptive clipping, which introduces a tunable lower bound to prevent excessive gradient suppression. Our method improves the accuracy of the worst-performing class on average over 10 percentage points on skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and over 5 percentage points over constant clipping.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs</title>
<link>https://arxiv.org/abs/2506.01404</link>
<guid>https://arxiv.org/abs/2506.01404</guid>
<content:encoded><![CDATA[

arXiv:2506.01404v1 Announce Type: new 
Abstract: This paper introduces an innovative error feedback framework designed to mitigate quantization noise in distributed graph filtering, where communications are constrained to quantized messages. It comes from error spectrum shaping techniques from state-space digital filters, and therefore establishes connections between quantized filtering processes over different domains. In contrast to existing error compensation methods, our framework quantitatively feeds back the quantization noise for exact compensation. We examine the framework under three key scenarios: (i) deterministic graph filtering, (ii) graph filtering over random graphs, and (iii) graph filtering with random node-asynchronous updates. Rigorous theoretical analysis demonstrates that the proposed framework significantly reduces the effect of quantization noise, and we provide closed-form solutions for the optimal error feedback coefficients. Moreover, this quantitative error feedback mechanism can be seamlessly integrated into communication-efficient decentralized optimization frameworks, enabling lower error floors. Numerical experiments validate the theoretical results, consistently showing that our method outperforms conventional quantization strategies in terms of both accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification</title>
<link>https://arxiv.org/abs/2506.01405</link>
<guid>https://arxiv.org/abs/2506.01405</guid>
<content:encoded><![CDATA[

arXiv:2506.01405v1 Announce Type: new 
Abstract: The identification of drug-target interactions (DTI) is crucial for drug discovery and repositioning, as it reveals potential uses of existing drugs, aiding in the acceleration of the drug development process and reducing associated costs. Despite the similarity information in DTI is important, most models are limited to mining direct similarity information within homogeneous graphs, overlooking the potential yet rich similarity information in heterogeneous graphs. Inspired by real-world social interaction behaviors, we propose SOC-DGL, which comprises two specialized modules: the Affinity-Driven Graph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL) module. The ADGL module adopts a comprehensive social interaction strategy, leveraging an affinity-enhanced global drug-target graph to learn both global DTI and the individual similarity information of drugs and targets. In contrast, the EDGL module employs a higher-order social interaction strategy, amplifying the influence of even-hop neighbors through an even-polynomial graph filter grounded in balance theory, enabling the indirect mining of higher-order homogeneous information. This dual approach enables SOC-DGL to effectively and comprehensively capture similarity information across diverse interaction scales within the affinity matrices and drug-target association matrices, significantly enhancing the model's generalization capability and predictive accuracy in DTI tasks. To address the issue of imbalance in drug-target interaction datasets, this paper proposes an adjustable imbalance loss function that mitigates the impact of sample imbalance by adjusting the weight of negative samples and a parameter. Extensive experiments on four benchmark datasets demonstrate significant accuracy improvements achieved by SOC-DGL, particularly in scenarios involving data imbalance and unseen drugs or targets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Latent Space Optimization with Nebula Variational Coding</title>
<link>https://arxiv.org/abs/2506.01414</link>
<guid>https://arxiv.org/abs/2506.01414</guid>
<content:encoded><![CDATA[

arXiv:2506.01414v1 Announce Type: new 
Abstract: Deep learning approaches process data in a layer-by-layer way with intermediate (or latent) features. We aim at designing a general solution to optimize the latent manifolds to improve the performance on classification, segmentation, completion and/or reconstruction through probabilistic models. This paper proposes a variational inference model which leads to a clustered embedding. We introduce additional variables in the latent space, called \textbf{nebula anchors}, that guide the latent variables to form clusters during training. To prevent the anchors from clustering among themselves, we employ the variational constraint that enforces the latent features within an anchor to form a Gaussian distribution, resulting in a generative model we refer as Nebula Variational Coding (NVC). Since each latent feature can be labeled with the closest anchor, we also propose to apply metric learning in a self-supervised way to make the separation between clusters more explicit. As a consequence, the latent variables of our variational coder form clusters which adapt to the generated semantic of the training data, \textit{e.g.} the categorical labels of each sample. We demonstrate experimentally that it can be used within different architectures designed to solve different problems including text sequence, images, 3D point clouds and volumetric data, validating the advantage of our proposed method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variance-Based Defense Against Blended Backdoor Attacks</title>
<link>https://arxiv.org/abs/2506.01444</link>
<guid>https://arxiv.org/abs/2506.01444</guid>
<content:encoded><![CDATA[

arXiv:2506.01444v1 Announce Type: new 
Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things</title>
<link>https://arxiv.org/abs/2506.01450</link>
<guid>https://arxiv.org/abs/2506.01450</guid>
<content:encoded><![CDATA[

arXiv:2506.01450v1 Announce Type: new 
Abstract: Industrial Internet of Things environments increasingly rely on advanced Anomaly Detection and explanation techniques to rapidly detect and mitigate cyberincidents, thereby ensuring operational safety. The sequential nature of data collected from these environments has enabled improvements in Anomaly Detection using Machine Learning and Deep Learning models by processing time windows rather than treating the data as tabular. However, conventional explanation methods often neglect this temporal structure, leading to imprecise or less actionable explanations. This work presents ShaTS (Shapley values for Time Series models), which is a model-agnostic explainable Artificial Intelligence method designed to enhance the precision of Shapley value explanations for time series models. ShaTS addresses the shortcomings of traditional approaches by incorporating an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS accurately identifies critical time instants, precisely pinpoints the sensors, actuators, and processes affected by anomalies, and outperforms SHAP in terms of both explainability and resource efficiency, fulfilling the real-time requirements of industrial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-aware Hypergraph Generation via Next-Scale Prediction</title>
<link>https://arxiv.org/abs/2506.01467</link>
<guid>https://arxiv.org/abs/2506.01467</guid>
<content:encoded><![CDATA[

arXiv:2506.01467v1 Announce Type: new 
Abstract: Hypergraphs generalize traditional graphs by allowing hyperedges to connect multiple nodes, making them well-suited for modeling complex structures with higher-order relationships, such as 3D meshes, molecular systems, and electronic circuits. While topology is central to hypergraph structure, many real-world applications also require node and hyperedge features. Existing hypergraph generation methods focus solely on topology, often overlooking feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph generation via next-scale prediction), a hierarchical approach that jointly generates hypergraph topology and features. FAHNES builds a multi-scale representation through node coarsening, then learns to reconstruct finer levels via localized expansion and refinement, guided by a new node budget mechanism that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs, 3D meshes, and molecular datasets. FAHNES achieves competitive results in reconstructing topology and features, establishing a foundation for future research in featured hypergraph generative modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions</title>
<link>https://arxiv.org/abs/2506.01478</link>
<guid>https://arxiv.org/abs/2506.01478</guid>
<content:encoded><![CDATA[

arXiv:2506.01478v1 Announce Type: new 
Abstract: Understanding the interaction between different drugs (drug-drug interaction or DDI) is critical for ensuring patient safety and optimizing therapeutic outcomes. Existing DDI datasets primarily focus on textual information, overlooking multimodal data that reflect complex drug mechanisms. In this paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark learning methods to study it. In brief, MUDI provides a comprehensive multimodal representation of drugs by combining pharmacological text, chemical formulas, molecular structure graphs, and images across 310,532 annotated drug pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to effectively evaluate machine-learning based generalization, MUDI consists of unseen drug pairs in the test set. We evaluate benchmark models using both late fusion voting and intermediate fusion strategies. All data, annotations, evaluation scripts, and baselines are released under an open research license.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?</title>
<link>https://arxiv.org/abs/2506.01482</link>
<guid>https://arxiv.org/abs/2506.01482</guid>
<content:encoded><![CDATA[

arXiv:2506.01482v1 Announce Type: new 
Abstract: Stage lighting plays an essential role in live music performances, influencing the engaging experience of both musicians and audiences. Given the high costs associated with hiring or training professional lighting engineers, Automatic Stage Lighting Control (ASLC) has gained increasing attention. However, most existing approaches only classify music into limited categories and map them to predefined light patterns, resulting in formulaic and monotonous outcomes that lack rationality. To address this issue, this paper presents an end-to-end solution that directly learns from experienced lighting engineers -- Skip-BART. To the best of our knowledge, this is the first work to conceptualize ASLC as a generative task rather than merely a classification problem. Our method modifies the BART model to take audio music as input and produce light hue and value (intensity) as output, incorporating a novel skip connection mechanism to enhance the relationship between music and light within the frame grid.We validate our method through both quantitative analysis and an human evaluation, demonstrating that Skip-BART outperforms conventional rule-based methods across all evaluation metrics and shows only a limited gap compared to real lighting engineers.Specifically, our method yields a p-value of 0.72 in a statistical comparison based on human evaluations with human lighting engineers, suggesting that the proposed approach closely matches human lighting engineering performance. To support further research, we have made our self-collected dataset, code, and trained model parameters available at https://github.com/RS2002/Skip-BART .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-agnostic Mitigation Strategies of Data Imbalance for Regression</title>
<link>https://arxiv.org/abs/2506.01486</link>
<guid>https://arxiv.org/abs/2506.01486</guid>
<content:encoded><![CDATA[

arXiv:2506.01486v1 Announce Type: new 
Abstract: Data imbalance persists as a pervasive challenge in regression tasks, introducing bias in model performance and undermining predictive reliability. This is particularly detrimental in applications aimed at predicting rare events that fall outside the domain of the bulk of the training data. In this study, we review the current state-of-the-art regarding sampling-based methods and cost-sensitive learning. Additionally, we propose novel approaches to mitigate model bias. To better asses the importance of data, we introduce the density-distance and density-ratio relevance functions, which effectively integrate empirical frequency of data with domain-specific preferences, offering enhanced interpretability for end-users. Furthermore, we present advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and improve existing sampling methods. In a comprehensive quantitative evaluation, we benchmark state-of-the-art methods on 10 synthetic and 42 real-world datasets, using neural networks, XGBoosting trees and Random Forest models. Our analysis reveals that while most strategies improve performance on rare samples, they often degrade it on frequent ones. We demonstrate that constructing an ensemble of models -- one trained with imbalance mitigation and another without -- can significantly reduce these negative effects. The key findings underscore the superior performance of our novel crbSMOGN sampling technique with the density-ratio relevance function for neural networks, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities</title>
<link>https://arxiv.org/abs/2506.01490</link>
<guid>https://arxiv.org/abs/2506.01490</guid>
<content:encoded><![CDATA[

arXiv:2506.01490v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. In real-world scenarios, practical factors often lead to uncertain modality missingness. Existing methods for handling modality missingness are based on data reconstruction or common subspace projections. However, these methods neglect the confidence in multimodal combinations and impose constraints on intra-class representation, hindering the capture of modality-specific information and resulting in suboptimal performance. To address these challenges, we propose a Confidence-Aware Self-Distillation (CASD) strategy that effectively incorporates multimodal probabilistic embeddings via a mixture of Student's $t$-distributions, enhancing its robustness by incorporating confidence and accommodating heavy-tailed properties. This strategy estimates joint distributions with uncertainty scores and reduces uncertainty in the student network by consistency distillation. Furthermore, we introduce a reparameterization representation module that facilitates CASD in robust multimodal learning by sampling embeddings from the joint distribution for the prediction module to calculate the task loss. As a result, the directional constraint from the loss minimization is alleviated by the sampled representation. Experimental results on three benchmark datasets demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme</title>
<link>https://arxiv.org/abs/2506.01502</link>
<guid>https://arxiv.org/abs/2506.01502</guid>
<content:encoded><![CDATA[

arXiv:2506.01502v1 Announce Type: new 
Abstract: Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Importance of Blank for CTC-Based Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.01503</link>
<guid>https://arxiv.org/abs/2506.01503</guid>
<content:encoded><![CDATA[

arXiv:2506.01503v1 Announce Type: new 
Abstract: With the rise of large pre-trained foundation models for automatic speech recognition new challenges appear. While the performance of these models is good, runtime and cost of inference increases. One approach to make use of their strength while retaining efficiency is to distill their knowledge to smaller models during training. In this work, we explore different CTC-based distillation variants, focusing on blank token handling. We show that common approaches like blank elimination do not always work off the shelf. We explore new blank selection patterns as a potential sweet spot between standard knowledge distillation and blank elimination mechanisms. Through the introduction of a symmetric selection method, we are able to remove the CTC loss during knowledge distillation with minimal to no performance degradation. With this, we make the training independent from target labels, potentially allowing for distillation on untranscribed audio data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows</title>
<link>https://arxiv.org/abs/2506.01522</link>
<guid>https://arxiv.org/abs/2506.01522</guid>
<content:encoded><![CDATA[

arXiv:2506.01522v1 Announce Type: new 
Abstract: Variational Autoencoders (VAEs) are powerful generative models widely used for learning interpretable latent spaces, quantifying uncertainty, and compressing data for downstream generative tasks. VAEs typically rely on diagonal Gaussian posteriors due to computational constraints. Using arguments grounded in differential geometry, we demonstrate inherent limitations in the representational capacity of diagonal covariance VAEs, as illustrated by explicit low-dimensional examples. In response, we show that a regularized variant of the recently introduced Free-form Injective Flow (FIF) can be interpreted as a VAE featuring a highly flexible, implicitly defined posterior. Crucially, this regularization yields a posterior equivalent to a full Gaussian covariance distribution, yet maintains computational costs comparable to standard diagonal covariance VAEs. Experiments on image datasets validate our approach, demonstrating that incorporating full covariance substantially improves model likelihood.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</title>
<link>https://arxiv.org/abs/2506.01523</link>
<guid>https://arxiv.org/abs/2506.01523</guid>
<content:encoded><![CDATA[

arXiv:2506.01523v1 Announce Type: new 
Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, when viewed as `loss + regularization,' the standard RLHF objective lacks theoretical justification and incentivizes degenerate, deterministic solutions, an issue that variants such as Direct Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by framing it as \emph{distribution learning} from pairwise preference feedback by explicitly modeling how information about the target language model bleeds through the preference data. This explicit modeling leads us to propose three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We theoretically show that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Finally, we empirically demonstrate that our distribution learning framework, especially preference distillation, consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Abstract World Models with a Group-Structured Latent Space</title>
<link>https://arxiv.org/abs/2506.01529</link>
<guid>https://arxiv.org/abs/2506.01529</guid>
<content:encoded><![CDATA[

arXiv:2506.01529v1 Announce Type: new 
Abstract: Learning meaningful abstract models of Markov Decision Processes (MDPs) is crucial for improving generalization from limited data. In this work, we show how geometric priors can be imposed on the low-dimensional representation manifold of a learned transition model. We incorporate known symmetric structures via appropriate choices of the latent space and the associated group actions, which encode prior knowledge about invariances in the environment. In addition, our framework allows the embedding of additional unstructured information alongside these symmetries. We show experimentally that this leads to better predictions of the latent transition model than fully unstructured approaches, as well as better learning on downstream RL tasks, in environments with rotational and translational features, including in first-person views of 3D environments. Additionally, our experiments show that this leads to simpler and more disentangled representations. The full code is available on GitHub to ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments</title>
<link>https://arxiv.org/abs/2506.01533</link>
<guid>https://arxiv.org/abs/2506.01533</guid>
<content:encoded><![CDATA[

arXiv:2506.01533v1 Announce Type: new 
Abstract: In medicine, treatments often influence multiple, interdependent outcomes, such as primary endpoints, complications, adverse events, or other secondary endpoints. Hence, to make optimal treatment decisions, clinicians are interested in learning the distribution of multi-dimensional treatment outcomes. However, the vast majority of machine learning methods for predicting treatment effects focus on single-outcome settings, despite the fact that medical data often include multiple, interdependent outcomes. To address this limitation, we propose a novel diffusion-based method called DIME to learn the joint distribution of multiple outcomes of medical treatments. We addresses three challenges relevant in medical practice: (i)it is tailored to learn the joint interventional distribution of multiple medical outcomes, which enables reliable decision-making with uncertainty quantification rather than relying solely on point estimates; (ii)it explicitly captures the dependence structure between outcomes; (iii)it can handle outcomes of mixed type, including binary, categorical, and continuous variables. In DIME, we take into account the fundamental problem of causal inference through causal masking. For training, our method decomposes the joint distribution into a series of conditional distributions with a customized conditional masking to account for the dependence structure across outcomes. For inference, our method auto-regressively generates predictions. This allows our method to move beyond point estimates of causal quantities and thus learn the joint interventional distribution. To the best of our knowledge, DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments. Across various experiments, we demonstrate that our method effectively learns the joint distribution and captures shared information among multiple outcomes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Destruction Processes for Diffusion Samplers</title>
<link>https://arxiv.org/abs/2506.01541</link>
<guid>https://arxiv.org/abs/2506.01541</guid>
<content:encoded><![CDATA[

arXiv:2506.01541v1 Announce Type: new 
Abstract: This paper explores the challenges and benefits of a trainable destruction process in diffusion samplers -- diffusion-based generative models trained to sample an unnormalised density without access to data samples. Contrary to the majority of work that views diffusion samplers as approximations to an underlying continuous-time model, we view diffusion models as discrete-time policies trained to produce samples in very few generation steps. We propose to trade some of the elegance of the underlying theory for flexibility in the definition of the generative and destruction policies. In particular, we decouple the generation and destruction variances, enabling both transition kernels to be learned as unconstrained Gaussian densities. We show that, when the number of steps is limited, training both generation and destruction processes results in faster convergence and improved sampling quality on various benchmarks. Through a robust ablation study, we investigate the design choices necessary to facilitate stable training. Finally, we show the scalability of our approach through experiments on GAN latent space sampling for conditional image generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Variational Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2506.01544</link>
<guid>https://arxiv.org/abs/2506.01544</guid>
<content:encoded><![CDATA[

arXiv:2506.01544v1 Announce Type: new 
Abstract: We introduce Temporal Variational Implicit Neural Representations (TV-INRs), a probabilistic framework for modeling irregular multivariate time series that enables efficient individualized imputation and forecasting. By integrating implicit neural representations with latent variable models, TV-INRs learn distributions over time-continuous generator functions conditioned on signal-specific covariates. Unlike existing approaches that require extensive training, fine-tuning or meta-learning, our method achieves accurate individualized predictions through a single forward pass. Our experiments demonstrate that with a single TV-INRs instance, we can accurately solve diverse imputation and forecasting tasks, offering a computationally efficient and scalable solution for real-world applications. TV-INRs excel especially in low-data regimes, where it outperforms existing methods by an order of magnitude in mean squared error for imputation task.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Incremental Learning for Algorithm Selection</title>
<link>https://arxiv.org/abs/2506.01545</link>
<guid>https://arxiv.org/abs/2506.01545</guid>
<content:encoded><![CDATA[

arXiv:2506.01545v1 Announce Type: new 
Abstract: Algorithm selection is commonly used to predict the best solver from a portfolio per per-instance. In many real scenarios, instances arrive in a stream: new instances become available over time, while the number of class labels can also grow as new data distributions arrive downstream. As a result, the classification model needs to be periodically updated to reflect additional solvers without catastrophic forgetting of past data. In machine-learning (ML), this is referred to as Class Incremental Learning (CIL). While commonly addressed in ML settings, its relevance to algorithm-selection in optimisation has not been previously studied. Using a bin-packing dataset, we benchmark 8 continual learning methods with respect to their ability to withstand catastrophic forgetting. We find that rehearsal-based methods significantly outperform other CIL methods. While there is evidence of forgetting, the loss is small at around 7%. Hence, these methods appear to be a viable approach to continual learning in streaming optimisation scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers</title>
<link>https://arxiv.org/abs/2506.01552</link>
<guid>https://arxiv.org/abs/2506.01552</guid>
<content:encoded><![CDATA[

arXiv:2506.01552v1 Announce Type: new 
Abstract: Hierarchical classification offers an approach to incorporate the concept of mistake severity by leveraging a structured, labeled hierarchy. However, decoding in such settings frequently relies on heuristic decision rules, which may not align with task-specific evaluation metrics. In this work, we propose a framework for the optimal decoding of an output probability distribution with respect to a target metric. We derive optimal decision rules for increasingly complex prediction settings, providing universal algorithms when candidates are limited to the set of nodes. In the most general case of predicting a subset of nodes, we focus on rules dedicated to the hierarchical $hF_{\beta}$ scores, tailored to hierarchical settings. To demonstrate the practical utility of our approach, we conduct extensive empirical evaluations, showcasing the superiority of our proposed optimal strategies, particularly in underdetermined scenarios. These results highlight the potential of our methods to enhance the performance and reliability of hierarchical classifiers in real-world applications. The code is available at https://github.com/RomanPlaud/hierarchical_decision_rules
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization</title>
<link>https://arxiv.org/abs/2506.01562</link>
<guid>https://arxiv.org/abs/2506.01562</guid>
<content:encoded><![CDATA[

arXiv:2506.01562v1 Announce Type: new 
Abstract: The softmax function is a fundamental building block of deep neural networks, commonly used to define output distributions in classification tasks or attention weights in transformer architectures. Despite its widespread use and proven effectiveness, its influence on learning dynamics and learned representations remains poorly understood, limiting our ability to optimize model behavior. In this paper, we study the pivotal role of the softmax function in shaping the model's representation. We introduce the concept of rank deficit bias - a phenomenon in which softmax-based deep networks find solutions of rank much lower than the number of classes. This bias depends on the softmax function's logits norm, which is implicitly influenced by hyperparameters or directly modified by softmax temperature. Furthermore, we demonstrate how to exploit the softmax dynamics to learn compressed representations or to enhance their performance on out-of-distribution data. We validate our findings across diverse architectures and real-world datasets, highlighting the broad applicability of temperature tuning in improving model performance. Our work provides new insights into the mechanisms of softmax, enabling better control over representation learning in deep neural networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory First: A Curriculum for Discovering Diverse Policies</title>
<link>https://arxiv.org/abs/2506.01568</link>
<guid>https://arxiv.org/abs/2506.01568</guid>
<content:encoded><![CDATA[

arXiv:2506.01568v1 Announce Type: new 
Abstract: Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima. In this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel. However, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity. To improve diversity optimization in RL, we therefore propose a curriculum that first explores at the trajectory level before learning step-based policies. In our empirical evaluation, we provide novel insights into the shortcoming of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Topology Evolution in Multilayer Perceptrons</title>
<link>https://arxiv.org/abs/2506.01569</link>
<guid>https://arxiv.org/abs/2506.01569</guid>
<content:encoded><![CDATA[

arXiv:2506.01569v1 Announce Type: new 
Abstract: This paper introduces a topological framework for interpreting the internal representations of Multilayer Perceptrons (MLPs). We construct a simplicial tower, a sequence of simplicial complexes connected by simplicial maps, that captures how data topology evolves across network layers. Our approach enables bi-persistence analysis: layer persistence tracks topological features within each layer across scales, while MLP persistence reveals how these features transform through the network. We prove stability theorems for our topological descriptors and establish that linear separability in latent spaces is related to disconnected components in the nerve complexes. To make our framework practical, we develop a combinatorial algorithm for computing MLP persistence and introduce trajectory-based visualisations that track data flow through the network. Experiments on synthetic and real-world medical data demonstrate our method's ability to identify redundant layers, reveal critical topological transitions, and provide interpretable insights into how MLPs progressively organise data for classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayes optimal learning of attention-indexed models</title>
<link>https://arxiv.org/abs/2506.01582</link>
<guid>https://arxiv.org/abs/2506.01582</guid>
<content:encoded><![CDATA[

arXiv:2506.01582v1 Announce Type: new 
Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for analyzing learning in deep attention layers. Inspired by multi-index models, AIM captures how token-level outputs emerge from layered bilinear interactions over high-dimensional embeddings. Unlike prior tractable attention models, AIM allows full-width key and query matrices, aligning more closely with practical transformers. Using tools from statistical mechanics and random matrix theory, we derive closed-form predictions for Bayes-optimal generalization error and identify sharp phase transitions as a function of sample complexity, model width, and sequence length. We propose a matching approximate message passing algorithm and show that gradient descent can reach optimal performance. AIM offers a solvable playground for understanding learning in modern attention architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VirnyFlow: A Design Space for Responsible Model Development</title>
<link>https://arxiv.org/abs/2506.01584</link>
<guid>https://arxiv.org/abs/2506.01584</guid>
<content:encoded><![CDATA[

arXiv:2506.01584v1 Announce Type: new 
Abstract: Developing machine learning (ML) models requires a deep understanding of real-world problems, which are inherently multi-objective. In this paper, we present VirnyFlow, the first design space for responsible model development, designed to assist data scientists in building ML pipelines that are tailored to the specific context of their problem. Unlike conventional AutoML frameworks, VirnyFlow enables users to define customized optimization criteria, perform comprehensive experimentation across pipeline stages, and iteratively refine models in alignment with real-world constraints. Our system integrates evaluation protocol definition, multi-objective Bayesian optimization, cost-aware multi-armed bandits, query optimization, and distributed parallelism into a unified architecture. We show that VirnyFlow significantly outperforms state-of-the-art AutoML systems in both optimization quality and scalability across five real-world benchmarks, offering a flexible, efficient, and responsible alternative to black-box automation in ML development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</title>
<link>https://arxiv.org/abs/2506.01594</link>
<guid>https://arxiv.org/abs/2506.01594</guid>
<content:encoded><![CDATA[

arXiv:2506.01594v1 Announce Type: new 
Abstract: As machine learning models are increasingly embedded into society through high-stakes decision-making, selecting the right algorithm for a given task, audience, and sector presents a critical challenge, particularly in the context of fairness. Traditional assessments of model fairness have often framed fairness as an objective mathematical property, treating model selection as an optimization problem under idealized informational conditions. This overlooks model multiplicity as a consideration--that multiple models can deliver similar performance while exhibiting different fairness characteristics. Legal scholars have engaged this challenge through the concept of Less Discriminatory Algorithms (LDAs), which frames model selection as a civil rights obligation. In real-world deployment, this normative challenge is bounded by constraints on fairness experimentation, e.g., regulatory standards, institutional priorities, and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s relational fairness approach using updated 2021 Home Mortgage Disclosure Act (HMDA) data, and proposes an expansion of the scope of the LDA search process. We argue that extending the LDA search horizontally, considering fairness across model families themselves, provides a lightweight complement, or alternative, to within-model hyperparameter optimization, when operationalizing fairness in non-experimental, resource constrained settings. Fairness metrics alone offer useful, but insufficient signals to accurately evaluate candidate LDAs. Rather, by using a horizontal LDA search approach with the relational trade-off framework, we demonstrate a responsible minimum viable LDA search on real-world lending outcomes. Organizations can modify this approach to systematically compare, evaluate, and select LDAs that optimize fairness and accuracy in a sector-based contextualized manner.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Improving Laplacian Positional Encodings For Temporal GNNs</title>
<link>https://arxiv.org/abs/2506.01596</link>
<guid>https://arxiv.org/abs/2506.01596</guid>
<content:encoded><![CDATA[

arXiv:2506.01596v1 Announce Type: new 
Abstract: Temporal graph learning has applications in recommendation systems, traffic forecasting, and social network analysis. Although multiple architectures have been introduced, progress in positional encoding for temporal graphs remains limited. Extending static Laplacian eigenvector approaches to temporal graphs through the supra-Laplacian has shown promise, but also poses key challenges: high eigendecomposition costs, limited theoretical understanding, and ambiguity about when and how to apply these encodings. In this paper, we address these issues by (1) offering a theoretical framework that connects supra-Laplacian encodings to per-time-slice encodings, highlighting the benefits of leveraging additional temporal connectivity, (2) introducing novel methods to reduce the computational overhead, achieving up to 56x faster runtimes while scaling to graphs with 50,000 active nodes, and (3) conducting an extensive experimental study to identify which models, tasks, and datasets benefit most from these encodings. Our findings reveal that while positional encodings can significantly boost performance in certain scenarios, their effectiveness varies across different models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Newton Algorithm in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2506.01597</link>
<guid>https://arxiv.org/abs/2506.01597</guid>
<content:encoded><![CDATA[

arXiv:2506.01597v1 Announce Type: new 
Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel Hilbert Spaces (RKHS) offer powerful representational capabilities. While second-order optimization methods like Newton's method demonstrate faster convergence than first-order approaches, current RKHS-based policy optimization remains constrained to first-order techniques. This limitation stems primarily from the intractability of explicitly computing and inverting the infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in RKHS, the first second-order optimization framework specifically designed for RL policies represented in RKHS. Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function. Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume. We establish theoretical guarantees proving convergence to a local optimum with a local quadratic convergence rate. Empirical evaluations on a toy financial asset allocation problem validate these theoretical properties, while experiments on standard RL benchmarks demonstrate that Policy Newton in RKHS achieves superior convergence speed and higher episodic rewards compared to established first-order RKHS approaches and parametric second-order methods. Our work bridges a critical gap between non-parametric policy representations and second-order optimization methods in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations</title>
<link>https://arxiv.org/abs/2506.01598</link>
<guid>https://arxiv.org/abs/2506.01598</guid>
<content:encoded><![CDATA[

arXiv:2506.01598v1 Announce Type: new 
Abstract: Neural operators, which aim to approximate mappings between infinite-dimensional function spaces, have been widely applied in the simulation and prediction of physical systems. However, the limited representational capacity of network architectures, combined with their heavy reliance on large-scale data, often hinder effective training and result in poor extrapolation performance. In this paper, inspired by traditional numerical methods, we propose a novel physics guided multi-step neural operator (PMNO) architecture to address these challenges in long-horizon prediction of complex physical systems. Distinct from general operator learning methods, the PMNO framework replaces the single-step input with multi-step historical data in the forward pass and introduces an implicit time-stepping scheme based on the Backward Differentiation Formula (BDF) during backpropagation. This design not only strengthens the model's extrapolation capacity but also facilitates more efficient and stable training with fewer data samples, especially for long-term predictions. Meanwhile, a causal training strategy is employed to circumvent the need for multi-stage training and to ensure efficient end-to-end optimization. The neural operator architecture possesses resolution-invariant properties, enabling the trained model to perform fast extrapolation on arbitrary spatial resolutions. We demonstrate the superior predictive performance of PMNO predictor across a diverse range of physical systems, including 2D linear system, modeling over irregular domain, complex-valued wave dynamics, and reaction-diffusion processes. Depending on the specific problem setting, various neural operator architectures, including FNO, DeepONet, and their variants, can be seamlessly integrated into the PMNO framework.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting Neural Models Latent Geometries with Relative Geodesic Representations</title>
<link>https://arxiv.org/abs/2506.01599</link>
<guid>https://arxiv.org/abs/2506.01599</guid>
<content:encoded><![CDATA[

arXiv:2506.01599v1 Announce Type: new 
Abstract: Neural models learn representations of high-dimensional data on low-dimensional manifolds. Multiple factors, including stochasticities in the training process, model architectures, and additional inductive biases, may induce different representations, even when learning the same task on the same data. However, it has recently been shown that when a latent structure is shared between distinct latent spaces, relative distances between representations can be preserved, up to distortions. Building on this idea, we demonstrate that exploiting the differential-geometric structure of latent spaces of neural models, it is possible to capture precisely the transformations between representational spaces trained on similar data distributions. Specifically, we assume that distinct neural models parametrize approximately the same underlying manifold, and introduce a representation based on the pullback metric that captures the intrinsic structure of the latent space, while scaling efficiently to large models. We validate experimentally our method on model stitching and retrieval tasks, covering autoencoders and vision foundation discriminative models, across diverse architectures, datasets, and pretraining schemes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains</title>
<link>https://arxiv.org/abs/2506.01614</link>
<guid>https://arxiv.org/abs/2506.01614</guid>
<content:encoded><![CDATA[

arXiv:2506.01614v1 Announce Type: new 
Abstract: This paper introduces a Machine Learning (ML) approach for scalability of UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding struggle with distributing UTXOs effectively across validators, creating substantial communication overhead due to child-parent transaction dependencies. This overhead, which arises from the need to locate parent UTXOs, significantly hampers transaction processing speeds. Our solution uses ML to optimize not only UTXO set sharding but also the routing of incoming transactions, ensuring that transactions are directed to shards containing their parent UTXOs. At the heart of our approach is a framework that combines contrastive and unsupervised learning to create an embedding space for transaction outputs. This embedding allows the model to group transaction outputs based on spending relationships, making it possible to route transactions efficiently to the correct validation microservices. Trained on historical transaction data with triplet loss and online semi-hard negative mining, the model embeds parent-child spending patterns directly into its parameters, thus eliminating the need for costly, real-time parent transaction lookups. This significantly reduces cross-shard communication overhead, boosting throughput and scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[

arXiv:2506.01625v1 Announce Type: new 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</title>
<link>https://arxiv.org/abs/2506.01631</link>
<guid>https://arxiv.org/abs/2506.01631</guid>
<content:encoded><![CDATA[

arXiv:2506.01631v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01639</link>
<guid>https://arxiv.org/abs/2506.01639</guid>
<content:encoded><![CDATA[

arXiv:2506.01639v1 Announce Type: new 
Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum entropy reinforcement learning, traditionally relies on minimizing reverse Kullback-Leibler (KL) divergence for policy updates. However, this approach leads to an intractable optimal projection policy, necessitating gradient-based approximations that can suffer from instability and poor sample efficiency. This paper investigates the alternative use of forward KL divergence within SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an explicit optimal projection policy -- corresponding to the mean and variance of the target Boltzmann distribution's action marginals. Building on the distinct advantages of both KL directions, we propose Bidirectional SAC, an algorithm that first initializes the policy using the explicit forward KL projection and then refines it by optimizing the reverse KL divergence. Comprehensive experiments on continuous control benchmarks show that Bidirectional SAC significantly outperforms standard SAC and other baselines, achieving up to a $30\%$ increase in episodic rewards, alongside enhanced sample efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning</title>
<link>https://arxiv.org/abs/2506.01656</link>
<guid>https://arxiv.org/abs/2506.01656</guid>
<content:encoded><![CDATA[

arXiv:2506.01656v1 Announce Type: new 
Abstract: Mixture of Experts (MoE), an ensemble of specialized models equipped with a router that dynamically distributes each input to appropriate experts, has achieved successful results in the field of machine learning. However, theoretical understanding of this architecture is falling behind due to its inherent complexity. In this paper, we theoretically study the sample and runtime complexity of MoE following the stochastic gradient descent (SGD) when learning a regression task with an underlying cluster structure of single index models. On the one hand, we prove that a vanilla neural network fails in detecting such a latent organization as it can only process the problem as a whole. This is intrinsically related to the concept of information exponent which is low for each cluster, but increases when we consider the entire task. On the other hand, we show that a MoE succeeds in dividing this problem into easier subproblems by leveraging the ability of each expert to weakly recover the simpler function corresponding to an individual cluster. To the best of our knowledge, this work is among the first to explore the benefits of the MoE framework by examining its SGD dynamics in the context of nonlinear regression.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Safe Reinforcement Learning from Analytic Gradients</title>
<link>https://arxiv.org/abs/2506.01665</link>
<guid>https://arxiv.org/abs/2506.01665</guid>
<content:encoded><![CDATA[

arXiv:2506.01665v1 Announce Type: new 
Abstract: Deploying autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research which aims to provide such guarantees using safeguards. These safeguards should be integrated during training to prevent a large sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance and sample efficiency. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them with a state-of-the-art learning algorithm and a differentiable simulation. We evaluate how different safeguards affect policy optimisation using numerical experiments on two classical control tasks. The results demonstrate safeguarded training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Impact ControlNet: Advancing Multi-ControlNet Integration</title>
<link>https://arxiv.org/abs/2506.01672</link>
<guid>https://arxiv.org/abs/2506.01672</guid>
<content:encoded><![CDATA[

arXiv:2506.01672v1 Announce Type: new 
Abstract: With the advancement of diffusion models, there is a growing demand for high-quality, controllable image generation, particularly through methods that utilize one or multiple control signals based on ControlNet. However, in current ControlNet training, each control is designed to influence all areas of an image, which can lead to conflicts when different control signals are expected to manage different parts of the image in practical applications. This issue is especially pronounced with edge-type control conditions, where regions lacking boundary information often represent low-frequency signals, referred to as silent control signals. When combining multiple ControlNets, these silent control signals can suppress the generation of textures in related areas, resulting in suboptimal outcomes. To address this problem, we propose Minimal Impact ControlNet. Our approach mitigates conflicts through three key strategies: constructing a balanced dataset, combining and injecting feature signals in a balanced manner, and addressing the asymmetry in the score function's Jacobian matrix induced by ControlNet. These improvements enhance the compatibility of control signals, allowing for freer and more harmonious generation in areas with silent control signals.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses</title>
<link>https://arxiv.org/abs/2506.01722</link>
<guid>https://arxiv.org/abs/2506.01722</guid>
<content:encoded><![CDATA[

arXiv:2506.01722v1 Announce Type: new 
Abstract: We consider the problem setting of prediction with expert advice with possibly heavy-tailed losses, i.e.\ the only assumption on the losses is an upper bound on their second moments, denoted by $\theta$. We develop adaptive algorithms that do not require any prior knowledge about the range or the second moment of the losses. Existing adaptive algorithms have what is typically considered a lower-order term in their regret guarantees. We show that this lower-order term, which is often the maximum of the losses, can actually dominate the regret bound in our setting. Specifically, we show that even with small constant $\theta$, this lower-order term can scale as $\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We propose adaptive algorithms with improved regret bounds that avoid the dependence on such a lower-order term and guarantee $\mathcal{O}(\sqrt{\theta T\log(K)})$ regret in the worst case, and $\mathcal{O}(\theta \log(KT)/\Delta_{\min})$ regret when the losses are sampled i.i.d.\ from some fixed distribution, where $\Delta_{\min}$ is the difference between the mean losses of the second best expert and the best expert. Additionally, when the loss function is the squared loss, our algorithm also guarantees improved regret bounds over prior results.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled data augmentation for learning to solve quadratic programming problems</title>
<link>https://arxiv.org/abs/2506.01728</link>
<guid>https://arxiv.org/abs/2506.01728</guid>
<content:encoded><![CDATA[

arXiv:2506.01728v1 Announce Type: new 
Abstract: Linear and quadratic optimization are crucial in numerous real-world applications, from training machine learning models to integer-linear optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, requiring solving many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised learning framework based on contrastive learning, thereby pretraining MPNNs for enhanced performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Manifold Learning for Reduced Order Modeling</title>
<link>https://arxiv.org/abs/2506.01741</link>
<guid>https://arxiv.org/abs/2506.01741</guid>
<content:encoded><![CDATA[

arXiv:2506.01741v1 Announce Type: new 
Abstract: The problem of identifying geometric structure in data is a cornerstone of (unsupervised) learning. As a result, Geometric Representation Learning has been widely applied across scientific and engineering domains. In this work, we investigate the use of Geometric Representation Learning for the data-driven discovery of system dynamics from spatial-temporal data. We propose to encode similarity structure in such data in a spatial-temporal proximity graph, to which we apply a range of classical and deep learning-based manifold learning approaches to learn reduced order dynamics. We observe that while manifold learning is generally capable of recovering reduced order dynamics, the quality of the learned representations varies substantially across different algorithms and hyperparameter choices. This is indicative of high sensitivity to the inherent geometric assumptions of the respective approaches and suggests a need for careful hyperparameter tuning, which can be expensive in practise. To overcome these challenges, we propose a framework for Automated Manifold Learning, which selects a manifold learning approach and corresponding hyperparameter choices based on representative subsamples of the input graph. We demonstrate that the proposed framework leads to performance gains both in scalability and in the learned representations' accuracy in capturing local and global geometric features of the underlying system dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems</title>
<link>https://arxiv.org/abs/2506.01777</link>
<guid>https://arxiv.org/abs/2506.01777</guid>
<content:encoded><![CDATA[

arXiv:2506.01777v1 Announce Type: new 
Abstract: Federated Unlearning (FU) enables clients to remove the influence of specific data from a collaboratively trained shared global model, addressing regulatory requirements such as GDPR and CCPA. However, this unlearning process introduces a new privacy risk: A malicious server may exploit unlearning updates to reconstruct the data requested for removal, a form of Data Reconstruction Attack (DRA). While DRAs for machine unlearning have been studied extensively in centralized Machine Learning-as-a-Service (MLaaS) settings, their applicability to FU remains unclear due to the decentralized, client-driven nature of FU. This work presents DRAUN, the first attack framework to reconstruct unlearned data in FU systems. DRAUN targets optimization-based unlearning methods, which are widely adopted for their efficiency. We theoretically demonstrate why existing DRAs targeting machine unlearning in MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate our approach through extensive experiments on four datasets and four model architectures, evaluating its performance against five popular unlearning methods, effectively demonstrating that state-of-the-art FU methods remain vulnerable to DRAs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2506.01780</link>
<guid>https://arxiv.org/abs/2506.01780</guid>
<content:encoded><![CDATA[

arXiv:2506.01780v1 Announce Type: new 
Abstract: This paper introduces FedGenGMM, a novel one-shot federated learning approach for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios. In federated learning (FL), where multiple decentralized clients collaboratively train models without sharing raw data, significant challenges include statistical heterogeneity, high communication costs, and privacy concerns. FedGenGMM addresses these issues by allowing local GMM models, trained independently on client devices, to be aggregated through a single communication round. This approach leverages the generative property of GMMs, enabling the creation of a synthetic dataset on the server side to train a global model efficiently. Evaluation across diverse datasets covering image, tabular, and time series data demonstrates that FedGenGMM consistently achieves performance comparable to non-federated and iterative federated methods, even under significant data heterogeneity. Additionally, FedGenGMM significantly reduces communication overhead, maintains robust performance in anomaly detection tasks, and offers flexibility in local model complexities, making it particularly suitable for edge computing environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.01781</link>
<guid>https://arxiv.org/abs/2506.01781</guid>
<content:encoded><![CDATA[

arXiv:2506.01781v1 Announce Type: new 
Abstract: Customer service chatbots are conversational systems aimed at addressing customer queries, often by directing them to automated workflows. A crucial aspect of this process is the classification of the customer's intent. Presently, most intent classification models for customer care utilise only customer query for intent prediction. This may result in low-accuracy models, which cannot handle ambiguous queries. An ambiguous query like "I didn't receive my package" could indicate a delayed order, or an order that was delivered but the customer failed to receive it. Resolution of each of these scenarios requires the execution of very different sequence of steps. Utilizing additional information, such as the customer's order delivery status, in the right manner can help identify the intent for such ambiguous queries. In this paper, we have introduced a context-aware NLU model that incorporates both, the customer query and contextual information from the customer's order status for predicting customer intent. A novel selective attention module is used to extract relevant context features. We have also proposed a multi-task learning paradigm for the effective utilization of different label types available in our training data. Our suggested method, Multi-Task Learning Contextual NLU with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8% increase in top 2 accuracy score over the baseline model which only uses user queries, and a 3.5% improvement over existing state-of-the-art models that combine query and context. We have deployed our model to production for Walmart's customer care domain. Accurate intent prediction through MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby significantly reducing escalations to human agents, leading to almost a million dollars in yearly savings for the company.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability</title>
<link>https://arxiv.org/abs/2506.01789</link>
<guid>https://arxiv.org/abs/2506.01789</guid>
<content:encoded><![CDATA[

arXiv:2506.01789v1 Announce Type: new 
Abstract: High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs</title>
<link>https://arxiv.org/abs/2506.01790</link>
<guid>https://arxiv.org/abs/2506.01790</guid>
<content:encoded><![CDATA[

arXiv:2506.01790v1 Announce Type: new 
Abstract: We study how training data contributes to the emergence of toxic behaviors in large-language models. Most prior work on reducing model toxicity adopts $reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a $proactive$ approach$-$IF-Guide$-$which leverages influence functions to identify harmful tokens within any training data and suppress their impact during training. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-Guide does not rely on human-preference data, which is typically required by existing alignment methods. In evaluation, we demonstrate that IF-Guide substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$ compared to uncensored models, and up to 3$\times$ compared to baseline alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning scenarios. IF-Guide is computationally efficient: a billion-parameter model is $not$ $necessary$ for computing influence scores; a million-parameter model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy for identifying harmful data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique</title>
<link>https://arxiv.org/abs/2506.01815</link>
<guid>https://arxiv.org/abs/2506.01815</guid>
<content:encoded><![CDATA[

arXiv:2506.01815v1 Announce Type: new 
Abstract: We provide an introduction to the topic of path signatures as means of feature extraction for machine learning from data streams. The article stresses the mathematical theory underlying the signature methodology, highlighting the conceptual character without plunging into the technical details of rigorous proofs. These notes are based on an introductory presentation given to students of the Research Experience for Undergraduates in Industrial Mathematics and Statistics at Worcester Polytechnic Institute in June 2024.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming</title>
<link>https://arxiv.org/abs/2506.01826</link>
<guid>https://arxiv.org/abs/2506.01826</guid>
<content:encoded><![CDATA[

arXiv:2506.01826v1 Announce Type: new 
Abstract: Signed graphs are equipped with both positive and negative edge weights, encoding pairwise correlations as well as anti-correlations in data. A balanced signed graph is a signed graph with no cycles containing an odd number of negative edges. Laplacian of a balanced signed graph has eigenvectors that map via a simple linear transform to ones in a corresponding positive graph Laplacian, thus enabling reuse of spectral filtering tools designed for positive graphs. We propose an efficient method to learn a balanced signed graph Laplacian directly from data. Specifically, extending a previous linear programming (LP) based sparse inverse covariance estimation method called CLIME, we formulate a new LP problem for each Laplacian column $i$, where the linear constraints restrict weight signs of edges stemming from node $i$, so that nodes of same / different polarities are connected by positive / negative edges. Towards optimal model selection, we derive a suitable CLIME parameter $\rho$ based on a combination of the Hannan-Quinn information criterion and a minimum feasibility criterion. We solve the LP problem efficiently by tailoring a sparse LP method based on ADMM. We theoretically prove local solution convergence of our proposed iterative algorithm. Extensive experimental results on synthetic and real-world datasets show that our balanced graph learning method outperforms competing methods and enables reuse of spectral filters, wavelets, and graph convolutional nets (GCN) constructed for positive graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts</title>
<link>https://arxiv.org/abs/2506.01827</link>
<guid>https://arxiv.org/abs/2506.01827</guid>
<content:encoded><![CDATA[

arXiv:2506.01827v1 Announce Type: new 
Abstract: As machine learning algorithms are shown to be an increasingly valuable tool, the demand for their access has grown accordingly. Oftentimes, it is infeasible to run inference with larger models without an accelerator, which may be unavailable in environments that have constraints such as energy consumption, security, or cost. To increase the availability of these models, we aim to im- prove the LLM inference speed on a CPU-only environment by modifying the cache architecture. To determine what improvements could be made, we conducted two experiments using Llama.cpp and the QWEN model: running various cache configurations and evaluating their performance, and outputting a trace of the memory footprint. Using these experiments, we investigate the memory access patterns and performance characteristics to identify potential optimizations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model</title>
<link>https://arxiv.org/abs/2506.01833</link>
<guid>https://arxiv.org/abs/2506.01833</guid>
<content:encoded><![CDATA[

arXiv:2506.01833v1 Announce Type: new 
Abstract: Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our $\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive $\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners. The code is available at https://github.com/ZhuJiwei111/SPACE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics</title>
<link>https://arxiv.org/abs/2506.01844</link>
<guid>https://arxiv.org/abs/2506.01844</guid>
<content:encoded><![CDATA[

arXiv:2506.01844v1 Announce Type: new 
Abstract: Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trojan Horse Hunt in Time Series Forecasting for Space Operations</title>
<link>https://arxiv.org/abs/2506.01849</link>
<guid>https://arxiv.org/abs/2506.01849</guid>
<content:encoded><![CDATA[

arXiv:2506.01849v1 Announce Type: new 
Abstract: This competition hosted on Kaggle (https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first part of a series of follow-up competitions and hackathons related to the "Assurance for Space Domain AI Applications" project funded by the European Space Agency (https://assurance-ai.space-codev.org/). The competition idea is based on one of the real-life AI security threats identified within the project -- the adversarial poisoning of continuously fine-tuned satellite telemetry forecasting models. The task is to develop methods for finding and reconstructing triggers (trojans) in advanced models for satellite telemetry forecasting used in safety-critical space operations. Participants are provided with 1) a large public dataset of real-life multivariate satellite telemetry (without triggers), 2) a reference model trained on the clean data, 3) a set of poisoned neural hierarchical interpolation (N-HiTS) models for time series forecasting trained on the dataset with injected triggers, and 4) Jupyter notebook with the training pipeline and baseline algorithm (the latter will be published in the last month of the competition). The main task of the competition is to reconstruct a set of 45 triggers (i.e., short multivariate time series segments) injected into the training data of the corresponding set of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and duration) of these triggers must be identified by participants. The popular Neural Cleanse method is adopted as a baseline, but it is not designed for time series analysis and new approaches are necessary for the task. The impact of the competition is not limited to the space domain, but also to many other safety-critical applications of advanced time series analysis where model poisoning may lead to serious consequences.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trade-offs in Data Memorization via Strong Data Processing Inequalities</title>
<link>https://arxiv.org/abs/2506.01855</link>
<guid>https://arxiv.org/abs/2506.01855</guid>
<content:encoded><![CDATA[

arXiv:2506.01855v1 Announce Type: new 
Abstract: Recent research demonstrated that training large language models involves memorization of a significant fraction of training data. Such memorization can lead to privacy violations when training on sensitive user data and thus motivates the study of data memorization's role in learning. In this work, we develop a general approach for proving lower bounds on excess data memorization, that relies on a new connection between strong data processing inequalities and data memorization. We then demonstrate that several simple and natural binary classification problems exhibit a trade-off between the number of samples available to a learning algorithm, and the amount of information about the training data that a learning algorithm needs to memorize to be accurate. In particular, $\Omega(d)$ bits of information about the training data need to be memorized when $O(1)$ $d$-dimensional examples are available, which then decays as the number of examples grows at a problem-specific rate. Further, our lower bounds are generally matched (up to logarithmic factors) by simple learning algorithms. We also extend our lower bounds to more general mixture-of-clusters models. Our definitions and results build on the work of Brown et al. (2021) and address several limitations of the lower bounds in their work.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Scaling Laws for Compressed Representations</title>
<link>https://arxiv.org/abs/2506.01863</link>
<guid>https://arxiv.org/abs/2506.01863</guid>
<content:encoded><![CDATA[

arXiv:2506.01863v1 Announce Type: new 
Abstract: Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials</title>
<link>https://arxiv.org/abs/2506.01868</link>
<guid>https://arxiv.org/abs/2506.01868</guid>
<content:encoded><![CDATA[

arXiv:2506.01868v1 Announce Type: new 
Abstract: As a machine-learned potential, the neuroevolution potential (NEP) method features exceptional computational efficiency and has been successfully applied in materials science. Constructing high-quality training datasets is crucial for developing accurate NEP models. However, the preparation and screening of NEP training datasets remain a bottleneck for broader applications due to their time-consuming, labor-intensive, and resource-intensive nature. In this work, we have developed NepTrain and NepTrainKit, which are dedicated to initializing and managing training datasets to generate high-quality training sets while automating NEP model training. NepTrain is an open-source Python package that features a bond length filtering method to effectively identify and remove non-physical structures from molecular dynamics trajectories, thereby ensuring high-quality training datasets. NepTrainKit is a graphical user interface (GUI) software designed specifically for NEP training datasets, providing functionalities for data editing, visualization, and interactive exploration. It integrates key features such as outlier identification, farthest-point sampling, non-physical structure detection, and configuration type selection. The combination of these tools enables users to process datasets more efficiently and conveniently. Using $\rm CsPbI_3$ as a case study, we demonstrate the complete workflow for training NEP models with NepTrain and further validate the models through materials property predictions. We believe this toolkit will greatly benefit researchers working with machine learning interatomic potentials.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.01869</link>
<guid>https://arxiv.org/abs/2506.01869</guid>
<content:encoded><![CDATA[

arXiv:2506.01869v1 Announce Type: new 
Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine Learning (ML) models that are efficient, cost-effective, and mindful of resource constraints. This field aims to achieve acceptable performance while minimizing the use of computational resources, time, energy, and data for both training and inference. FML strategies can be broadly categorized into input frugality, learning process frugality, and model frugality, each focusing on reducing resource consumption at different stages of the ML pipeline. This chapter explores recent advancements, applications, and open challenges in FML, emphasizing its importance for smart environments that incorporate edge computing and IoT devices, which often face strict limitations in bandwidth, energy, or latency. Technological enablers such as model compression, energy-efficient hardware, and data-efficient learning techniques are discussed, along with adaptive methods including parameter regularization, knowledge distillation, and dynamic architecture design that enable incremental model updates without full retraining. Furthermore, it provides a comprehensive taxonomy of frugal methods, discusses case studies across diverse domains, and identifies future research directions to drive innovation in this evolving field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Explore: An In-Context Learning Approach for Pure Exploration</title>
<link>https://arxiv.org/abs/2506.01876</link>
<guid>https://arxiv.org/abs/2506.01876</guid>
<content:encoded><![CDATA[

arXiv:2506.01876v1 Announce Type: new 
Abstract: In this work, we study the active sequential hypothesis testing problem, also known as pure exploration, where the goal is to actively control a data collection process to efficiently identify the correct hypothesis underlying a decision problem. While relevant across multiple domains, devising adaptive exploration strategies remains challenging, particularly due to difficulties in encoding appropriate inductive biases. Existing Reinforcement Learning (RL)-based methods often underperform when relevant information structures are inadequately represented, whereas more complex methods, like Best Arm Identification (BAI) techniques, may be difficult to devise and typically rely on explicit modeling assumptions. To address these limitations, we introduce In-Context Pure Exploration (ICPE), an in-context learning approach that uses Transformers to learn exploration strategies directly from experience. ICPE combines supervised learning and reinforcement learning to identify and exploit latent structure across related tasks, without requiring prior assumptions. Numerical results across diverse synthetic and semi-synthetic benchmarks highlight ICPE's capability to achieve robust performance performance in deterministic, stochastic, and structured settings. These results demonstrate ICPE's ability to match optimal instance-dependent algorithms using only deep learning techniques, making it a practical and general approach to data-efficient exploration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics</title>
<link>https://arxiv.org/abs/2506.01883</link>
<guid>https://arxiv.org/abs/2506.01883</guid>
<content:encoded><![CDATA[

arXiv:2506.01883v1 Announce Type: new 
Abstract: Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$ speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and an 18$\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agnostic Reinforcement Learning: Foundations and Algorithms</title>
<link>https://arxiv.org/abs/2506.01884</link>
<guid>https://arxiv.org/abs/2506.01884</guid>
<content:encoded><![CDATA[

arXiv:2506.01884v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success across numerous challenging domains. However, we lack a strong theoretical understanding of the statistical complexity of RL in environments with large state spaces, where function approximation is required for sample-efficient learning. This thesis addresses this gap by rigorously examining the statistical complexity of RL with function approximation from a learning theoretic perspective. Departing from a long history of prior work, we consider the weakest form of function approximation, called agnostic policy learning, in which the learner seeks to find the best policy in a given class $\Pi$, with no guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes: environment access -- how a learner collects data from the environment; coverage conditions -- intrinsic properties of the underlying MDP measuring the expansiveness of state-occupancy measures for policies in the class $\Pi$, and representational conditions -- structural assumptions on the class $\Pi$ itself. Within this comprehensive framework, we (1) design new learning algorithms with theoretical guarantees and (2) characterize fundamental performance bounds of any algorithm. Our results reveal significant statistical separations that highlight the power and limitations of agnostic policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection</title>
<link>https://arxiv.org/abs/2506.01890</link>
<guid>https://arxiv.org/abs/2506.01890</guid>
<content:encoded><![CDATA[

arXiv:2506.01890v1 Announce Type: new 
Abstract: Early detection of cognitive disorders such as Alzheimer's disease is critical for enabling timely clinical intervention and improving patient outcomes. In this work, we introduce CogniAlign, a multimodal architecture for Alzheimer's detection that integrates audio and textual modalities, two non-intrusive sources of information that offer complementary insights into cognitive health. Unlike prior approaches that fuse modalities at a coarse level, CogniAlign leverages a word-level temporal alignment strategy that synchronizes audio embeddings with corresponding textual tokens based on transcription timestamps. This alignment supports the development of token-level fusion techniques, enabling more precise cross-modal interactions. To fully exploit this alignment, we propose a Gated Cross-Attention Fusion mechanism, where audio features attend over textual representations, guided by the superior unimodal performance of the text modality. In addition, we incorporate prosodic cues, specifically interword pauses, by inserting pause tokens into the text and generating audio embeddings for silent intervals, further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset, where it achieves an accuracy of 90.36%, outperforming existing state-of-the-art methods. A detailed ablation study confirms the advantages of our alignment strategy, attention-based fusion, and prosodic modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLorc: Momentum Low-rank Compression for Large Language Model Adaptation</title>
<link>https://arxiv.org/abs/2506.01897</link>
<guid>https://arxiv.org/abs/2506.01897</guid>
<content:encoded><![CDATA[

arXiv:2506.01897v1 Announce Type: new 
Abstract: With increasing size of large language models (LLMs), full-parameter fine-tuning imposes substantial memory demands. To alleviate this, we propose a novel memory-efficient training paradigm called Momentum Low-rank compression (MLorc). By directly compressing and reconstructing momentum rather than gradients, MLorc avoids imposing a fixed-rank constraint on weight update matrices and better preserves the training dynamics of full-parameter fine-tuning, in contrast to existing low-rank approaches such as LoRA and GaLore. Empirically, MLorc consistently outperforms other memory-efficient training methods, matches or even exceeds the performance of full fine-tuning with a small rank (e.g., $r=4$), and generalizes well across different optimizers -- all while not compromising time or memory efficiency. Furthermore, we provide a theoretical guarantee for its convergence under reasonable assumptions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data</title>
<link>https://arxiv.org/abs/2506.01907</link>
<guid>https://arxiv.org/abs/2506.01907</guid>
<content:encoded><![CDATA[

arXiv:2506.01907v1 Announce Type: new 
Abstract: Privacy-preserving data publication, including synthetic data sharing, often experiences trade-offs between privacy and utility. Synthetic data is generally more effective than data anonymization in balancing this trade-off, however, not without its own challenges. Synthetic data produced by generative models trained on source data may inadvertently reveal information about outliers. Techniques specifically designed for preserving privacy, such as introducing noise to satisfy differential privacy, often incur unpredictable and significant losses in utility. In this work we show that, with the right mechanism of synthetic data generation, we can achieve strong privacy protection without significant utility loss. Synthetic data generators producing contracting data patterns, such as Synthetic Minority Over-sampling Technique (SMOTE), can enhance a differentially private data generator, leveraging the strengths of both. We prove in theory and through empirical demonstration that this SMOTE-DP technique can produce synthetic data that not only ensures robust privacy protection but maintains utility in downstream learning tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Gradient Norm Clipping &amp; Non-Euclidean $(L_0,L_1)$-Smoothness</title>
<link>https://arxiv.org/abs/2506.01913</link>
<guid>https://arxiv.org/abs/2506.01913</guid>
<content:encoded><![CDATA[

arXiv:2506.01913v1 Announce Type: new 
Abstract: This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning and demonstrate their properties on image classification and language modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models</title>
<link>https://arxiv.org/abs/2506.01919</link>
<guid>https://arxiv.org/abs/2506.01919</guid>
<content:encoded><![CDATA[

arXiv:2506.01919v1 Announce Type: new 
Abstract: Transformer based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Despite their empirical success, a comprehensive theoretical understanding of this phenomenon remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model, i.e, Hidden Markov Models, which are fundamental to many language tasks, we observe that: first, lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens; second, on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement. Building on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer's effectiveness and efficiency on sequence learning across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images</title>
<link>https://arxiv.org/abs/2411.19276</link>
<guid>https://arxiv.org/abs/2411.19276</guid>
<content:encoded><![CDATA[

arXiv:2411.19276v2 Announce Type: cross 
Abstract: In this study, we compare the performance of randomized classical and quantum neural networks (NNs) as well as classical and quantum-classical hybrid convolutional neural networks (CNNs) for the task of binary image classification. We use two distinct methodologies: using randomized NNs on dimensionality-reduced data, and applying CNNs to full image data. We evaluate these approaches on three data sets of increasing complexity: an artificial hypercube dataset, MNIST handwritten digits and real-world industrial images. We analyze correlations between classification accuracy and quantum model hyperparameters, including the number of trainable parameters, feature encoding methods, circuit layers, entangling gate type and structure, gate entangling power, and measurement operators. For random quantum NNs, we compare their performance against literature models. Classical and quantum/hybrid models achieved statistically equivalent classification accuracies across most datasets, with no approach demonstrating consistent superiority. We observe that quantum models show lower variance with respect to initial training parameters, suggesting better training stability. Among the hyperparameters analyzed, only the number of trainable parameters showed a positive correlation with the model performance. Around 94% of the best-performing quantum NNs had entangling gates, although for hybrid CNNs, models without entanglement performed equally well but took longer to converge. Cross-dataset performance analysis revealed limited transferability of quantum models between different classification tasks. Our study provides an industry perspective on quantum machine learning for practical image classification tasks, highlighting both current limitations and potential avenues for further research in quantum circuit design, entanglement utilization, and model transferability across varied applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization</title>
<link>https://arxiv.org/abs/2506.00002</link>
<guid>https://arxiv.org/abs/2506.00002</guid>
<content:encoded><![CDATA[

arXiv:2506.00002v1 Announce Type: cross 
Abstract: Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classical digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hardware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamental challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$ times speedup, depending on the difficulty of the generation tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[

arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies</title>
<link>https://arxiv.org/abs/2506.00008</link>
<guid>https://arxiv.org/abs/2506.00008</guid>
<content:encoded><![CDATA[

arXiv:2506.00008v1 Announce Type: cross 
Abstract: The rapid growth of large-language models (LLMs) is driving a new wave of specialized hardware for inference. This paper presents the first workload-centric, cross-architectural performance study of commercial AI accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale engines. We compare memory hierarchies, compute fabrics, and on-chip interconnects, and observe up to 3.7x performance variation across architectures as batch size and sequence length change. Four scaling techniques for trillion-parameter models are examined; expert parallelism offers an 8.4x parameter-to-compute advantage but incurs 2.1x higher latency variance than tensor parallelism. These findings provide quantitative guidance for matching workloads to accelerators and reveal architectural gaps that next-generation designs must address.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Physical Reasoning with the PHYSICS Dataset</title>
<link>https://arxiv.org/abs/2506.00022</link>
<guid>https://arxiv.org/abs/2506.00022</guid>
<content:encoded><![CDATA[

arXiv:2506.00022v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00033</link>
<guid>https://arxiv.org/abs/2506.00033</guid>
<content:encoded><![CDATA[

arXiv:2506.00033v1 Announce Type: cross 
Abstract: The large underlying assumption of climate models today relies on the basis of a "confident" initial condition, a reasonably plausible snapshot of the Earth for which all future predictions depend on. However, given the inherently chaotic nature of our system, this assumption is complicated by sensitive dependence, where small uncertainties in initial conditions can lead to exponentially diverging outcomes over time. This challenge is particularly salient at global spatial scales and over centennial timescales, where data gaps are not just common but expected. The source of uncertainty is two-fold: (1) sparse, noisy observations from satellites and ground stations, and (2) internal variability stemming from the simplifying approximations within the models themselves.
  In practice, data assimilation methods are used to reconcile this missing information by conditioning model states on partial observations. Our work builds on this idea but operates at the extreme end of sparsity. We propose a conditional data imputation framework that reconstructs full temperature fields from as little as 1% observational coverage. The method leverages a diffusion model guided by a prekriged mask, effectively inferring the full-state fields from minimal data points. We validate our framework over the Southern Great Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the summer months of 2018-2020. Across varying observational densities--from swath data to isolated in-situ sensors--our model achieves strong reconstruction accuracy, highlighting its potential to fill in critical data gaps in both historical reanalysis and real-time forecasting pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models</title>
<link>https://arxiv.org/abs/2506.00037</link>
<guid>https://arxiv.org/abs/2506.00037</guid>
<content:encoded><![CDATA[

arXiv:2506.00037v1 Announce Type: cross 
Abstract: Text embedding models enable semantic search, powering several NLP applications like Retrieval Augmented Generation by efficient information retrieval (IR). However, text embedding models are commonly studied in scenarios where the training data is static, thus limiting its applications to dynamic scenarios where new training data emerges over time. IR methods generally encode a huge corpus of documents to low-dimensional embeddings and store them in a database index. During retrieval, a semantic search over the corpus is performed and the document whose embedding is most similar to the query embedding is returned. When updating an embedding model with new training data, using the already indexed corpus is suboptimal due to the non-compatibility issue, since the model which was used to obtain the embeddings of the corpus has changed. While re-indexing of old corpus documents using the updated model enables compatibility, it requires much higher computation and time. Thus, it is critical to study how the already indexed corpus can still be effectively used without the need of re-indexing. In this work, we establish a continual learning benchmark with large-scale datasets and continually train dense retrieval embedding models on query-document pairs from new datasets in each task and observe forgetting on old tasks due to significant drift of embeddings. We employ embedding distillation on both query and document embeddings to maintain stability and propose a novel query drift compensation method during retrieval to project new model query embeddings to the old embedding space. This enables compatibility with previously indexed corpus embeddings extracted using the old model and thus reduces the forgetting. We show that the proposed method significantly improves performance without any re-indexing. Code is available at https://github.com/dipamgoswami/QDC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval</title>
<link>https://arxiv.org/abs/2506.00041</link>
<guid>https://arxiv.org/abs/2506.00041</guid>
<content:encoded><![CDATA[

arXiv:2506.00041v1 Announce Type: cross 
Abstract: Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability. In this work, we propose a novel interpretability framework that leverages Sparse Autoencoders (SAEs) to decompose previously uninterpretable dense embeddings from DPR models into distinct, interpretable latent concepts. We generate natural language descriptions for each latent concept, enabling human interpretations of both the dense embeddings and the query-document similarity scores of DPR models. We further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework that directly utilizes the extracted latent concepts as indexing units. CL-SR effectively combines the semantic expressiveness of dense embeddings with the transparency and efficiency of sparse representations. We show that CL-SR achieves high index-space and computational efficiency while maintaining robust performance across vocabulary and semantic mismatches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic intraday electricity price forecasting using generative machine learning</title>
<link>https://arxiv.org/abs/2506.00044</link>
<guid>https://arxiv.org/abs/2506.00044</guid>
<content:encoded><![CDATA[

arXiv:2506.00044v1 Announce Type: cross 
Abstract: The growing importance of intraday electricity trading in Europe calls for improved price forecasting and tailored decision-support tools. In this paper, we propose a novel generative neural network model to generate probabilistic path forecasts for intraday electricity prices and use them to construct effective trading strategies for Germany's continuous-time intraday market. Our method demonstrates competitive performance in terms of statistical evaluation metrics compared to two state-of-the-art statistical benchmark approaches. To further assess its economic value, we consider a realistic fixed-volume trading scenario and propose various strategies for placing market sell orders based on the path forecasts. Among the different trading strategies, the price paths generated by our generative model lead to higher profit gains than the benchmark methods. Our findings highlight the potential of generative machine learning tools in electricity price forecasting and underscore the importance of economic evaluation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL</title>
<link>https://arxiv.org/abs/2506.00048</link>
<guid>https://arxiv.org/abs/2506.00048</guid>
<content:encoded><![CDATA[

arXiv:2506.00048v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are powerful tools for recommendation systems, but they often struggle under data sparsity and noise. To address these issues, we implemented LightGCL, a graph contrastive learning model that uses Singular Value Decomposition (SVD) for robust graph augmentation, preserving semantic integrity without relying on stochastic or heuristic perturbations. LightGCL enables structural refinement and captures global collaborative signals, achieving significant gains over state-of-the-art models across benchmark datasets. Our experiments also demonstrate improved fairness and resilience to popularity bias, making it well-suited for real-world recommender systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving statistical learning methods via features selection without replacement sampling and random projection</title>
<link>https://arxiv.org/abs/2506.00053</link>
<guid>https://arxiv.org/abs/2506.00053</guid>
<content:encoded><![CDATA[

arXiv:2506.00053v1 Announce Type: cross 
Abstract: Cancer is fundamentally a genetic disease characterized by genetic and epigenetic alterations that disrupt normal gene expression, leading to uncontrolled cell growth and metastasis. High-dimensional microarray datasets pose challenges for classification models due to the "small n, large p" problem, resulting in overfitting. This study makes three different key contributions: 1) we propose a machine learning-based approach integrating the Feature Selection Without Re-placement (FSWOR) technique and a projection method to improve classification accuracy. 2) We apply the Kendall statistical test to identify the most significant genes from the brain cancer mi-croarray dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3) we apply machine learning models using k-fold cross validation techniques in which our model incorpo-rates ensemble classifiers with LDA projection and Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by 9.09%. The results demonstrate the effectiveness of our ap-proach in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting. This study contributes to cancer biomarker discovery, offering a robust computational method for analyzing microarray data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education</title>
<link>https://arxiv.org/abs/2506.00057</link>
<guid>https://arxiv.org/abs/2506.00057</guid>
<content:encoded><![CDATA[

arXiv:2506.00057v1 Announce Type: cross 
Abstract: Educators teaching entry-level university engineering modules face the challenge of identifying which topics students find most difficult and how to support diverse student needs effectively. This study demonstrates a rigorous yet interpretable statistical approach -- hierarchical Bayesian modeling -- that leverages detailed student response data to quantify both skill difficulty and individual student abilities. Using a large-scale dataset from an undergraduate Statics course, we identified clear patterns of skill mastery and uncovered distinct student subgroups based on their learning trajectories. Our analysis reveals that certain concepts consistently present challenges, requiring targeted instructional support, while others are readily mastered and may benefit from enrichment activities. Importantly, the hierarchical Bayesian method provides educators with intuitive, reliable metrics without sacrificing predictive accuracy. This approach allows for data-informed decisions, enabling personalized teaching strategies to improve student engagement and success. By combining robust statistical methods with clear interpretability, this study equips educators with actionable insights to better support diverse learner populations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?</title>
<link>https://arxiv.org/abs/2506.00062</link>
<guid>https://arxiv.org/abs/2506.00062</guid>
<content:encoded><![CDATA[

arXiv:2506.00062v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a common practice to adapt general-purpose models to the telecom domain. However, little attention has been paid to how this process may compromise model safety. Recent research has shown that even benign fine-tuning can degrade the safety alignment of LLMs, causing them to respond to harmful or unethical user queries. In this paper, we investigate this issue for telecom-tuned LLMs using three representative datasets featured by the GenAINet initiative. We show that safety degradation persists even for structured and seemingly harmless datasets such as 3GPP standards and tabular records, indicating that telecom-specific data is not immune to safety erosion during fine-tuning. We further extend our analysis to publicly available Telecom LLMs trained via continual pre-training, revealing that safety alignment is often severely lacking, primarily due to the omission of safety-focused instruction tuning. To address these issues in both fine-tuned and pre-trained models, we conduct extensive experiments and evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and SafeMERGE) using established red-teaming benchmarks. The results show that, across all settings, the proposed defenses can effectively restore safety after harmful degradation without compromising downstream task performance, leading to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as a diagnostic study and practical guide for safety realignment in telecom-tuned LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning for real-world deployments of Telecom LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs</title>
<link>https://arxiv.org/abs/2506.00072</link>
<guid>https://arxiv.org/abs/2506.00072</guid>
<content:encoded><![CDATA[

arXiv:2506.00072v1 Announce Type: cross 
Abstract: This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry</title>
<link>https://arxiv.org/abs/2506.00076</link>
<guid>https://arxiv.org/abs/2506.00076</guid>
<content:encoded><![CDATA[

arXiv:2506.00076v1 Announce Type: cross 
Abstract: Television networks face high financial risk when making programming decisions, often relying on limited historical data to forecast episodic viewership. This study introduces a machine learning framework that integrates natural language processing (NLP) features from over 25000 television episodes with traditional viewership data to enhance predictive accuracy. By extracting emotional tone, cognitive complexity, and narrative structure from episode dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost, and feature selection models. While prior viewership remains a strong baseline predictor, NLP features contribute meaningful improvements for some series. We also introduce a similarity scoring method based on Euclidean distance between aggregate dialogue vectors to compare shows by content. Tested across diverse genres, including Better Call Saul and Abbott Elementary, our framework reveals genre-specific performance and offers interpretable metrics for writers, executives, and marketers seeking data-driven insight into audience behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian mixture models as a proxy for interacting language models</title>
<link>https://arxiv.org/abs/2506.00077</link>
<guid>https://arxiv.org/abs/2506.00077</guid>
<content:encoded><![CDATA[

arXiv:2506.00077v1 Announce Type: cross 
Abstract: Large language models (LLMs) are a powerful tool with the ability to match human capabilities and behavior in many settings. Retrieval-augmented generation (RAG) further allows LLMs to generate diverse output depending on the contents of their RAG database. This motivates their use in the social sciences to study human behavior between individuals when large-scale experiments are infeasible. However, LLMs depend on complex, computationally expensive algorithms. In this paper, we introduce interacting Gaussian mixture models (GMMs) as an alternative to similar frameworks using LLMs. We compare a simplified model of GMMs to select experimental simulations of LLMs whose updating and response depend on feedback from other LLMs. We find that interacting GMMs capture important features of the dynamics in interacting LLMs, and we investigate key similarities and differences between interacting LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture models, potential modifications, and future research directions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values</title>
<link>https://arxiv.org/abs/2506.00079</link>
<guid>https://arxiv.org/abs/2506.00079</guid>
<content:encoded><![CDATA[

arXiv:2506.00079v1 Announce Type: cross 
Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2506.00088</link>
<guid>https://arxiv.org/abs/2506.00088</guid>
<content:encoded><![CDATA[

arXiv:2506.00088v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey</title>
<link>https://arxiv.org/abs/2506.00098</link>
<guid>https://arxiv.org/abs/2506.00098</guid>
<content:encoded><![CDATA[

arXiv:2506.00098v1 Announce Type: cross 
Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robot's behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC</title>
<link>https://arxiv.org/abs/2506.00102</link>
<guid>https://arxiv.org/abs/2506.00102</guid>
<content:encoded><![CDATA[

arXiv:2506.00102v1 Announce Type: cross 
Abstract: The pursuit of discovering new phenomena at the Large Hadron Collider (LHC) demands constant innovation in algorithms and technologies. Tensor networks are mathematical models on the intersection of classical and quantum machine learning, which present a promising and efficient alternative for tackling these challenges. In this work, we propose a tensor network-based strategy for anomaly detection at the LHC and demonstrate its superior performance in identifying new phenomena compared to established quantum methods. Our model is a parametrized Matrix Product State with an isometric feature map, processing a latent representation of simulated LHC data generated by an autoencoder. Our results highlight the potential of tensor networks to enhance new-physics discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generator Based Inference (GBI)</title>
<link>https://arxiv.org/abs/2506.00119</link>
<guid>https://arxiv.org/abs/2506.00119</guid>
<content:encoded><![CDATA[

arXiv:2506.00119v1 Announce Type: cross 
Abstract: Statistical inference in physics is often based on samples from a generator (sometimes referred to as a ``forward model") that emulate experimental data and depend on parameters of the underlying theory. Modern machine learning has supercharged this workflow to enable high-dimensional and unbinned analyses to utilize much more information than ever before. We propose a general framework for describing the integration of machine learning with generators called Generator Based Inference (GBI). A well-studied special case of this setup is Simulation Based Inference (SBI) where the generator is a physics-based simulator. In this work, we examine other methods within the GBI toolkit that use data-driven methods to build the generator. In particular, we focus on resonant anomaly detection, where the generator describing the background is learned from sidebands. We show how to perform machine learning-based parameter estimation in this context with data-derived generators. This transforms the statistical outputs of anomaly detection to be directly interpretable and the performance on the LHCO community benchmark dataset establishes a new state-of-the-art for anomaly detection sensitivity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models</title>
<link>https://arxiv.org/abs/2506.00128</link>
<guid>https://arxiv.org/abs/2506.00128</guid>
<content:encoded><![CDATA[

arXiv:2506.00128v1 Announce Type: cross 
Abstract: Effective prioritization of issue reports in software engineering helps to optimize resource allocation and information recovery. However, manual issue classification is laborious and lacks scalability. As an alternative, many open source software (OSS) projects employ automated processes for this task, yet this method often relies on large datasets for adequate training. Traditionally, machine learning techniques have been used for issue classification. More recently, large language models (LLMs) have emerged as powerful tools for addressing a range of software engineering challenges, including code and test generation, mapping new requirements to legacy software endpoints, and conducting code reviews. The following research investigates an automated approach to issue classification based on LLMs. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports, mitigating the necessity for extensive training data while also maintaining reliability in classification. In our research, we developed an LLM-based approach for accurately labeling issues by selecting two of the most prominent large language models. We then compared their performance across multiple datasets. Our findings show that GPT-4o achieved the best results in classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o outperformed DeepSeek R1, achieving an F1 score 20% higher when both models were trained on the same dataset from the NLBSE 2023 competition, which was ten times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved 59.33%. Increasing the dataset size did not improve the F1 score, reducing the dependence on massive datasets for building an efficient solution to issue classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation</title>
<link>https://arxiv.org/abs/2506.00129</link>
<guid>https://arxiv.org/abs/2506.00129</guid>
<content:encoded><![CDATA[

arXiv:2506.00129v1 Announce Type: cross 
Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on improving the representational capacity of large language models to incorporate Sign Language features. This work explores an alternative direction: enhancing the geometric properties of skeletal representations themselves. We propose Geo-Sign, a method that leverages the properties of hyperbolic geometry to model the hierarchical structure inherent in sign language kinematics. By projecting skeletal features derived from Spatio-Temporal Graph Convolutional Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more discriminative embeddings, particularly for fine-grained motions like finger articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet mean aggregation scheme, and a geometric contrastive loss operating directly in hyperbolic space. These components are integrated into an end-to-end translation framework as a regularisation function, to enhance the representations within the language model. This work demonstrates the potential of hyperbolic geometry to improve skeletal representations for Sign Language Translation, improving on SOTA RGB methods while preserving privacy and improving computational efficiency. Code available here: https://github.com/ed-fish/geo-sign.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things</title>
<link>https://arxiv.org/abs/2506.00133</link>
<guid>https://arxiv.org/abs/2506.00133</guid>
<content:encoded><![CDATA[

arXiv:2506.00133v1 Announce Type: cross 
Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low bandwidth, high latency, mobility, and limited energy resources. Traditional routing protocols like RPL, which were designed for land-based networks, do not perform well in these underwater conditions. This paper introduces RL-RPL-UA, a new routing protocol that uses reinforcement learning to improve performance in underwater environments. Each node includes a lightweight RL agent that selects the best parent node based on local information such as packet delivery ratio, buffer level, link quality, and remaining energy. RL-RPL-UA keeps full compatibility with standard RPL messages and adds a dynamic objective function to support real-time decision-making. Simulations using Aqua-Sim show that RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per packet by 14.8%, and extends network lifetime by 80 seconds compared to traditional methods. These results suggest that RL-RPL-UA is a promising and energy-efficient routing solution for underwater networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-QA: A Benchmark for Personalized Long-form Question Answering</title>
<link>https://arxiv.org/abs/2506.00137</link>
<guid>https://arxiv.org/abs/2506.00137</guid>
<content:encoded><![CDATA[

arXiv:2506.00137v1 Announce Type: cross 
Abstract: Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models (LLMs). Our results show that incorporating the personalized context provided leads to performance improvements of up to 39%. The benchmark is publicly released to support future research in this area.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation</title>
<link>https://arxiv.org/abs/2506.00138</link>
<guid>https://arxiv.org/abs/2506.00138</guid>
<content:encoded><![CDATA[

arXiv:2506.00138v1 Announce Type: cross 
Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in sparse reward and reward-free environments, including class of methods known as intrinsic motivation, exhibit inconsistent exploration patterns and thus fail to produce robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in unconstrained, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed to capture robust autonomous exploration observed in animals. Our method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. We demonstrate that artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Profit and Fairness in Risk-Based Pricing Markets</title>
<link>https://arxiv.org/abs/2506.00140</link>
<guid>https://arxiv.org/abs/2506.00140</guid>
<content:encoded><![CDATA[

arXiv:2506.00140v1 Announce Type: cross 
Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer groups from essential resources such as health insurance and consumer credit. We show that a regulator can realign private incentives with social objectives through a learned, interpretable tax schedule. First, we provide a formal proposition that bounding each firm's \emph{local} demographic gap implicitly bounds the \emph{global} opt-out disparity, motivating firm-level penalties. Building on this insight we introduce \texttt{MarketSim} -- an open-source, scalable simulator of heterogeneous consumers and profit-maximizing firms -- and train a reinforcement learning (RL) social planner (SP) that selects a bracketed fairness-tax while remaining close to a simple linear prior via an $\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and easily interpretable. In two empirically calibrated markets, i.e., U.S. health-insurance and consumer-credit, our planner simultaneously raises demand-fairness by up to $16\%$ relative to unregulated Free Market while outperforming a fixed linear schedule in terms of social welfare without explicit coordination. These results illustrate how AI-assisted regulation can convert a competitive social dilemma into a win-win equilibrium, providing a principled and practical framework for fairness-aware market oversight.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures</title>
<link>https://arxiv.org/abs/2506.00165</link>
<guid>https://arxiv.org/abs/2506.00165</guid>
<content:encoded><![CDATA[

arXiv:2506.00165v1 Announce Type: cross 
Abstract: Randomized dimensionality reduction is a widely-used algorithmic technique for speeding up large-scale Euclidean optimization problems. In this paper, we study dimension reduction for a variety of maximization problems, including max-matching, max-spanning tree, max TSP, as well as various measures for dataset diversity. For these problems, we show that the effect of dimension reduction is intimately tied to the \emph{doubling dimension} $\lambda_X$ of the underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of point sets. Specifically, we prove that a target dimension of $O(\lambda_X)$ suffices to approximately preserve the value of any near-optimal solution,which we also show is necessary for some of these problems. This is in contrast to classical dimension reduction results, whose dependence increases with the dataset size $|X|$. We also provide empirical results validating the quality of solutions found in the projected space, as well as speedups due to dimensionality reduction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds</title>
<link>https://arxiv.org/abs/2506.00171</link>
<guid>https://arxiv.org/abs/2506.00171</guid>
<content:encoded><![CDATA[

arXiv:2506.00171v1 Announce Type: cross 
Abstract: We study the problem of estimating eigenpairs of elliptic differential operators from samples of a distribution $\rho$ supported on a manifold $M$. The operators discussed in the paper are relevant in unsupervised learning and in particular are obtained by taking suitable scaling limits of widely used graph Laplacians over data clouds. We study the minimax risk for this eigenpair estimation problem and explore the rates of approximation that can be achieved by commonly used graph Laplacians built from random data. More concretely, assuming that $\rho$ belongs to a certain family of distributions with controlled second derivatives, and assuming that the $d$-dimensional manifold $M$ where $\rho$ is supported has bounded geometry, we prove that the statistical minimax rate for approximating eigenvalues and eigenvectors in the $H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a closely related density estimation problem. We then revisit the literature studying Laplacians over proximity graphs in the large data limit and prove that, under slightly stronger regularity assumptions on the data generating model, eigenpairs of graph Laplacians induce manifold agnostic estimators with an error of approximation that, up to logarithmic corrections, matches our lower bounds. Our analysis allows us to expand the existing literature on graph-based learning in at least two significant ways: 1) we consider stronger norms to measure the error of approximation than the ones that had been analyzed in the past; 2) our rates of convergence are uniform over a family of smooth distributions and do not just apply to densities with special symmetries, and, as a consequence of our lower bounds, are essentially sharp when the connectivity of the graph is sufficiently high.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Validation of the Independent Chip Model</title>
<link>https://arxiv.org/abs/2506.00180</link>
<guid>https://arxiv.org/abs/2506.00180</guid>
<content:encoded><![CDATA[

arXiv:2506.00180v1 Announce Type: cross 
Abstract: The independent chip model (ICM) forms a cornerstone of all modern poker tournament strategy. However, despite its prominence, the ICM's performance in the real world has not been sufficiently scrutinized, especially at a large scale. In this paper, we introduce our new dataset of poker tournaments, consisting of results of over ten thousand events. Then, using this dataset, we perform two experiments as part of a large-scale empirical validation of the ICM. First, we verify that the ICM performs more accurately than a baseline we propose. Second, we obtain empirical evidence of the ICM underestimating the performances of players with larger stacks while overestimating those who are short-stacked. Our contributions may be useful to future researchers developing new algorithms for estimating a player's value in poker tournaments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overfitting has a limitation: a model-independent generalization error bound based on R\'enyi entropy</title>
<link>https://arxiv.org/abs/2506.00182</link>
<guid>https://arxiv.org/abs/2506.00182</guid>
<content:encoded><![CDATA[

arXiv:2506.00182v1 Announce Type: cross 
Abstract: Will further scaling up of machine learning models continue to bring success? A significant challenge in answering this question lies in understanding generalization error, which is the impact of overfitting. Understanding generalization error behavior of increasingly large-scale machine learning models remains a significant area of investigation, as conventional analyses often link error bounds to model complexity, failing to fully explain the success of extremely large architectures. This research introduces a novel perspective by establishing a model-independent upper bound for generalization error applicable to algorithms whose outputs are determined solely by the data's histogram, such as empirical risk minimization or gradient-based methods. Crucially, this bound is shown to depend only on the R\'enyi entropy of the data-generating distribution, suggesting that a small generalization error can be maintained even with arbitrarily large models, provided the data quantity is sufficient relative to this entropy. This framework offers a direct explanation for the phenomenon where generalization performance degrades significantly upon injecting random noise into data, where the performance degrade is attributed to the consequent increase in the data distribution's R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be data-distribution-dependent, demonstrating that an amount of data corresponding to the R\'enyi entropy is indeed essential for successful learning, thereby highlighting the tightness of our proposed generalization bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Limits of Beam Search Decoding for Transducer-based ASR models</title>
<link>https://arxiv.org/abs/2506.00185</link>
<guid>https://arxiv.org/abs/2506.00185</guid>
<content:encoded><![CDATA[

arXiv:2506.00185v1 Announce Type: cross 
Abstract: Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Graph Backdoor Attack</title>
<link>https://arxiv.org/abs/2506.00191</link>
<guid>https://arxiv.org/abs/2506.00191</guid>
<content:encoded><![CDATA[

arXiv:2506.00191v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex, multi-typed relationships across diverse domains, yet their vulnerability to backdoor attacks remains unexplored. To address this gap, we conduct the first investigation into the susceptibility of HGNNs to existing graph backdoor attacks, revealing three critical issues: (1) high attack budget required for effective backdoor injection, (2) inefficient and unreliable backdoor activation, and (3) inaccurate attack effectiveness evaluation. To tackle these issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first backdoor attack specifically designed for HGNNs, introducing a novel relation-based trigger mechanism that establishes specific connections between a strategically selected trigger node and poisoned nodes via the backdoor metapath. HGBA achieves efficient and stealthy backdoor injection with minimal structural modifications and supports easy backdoor activation through two flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally, we improve the ASR measurement protocol, enabling a more accurate assessment of attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses multiple state-of-the-art graph backdoor attacks in black-box settings, efficiently attacking HGNNs with low attack budgets. Ablation studies show that the strength of HBGA benefits from our trigger node selection method and backdoor metapath selection strategy. In addition, HGBA shows superior robustness against node feature perturbations and multiple types of existing graph backdoor defense mechanisms. Finally, extension experiments demonstrate that the relation-based trigger mechanism can effectively extend to tasks in homogeneous graph scenarios, thereby posing severe threats to broader security-critical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs</title>
<link>https://arxiv.org/abs/2506.00197</link>
<guid>https://arxiv.org/abs/2506.00197</guid>
<content:encoded><![CDATA[

arXiv:2506.00197v1 Announce Type: cross 
Abstract: Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring Radiology Reports: Challenging LLMs with Lightweight Models</title>
<link>https://arxiv.org/abs/2506.00200</link>
<guid>https://arxiv.org/abs/2506.00200</guid>
<content:encoded><![CDATA[

arXiv:2506.00200v1 Announce Type: cross 
Abstract: Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)-specifically T5 and BERT2BERT-for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B-70B), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol</title>
<link>https://arxiv.org/abs/2506.00223</link>
<guid>https://arxiv.org/abs/2506.00223</guid>
<content:encoded><![CDATA[

arXiv:2506.00223v1 Announce Type: cross 
Abstract: Accurate prediction of molecular solubility is a cornerstone of early-stage drug discovery, yet conventional machine learning models face significant challenges due to limited labeled data and the high-dimensional nature of molecular descriptors. To address these issues, we propose LatMixSol, a novel latent space augmentation framework that combines autoencoder-based feature compression with guided interpolation to enrich training data. Our approach first encodes molecular descriptors into a low-dimensional latent space using a two-layer autoencoder. Spectral clustering is then applied to group chemically similar molecules, enabling targeted MixUp-style interpolation within clusters. Synthetic samples are generated by blending latent vectors of cluster members and decoding them back to the original feature space. Evaluated on the Huuskonen solubility benchmark, LatMixSol demonstrates consistent improvements across three of four gradient-boosted regressors (CatBoost, LightGBM, HistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared increases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant enhancement with a 7.6% RMSE improvement. Our analysis confirms that cluster-guided latent space augmentation preserves chemical validity while expanding dataset diversity, offering a computationally efficient strategy to enhance predictive models in resource-constrained drug discovery pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Principal Component Analysis</title>
<link>https://arxiv.org/abs/2506.00226</link>
<guid>https://arxiv.org/abs/2506.00226</guid>
<content:encoded><![CDATA[

arXiv:2506.00226v1 Announce Type: cross 
Abstract: This paper proposes an innovative extension of Principal Component Analysis (PCA) that transcends the traditional assumption of data lying in Euclidean space, enabling its application to data on Riemannian manifolds. The primary challenge addressed is the lack of vector space operations on such manifolds. Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA) as a geometric approach to analyze data on Riemannian manifolds, particularly effective for structured datasets like medical images, where the manifold's intrinsic structure is apparent. However, PGA's applicability is limited when dealing with general datasets that lack an implicit local distance notion. In this work, we introduce a generalized framework, termed {\em Riemannian Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with a local distance structure. Specifically, we adapt the PCA methodology to Riemannian manifolds by equipping data tables with local metrics, enabling the incorporation of manifold geometry. This framework provides a unified approach for dimensionality reduction and statistical analysis directly on manifolds, opening new possibilities for datasets with region-specific or part-specific distance notions, ensuring respect for their intrinsic geometric properties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sorrel: A simple and flexible framework for multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2506.00228</link>
<guid>https://arxiv.org/abs/2506.00228</guid>
<content:encoded><![CDATA[

arXiv:2506.00228v1 Announce Type: cross 
Abstract: We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple Python interface for generating and testing new multi-agent reinforcement learning environments. This interface places a high degree of emphasis on simplicity and accessibility, and uses a more psychologically intuitive structure for the basic agent-environment loop, making it a useful tool for social scientists to investigate how learning and social interaction leads to the development and change of group dynamics. In this short paper, we outline the basic design philosophy and features of Sorrel.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment</title>
<link>https://arxiv.org/abs/2506.00238</link>
<guid>https://arxiv.org/abs/2506.00238</guid>
<content:encoded><![CDATA[

arXiv:2506.00238v1 Announce Type: cross 
Abstract: Natural disasters usually affect vast areas and devastate infrastructures. Performing a timely and efficient response is crucial to minimize the impact on affected communities, and data-driven approaches are the best choice. Visual question answering (VQA) models help management teams to achieve in-depth understanding of damages. However, recently published models do not possess the ability to answer open-ended questions and only select the best answer among a predefined list of answers. If we want to ask questions with new additional possible answers that do not exist in the predefined list, the model needs to be fin-tuned/retrained on a new collected and annotated dataset, which is a time-consuming procedure. In recent years, large-scale Vision-Language Models (VLMs) have earned significant attention. These models are trained on extensive datasets and demonstrate strong performance on both unimodal and multimodal vision/language downstream tasks, often without the need for fine-tuning. In this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and investigate the performance of on post-disaster FloodNet dataset. Since the proposed method takes advantage of zero-shot learning, it can be applied on new datasets without fine-tuning. In addition, ZeShot-VQA is able to process and generate answers that has been not seen during the training procedure, which demonstrates its flexibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How hard is learning to cut? Trade-offs and sample complexity</title>
<link>https://arxiv.org/abs/2506.00252</link>
<guid>https://arxiv.org/abs/2506.00252</guid>
<content:encoded><![CDATA[

arXiv:2506.00252v1 Announce Type: cross 
Abstract: In the recent years, branch-and-cut algorithms have been the target of data-driven approaches designed to enhance the decision making in different phases of the algorithm such as branching, or the choice of cutting planes (cuts). In particular, for cutting plane selection two score functions have been proposed in the literature to evaluate the quality of a cut: branch-and-cut tree size and gap closed. In this paper, we present new sample complexity lower bounds, valid for both scores. We show that for a wide family of classes $\mathcal{F}$ that maps an instance to a cut, learning over an unknown distribution of the instances to minimize those scores requires at least (up to multiplicative constants) as many samples as learning from the same class function $\mathcal{F}$ any generic target function (using square loss). Our results also extend to the case of learning from a restricted set of cuts, namely those from the Simplex tableau. To the best of our knowledge, these constitute the first lower bounds for the learning-to-cut framework. We compare our bounds to known upper bounds in the case of neural networks and show they are nearly tight. We illustrate our results with a graph neural network selection evaluated on set covering and facility location integer programming models and we empirically show that the gap closed score is an effective proxy to minimize the branch-and-cut tree size. Although the gap closed score has been extensively used in the integer programming literature, this is the first principled analysis discussing both scores at the same time both theoretically and computationally.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Data Sketching for Varying Coefficient Regression Models</title>
<link>https://arxiv.org/abs/2506.00270</link>
<guid>https://arxiv.org/abs/2506.00270</guid>
<content:encoded><![CDATA[

arXiv:2506.00270v1 Announce Type: cross 
Abstract: Varying coefficient models are popular for estimating nonlinear regression functions in functional data models. Their Bayesian variants have received limited attention in large data applications, primarily due to prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We introduce Bayesian data sketching for varying coefficient models to obviate computational challenges presented by large sample sizes. To address the challenges of analyzing large data, we compress the functional response vector and predictor matrix by a random linear transformation to achieve dimension reduction and conduct inference on the compressed data. Our approach distinguishes itself from several existing methods for analyzing large functional data in that it requires neither the development of new models or algorithms, nor any specialized computational hardware while delivering fully model-based Bayesian inference. Well-established methods and algorithms for varying coefficient regression models can be applied to the compressed data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction</title>
<link>https://arxiv.org/abs/2506.00273</link>
<guid>https://arxiv.org/abs/2506.00273</guid>
<content:encoded><![CDATA[

arXiv:2506.00273v1 Announce Type: cross 
Abstract: This paper introduces SoundSculpt, a neural network designed to extract target sound fields from ambisonic recordings. SoundSculpt employs an ambisonic-in-ambisonic-out architecture and is conditioned on both spatial information (e.g., target direction obtained by pointing at an immersive video) and semantic embeddings (e.g., derived from image segmentation and captioning). Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt demonstrates superior performance compared to various signal processing baselines. Our results further reveal that while spatial conditioning alone can be effective, the combination of spatial and semantic information is beneficial in scenarios where there are secondary sound sources spatially close to the target. Additionally, we compare two different semantic embeddings derived from a text description of the target sound using text encoders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning</title>
<link>https://arxiv.org/abs/2506.00279</link>
<guid>https://arxiv.org/abs/2506.00279</guid>
<content:encoded><![CDATA[

arXiv:2506.00279v1 Announce Type: cross 
Abstract: Despite extensive research on the relationship between sleep and cognition, the connection between sleep microstructure and human performance across specific cognitive domains remains underexplored. This study investigates whether deep learning models can predict executive functions, particularly cognitive adaptability and conceptual reasoning from physiological processes during a night's sleep. To address this, we introduce CogPSGFormer, a multi-scale convolutional-transformer model designed to process multi-modal polysomnographic data. This model integrates one-channel ECG and EEG signals along with extracted features, including EEG power bands and heart rate variability parameters, to capture complementary information across modalities. A thorough evaluation of the CogPSGFormer architecture was conducted to optimize the processing of extended sleep signals and identify the most effective configuration. The proposed framework was evaluated on 817 individuals from the STAGES dataset using cross-validation. The model achieved 80.3\% accuracy in classifying individuals into low vs. high cognitive performance groups on unseen data based on Penn Conditional Exclusion Test (PCET) scores. These findings highlight the effectiveness of our multi-scale feature extraction and multi-modal learning approach in leveraging sleep-derived signals for cognitive performance prediction. To facilitate reproducibility, our code is publicly accessible (https://github.com/boshrakh95/CogPSGFormer.git).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Gaussian Splat Vulnerabilities</title>
<link>https://arxiv.org/abs/2506.00280</link>
<guid>https://arxiv.org/abs/2506.00280</guid>
<content:encoded><![CDATA[

arXiv:2506.00280v1 Announce Type: cross 
Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLM-One: Diffusion Language Models for One-Step Sequence Generation</title>
<link>https://arxiv.org/abs/2506.00290</link>
<guid>https://arxiv.org/abs/2506.00290</guid>
<content:encoded><![CDATA[

arXiv:2506.00290v1 Announce Type: cross 
Abstract: This paper introduces DLM-One, a score-distillation-based framework for one-step sequence generation with continuous diffusion language models (DLMs). DLM-One eliminates the need for iterative refinement by aligning the scores of a student model's outputs in the continuous token embedding space with the score function of a pretrained teacher DLM. We investigate whether DLM-One can achieve substantial gains in sampling efficiency for language modeling. Through comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we show that DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks used to evaluate the teacher models. We further analyze the method's empirical behavior across multiple datasets, providing initial insights into its generality and practical applicability. Our findings position one-step diffusion as a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models operating in embedding space for natural language processing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Aerodynamics for the Control of Flying Humanoid Robots</title>
<link>https://arxiv.org/abs/2506.00305</link>
<guid>https://arxiv.org/abs/2506.00305</guid>
<content:encoded><![CDATA[

arXiv:2506.00305v1 Announce Type: cross 
Abstract: Robots with multi-modal locomotion are an active research field due to their versatility in diverse environments. In this context, additional actuation can provide humanoid robots with aerial capabilities. Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint. The technological contribution includes the mechanical design of iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine integration, and hardware modifications for wind tunnel experiments on humanoid robots for precise aerodynamic forces and surface pressure measurements. The scientific contribution offers a comprehensive approach to model and control aerodynamic forces using classical and learning techniques. Computational Fluid Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the aerodynamic dataset, enabling the training of a Deep Neural Network and a linear regression model. These models are integrated into a simulator for designing aerodynamic-aware controllers, validated through flight simulations and balancing experiments on the iRonCub-Mk1 physical prototype.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Token Sequence Compression via Meta-Tokens</title>
<link>https://arxiv.org/abs/2506.00307</link>
<guid>https://arxiv.org/abs/2506.00307</guid>
<content:encoded><![CDATA[

arXiv:2506.00307v1 Announce Type: cross 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-of-Two (PoT) Weights in Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2506.00315</link>
<guid>https://arxiv.org/abs/2506.00315</guid>
<content:encoded><![CDATA[

arXiv:2506.00315v1 Announce Type: cross 
Abstract: Complexity of Neural Networks is increasing rapidly due to the massive increase in model parameters. Specifically, in Large Language Models (LLMs), the number of model parameters has grown exponentially in the past few years, for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This raises a significant challenge for implementation, especially for Edge devices where memory and processing power are very limited. In this work, we investigate reducing LLM complexity with special type of quantization, power of two (PoT), for linear layers weights and transformer tables. PoT not only provides memory reduction but more importantly provides significant computational reduction through converting multiplication to bit shifting. We obtained preliminary results of PoT quantization on Nano-GPT implementation using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The PoT quantization results are shown to be very promising with cross entropy loss degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent power levels.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</title>
<link>https://arxiv.org/abs/2506.00320</link>
<guid>https://arxiv.org/abs/2506.00320</guid>
<content:encoded><![CDATA[

arXiv:2506.00320v1 Announce Type: cross 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation</title>
<link>https://arxiv.org/abs/2506.00322</link>
<guid>https://arxiv.org/abs/2506.00322</guid>
<content:encoded><![CDATA[

arXiv:2506.00322v1 Announce Type: cross 
Abstract: We propose dpmm, an open-source library for synthetic data generation with Differentially Private (DP) guarantees. It includes three popular marginal models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer richer functionality compared to alternative implementations. Additionally, we adopt best practices to provide end-to-end DP guarantees and address well-known DP-related vulnerabilities. Our goal is to accommodate a wide audience with easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The iNaturalist Sounds Dataset</title>
<link>https://arxiv.org/abs/2506.00343</link>
<guid>https://arxiv.org/abs/2506.00343</guid>
<content:encoded><![CDATA[

arXiv:2506.00343v1 Announce Type: cross 
Abstract: We present the iNaturalist Sounds Dataset (iNatSounds), a collection of 230,000 audio files capturing sounds from over 5,500 species, contributed by more than 27,000 recordists worldwide. The dataset encompasses sounds from birds, mammals, insects, reptiles, and amphibians, with audio and species labels derived from observations submitted to iNaturalist, a global citizen science platform. Each recording in the dataset varies in length and includes a single species annotation. We benchmark multiple backbone architectures, comparing multiclass classification objectives with multilabel objectives. Despite weak labeling, we demonstrate that iNatSounds serves as a useful pretraining resource by benchmarking it on strongly labeled downstream evaluation datasets. The dataset is available as a single, freely accessible archive, promoting accessibility and research in this important domain. We envision models trained on this data powering next-generation public engagement applications, and assisting biologists, ecologists, and land use managers in processing large audio collections, thereby contributing to the understanding of species compositions in diverse soundscapes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings</title>
<link>https://arxiv.org/abs/2506.00348</link>
<guid>https://arxiv.org/abs/2506.00348</guid>
<content:encoded><![CDATA[

arXiv:2506.00348v1 Announce Type: cross 
Abstract: Knowledge of accurate relative skills in any competitive system is essential, but foundational approaches such as ELO discard extremely relevant performance data by concentrating exclusively on binary outcomes. While margin of victory (MOV) extensions exist, they often lack a definitive method for incorporating this information. We introduce Margin of Victory Differential Analysis (MOVDA), a framework that enhances traditional rating systems by using the deviation between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a domain-specific, non-linear function (a scaled hyperbolic tangent that captures saturation effects and home advantage) to predict expected MOV based on rating differentials. Crucially, the $\textit{difference}$ between the true and expected MOV provides a subtle and weighted signal for rating updates, highlighting informative deviations in all levels of contests. Extensive experiments on professional NBA basketball data (from 2013 to 2023, with 13,619 games) show that MOVDA significantly outperforms standard ELO and Bayesian baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly accelerates rating convergence by $13.5\%$, while maintaining the computational efficiency of the original ELO updates. MOVDA offers a theoretically motivated, empirically superior, and computationally lean approach to integrating performance magnitude into skill rating for competitive environments like the NBA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</title>
<link>https://arxiv.org/abs/2506.00358</link>
<guid>https://arxiv.org/abs/2506.00358</guid>
<content:encoded><![CDATA[

arXiv:2506.00358v1 Announce Type: cross 
Abstract: While recent audio-visual models have demonstrated impressive performance, their robustness to distributional shifts at test-time remains not fully understood. Existing robustness benchmarks mainly focus on single modalities, making them insufficient for thoroughly assessing the robustness of audio-visual models. Motivated by real-world scenarios where shifts can occur $\textit{simultaneously}$ in both audio and visual modalities, we introduce $\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the test-time robustness of audio-visual recognition models. $\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets, $\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and $\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through extensive evaluations, we observe that state-of-the-art supervised and self-supervised audio-visual models exhibit declining robustness as corruption severity increases. Furthermore, online test-time adaptation (TTA) methods, on $\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a simple TTA approach enabling on-the-fly cross-modal fusion by penalizing high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective and robust audio-visual TTA approaches. Our code is available $\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-shift robust federated feature screening for high-dimensional classification</title>
<link>https://arxiv.org/abs/2506.00379</link>
<guid>https://arxiv.org/abs/2506.00379</guid>
<content:encoded><![CDATA[

arXiv:2506.00379v1 Announce Type: cross 
Abstract: Distributed and federated learning are important tools for high-dimensional classification of large datasets. To reduce computational costs and overcome the curse of dimensionality, feature screening plays a pivotal role in eliminating irrelevant features during data preprocessing. However, data heterogeneity, particularly label shifting across different clients, presents significant challenges for feature screening. This paper introduces a general framework that unifies existing screening methods and proposes a novel utility, label-shift robust federated feature screening (LR-FFS), along with its federated estimation procedure. The framework facilitates a uniform analysis of methods and systematically characterizes their behaviors under label shift conditions. Building upon this framework, LR-FFS leverages conditional distribution functions and expectations to address label shift without adding computational burdens and remains robust against model misspecification and outliers. Additionally, the federated procedure ensures computational efficiency and privacy protection while maintaining screening effectiveness comparable to centralized processing. We also provide a false discovery rate (FDR) control method for federated feature screening. Experimental results and theoretical analyses demonstrate LR-FFS's superior performance across diverse client environments, including those with varying class distributions, sample sizes, and missing categorical data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2506.00385</link>
<guid>https://arxiv.org/abs/2506.00385</guid>
<content:encoded><![CDATA[

arXiv:2506.00385v1 Announce Type: cross 
Abstract: Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLMs via Adaptive Parallel Decoding</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[

arXiv:2506.00413v1 Announce Type: cross 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free</title>
<link>https://arxiv.org/abs/2506.00433</link>
<guid>https://arxiv.org/abs/2506.00433</guid>
<content:encoded><![CDATA[

arXiv:2506.00433v1 Announce Type: cross 
Abstract: High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight framework that enables any latent diffusion model to scale to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces three key components: (1) a scale-consistent variational autoencoder objective that enhances the spectral fidelity of latent representations; (2) wavelet energy maps that identify and localize detail-rich spatial regions within the latent space; and (3) a time-dependent masking strategy that focuses denoising supervision on high-frequency components during training. LWD requires no architectural modifications and incurs no additional computational overhead. Despite its simplicity, it consistently improves perceptual quality and reduces FID in ultra-high-resolution image synthesis, outperforming strong baseline models. These results highlight the effectiveness of frequency-aware, signal-driven supervision as a principled and efficient approach for high-resolution generative modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling</title>
<link>https://arxiv.org/abs/2506.00446</link>
<guid>https://arxiv.org/abs/2506.00446</guid>
<content:encoded><![CDATA[

arXiv:2506.00446v1 Announce Type: cross 
Abstract: Off-policy evaluation (OPE) in ranking settings with large ranking action spaces, which stems from an increase in both the number of unique actions and length of the ranking, is essential for assessing new recommender policies using only logged bandit data from previous versions. To address the high variance issues associated with existing estimators, we introduce two new assumptions: no direct effect on rankings and user behavior model on ranking embedding spaces. We then propose the generalized marginalized inverse propensity score (GMIPS) estimator with statistically desirable properties compared to existing ones. Finally, we demonstrate that the GMIPS achieves the lowest MSE. Notably, among GMIPS variants, the marginalized reward interaction IPS (MRIPS) incorporates a doubly marginalized importance weight based on a cascade behavior assumption on ranking embeddings. MRIPS effectively balances the trade-off between bias and variance, even as the ranking action spaces increase and the above assumptions may not hold, as evidenced by our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DV365: Extremely Long User History Modeling at Instagram</title>
<link>https://arxiv.org/abs/2506.00450</link>
<guid>https://arxiv.org/abs/2506.00450</guid>
<content:encoded><![CDATA[

arXiv:2506.00450v1 Announce Type: cross 
Abstract: Long user history is highly valuable signal for recommendation systems, but effectively incorporating it often comes with high cost in terms of data center power consumption and GPU. In this work, we chose offline embedding over end-to-end sequence length optimization methods to enable extremely long user sequence modeling as a cost-effective solution, and propose a new user embedding learning strategy, multi-slicing and summarization, that generates highly generalizable user representation of user's long-term stable interest. History length we encoded in this embedding is up to 70,000 and on average 40,000. This embedding, named as DV365, is proven highly incremental on top of advanced attentive user sequence models deployed in Instagram. Produced by a single upstream foundational model, it is launched in 15 different models across Instagram and Threads with significant impact, and has been production battle-proven for >1 year since our first launch.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[

arXiv:2506.00455v1 Announce Type: cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address this challenge, we introduce a novel machine learning method using diffusion-based molecular generation to enhance odour localization accuracy that can be used by itself or with automated olfactory dataset construction pipelines with vision-language models (VLMs) This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and the training data of VLMs, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors which emulate human olfactory recognition through electronic sensor arrays. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making in environments where olfactory cues are essential. Our methodology represents a foundational advancement in the field of robotic olfaction, offering a scalable solution to the challenges posed by limited olfactory data and sensor ambiguities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark</title>
<link>https://arxiv.org/abs/2506.00462</link>
<guid>https://arxiv.org/abs/2506.00462</guid>
<content:encoded><![CDATA[

arXiv:2506.00462v1 Announce Type: cross 
Abstract: Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested ``in the wild''. Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation</title>
<link>https://arxiv.org/abs/2506.00471</link>
<guid>https://arxiv.org/abs/2506.00471</guid>
<content:encoded><![CDATA[

arXiv:2506.00471v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for seismic wavefield modeling, yet they typically require time-consuming retraining when applied to different velocity models. Moreover, their training can suffer from slow convergence due to the complexity of of the wavefield solution. To address these challenges, we introduce a latent diffusion-based strategy for rapid and effective PINN initialization. First, we train multiple PINNs to represent frequency-domain scattered wavefields for various velocity models, then flatten each trained network's parameters into a one-dimensional vector, creating a comprehensive parameter dataset. Next, we employ an autoencoder to learn latent representations of these parameter vectors, capturing essential patterns across diverse PINN's parameters. We then train a conditional diffusion model to store the distribution of these latent vectors, with the corresponding velocity models serving as conditions. Once trained, this diffusion model can generate latent vectors corresponding to new velocity models, which are subsequently decoded by the autoencoder into complete PINN parameters. Experimental results indicate that our method significantly accelerates training and maintains high accuracy across in-distribution and out-of-distribution velocity scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00479</link>
<guid>https://arxiv.org/abs/2506.00479</guid>
<content:encoded><![CDATA[

arXiv:2506.00479v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet their significant computational demands hinder practical deployment. While efforts to improve LVLM efficiency are growing, existing methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics. In this work, we systematically evaluate mainstream acceleration techniques for LVLMs, categorized into token and parameter compression. We introduce EffiVLM-Bench, a unified framework for assessing not only absolute performance but also generalization and loyalty, while exploring Pareto-optimal trade-offs. Our extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs. We open-source code and recipes for EffiVLM-Bench to foster future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2506.00483</link>
<guid>https://arxiv.org/abs/2506.00483</guid>
<content:encoded><![CDATA[

arXiv:2506.00483v1 Announce Type: cross 
Abstract: Multi-hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps. We introduce Auto-Patch, a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch selectively modifies internal representations using a learned classifier. Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from 18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to Chain-of-Thought prompting (27.44\%). Our results highlight the potential of dynamic hidden state interventions for advancing complex reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities</title>
<link>https://arxiv.org/abs/2506.00548</link>
<guid>https://arxiv.org/abs/2506.00548</guid>
<content:encoded><![CDATA[

arXiv:2506.00548v1 Announce Type: cross 
Abstract: Existing attacks against multimodal language models (MLLMs) primarily communicate instructions through text accompanied by adversarial images. In contrast, we exploit the capabilities of MLLMs to interpret non-textual instructions, specifically, adversarial images or audio generated by our novel method, Con Instruction. We optimize these adversarial examples to align closely with target instructions in the embedding space, revealing the detrimental implications of MLLMs' sophisticated understanding. Unlike prior work, our method does not require training data or preprocessing of textual instructions. While these non-textual adversarial examples can effectively bypass MLLM safety mechanisms, their combination with various text inputs substantially amplifies attack success. We further introduce a new Attack Response Categorization (ARC) framework, which evaluates both the quality of the model's response and its relevance to the malicious instructions. Experimental results demonstrate that Con Instruction effectively bypasses safety mechanisms in multiple vision- and audio-language models, including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard benchmarks: AdvBench and SafeBench. Specifically, our method achieves the highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On the defense side, we explore various countermeasures against our attacks and uncover a substantial performance gap among existing techniques. Our implementation is made publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score Matching With Missing Data</title>
<link>https://arxiv.org/abs/2506.00557</link>
<guid>https://arxiv.org/abs/2506.00557</guid>
<content:encoded><![CDATA[

arXiv:2506.00557v1 Announce Type: cross 
Abstract: Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification</title>
<link>https://arxiv.org/abs/2506.00589</link>
<guid>https://arxiv.org/abs/2506.00589</guid>
<content:encoded><![CDATA[

arXiv:2506.00589v1 Announce Type: cross 
Abstract: Many core problems in robotics can be framed as constrained optimization problems. Often on these problems, the robotic system has uncertainty, or it would be advantageous to identify multiple high quality feasible solutions. To enable this, we present two novel frameworks for applying principles of constrained optimization to the new variational inference algorithm Stein variational gradient descent. Our general framework supports multiple types of constrained optimizers and can handle arbitrary constraints. We demonstrate on a variety of problems that we are able to learn to approximate distributions without violating constraints. Specifically, we show that we can build distributions of: robot motion plans that exactly avoid collisions, robot arm joint angles on the SE(3) manifold with exact table placement constraints, and object poses from point clouds with table placement constraints.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PackHero: A Scalable Graph-based Approach for Efficient Packer Identification</title>
<link>https://arxiv.org/abs/2506.00659</link>
<guid>https://arxiv.org/abs/2506.00659</guid>
<content:encoded><![CDATA[

arXiv:2506.00659v1 Announce Type: cross 
Abstract: Anti-analysis techniques, particularly packing, challenge malware analysts, making packer identification fundamental. Existing packer identifiers have significant limitations: signature-based methods lack flexibility and struggle against dynamic evasion, while Machine Learning approaches require extensive training data, limiting scalability and adaptability. Consequently, achieving accurate and adaptable packer identification remains an open problem. This paper presents PackHero, a scalable and efficient methodology for identifying packers using a novel static approach. PackHero employs a Graph Matching Network and clustering to match and group Call Graphs from programs packed with known packers. We evaluate our approach on a public dataset of malware and benign samples packed with various packers, demonstrating its effectiveness and scalability across varying sample sizes. PackHero achieves a macro-average F1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100 samples. Notably, PackHero requires fewer samples to achieve stable performance compared to other Machine Learning-based tools. Overall, PackHero matches the performance of State-of-the-art signature-based tools, outperforming them in handling Virtualization-based packers such as Themida/Winlicense, with a recall of 100%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout</title>
<link>https://arxiv.org/abs/2506.00662</link>
<guid>https://arxiv.org/abs/2506.00662</guid>
<content:encoded><![CDATA[

arXiv:2506.00662v1 Announce Type: cross 
Abstract: INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating robust classification from genomic data. METHODS: We developed a transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for uncertainty estimation in AD classification from whole-genome sequencing (WGS). We combined a transformer that preserves single-nucleotide polymorphism (SNP) sequence structure with a concurrent random forest using flattened genotypes. An uncertainty threshold separated samples into an uncertain (high-variance) group and a more certain (low-variance) group. RESULTS: We analyzed 1050 individuals, holding out half for testing. Overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636, respectively. Excluding the uncertain group improved accuracy from 0.6263 to 0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase). DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus improving reliability in AD genomic classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.00664</link>
<guid>https://arxiv.org/abs/2506.00664</guid>
<content:encoded><![CDATA[

arXiv:2506.00664v1 Announce Type: cross 
Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization</title>
<link>https://arxiv.org/abs/2506.00674</link>
<guid>https://arxiv.org/abs/2506.00674</guid>
<content:encoded><![CDATA[

arXiv:2506.00674v1 Announce Type: cross 
Abstract: The Boolean satisfiability (SAT) problem lies at the core of many applications in combinatorial optimization, software verification, cryptography, and machine learning. While state-of-the-art solvers have demonstrated high efficiency in handling conjunctive normal form (CNF) formulas, numerous applications require non-CNF (hybrid) constraints, such as XOR, cardinality, and Not-All-Equal constraints. Recent work leverages polynomial representations to represent such hybrid constraints, but it relies on box constraints that can limit the use of powerful unconstrained optimizers. In this paper, we propose unconstrained continuous optimization formulations for hybrid SAT solving by penalty terms. We provide theoretical insights into when these penalty terms are necessary and demonstrate empirically that unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid benchmarks. Our results highlight the potential of combining continuous optimization and machine-learning-based methods for effective hybrid SAT solving.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Upsample and Upmix Audio in the Latent Domain</title>
<link>https://arxiv.org/abs/2506.00681</link>
<guid>https://arxiv.org/abs/2506.00681</guid>
<content:encoded><![CDATA[

arXiv:2506.00681v1 Announce Type: cross 
Abstract: Neural audio autoencoders create compact latent representations that preserve perceptually important information, serving as the foundation for both modern audio compression systems and generation approaches like next-token prediction and latent diffusion. Despite their prevalence, most audio processing operations, such as spatial and spectral up-sampling, still inefficiently operate on raw waveforms or spectral representations rather than directly on these compressed representations. We propose a framework that performs audio processing operations entirely within an autoencoder's latent space, eliminating the need to decode to raw audio formats. Our approach dramatically simplifies training by operating solely in the latent domain, with a latent L1 reconstruction term, augmented by a single latent adversarial discriminator. This contrasts sharply with raw-audio methods that typically require complex combinations of multi-scale losses and discriminators. Through experiments in bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational efficiency gains of up to 100x while maintaining quality comparable to post-processing on raw audio. This work establishes a more efficient paradigm for audio processing pipelines that already incorporate autoencoders, enabling significantly faster and more resource-efficient workflows across various audio tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments</title>
<link>https://arxiv.org/abs/2506.00694</link>
<guid>https://arxiv.org/abs/2506.00694</guid>
<content:encoded><![CDATA[

arXiv:2506.00694v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Project page: https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Centric Token Interpretation for Vector-Quantized Generative Models</title>
<link>https://arxiv.org/abs/2506.00698</link>
<guid>https://arxiv.org/abs/2506.00698</guid>
<content:encoded><![CDATA[

arXiv:2506.00698v1 Announce Type: cross 
Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for image generation. However, the key component of VQGMs -- the codebook of discrete tokens -- is still not well understood, e.g., which tokens are critical to generate an image of a certain concept? This paper introduces Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting VQGMs by identifying concept-specific token combinations. Our framework employs two methods: (1) a sample-level explanation method that analyzes token importance scores in individual images, and (2) a codebook-level explanation method that explores the entire codebook to find globally relevant tokens. Experimental results demonstrate CORTEX's efficacy in providing clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX is useful in applications such as targeted image editing and shortcut feature detection. Our code is available at https://github.com/YangTianze009/CORTEX.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[

arXiv:2506.00708v1 Announce Type: cross 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[

arXiv:2506.00713v1 Announce Type: cross 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Inpainted Objects In-N-Out of Context</title>
<link>https://arxiv.org/abs/2506.00721</link>
<guid>https://arxiv.org/abs/2506.00721</guid>
<content:encoded><![CDATA[

arXiv:2506.00721v1 Announce Type: cross 
Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel dataset addressing the scarcity of out-of-context examples in existing vision datasets. By systematically replacing objects in COCO images through diffusion-based inpainting, we create 97,722 unique images featuring both contextually coherent and inconsistent scenes, enabling effective context learning. Each inpainted object is meticulously verified and categorized as in- or out-of-context through a multimodal large language model assessment. Our analysis reveals significant patterns in semantic priors that influence inpainting success across object categories. We demonstrate three key tasks enabled by COinCO: (1) training context classifiers that effectively determine whether existing objects belong in their context; (2) a novel Objects-from-Context prediction task that determines which new objects naturally belong in given scenes at both instance and clique levels, and (3) context-enhanced fake detection on state-of-the-art methods without fine-tuning. COinCO provides a controlled testbed with contextual variations, establishing a foundation for advancing context-aware visual understanding in computer vision and image forensics. Our code and data are at: https://github.com/YangTianze009/COinCO.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra</title>
<link>https://arxiv.org/abs/2506.00725</link>
<guid>https://arxiv.org/abs/2506.00725</guid>
<content:encoded><![CDATA[

arXiv:2506.00725v1 Announce Type: cross 
Abstract: Defects are ubiquitous in solids and strongly influence materials' mechanical and functional properties. However, non-destructive characterization and quantification of defects, especially when multiple types coexist, remain a long-standing challenge. Here we introduce DefectNet, a foundation machine learning model that predicts the chemical identity and concentration of substitutional point defects with multiple coexisting elements directly from vibrational spectra, specifically phonon density-of-states (PDoS). Trained on over 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a tailored attention mechanism to identify up to six distinct defect elements at concentrations ranging from 0.2% to 25%. The model generalizes well to unseen crystals across 56 elements and can be fine-tuned on experimental data. Validation using inelastic scattering measurements of SiGe alloys and MgB$_2$ superconductor demonstrates its accuracy and transferability. Our work establishes vibrational spectroscopy as a viable, non-destructive probe for point defect quantification in bulk materials, and highlights the promise of foundation models in data-driven defect engineering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?</title>
<link>https://arxiv.org/abs/2506.00751</link>
<guid>https://arxiv.org/abs/2506.00751</guid>
<content:encoded><![CDATA[

arXiv:2506.00751v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[

arXiv:2506.00785v1 Announce Type: cross 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer</title>
<link>https://arxiv.org/abs/2506.00800</link>
<guid>https://arxiv.org/abs/2506.00800</guid>
<content:encoded><![CDATA[

arXiv:2506.00800v1 Announce Type: cross 
Abstract: Automated Audio Captioning (AAC) aims to describe the semantic contexts of general sounds, including acoustic events and scenes, by leveraging effective acoustic features. To enhance performance, an AAC method, EnCLAP, employed discrete tokens from EnCodec as an effective input for fine-tuning a language model BART. However, EnCodec is designed to reconstruct waveforms rather than capture the semantic contexts of general sounds, which AAC should describe. To address this issue, we propose CLAP-ART, an AAC method that utilizes ``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich discrete tokens from pre-trained audio representations through vector quantization. We experimentally confirmed that CLAP-ART outperforms baseline EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens derived from semantically rich AR are beneficial for AAC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning</title>
<link>https://arxiv.org/abs/2506.00813</link>
<guid>https://arxiv.org/abs/2506.00813</guid>
<content:encoded><![CDATA[

arXiv:2506.00813v1 Announce Type: cross 
Abstract: Tabular-image multimodal learning, which integrates structured tabular data with imaging data, holds great promise for a variety of tasks, especially in medical applications. Yet, two key challenges remain: (1) the lack of a standardized, pretrained representation for tabular data, as is commonly available in vision and language domains; and (2) the difficulty of handling missing values in the tabular modality, which are common in real-world medical datasets. To address these issues, we propose the TabPFN-Integrated Multimodal Engine (TIME), a novel multimodal framework that builds on the recently introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen tabular encoder to generate robust, strong embeddings that are naturally resilient to missing data, and combines them with image features from pretrained vision backbones. We explore a range of fusion strategies and tabular encoders, and evaluate our approach on both natural and medical datasets. Extensive experiments demonstrate that TIME consistently outperforms competitive baselines across both complete and incomplete tabular inputs, underscoring its practical value in real-world multimodal learning scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Linear Markov Decision Process</title>
<link>https://arxiv.org/abs/2506.00818</link>
<guid>https://arxiv.org/abs/2506.00818</guid>
<content:encoded><![CDATA[

arXiv:2506.00818v1 Announce Type: cross 
Abstract: The linear Markov Decision Process (MDP) framework offers a principled foundation for reinforcement learning (RL) with strong theoretical guarantees and sample efficiency. However, its restrictive assumption-that both transition dynamics and reward functions are linear in the same feature space-limits its applicability in real-world domains, where rewards often exhibit nonlinear or discrete structures. Motivated by applications such as healthcare and e-commerce, where data is scarce and reward signals can be binary or count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an extension of the linear MDP framework-that models rewards using generalized linear models (GLMs) while maintaining linear transition dynamics. We establish the Bellman completeness of GLMDPs with respect to a new function class that accommodates nonlinear rewards and develop two offline RL algorithms: Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant (SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our algorithms achieve theoretical guarantees on policy suboptimality and demonstrate improved sample efficiency in settings where reward labels are expensive or limited.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[

arXiv:2506.00826v1 Announce Type: cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System</title>
<link>https://arxiv.org/abs/2506.00828</link>
<guid>https://arxiv.org/abs/2506.00828</guid>
<content:encoded><![CDATA[

arXiv:2506.00828v1 Announce Type: cross 
Abstract: In a single-slot recommendation system, users are only exposed to one item at a time, and the system cannot collect user feedback on multiple items simultaneously. Therefore, only pointwise modeling solutions can be adopted, focusing solely on modeling the likelihood of clicks or conversions for items by users to learn user-item preferences, without the ability to capture the ranking information among different items directly. However, since user-side information is often much more abundant than item-side information, the model can quickly learn the differences in user intrinsic tendencies, which are independent of the items they are exposed to. This can cause these intrinsic tendencies to become a shortcut bias for the model, leading to insufficient mining of the most concerned user-item preferences. To solve this challenge, we introduce the Breaker model. Breaker integrates an auxiliary task of user representation clustering with a multi-tower structure for cluster-specific preference modeling. By clustering user representations, we ensure that users within each cluster exhibit similar characteristics, which increases the complexity of the pointwise recommendation task on the user side. This forces the multi-tower structure with cluster-driven parameter learning to better model user-item preferences, ultimately eliminating shortcut biases related to user intrinsic tendencies. In terms of training, we propose a delayed parameter update mechanism to enhance training stability and convergence, enabling end-to-end joint training of the auxiliary clustering and classification tasks. Both offline and online experiments demonstrate that our method surpasses the baselines. It has already been deployed and is actively serving tens of millions of users daily on Meituan, one of the most popular e-commerce platforms for services.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPKE: Complex Question Answering under Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.00829</link>
<guid>https://arxiv.org/abs/2506.00829</guid>
<content:encoded><![CDATA[

arXiv:2506.00829v1 Announce Type: cross 
Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Path Guiding with Distribution Factorization</title>
<link>https://arxiv.org/abs/2506.00839</link>
<guid>https://arxiv.org/abs/2506.00839</guid>
<content:encoded><![CDATA[

arXiv:2506.00839v1 Announce Type: cross 
Abstract: In this paper, we present a neural path guiding method to aid with Monte Carlo (MC) integration in rendering. Existing neural methods utilize distribution representations that are either fast or expressive, but not both. We propose a simple, but effective, representation that is sufficiently expressive and reasonably fast. Specifically, we break down the 2D distribution over the directional domain into two 1D probability distribution functions (PDF). We propose to model each 1D PDF using a neural network that estimates the distribution at a set of discrete coordinates. The PDF at an arbitrary location can then be evaluated and sampled through interpolation. To train the network, we maximize the similarity of the learned and target distributions. To reduce the variance of the gradient during optimizations and estimate the normalization factor, we propose to cache the incoming radiance using an additional network. Through extensive experiments, we demonstrate that our approach is better than the existing methods, particularly in challenging scenes with complex light transport.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[

arXiv:2506.00854v1 Announce Type: cross 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models</title>
<link>https://arxiv.org/abs/2506.00863</link>
<guid>https://arxiv.org/abs/2506.00863</guid>
<content:encoded><![CDATA[

arXiv:2506.00863v1 Announce Type: cross 
Abstract: Emotion recognition in low-resource languages like Marathi remains challenging due to limited annotated data. We present L3Cube-MahaEmotions, a high-quality Marathi emotion recognition dataset with 11 fine-grained emotion labels. The training data is synthetically annotated using large language models (LLMs), while the validation and test sets are manually labeled to serve as a reliable gold-standard benchmark. Building on the MahaSent dataset, we apply the Chain-of-Translation (CoTR) prompting technique, where Marathi sentences are translated into English and emotion labeled via a single prompt. GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data annotation due to superior label quality. We evaluate model performance using standard metrics and explore label aggregation strategies (e.g., Union, Intersection). While GPT-4 predictions outperform fine-tuned BERT models, BERT-based models trained on synthetic labels fail to surpass GPT-4. This highlights both the importance of high-quality human-labeled data and the inherent complexity of emotion recognition. An important finding of this work is that generic LLMs like GPT-4 and Llama3-405B generalize better than fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection Pursuit Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2506.00866</link>
<guid>https://arxiv.org/abs/2506.00866</guid>
<content:encoded><![CDATA[

arXiv:2506.00866v1 Announce Type: cross 
Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for its broad applications across multiple domains, such as covariate shift adaptation, causal inference, independence tests and beyond. Parametric methods for estimating the density ratio possibly lead to biased results if models are misspecified, while conventional non-parametric methods suffer from the curse of dimensionality when the dimension of data is large. To address these challenges, in this paper, we propose a novel approach for DRE based on the projection pursuit (PP) approximation. The proposed method leverages PP to mitigate the impact of high dimensionality while retaining the model flexibility needed for the accuracy of DRE. We establish the consistency and the convergence rate for the proposed estimator. Experimental results demonstrate that our proposed method outperforms existing alternatives in various applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODEMENV: Benchmarking Large Language Models on Code Migration</title>
<link>https://arxiv.org/abs/2506.00894</link>
<guid>https://arxiv.org/abs/2506.00894</guid>
<content:encoded><![CDATA[

arXiv:2506.00894v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras</title>
<link>https://arxiv.org/abs/2506.00904</link>
<guid>https://arxiv.org/abs/2506.00904</guid>
<content:encoded><![CDATA[

arXiv:2506.00904v1 Announce Type: cross 
Abstract: The construction industry faces significant challenges in optimizing equipment utilization, as underused machinery leads to increased operational costs and project delays. Accurate and timely monitoring of equipment activity is therefore key to identifying idle periods and improving overall efficiency. This paper presents the Edge-IMI framework for detecting idle construction machinery, specifically designed for integration with surveillance camera systems. The proposed solution consists of three components: object detection, tracking, and idle state identification, which are tailored for execution on resource-constrained, CPU-based edge computing devices. The performance of Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS benchmarks. Experimental results confirm that the object detector achieves an F1 score of 71.75%, indicating robust real-world detection capabilities. The logistic regression-based idle identification module reliably distinguishes between active and idle machinery with minimal false positives. Integrating all three modules, Edge-IMI enables efficient on-site inference, reducing reliance on high-bandwidth cloud services and costly hardware accelerators. We also evaluate the performance of object detection models on Raspberry Pi 5 and an Intel NUC platforms, as example edge computing platforms. We assess the feasibility of real-time processing and the impact of model optimization techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search</title>
<link>https://arxiv.org/abs/2506.00925</link>
<guid>https://arxiv.org/abs/2506.00925</guid>
<content:encoded><![CDATA[

arXiv:2506.00925v1 Announce Type: cross 
Abstract: Designing protein sequences that fold into a target 3D structure, known as protein inverse folding, is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by expanding the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise</title>
<link>https://arxiv.org/abs/2506.00933</link>
<guid>https://arxiv.org/abs/2506.00933</guid>
<content:encoded><![CDATA[

arXiv:2506.00933v1 Announce Type: cross 
Abstract: Integral equations are widely used in fields such as applied modeling, medical imaging, and system identification, providing a powerful framework for solving deterministic problems. While parameter identification for differential equations has been extensively studied, the focus on integral equations, particularly stochastic Volterra integral equations, remains limited. This research addresses the parameter identification problem, also known as the equation reconstruction problem, in Volterra integral equations driven by Gaussian noise. We propose an improved deep neural networks framework for estimating unknown parameters in the drift term of these equations. The network represents the primary variables and their integrals, enhancing parameter estimation accuracy by incorporating inter-output relationships into the loss function. Additionally, the framework extends beyond parameter identification to predict the system's behavior outside the integration interval. Prediction accuracy is validated by comparing predicted and true trajectories using a 95% confidence interval. Numerical experiments demonstrate the effectiveness of the proposed deep neural networks framework in both parameter identification and prediction tasks, showing robust performance under varying noise levels and providing accurate solutions for modeling stochastic systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: From Ad-hoc to Proactive Search in Conversations</title>
<link>https://arxiv.org/abs/2506.00983</link>
<guid>https://arxiv.org/abs/2506.00983</guid>
<content:encoded><![CDATA[

arXiv:2506.00983v1 Announce Type: cross 
Abstract: Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Local Intrinsic Dimensions of Contextual Language Models</title>
<link>https://arxiv.org/abs/2506.01034</link>
<guid>https://arxiv.org/abs/2506.01034</guid>
<content:encoded><![CDATA[

arXiv:2506.01034v1 Announce Type: cross 
Abstract: Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[

arXiv:2506.01062v1 Announce Type: cross 
Abstract: We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety</title>
<link>https://arxiv.org/abs/2506.01069</link>
<guid>https://arxiv.org/abs/2506.01069</guid>
<content:encoded><![CDATA[

arXiv:2506.01069v1 Announce Type: cross 
Abstract: Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning DNF through Generalized Fourier Representations</title>
<link>https://arxiv.org/abs/2506.01075</link>
<guid>https://arxiv.org/abs/2506.01075</guid>
<content:encoded><![CDATA[

arXiv:2506.01075v1 Announce Type: cross 
Abstract: The Fourier representation for the uniform distribution over the Boolean cube has found numerous applications in algorithms and complexity analysis. Notably, in learning theory, learnability of Disjunctive Normal Form (DNF) under uniform as well as product distributions has been established through such representations. This paper makes five main contributions. First, it introduces a generalized Fourier expansion that can be used with any distribution $D$ through the representation of the distribution as a Bayesian network (BN). Second, it shows that the main algorithmic tools for learning with the Fourier representation, that use membership queries to approximate functions by recovering their heavy Fourier coefficients, can be used with slight modifications with the generalized expansion. These results hold for any distribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under the new expansion, showing that it is bounded for a class of distributions which can be represented by difference bounded tree BN, where a parent node in the BN representation can change the conditional expectation of a child node by at most $\alpha<0.5$. Lower bounds are presented to show that such constraints are necessary. The fourth contribution uses these results to show the learnability of DNF with membership queries under difference bounded tree BN. The final contribution is to develop an algorithm for learning difference-bounded tree BN distributions, thus extending the DNF learnability result to cases where the distribution is not known in advance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion posterior sampling for informative likelihoods</title>
<link>https://arxiv.org/abs/2506.01083</link>
<guid>https://arxiv.org/abs/2506.01083</guid>
<content:encoded><![CDATA[

arXiv:2506.01083v1 Announce Type: cross 
Abstract: Sequential Monte Carlo (SMC) methods have recently shown successful results for conditional sampling of generative diffusion models. In this paper we propose a new diffusion posterior SMC sampler achieving improved statistical efficiencies, particularly under outlier conditions or highly informative likelihoods. The key idea is to construct an observation path that correlates with the diffusion model and to design the sampler to leverage this correlation for more efficient sampling. Empirical results conclude the efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression</title>
<link>https://arxiv.org/abs/2506.01084</link>
<guid>https://arxiv.org/abs/2506.01084</guid>
<content:encoded><![CDATA[

arXiv:2506.01084v1 Announce Type: cross 
Abstract: Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\%, with significant improvements in inference latency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[

arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization</title>
<link>https://arxiv.org/abs/2506.01143</link>
<guid>https://arxiv.org/abs/2506.01143</guid>
<content:encoded><![CDATA[

arXiv:2506.01143v1 Announce Type: cross 
Abstract: Modern machine learning models are often trained in a setting where the number of parameters exceeds the number of training samples. To understand the implicit bias of gradient descent in such overparameterized models, prior work has studied diagonal linear neural networks in the regression setting. These studies have shown that, when initialized with small weights, gradient descent tends to favor solutions with minimal $\ell^1$-norm - an effect known as implicit regularization. In this paper, we investigate implicit regularization in diagonal linear neural networks of depth $D\ge 2$ for overparameterized linear regression problems. We focus on analyzing the approximation error between the limit point of gradient flow trajectories and the solution to the $\ell^1$-minimization problem. By deriving tight upper and lower bounds on the approximation error, we precisely characterize how the approximation error depends on the scale of initialization $\alpha$. Our results reveal a qualitative difference between depths: for $D \ge 3$, the error decreases linearly with $\alpha$, whereas for $D=2$, it decreases at rate $\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly characterized. Interestingly, this parameter is closely linked to so-called null space property constants studied in the sparse recovery literature. We demonstrate the asymptotic tightness of our bounds through explicit examples. Numerical experiments corroborate our theoretical findings and suggest that deeper networks, i.e., $D \ge 3$, may lead to better generalization, particularly for realistic initialization scales.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition</title>
<link>https://arxiv.org/abs/2506.01147</link>
<guid>https://arxiv.org/abs/2506.01147</guid>
<content:encoded><![CDATA[

arXiv:2506.01147v1 Announce Type: cross 
Abstract: System-generated logs are typically converted into categorical log templates through parsing. These templates are crucial for generating actionable insights in various downstream tasks. However, existing parsers often fail to capture fine-grained template details, leading to suboptimal accuracy and reduced utility in downstream tasks requiring precise pattern identification. We propose a character-level log parser utilizing a novel neural architecture that aggregates character embeddings. Our approach estimates a sequence of binary-coded decimals to achieve highly granular log templates extraction. Our low-resource character-level parser, tested on revised Loghub-2k and a manually annotated industrial dataset, matches LLM-based parsers in accuracy while outperforming semantic parsers in efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor</title>
<link>https://arxiv.org/abs/2506.01162</link>
<guid>https://arxiv.org/abs/2506.01162</guid>
<content:encoded><![CDATA[

arXiv:2506.01162v1 Announce Type: cross 
Abstract: Estimating the density of a distribution from its samples is a fundamental problem in statistics. Hypothesis selection addresses the setting where, in addition to a sample set, we are given $n$ candidate distributions -- referred to as hypotheses -- and the goal is to determine which one best describes the underlying data distribution. This problem is known to be solvable very efficiently, requiring roughly $O(\log n)$ samples and running in $\tilde{O}(n)$ time. The quality of the output is measured via the total variation distance to the unknown distribution, and the approximation factor of the algorithm determines how large this distance is compared to the optimal distance achieved by the best candidate hypothesis. It is known that $\alpha = 3$ is the optimal approximation factor for this problem. We study hypothesis selection under the constraint of differential privacy. We propose a differentially private algorithm in the central model that runs in nearly-linear time with respect to the number of hypotheses, achieves the optimal approximation factor, and incurs only a modest increase in sample complexity, which remains polylogarithmic in $n$. This resolves an open question posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work, existing upper bounds required quadratic time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration</title>
<link>https://arxiv.org/abs/2506.01166</link>
<guid>https://arxiv.org/abs/2506.01166</guid>
<content:encoded><![CDATA[

arXiv:2506.01166v1 Announce Type: cross 
Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network DNN accelerators - particularly important for emerging Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate MAC units. The proposed architecture achieves saving by 37% and 68% in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFBench: An Extensive Benchmark for Fatigue Analysis</title>
<link>https://arxiv.org/abs/2506.01173</link>
<guid>https://arxiv.org/abs/2506.01173</guid>
<content:encoded><![CDATA[

arXiv:2506.01173v1 Announce Type: cross 
Abstract: Fatigue-induced crack growth is a leading cause of structural failure across critical industries such as aerospace, civil engineering, automotive, and energy. Accurate prediction of stress intensity factors (SIFs) -- the key parameters governing crack propagation in linear elastic fracture mechanics -- is essential for assessing fatigue life and ensuring structural integrity. While machine learning (ML) has shown great promise in SIF prediction, its advancement has been severely limited by the lack of rich, transparent, well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale benchmark database designed to support ML-based SIF prediction. SIFBench contains over 5 million different crack and component geometries derived from high-fidelity finite element simulations across 37 distinct scenarios, and provides a unified Python interface for seamless data access and customization. We report baseline results using a range of popular ML models -- including random forests, support vector machines, feedforward neural networks, and Fourier neural operators -- alongside comprehensive evaluation metrics and template code for model training, validation, and assessment. By offering a standardized and scalable resource, SIFBench substantially lowers the entry barrier and fosters the development and application of ML methods in damage tolerance design and predictive maintenance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data</title>
<link>https://arxiv.org/abs/2506.01189</link>
<guid>https://arxiv.org/abs/2506.01189</guid>
<content:encoded><![CDATA[

arXiv:2506.01189v1 Announce Type: cross 
Abstract: Despite progress in the rapidly developing field of geometric deep learning, performing statistical analysis on geometric data--where each observation is a shape such as a curve, graph, or surface--remains challenging due to the non-Euclidean nature of shape spaces, which are defined as equivalence classes under invariance groups. Building machine learning frameworks that incorporate such invariances, notably to shape parametrization, is often crucial to ensure generalizability of the trained models to new observations. This work proposes SVarM to exploit varifold representations of shapes as measures and their duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$. This method provides a general framework akin to linear support vector machines but operating instead over the infinite-dimensional space of varifolds. We develop classification and regression models on shape datasets by introducing a neural network-based representation of the trainable test function $h$. This approach demonstrates strong performance and robustness across various shape graph and surface datasets, achieving results comparable to state-of-the-art methods while significantly reducing the number of trainable parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures</title>
<link>https://arxiv.org/abs/2506.01197</link>
<guid>https://arxiv.org/abs/2506.01197</guid>
<content:encoded><![CDATA[

arXiv:2506.01197v1 Announce Type: cross 
Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</title>
<link>https://arxiv.org/abs/2506.01215</link>
<guid>https://arxiv.org/abs/2506.01215</guid>
<content:encoded><![CDATA[

arXiv:2506.01215v1 Announce Type: cross 
Abstract: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Mixed Precision Quantization for Learned Image Compression</title>
<link>https://arxiv.org/abs/2506.01221</link>
<guid>https://arxiv.org/abs/2506.01221</guid>
<content:encoded><![CDATA[

arXiv:2506.01221v1 Announce Type: cross 
Abstract: Despite its improvements in coding performance compared to traditional codecs, Learned Image Compression (LIC) suffers from large computational costs for storage and deployment. Model quantization offers an effective solution to reduce the computational complexity of LIC models. However, most existing works perform fixed-precision quantization which suffers from sub-optimal utilization of resources due to the varying sensitivity to quantization of different layers of a neural network. In this paper, we propose a Flexible Mixed Precision Quantization (FMPQ) method that assigns different bit-widths to different layers of the quantized network using the fractional change in rate-distortion loss as the bit-assignment criterion. We also introduce an adaptive search algorithm which reduces the time-complexity of searching for the desired distribution of quantization bit-widths given a fixed model size. Evaluation of our method shows improved BD-Rate performance under similar model size constraints compared to other works on quantization of LIC models. We have made the source code available at gitlab.com/viper-purdue/fmpq.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN</title>
<link>https://arxiv.org/abs/2506.01226</link>
<guid>https://arxiv.org/abs/2506.01226</guid>
<content:encoded><![CDATA[

arXiv:2506.01226v1 Announce Type: cross 
Abstract: We study parameterizations of stabilizing nonlinear policies for learning-based control. We propose a structure based on a nonlinear version of the Youla-Ku\v{c}era parameterization combined with robust neural networks such as the recurrent equilibrium network (REN). The resulting parameterizations are unconstrained, and hence can be searched over with first-order optimization methods, while always ensuring closed-loop stability by construction. We study the combination of (a) nonlinear dynamics, (b) partial observation, and (c) incremental closed-loop stability requirements (contraction and Lipschitzness). We find that with any two of these three difficulties, a contracting and Lipschitz Youla parameter always leads to contracting and Lipschitz closed loops. However, if all three hold, then incremental stability can be lost with exogenous disturbances. Instead, a weaker condition is maintained, which we call d-tube contraction and Lipschitzness. We further obtain converse results showing that the proposed parameterization covers all contracting and Lipschitz closed loops for certain classes of nonlinear systems. Numerical experiments illustrate the utility of our parameterization when learning controllers with built-in stability certificates for: i) ``economic'' rewards without stabilizing effects; ii) short training horizons; and iii) uncertain systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors</title>
<link>https://arxiv.org/abs/2506.01247</link>
<guid>https://arxiv.org/abs/2506.01247</guid>
<content:encoded><![CDATA[

arXiv:2506.01247v1 Announce Type: cross 
Abstract: Steering vision foundation models at inference time without retraining or access to large labeled datasets is a desirable yet challenging objective, particularly in dynamic or resource-constrained settings. In this paper, we introduce Visual Sparse Steering (VS2), a lightweight, test-time method that guides vision models using steering vectors derived from sparse features learned by top-$k$ Sparse Autoencoders without requiring contrastive data. Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a retrieval-augmented variant that selectively amplifies relevant sparse features using pseudo-labeled neighbors at inference time. With oracle positive/negative sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44% on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2 and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing that sparse steering benefits specific classes by disambiguating visually or taxonomically proximate categories rather than providing a uniform boost. Finally, to better align the sparse features learned through the SAE reconstruction task with those relevant for downstream performance, we propose Prototype-Aligned Sparse Steering (PASS). By incorporating a prototype-alignment loss during SAE training, using labels only during training while remaining fully test-time unsupervised, PASS consistently, though modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100 with ViT-B/32.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence intervals for forced alignment boundaries using model ensembles</title>
<link>https://arxiv.org/abs/2506.01256</link>
<guid>https://arxiv.org/abs/2506.01256</guid>
<content:encoded><![CDATA[

arXiv:2506.01256v1 Announce Type: cross 
Abstract: Forced alignment is a common tool to align audio with orthographic and phonetic transcriptions. Most forced alignment tools provide only a single estimate of a boundary. The present project introduces a method of deriving confidence intervals for these boundaries using a neural network ensemble technique. Ten different segment classifier neural networks were previously trained, and the alignment process is repeated with each model. The alignment ensemble is then used to place the boundary at the median of the boundaries in the ensemble, and 97.85% confidence intervals are constructed using order statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a slight improvement over using just a single model. The confidence intervals are incorporated into Praat TextGrids using a point tier, and they are also output as a table for researchers to analyze separately as diagnostics or to incorporate uncertainty into their analyses.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation</title>
<link>https://arxiv.org/abs/2506.01267</link>
<guid>https://arxiv.org/abs/2506.01267</guid>
<content:encoded><![CDATA[

arXiv:2506.01267v1 Announce Type: cross 
Abstract: Despite tremendous advancements of machine learning models and algorithms in various application domains, they are known to be vulnerable to subtle, natural or intentionally crafted perturbations in future input data, known as adversarial attacks. While numerous adversarial learning methods have been proposed, fundamental questions about their statistical optimality in robust loss remain largely unanswered. In particular, the minimax rate of convergence and the construction of rate-optimal estimators under future $X$-attacks are yet to be worked out.
  In this paper, we address this issue in the context of nonparametric regression, under suitable assumptions on the smoothness of the regression function and the geometric structure of the input perturbation set. We first establish the minimax rate of convergence under adversarial $L_q$-risks with $1 \leq q \leq \infty$ and propose a piecewise local polynomial estimator that achieves the minimax optimality. The established minimax rate elucidates how the smoothness level and perturbation magnitude affect the fundamental limit of adversarial learning under future $X$-attacks. Furthermore, we construct a data-driven adaptive estimator that is shown to achieve, within a logarithmic factor, the optimal rate across a broad scale of nonparametric and adversarial classes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction</title>
<link>https://arxiv.org/abs/2506.01268</link>
<guid>https://arxiv.org/abs/2506.01268</guid>
<content:encoded><![CDATA[

arXiv:2506.01268v1 Announce Type: cross 
Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework's single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \https://github.com/opendilab/CleanS2S.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable In-Context Q-Learning</title>
<link>https://arxiv.org/abs/2506.01299</link>
<guid>https://arxiv.org/abs/2506.01299</guid>
<content:encoded><![CDATA[

arXiv:2506.01299v1 Announce Type: cross 
Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Platform for Investigating Public Health Content with Efficient Concern Classification</title>
<link>https://arxiv.org/abs/2506.01308</link>
<guid>https://arxiv.org/abs/2506.01308</guid>
<content:encoded><![CDATA[

arXiv:2506.01308v1 Announce Type: cross 
Abstract: A recent rise in online content expressing concerns with public health initiatives has contributed to already stalled uptake of preemptive measures globally. Future public health efforts must attempt to understand such content, what concerns it may raise among readers, and how to effectively respond to it. To this end, we present ConcernScope, a platform that uses a teacher-student framework for knowledge transfer between large language models and light-weight classifiers to quickly and effectively identify the health concerns raised in a text corpus. The platform allows uploading massive files directly, automatically scraping specific URLs, and direct text editing. ConcernScope is built on top of a taxonomy of public health concerns. Intended for public health officials, we demonstrate several applications of this platform: guided data exploration to find useful examples of common concerns found in online community datasets, identification of trends in concerns through an example time series analysis of 186,000 samples, and finding trends in topic frequency before and after significant events.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Clustering in Mixture of Markov Chains</title>
<link>https://arxiv.org/abs/2506.01324</link>
<guid>https://arxiv.org/abs/2506.01324</guid>
<content:encoded><![CDATA[

arXiv:2506.01324v1 Announce Type: cross 
Abstract: We study the problem of clustering $T$ trajectories of length $H$, each generated by one of $K$ unknown ergodic Markov chains over a finite state space of size $S$. The goal is to accurately group trajectories according to their underlying generative model. We begin by deriving an instance-dependent, high-probability lower bound on the clustering error rate, governed by the weighted KL divergence between the transition kernels of the chains. We then present a novel two-stage clustering algorithm. In Stage~I, we apply spectral clustering using a new injective Euclidean embedding for ergodic Markov chains -- a contribution of independent interest that enables sharp concentration results. Stage~II refines the initial clusters via a single step of likelihood-based reassignment. Our method achieves a near-optimal clustering error with high probability, under the conditions $H = \tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH = \tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the minimum stationary probability of a state across the $K$ chains and $\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements provide significant improvements, if not at least comparable, to the state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm offers a key practical advantage: unlike existing approach, it requires no prior knowledge of model-specific quantities (e.g., separation between kernels or visitation probabilities). We conclude by discussing the inherent gap between our upper and lower bounds, providing insights into the unique structure of this clustering problem.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01347</link>
<guid>https://arxiv.org/abs/2506.01347</guid>
<content:encoded><![CDATA[

arXiv:2506.01347v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. We train Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum ($k$ up to $256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@$1$ but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@$k$ performance on MATH, AIME 2025, and AMC23. Our code is available at https://github.com/TianHongZXY/RLVR-Decomposed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoBrain: Synergizing Minds and Eyes For Human Action Understanding</title>
<link>https://arxiv.org/abs/2506.01353</link>
<guid>https://arxiv.org/abs/2506.01353</guid>
<content:encoded><![CDATA[

arXiv:2506.01353v1 Announce Type: cross 
Abstract: The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain --the world's first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a muiltimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Scientists Fail Without Strong Implementation Capability</title>
<link>https://arxiv.org/abs/2506.01372</link>
<guid>https://arxiv.org/abs/2506.01372</guid>
<content:encoded><![CDATA[

arXiv:2506.01372v1 Announce Type: cross 
Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Turbulence to Tranquility: AI-Driven Low-Altitude Network</title>
<link>https://arxiv.org/abs/2506.01378</link>
<guid>https://arxiv.org/abs/2506.01378</guid>
<content:encoded><![CDATA[

arXiv:2506.01378v1 Announce Type: cross 
Abstract: Low Altitude Economy (LAE) networks own transformative potential in urban mobility, emergency response, and aerial logistics. However, these networks face significant challenges in spectrum management, interference mitigation, and real-time coordination across dynamic and resource-constrained environments. After addressing these challenges, this study explores three core elements for enabling intelligent LAE networks as follows machine learning-based spectrum sensing and coexistence, artificial intelligence (AI)-optimized resource allocation and trajectory planning, and testbed-driven validation and standardization. We highlight how federated and reinforcement learning techniques support decentralized, adaptive decision-making under mobility and energy constraints. In addition, we discuss the role of real-world platforms such as AERPAW in bridging the gap between simulation and deployment and enabling iterative system refinement under realistic conditions. This study aims to provide a forward-looking roadmap toward developing efficient and interoperable AI-driven LAE ecosystems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Calls for Malware Detection and Classification: Methodologies and Applications</title>
<link>https://arxiv.org/abs/2506.01412</link>
<guid>https://arxiv.org/abs/2506.01412</guid>
<content:encoded><![CDATA[

arXiv:2506.01412v1 Announce Type: cross 
Abstract: As malware continues to become more complex and harder to detect, Malware Analysis needs to continue to evolve to stay one step ahead. One promising key area approach focuses on using system calls and API Calls, the core communication between user applications and the operating system and their kernels. These calls provide valuable insight into how software or programs behaves, making them an useful tool for spotting suspicious or harmful activity of programs and software. This chapter takes a deep down look at how system calls are used in malware detection and classification, covering techniques like static and dynamic analysis, as well as sandboxing. By combining these methods with advanced techniques like machine learning, statistical analysis, and anomaly detection, researchers can analyze system call patterns to tell the difference between normal and malicious behavior. The chapter also explores how these techniques are applied across different systems, including Windows, Linux, and Android, while also looking at the ways sophisticated malware tries to evade detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[

arXiv:2506.01413v1 Announce Type: cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title>
<link>https://arxiv.org/abs/2506.01420</link>
<guid>https://arxiv.org/abs/2506.01420</guid>
<content:encoded><![CDATA[

arXiv:2506.01420v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in sensitive domains, where their ability to infer personal data from seemingly benign text poses emerging privacy risks. While recent LLM-based anonymization methods help mitigate such risks, they often rely on proprietary models (e.g., GPT-4), raising concerns about cost and the potential exposure of sensitive data to untrusted external systems. To address this, we introduce SElf-refining Anonymization with Language model (SEAL), a novel distillation framework for training small language models (SLMs) to perform effective anonymization without relying on external costly models at inference time. We leverage adversarial interactions between an LLM anonymizer and an inference model to collect trajectories of anonymized texts and inferred attributes, which are used to distill anonymization, adversarial inference, and utility evaluation capabilities into SLMs via supervised fine-tuning and preference learning. The resulting models learn to both anonymize text and critique their outputs, enabling iterative improvement of anonymization quality via self-refinement. Experiments on SynthPAI, a dataset of synthetic personal profiles and text comments, demonstrate that SLMs trained with SEAL achieve substantial improvements in anonymization capabilities. Notably, 8B models attain a privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with self-refinement, even surpass it in terms of privacy. These results show the effectiveness of our adversarial distillation framework in training SLMs as efficient anonymizers. To facilitate further research, we release the full dataset used in our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes</title>
<link>https://arxiv.org/abs/2506.01456</link>
<guid>https://arxiv.org/abs/2506.01456</guid>
<content:encoded><![CDATA[

arXiv:2506.01456v1 Announce Type: cross 
Abstract: Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity</title>
<link>https://arxiv.org/abs/2506.01493</link>
<guid>https://arxiv.org/abs/2506.01493</guid>
<content:encoded><![CDATA[

arXiv:2506.01493v1 Announce Type: cross 
Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully scaled to billion-scale large text-to-image datasets. However, training such models entails a high training cost, limiting some applications and research usage. To reduce the cost, one promising direction is the incorporation of pre-trained models. The existing method of utilizing pre-trained models for a generator significantly reduced the training cost compared with the other large-scale GANs, but we found the model loses the diversity of generation for a given prompt by a large margin. To build an efficient and high-fidelity text-to-image GAN without compromise, we propose to use two specialized discriminators with Slicing Adversarial Networks (SANs) adapted for text-to-image tasks. Our proposed model, called SCAD, shows a notable enhancement in diversity for a given prompt with better sample fidelity. We also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID competitive with the latest large-scale GANs at two orders of magnitude less training cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiceMixer - Netlist-Level Circuit Evolution</title>
<link>https://arxiv.org/abs/2506.01497</link>
<guid>https://arxiv.org/abs/2506.01497</guid>
<content:encoded><![CDATA[

arXiv:2506.01497v1 Announce Type: cross 
Abstract: This paper introduces SpiceMixer, a genetic algorithm developed to synthesize novel analog circuits by evolving SPICE netlists. Unlike conventional methods, SpiceMixer operates directly on netlist lines, enabling compatibility with any component or subcircuit type and supporting general-purpose genetic operations. By using a normalized netlist format, the algorithm enhances the effectiveness of its genetic operators: crossover, mutation, and pruning. We show that SpiceMixer achieves superior performance in synthesizing standard cells (inverter, two-input NAND, and latch) and in designing an analog classifier circuit for the Iris dataset, reaching an accuracy of 89% on the test set. Across all evaluated tasks, SpiceMixer consistently outperforms existing synthesis methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing</title>
<link>https://arxiv.org/abs/2506.01566</link>
<guid>https://arxiv.org/abs/2506.01566</guid>
<content:encoded><![CDATA[

arXiv:2506.01566v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. However, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge devices. One promising approach to address this challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense processing of general matrix multiplications (GEMMs). FlexiSAGA supports seven different sparse and dense dataflows, enabling efficient processing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN/HW co-design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Dataset Distillation in the Wild</title>
<link>https://arxiv.org/abs/2506.01586</link>
<guid>https://arxiv.org/abs/2506.01586</guid>
<content:encoded><![CDATA[

arXiv:2506.01586v1 Announce Type: cross 
Abstract: Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading model performance. To these ends, we propose Multi-modal dataset Distillation in the Wild, i.e., MDW, the first framework to distill noisy multi-modal datasets into compact clean ones for effective and efficient model training. Specifically, MDW introduces learnable fine-grained correspondences during distillation and adaptively optimizes distilled data to emphasize correspondence-discriminative regions, thereby enhancing distilled data's information density and efficacy. Moreover, to capture robust cross-modal correspondence prior knowledge from real data, MDW proposes dual-track collaborative learning to avoid the risky data noise, alleviating information loss with certifiable noise tolerance. Extensive experiments validate MDW's theoretical and empirical efficacy with remarkable scalability, surpassing prior methods by over 15% across various compression ratios, highlighting its appealing practicality for applications with diverse efficacy and resource needs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy</title>
<link>https://arxiv.org/abs/2506.01602</link>
<guid>https://arxiv.org/abs/2506.01602</guid>
<content:encoded><![CDATA[

arXiv:2506.01602v1 Announce Type: cross 
Abstract: Word sense analysis is an essential analysis work for interpreting the linguistic and social backgrounds. The word sense change detection is a task of identifying and interpreting shifts in word meanings over time. This paper proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and quantify changes across time periods. This method enables both the identification of words undergoing sense shifts and the explanation of their evolution over multiple historical periods. To my knowledge, this is the first application of MMD to word sense change detection. Empirical assessment results demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title>
<link>https://arxiv.org/abs/2506.01608</link>
<guid>https://arxiv.org/abs/2506.01608</guid>
<content:encoded><![CDATA[

arXiv:2506.01608v1 Announce Type: cross 
Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech</title>
<link>https://arxiv.org/abs/2506.01618</link>
<guid>https://arxiv.org/abs/2506.01618</guid>
<content:encoded><![CDATA[

arXiv:2506.01618v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: https://github.com/idiap/RnV
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents need world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[

arXiv:2506.01622v1 Announce Type: cross 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer</title>
<link>https://arxiv.org/abs/2506.01623</link>
<guid>https://arxiv.org/abs/2506.01623</guid>
<content:encoded><![CDATA[

arXiv:2506.01623v1 Announce Type: cross 
Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a related one with minimal relearning. In contrast, reinforcement learning (RL) agents typically require extensive retraining even when new tasks share structural similarities with previously learned ones. In this work, we propose MAGIK, a novel framework that enables RL agents to transfer knowledge to analogous tasks without interacting with the target environment. Our approach leverages an imagination mechanism to map entities in the target task to their analogues in the source domain, allowing the agent to reuse its original policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK achieves effective zero-shot transfer using only a small number of human-labelled examples. We compare our approach to related baselines and highlight how it offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Cooperation in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2506.01624</link>
<guid>https://arxiv.org/abs/2506.01624</guid>
<content:encoded><![CDATA[

arXiv:2506.01624v1 Announce Type: cross 
Abstract: The development of AI agents based on large, open-domain language models (LLMs) has paved the way for the development of general-purpose AI assistants that can support human in tasks such as writing, coding, graphic design, and scientific research. A major challenge with such agents is that, by necessity, they are trained by observing relatively short-term interactions with humans. Such models can fail to generalize to long-term interactions, for example, interactions where a user has repeatedly corrected mistakes on the part of the agent. In this work, we argue that these challenges can be overcome by explicitly modeling humans' social intelligence, that is, their ability to build and maintain long-term relationships with other agents whose behavior cannot always be predicted. By mathematically modeling the strategies humans use to communicate and reason about one another over long periods of time, we may be able to derive new game theoretic objectives against which LLMs and future AI agents may be optimized.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</title>
<link>https://arxiv.org/abs/2506.01635</link>
<guid>https://arxiv.org/abs/2506.01635</guid>
<content:encoded><![CDATA[

arXiv:2506.01635v1 Announce Type: cross 
Abstract: Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees</title>
<link>https://arxiv.org/abs/2506.01641</link>
<guid>https://arxiv.org/abs/2506.01641</guid>
<content:encoded><![CDATA[

arXiv:2506.01641v1 Announce Type: cross 
Abstract: In recent years, deep reinforcement learning (DRL) algorithms have gained traction in home energy management systems. However, their adoption by energy management companies remains limited due to the black-box nature of DRL, which fails to provide transparent decision-making feedback. To address this, explainable reinforcement learning (XRL) techniques have emerged, aiming to make DRL decisions more transparent. Among these, soft differential decision tree (DDT) distillation provides a promising approach due to the clear decision rules they are based on, which can be efficiently computed. However, achieving high performance often requires deep, and completely full, trees, which reduces interpretability. To overcome this, we propose a novel asymmetric soft DDT construction method. Unlike traditional soft DDTs, our approach adaptively constructs trees by expanding nodes only when necessary. This improves the efficient use of decision nodes, which require a predetermined depth to construct full symmetric trees, enhancing both interpretability and performance. We demonstrate the potential of asymmetric DDTs to provide transparent, efficient, and high-performing decision-making in home energy management systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge</title>
<link>https://arxiv.org/abs/2506.01646</link>
<guid>https://arxiv.org/abs/2506.01646</guid>
<content:encoded><![CDATA[

arXiv:2506.01646v1 Announce Type: cross 
Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, "DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engram Memory Encoding and Retrieval: A Neurocomputational Perspective</title>
<link>https://arxiv.org/abs/2506.01659</link>
<guid>https://arxiv.org/abs/2506.01659</guid>
<content:encoded><![CDATA[

arXiv:2506.01659v1 Announce Type: cross 
Abstract: Despite substantial research into the biological basis of memory, the precise mechanisms by which experiences are encoded, stored, and retrieved in the brain remain incompletely understood. A growing body of evidence supports the engram theory, which posits that sparse populations of neurons undergo lasting physical and biochemical changes to support long-term memory. Yet, a comprehensive computational framework that integrates biological findings with mechanistic models remains elusive. This work synthesizes insights from cellular neuroscience and computational modeling to address key challenges in engram research: how engram neurons are identified and manipulated; how synaptic plasticity mechanisms contribute to stable memory traces; and how sparsity promotes efficient, interference-resistant representations. Relevant computational approaches -- such as sparse regularization, engram gating, and biologically inspired architectures like Sparse Distributed Memory and spiking neural networks -- are also examined. Together, these findings suggest that memory efficiency, capacity, and stability emerge from the interaction of plasticity and sparsity constraints. By integrating neurobiological and computational perspectives, this paper provides a comprehensive theoretical foundation for engram research and proposes a roadmap for future inquiry into the mechanisms underlying memory, with implications for the diagnosis and treatment of memory-related disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI Systems Must Be Contestable: Here's How to Make It Happen</title>
<link>https://arxiv.org/abs/2506.01662</link>
<guid>https://arxiv.org/abs/2506.01662</guid>
<content:encoded><![CDATA[

arXiv:2506.01662v1 Announce Type: cross 
Abstract: As AI regulations around the world intensify their focus on system safety, contestability has become a mandatory, yet ill-defined, safeguard. In XAI, "contestability" remains an empty promise: no formal definition exists, no algorithm guarantees it, and practitioners lack concrete guidance to satisfy regulatory requirements. Grounded in a systematic literature review, this paper presents the first rigorous formal definition of contestability in explainable AI, directly aligned with stakeholder requirements and regulatory mandates. We introduce a modular framework of by-design and post-hoc mechanisms spanning human-centered interfaces, technical architectures, legal processes, and organizational workflows. To operationalize our framework, we propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria. Through multiple case studies across diverse application domains, we reveal where state-of-the-art systems fall short and show how our framework drives targeted improvements. By converting contestability from regulatory theory into a practical framework, our work equips practitioners with the tools to embed genuine recourse and accountability into AI systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of discrete-continuous quantum circuits with multimodal diffusion models</title>
<link>https://arxiv.org/abs/2506.01666</link>
<guid>https://arxiv.org/abs/2506.01666</guid>
<content:encoded><![CDATA[

arXiv:2506.01666v1 Announce Type: cross 
Abstract: Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts</title>
<link>https://arxiv.org/abs/2506.01685</link>
<guid>https://arxiv.org/abs/2506.01685</guid>
<content:encoded><![CDATA[

arXiv:2506.01685v1 Announce Type: cross 
Abstract: In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Descriptive and Normative Theory of Human Beliefs in RLHF</title>
<link>https://arxiv.org/abs/2506.01692</link>
<guid>https://arxiv.org/abs/2506.01692</guid>
<content:encoded><![CDATA[

arXiv:2506.01692v1 Announce Type: cross 
Abstract: Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent -- and resulting preferences -- for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the \textit{mismatch} between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signature Maximum Mean Discrepancy Two-Sample Statistical Tests</title>
<link>https://arxiv.org/abs/2506.01718</link>
<guid>https://arxiv.org/abs/2506.01718</guid>
<content:encoded><![CDATA[

arXiv:2506.01718v1 Announce Type: cross 
Abstract: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning research which has gained popularity in recent years as a highly effective tool for comparing (finite-dimensional) distributions. Since it is designed as a kernel-based method, the MMD can be extended to path space valued distributions using the signature kernel. The resulting signature MMD (sig-MMD) can be used to define a metric between distributions on path space. Similarly to the original use case of the MMD as a test statistic within a two-sample testing framework, the sig-MMD can be applied to determine if two sets of paths are drawn from the same stochastic process. This work is dedicated to understanding the possibilities and challenges associated with applying the sig-MMD as a statistical tool in practice. We introduce and explain the sig-MMD, and provide easily accessible and verifiable examples for its practical use. We present examples that can lead to Type 2 errors in the hypothesis test, falsely indicating that samples have been drawn from the same underlying process (which generally occurs in a limited data setting). We then present techniques to mitigate the occurrence of this type of error.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-assimilated model-informed reinforcement learning</title>
<link>https://arxiv.org/abs/2506.01755</link>
<guid>https://arxiv.org/abs/2506.01755</guid>
<content:encoded><![CDATA[

arXiv:2506.01755v1 Announce Type: cross 
Abstract: The control of spatio-temporally chaos is challenging because of high dimensionality and unpredictability. Model-free reinforcement learning (RL) discovers optimal control policies by interacting with the system, typically requiring observations of the full physical state.In practice, sensors often provide only partial and noisy measurements (observations) of the system. The objective of this paper is to develop a framework that enables the control of chaotic systems with partial and noisy observability. The proposed method, data-assimilated model-informed reinforcement learning (DA-MIRL), integrates (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct the model prediction when observations become available; and (iii) an off-policy actor-critic RL algorithm to adaptively learn an optimal control strategy based on the corrected state estimates. We test DA-MIRL on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation. We estimate the full state of the environment with (i) a physics-based model, here, a coarse-grained model; and (ii) a data-driven model, here, the control-aware echo state network, which is proposed in this paper. We show that DA-MIRL successfully estimates and suppresses the chaotic dynamics of the environment in real time from partial observations and approximate models. This work opens opportunities for the control of partially observable chaotic systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs</title>
<link>https://arxiv.org/abs/2506.01770</link>
<guid>https://arxiv.org/abs/2506.01770</guid>
<content:encoded><![CDATA[

arXiv:2506.01770v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning</title>
<link>https://arxiv.org/abs/2506.01778</link>
<guid>https://arxiv.org/abs/2506.01778</guid>
<content:encoded><![CDATA[

arXiv:2506.01778v1 Announce Type: cross 
Abstract: We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An adaptive data sampling strategy for stabilizing dynamical systems via controller inference</title>
<link>https://arxiv.org/abs/2506.01816</link>
<guid>https://arxiv.org/abs/2506.01816</guid>
<content:encoded><![CDATA[

arXiv:2506.01816v1 Announce Type: cross 
Abstract: Learning stabilizing controllers from data is an important task in engineering applications; however, collecting informative data is challenging because unstable systems often lead to rapidly growing or erratic trajectories. In this work, we propose an adaptive sampling scheme that generates data while simultaneously stabilizing the system to avoid instabilities during the data collection. Under mild assumptions, the approach provably generates data sets that are informative for stabilization and have minimal size. The numerical experiments demonstrate that controller inference with the novel adaptive sampling approach learns controllers with up to one order of magnitude fewer data samples than unguided data generation. The results show that the proposed approach opens the door to stabilizing systems in edge cases and limit states where instabilities often occur and data collection is inherently difficult.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fodor and Pylyshyn's Legacy - Still No Human-like Systematic Compositionality in Neural Networks</title>
<link>https://arxiv.org/abs/2506.01820</link>
<guid>https://arxiv.org/abs/2506.01820</guid>
<content:encoded><![CDATA[

arXiv:2506.01820v1 Announce Type: cross 
Abstract: Strong meta-learning capabilities for systematic compositionality are emerging as an important skill for navigating the complex and changing tasks of today's world. However, in presenting models for robust adaptation to novel environments, it is important to refrain from making unsupported claims about the performance of meta-learning systems that ultimately do not stand up to scrutiny. While Fodor and Pylyshyn famously posited that neural networks inherently lack this capacity as they are unable to model compositional representations or structure-sensitive operations, and thus are not a viable model of the human mind, Lake and Baroni recently presented meta-learning as a pathway to compositionality. In this position paper, we critically revisit this claim and highlight limitations in the proposed meta-learning framework for compositionality. Our analysis shows that modern neural meta-learning systems can only perform such tasks, if at all, under a very narrow and restricted definition of a meta-learning setup. We therefore claim that `Fodor and Pylyshyn's legacy' persists, and to date, there is no human-like systematic compositionality learned in neural networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Streaming Discrete Speech Units</title>
<link>https://arxiv.org/abs/2506.01845</link>
<guid>https://arxiv.org/abs/2506.01845</guid>
<content:encoded><![CDATA[

arXiv:2506.01845v1 Announce Type: cross 
Abstract: Discrete speech units (DSUs) are derived from clustering the features of self-supervised speech models (S3Ms). DSUs offer significant advantages for on-device streaming speech applications due to their rich phonetic information, high transmission efficiency, and seamless integration with large language models. However, conventional DSU-based approaches are impractical as they require full-length speech input and computationally expensive S3Ms. In this work, we reduce both the attention window and the model size while preserving the effectiveness of DSUs. Our results demonstrate that we can reduce floating-point operations (FLOPs) by 50% with only a relative increase of 6.5% in character error rate (CER) on the ML-SUPERB 1h dataset. These findings highlight the potential of DSUs for real-time speech processing in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs</title>
<link>https://arxiv.org/abs/2506.01850</link>
<guid>https://arxiv.org/abs/2506.01850</guid>
<content:encoded><![CDATA[

arXiv:2506.01850v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning thermodynamic master equations for open quantum systems</title>
<link>https://arxiv.org/abs/2506.01882</link>
<guid>https://arxiv.org/abs/2506.01882</guid>
<content:encoded><![CDATA[

arXiv:2506.01882v1 Announce Type: cross 
Abstract: The characterization of Hamiltonians and other components of open quantum dynamical systems plays a crucial role in quantum computing and other applications. Scientific machine learning techniques have been applied to this problem in a variety of ways, including by modeling with deep neural networks. However, the majority of mathematical models describing open quantum systems are linear, and the natural nonlinearities in learnable models have not been incorporated using physical principles. We present a data-driven model for open quantum systems that includes learnable, thermodynamically consistent terms. The trained model is interpretable, as it directly estimates the system Hamiltonian and linear components of coupling to the environment. We validate the model on synthetic two and three-level data, as well as experimental two-level data collected from a quantum device at Lawrence Livermore National Laboratory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States</title>
<link>https://arxiv.org/abs/2506.01891</link>
<guid>https://arxiv.org/abs/2506.01891</guid>
<content:encoded><![CDATA[

arXiv:2506.01891v1 Announce Type: cross 
Abstract: Neural Quantum States (NQS) are a class of variational wave functions parametrized by neural networks (NNs) to study quantum many-body systems. In this work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold Networks (KANs), to represent quantum mechanical wave functions as nested univariate functions. We show that \sk wavefunction with learnable sinusoidal activation functions can capture the ground state energies, fidelities and various correlation functions of the 1D Transverse-Field Ising model, Anisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with different chain lengths. In our study of the $J_1-J_2$ model with $L=100$ sites, we find that the SineKAN model outperforms several previously explored neural quantum state ans\"atze, including Restricted Boltzmann Machines (RBMs), Long Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN), when compared to the results obtained from the Density Matrix Renormalization Group (DMRG) algorithm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-Learned Sampling of Conditioned Path Measures</title>
<link>https://arxiv.org/abs/2506.01904</link>
<guid>https://arxiv.org/abs/2506.01904</guid>
<content:encoded><![CDATA[

arXiv:2506.01904v1 Announce Type: cross 
Abstract: We propose algorithms for sampling from posterior path measures $P(C([0, T], \mathbb{R}^d))$ under a general prior process. This leverages ideas from (1) controlled equilibrium dynamics, which gradually transport between two path measures, and (2) optimization in $\infty$-dimensional probability space endowed with a Wasserstein metric, which can be used to evolve a density curve under the specified likelihood. The resulting algorithms are theoretically grounded and can be integrated seamlessly with neural networks for learning the target trajectory ensembles, without access to data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation</title>
<link>https://arxiv.org/abs/2506.01923</link>
<guid>https://arxiv.org/abs/2506.01923</guid>
<content:encoded><![CDATA[

arXiv:2506.01923v1 Announce Type: cross 
Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: https://amink8.github.io/TaxaDiffusion/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title>
<link>https://arxiv.org/abs/2506.01926</link>
<guid>https://arxiv.org/abs/2506.01926</guid>
<content:encoded><![CDATA[

arXiv:2506.01926v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Esoteric Language Models</title>
<link>https://arxiv.org/abs/2506.01928</link>
<guid>https://arxiv.org/abs/2506.01928</guid>
<content:encoded><![CDATA[

arXiv:2506.01928v1 Announce Type: cross 
Abstract: Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Generation from Contextually-Contradictory Prompts</title>
<link>https://arxiv.org/abs/2506.01929</link>
<guid>https://arxiv.org/abs/2506.01929</guid>
<content:encoded><![CDATA[

arXiv:2506.01929v1 Announce Type: cross 
Abstract: Text-to-image diffusion models excel at generating high-quality, diverse images from natural language prompts. However, they often fail to produce semantically accurate results when the prompt contains concept combinations that contradict their learned priors. We define this failure mode as contextual contradiction, where one concept implicitly negates another due to entangled associations learned during training. To address this, we propose a stage-aware prompt decomposition framework that guides the denoising process using a sequence of proxy prompts. Each proxy prompt is constructed to match the semantic content expected to emerge at a specific stage of denoising, while ensuring contextual coherence. To construct these proxy prompts, we leverage a large language model (LLM) to analyze the target prompt, identify contradictions, and generate alternative expressions that preserve the original intent while resolving contextual conflicts. By aligning prompt information with the denoising progression, our method enables fine-grained semantic control and accurate image generation in the presence of contextual contradictions. Experiments across a variety of challenging prompts show substantial improvements in alignment to the textual prompt.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should Decision-Makers Reveal Classifiers in Online Strategic Classification?</title>
<link>https://arxiv.org/abs/2506.01936</link>
<guid>https://arxiv.org/abs/2506.01936</guid>
<content:encoded><![CDATA[

arXiv:2506.01936v1 Announce Type: cross 
Abstract: Strategic classification addresses a learning problem where a decision-maker implements a classifier over agents who may manipulate their features in order to receive favorable predictions. In the standard model of online strategic classification, in each round, the decision-maker implements and publicly reveals a classifier, after which agents perfectly best respond based on this knowledge. However, in practice, whether to disclose the classifier is often debated -- some decision-makers believe that hiding the classifier can prevent misclassification errors caused by manipulation.
  In this paper, we formally examine how limiting the agents' access to the current classifier affects the decision-maker's performance. Specifically, we consider an extended online strategic classification setting where agents lack direct knowledge about the current classifier and instead manipulate based on a weighted average of historically implemented classifiers. Our main result shows that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or $k_{\text{in}}$ times more mistakes compared to the full-knowledge setting, where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph (representing how many distinct feature vectors can be manipulated to appear as a single one), and $\gamma$ is the discount factor indicating agents' memory of past classifiers. Our results demonstrate how withholding access to the classifier can backfire and degrade the decision-maker's performance in online strategic classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[

arXiv:2506.01939v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries</title>
<link>https://arxiv.org/abs/2506.01945</link>
<guid>https://arxiv.org/abs/2506.01945</guid>
<content:encoded><![CDATA[

arXiv:2506.01945v1 Announce Type: cross 
Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia, Nigeria, and T\"urkiye), are gaining influence in global stock markets, although they remain susceptible to the economic conditions of developed countries like the G7 (Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States). This interconnectedness and sensitivity of financial markets make understanding these relationships crucial for investors and policymakers to predict stock price movements accurately. To this end, we examined the main stock market indices of G7 and MINT countries from 2012 to 2024, using a recent graph neural network (GNN) algorithm called multivariate time series forecasting with graph neural network (MTGNN). This method allows for considering complex spatio-temporal connections in multivariate time series. In the implementations, MTGNN revealed that the US and Canada are the most influential G7 countries regarding stock indices in the forecasting process, and Indonesia and T\"urkiye are the most influential MINT countries. Additionally, our results showed that MTGNN outperformed traditional methods in forecasting the prices of stock market indices for MINT and G7 countries. Consequently, the study offers valuable insights into economic blocks' markets and presents a compelling empirical approach to analyzing global stock market dynamics using MTGNN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-ensemble: Mitigating Confidence Distortion for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01951</link>
<guid>https://arxiv.org/abs/2506.01951</guid>
<content:encoded><![CDATA[

arXiv:2506.01951v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) perform well in general fields, they exhibit a confidence distortion problem on multi-choice question-answering (MCQA), particularly as the number of answer choices increases. Specifically, on MCQA with many choices, LLMs suffer from under-confidence in correct predictions and over-confidence in incorrect ones, leading to a substantially degraded performance. To solve this problem, we propose Self-ensemble in this work. Our method splits the choices into several groups and ensembles LLM predictions across these groups to reach a final decision. The advantage of Self-ensemble is its plug-and-play nature, where it can be integrated into existing LLM architecture based on a designed attention mask and positional encoding, without requiring labeled datasets for parameter tuning. Experimental results on three LLMs and datasets demonstrate that Self-ensemble comprehensively addresses the confidence distortion problem of LLMs, outperforming standard inference as well as baseline methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks</title>
<link>https://arxiv.org/abs/2506.01952</link>
<guid>https://arxiv.org/abs/2506.01952</guid>
<content:encoded><![CDATA[

arXiv:2506.01952v1 Announce Type: cross 
Abstract: Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</title>
<link>https://arxiv.org/abs/2506.01954</link>
<guid>https://arxiv.org/abs/2506.01954</guid>
<content:encoded><![CDATA[

arXiv:2506.01954v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Process Image Generation</title>
<link>https://arxiv.org/abs/2506.01955</link>
<guid>https://arxiv.org/abs/2506.01955</guid>
<content:encoded><![CDATA[

arXiv:2506.01955v1 Announce Type: cross 
Abstract: Prior methods for controlling image generation are limited in their ability to be taught new tasks. In contrast, vision-language models, or VLMs, can learn tasks in-context and produce the correct outputs for a given input. We propose a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator. Our general framework enables a wide variety of new control tasks through the same text-and-image based interface. We showcase a handful of applications of this technique for different types of control signals, such as commonsense inferences and visual prompts. With our method, users can implement multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes. Project page: https://dual-process.github.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[

arXiv:2205.09337v2 Announce Type: replace 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[

arXiv:2205.10538v2 Announce Type: replace 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Aware Pseudo Labeling via Optimal Transport for Entity Alignment</title>
<link>https://arxiv.org/abs/2209.01847</link>
<guid>https://arxiv.org/abs/2209.01847</guid>
<content:encoded><![CDATA[

arXiv:2209.01847v3 Announce Type: replace 
Abstract: Entity alignment aims to discover unique equivalent entity pairs with the same meaning across different knowledge graphs (KGs). Existing models have focused on projecting KGs into a latent embedding space so that inherent semantics between entities can be captured for entity alignment. However, the adverse impacts of alignment conflicts have been largely overlooked during training, thereby limiting the entity alignment performance. To address this issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport model (CPL-OT) for entity alignment. The key idea is to iteratively pseudo-label alignment pairs empowered with conflict-aware optimal transport (OT) modeling to boost the precision of entity alignment. CPL-OT is composed of two key components -- entity embedding learning with global-local aggregation and iterative conflict-aware pseudo labeling -- that mutually reinforce each other. To mitigate alignment conflicts during pseudo labeling, we propose to use optimal transport as an effective means to warrant one-to-one entity alignment between two KGs with the minimal overall transport cost. Extensive experiments on benchmark datasets validate the superiority of CPL-OT over state-of-the-art baselines under both settings with and without prior alignment seeds.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning distributed representations with efficient SoftMax normalization</title>
<link>https://arxiv.org/abs/2303.17475</link>
<guid>https://arxiv.org/abs/2303.17475</guid>
<content:encoded><![CDATA[

arXiv:2303.17475v4 Announce Type: replace 
Abstract: Learning distributed representations, or embeddings, that encode the relational similarity patterns among objects is a relevant task in machine learning. A popular method to learn the embedding matrices $X, Y$ is optimizing a loss function of the term ${\rm SoftMax}(XY^T)$. The complexity required to calculate this term, however, runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time heuristic approximation to compute the normalization constants of ${\rm SoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some pre-trained embedding datasets that the proposed estimation method achieves higher or comparable accuracy with competing methods. From this result, we design an efficient and task-agnostic algorithm that learns the embeddings by optimizing the cross entropy between the softmax and a set of probability distributions given as inputs. The proposed algorithm is interpretable and easily adapted to arbitrary embedding problems. We consider a few use cases and observe similar or higher performances and a lower computational time than similar ``2Vec'' algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming</title>
<link>https://arxiv.org/abs/2304.09276</link>
<guid>https://arxiv.org/abs/2304.09276</guid>
<content:encoded><![CDATA[

arXiv:2304.09276v2 Announce Type: replace 
Abstract: Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with complex structures, we use the Lambda Calculus ({\lambda}-Calculus), a simple, but Turing-Complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. We will introduce the use of integrated neural learning and lambda calculi formalization. Finally, we explore execution of a program in {\lambda}-Calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks, Transformer Model, Sequence-to-Sequence Models, Computational Models
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Motor Symptom Presence and Severity in Parkinson's Disease from Wrist Accelerometer Time Series using ROCKET and InceptionTime</title>
<link>https://arxiv.org/abs/2304.11265</link>
<guid>https://arxiv.org/abs/2304.11265</guid>
<content:encoded><![CDATA[

arXiv:2304.11265v4 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a neurodegenerative condition characterized by frequently changing motor symptoms, necessitating continuous symptom monitoring for more targeted treatment. Classical time series classification and deep learning techniques have demonstrated limited efficacy in monitoring PD symptoms using wearable accelerometer data due to complex PD movement patterns and the small size of available datasets. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) as they are promising for PD symptom monitoring. InceptionTime's high learning capacity is well-suited to modeling complex movement patterns, while ROCKET is suited to small datasets. With random search methodology, we identify the highest-scoring InceptionTime architecture and compare its performance to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motion data from PD patients. Our findings indicate that all approaches can learn to estimate tremor severity and bradykinesia presence with moderate performance but encounter challenges in detecting dyskinesia. Among the presented approaches, ROCKET demonstrates higher scores in identifying dyskinesia, whereas InceptionTime exhibits slightly better performance in tremor and bradykinesia estimation. Notably, both methods outperform the multi-layer perceptron. In conclusion, InceptionTime can classify complex wrist motion time series and holds potential for continuous symptom monitoring in PD with further development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breiman meets Bellman: Non-Greedy Decision Trees with MDPs</title>
<link>https://arxiv.org/abs/2309.12701</link>
<guid>https://arxiv.org/abs/2309.12701</guid>
<content:encoded><![CDATA[

arXiv:2309.12701v5 Announce Type: replace 
Abstract: In supervised learning, decision trees are valued for their interpretability and performance. While greedy decision tree algorithms like CART remain widely used due to their computational efficiency, they often produce sub-optimal solutions with respect to a regularized training loss. Conversely, optimal decision tree methods can find better solutions but are computationally intensive and typically limited to shallow trees or binary features. We present Dynamic Programming Decision Trees (DPDT), a framework that bridges the gap between greedy and optimal approaches. DPDT relies on a Markov Decision Process formulation combined with heuristic split generation to construct near-optimal decision trees with significantly reduced computational complexity. Our approach dynamically limits the set of admissible splits at each node while directly optimizing the tree regularized training loss. Theoretical analysis demonstrates that DPDT can minimize regularized training losses at least as well as CART. Our empirical study shows on multiple datasets that DPDT achieves near-optimal loss with orders of magnitude fewer operations than existing optimal solvers. More importantly, extensive benchmarking suggests statistically significant improvements of DPDT over both CART and optimal decision trees in terms of generalization to unseen data. We demonstrate DPDT practicality through applications to boosting, where it consistently outperforms baselines. Our framework provides a promising direction for developing efficient, near-optimal decision tree algorithms that scale to practical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supercharging Graph Transformers with Advective Diffusion</title>
<link>https://arxiv.org/abs/2310.06417</link>
<guid>https://arxiv.org/abs/2310.06417</guid>
<content:encoded><![CDATA[

arXiv:2310.06417v3 Announce Type: replace 
Abstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Successor Features with Distributed Hebbian Temporal Memory</title>
<link>https://arxiv.org/abs/2310.13391</link>
<guid>https://arxiv.org/abs/2310.13391</guid>
<content:encoded><![CDATA[

arXiv:2310.13391v4 Announce Type: replace 
Abstract: This paper presents a novel approach to address the challenge of online sequence learning for decision making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on the factor graph formalism and a multi-component neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Features (SFs). Inspired by neurophysiological models of the neocortex, the algorithm uses distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning of traditional temporal memory algorithms such as RNN and HMM. Experimental results show that DHTM outperforms LSTM, RWKV and a biologically inspired HMM-like algorithm, CSCG, on non-stationary data sets. Our results suggest that DHTM is a promising approach to address the challenges of online sequence learning and planning in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Conflicts in Neural Networks and Their Implications for Explainability</title>
<link>https://arxiv.org/abs/2310.20363</link>
<guid>https://arxiv.org/abs/2310.20363</guid>
<content:encoded><![CDATA[

arXiv:2310.20363v2 Announce Type: replace 
Abstract: Artificial Neural Networks (ANNs) often represent conflicts between features, arising naturally during training as the network learns to integrate diverse and potentially disagreeing inputs to better predict the target variable. Despite their relevance to the ``reasoning'' processes of these models, the properties and implications of conflicts for understanding and explaining ANNs remain underexplored. In this paper, we develop a rigorous theory of conflicts in ANNs and demonstrate their impact on ANN explainability through two case studies. In the first case study, we use our theory of conflicts to inspire the design of a novel feature attribution method, which we call Conflict-Aware Feature-wise Explanations (CAFE). CAFE separates the positive and negative influences of features and biases, enabling more faithful explanations for models applied to tabular data. In the second case study, we take preliminary steps towards understanding the role of conflicts in out-of-distribution (OOD) scenarios. Through our experiments, we identify potentially useful connections between model conflicts and different kinds of distributional shifts in tabular and image data. Overall, our findings demonstrate the importance of accounting for conflicts in the development of more reliable explanation methods for AI systems, which are crucial for the beneficial use of these systems in the society.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective</title>
<link>https://arxiv.org/abs/2311.16646</link>
<guid>https://arxiv.org/abs/2311.16646</guid>
<content:encoded><![CDATA[

arXiv:2311.16646v2 Announce Type: replace 
Abstract: Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling and Uniqueness Sets in Graphon Signal Processing</title>
<link>https://arxiv.org/abs/2401.06279</link>
<guid>https://arxiv.org/abs/2401.06279</guid>
<content:encoded><![CDATA[

arXiv:2401.06279v2 Announce Type: replace 
Abstract: In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the convergence results to provide an algorithm that obtains approximately close to optimal sampling sets. Performing a set of numerical experiments, we evaluate the quality of these sampling sets. Our results open the door for the efficient computation of optimal sampling sets in graphs of large size.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCFC: Bridging Federated Clustering and Contrastive Learning</title>
<link>https://arxiv.org/abs/2401.06634</link>
<guid>https://arxiv.org/abs/2401.06634</guid>
<content:encoded><![CDATA[

arXiv:2401.06634v2 Announce Type: replace 
Abstract: Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCFC also shows superior performance in handling device failures from a practical viewpoint.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning</title>
<link>https://arxiv.org/abs/2401.13796</link>
<guid>https://arxiv.org/abs/2401.13796</guid>
<content:encoded><![CDATA[

arXiv:2401.13796v3 Announce Type: replace 
Abstract: Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Free Score-Based Sampling Methods with Ensembles</title>
<link>https://arxiv.org/abs/2401.17539</link>
<guid>https://arxiv.org/abs/2401.17539</guid>
<content:encoded><![CDATA[

arXiv:2401.17539v2 Announce Type: replace 
Abstract: Recent developments in generative modeling have utilized score-based methods coupled with stochastic differential equations to sample from complex probability distributions. However, these and other performant sampling methods generally require gradients of the target probability distribution, which can be unavailable or computationally prohibitive in many scientific and engineering applications. Here, we introduce ensembles within score-based sampling methods to develop gradient-free approximate sampling techniques that leverage the collective dynamics of particle ensembles to compute approximate reverse diffusion drifts. We introduce the underlying methodology, emphasizing its relationship with generative diffusion models and the previously introduced F\"ollmer sampler. We demonstrate the efficacy of the ensemble strategies through various examples, ranging from low- to medium-dimensionality sampling problems, including multi-modal and highly non-Gaussian probability distributions, and provide comparisons to traditional methods like the No-U-Turn Sampler. Additionally, we showcase these strategies in the context of a high-dimensional Bayesian inversion problem within the geophysical sciences. Our findings highlight the potential of ensemble strategies for modeling complex probability distributions in situations where gradients are unavailable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</title>
<link>https://arxiv.org/abs/2402.04435</link>
<guid>https://arxiv.org/abs/2402.04435</guid>
<content:encoded><![CDATA[

arXiv:2402.04435v2 Announce Type: replace 
Abstract: Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\method} in IP protection and maintaining high-performance for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Temperature Scaling and Conformal Prediction of Deep Classifiers</title>
<link>https://arxiv.org/abs/2402.05806</link>
<guid>https://arxiv.org/abs/2402.05806</guid>
<content:encoded><![CDATA[

arXiv:2402.05806v4 Announce Type: replace 
Abstract: In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied by some confidence indication. Two popular approaches for that aim are: 1) Calibration: modifies the classifier's softmax values such that the maximal value better estimates the correctness probability; and 2) Conformal Prediction (CP): produces a prediction set of candidate labels that contains the true label with a user-specified probability, guaranteeing marginal coverage but not, e.g., per class coverage. In practice, both types of indications are desirable, yet, so far the interplay between them has not been investigated. Focusing on the ubiquitous Temperature Scaling (TS) calibration, we start this paper with an extensive empirical study of its effect on prominent CP methods. We show that while TS calibration improves the class-conditional coverage of adaptive CP methods, surprisingly, it negatively affects their prediction set sizes. Motivated by this behavior, we explore the effect of TS on CP beyond its calibration application and reveal an intriguing trend under which it allows to trade prediction set size and conditional coverage of adaptive CP methods. Then, we establish a mathematical theory that explains the entire non-monotonic trend. Finally, based on our experiments and theory, we offer simple guidelines for practitioners to effectively combine adaptive CP with calibration, aligned with user-defined goals.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</title>
<link>https://arxiv.org/abs/2402.15734</link>
<guid>https://arxiv.org/abs/2402.15734</guid>
<content:encoded><![CDATA[

arXiv:2402.15734v4 Announce Type: replace 
Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions, and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data_efficient_nopt.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanAgent: Automating Data Standardization with LLM-based Agents</title>
<link>https://arxiv.org/abs/2403.08291</link>
<guid>https://arxiv.org/abs/2403.08291</guid>
<content:encoded><![CDATA[

arXiv:2403.08291v4 Announce Type: replace 
Abstract: Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplifying the LLM's code generation with concise API calls. We first propose Dataprep.Clean, a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists only need to provide their requirements once, allowing for a hands-free process. To demonstrate the practical utility of CleanAgent, we developed a user-friendly web application, allowing users to interact with it using real-world datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-PDE Methods for Rough Signature Kernels</title>
<link>https://arxiv.org/abs/2404.02926</link>
<guid>https://arxiv.org/abs/2404.02926</guid>
<content:encoded><![CDATA[

arXiv:2404.02926v2 Announce Type: replace 
Abstract: Signature kernels, inner products of path signatures, underpin several machine learning algorithms for multivariate time series analysis. For bounded variation paths, signature kernels were recently shown to solve a Goursat PDE. However, existing PDE solvers only use increments as input data, leading to first order approximation errors. These approaches become computationally intractable for highly oscillatory input paths, as they have to be resolved at a fine enough scale to accurately recover their signature kernel, resulting in significant time and memory complexities. In this paper, we extend the analysis to rough paths, and show, leveraging the framework of smooth rough paths, that the resulting rough signature kernels can be approximated by a novel system of PDEs whose coefficients involve higher order iterated integrals of the input rough paths. We show that this system of PDEs admits a unique solution and establish quantitative error bounds yielding a higher order approximation to rough signature kernels.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Four-hour thunderstorm nowcasting using deep diffusion models of satellite</title>
<link>https://arxiv.org/abs/2404.10512</link>
<guid>https://arxiv.org/abs/2404.10512</guid>
<content:encoded><![CDATA[

arXiv:2404.10512v4 Announce Type: replace 
Abstract: Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose deep diffusion models of satellite (DDMS) to establish an AI-based convection nowcasting system. Specifically, DDMS employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time. Additionally, it combines geostationary satellite brightness temperature data and domain knowledge from meteorological experts, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km$^2$), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints</title>
<link>https://arxiv.org/abs/2404.14202</link>
<guid>https://arxiv.org/abs/2404.14202</guid>
<content:encoded><![CDATA[

arXiv:2404.14202v4 Announce Type: replace 
Abstract: In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bypassing Skip-Gram Negative Sampling: Dimension Regularization as a More Efficient Alternative for Graph Embeddings</title>
<link>https://arxiv.org/abs/2405.00172</link>
<guid>https://arxiv.org/abs/2405.00172</guid>
<content:encoded><![CDATA[

arXiv:2405.00172v2 Announce Type: replace 
Abstract: A wide range of graph embedding objectives decompose into two components: one that enforces similarity, attracting the embeddings of nodes that are perceived as similar, and another that enforces dissimilarity, repelling the embeddings of nodes that are perceived as dissimilar. Without repulsion, the embeddings would collapse into trivial solutions. Skip-Gram Negative Sampling (SGNS) is a popular and efficient repulsion approach that prevents collapse by repelling each node from a sample of dissimilar nodes. In this work, we show that when repulsion is most needed and the embeddings approach collapse, SGNS node-wise repulsion is, in the aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are more scalable than node operations and produce a simpler geometric interpretation of the repulsion. Our theoretical result establishes dimension regularization as an effective and more efficient, compared to skip-gram node contrast, approach to enforcing dissimilarity among embeddings of nodes. We use this result to propose a flexible algorithm augmentation framework that improves the scalability of any existing algorithm using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream link-prediction performance while reducing GPU memory usage by up to 33.3% and training time by 23.4%. Moreover, we show that completely removing repulsion (a special case of our augmentation framework) in LINE reduces training time by 70.9% on average, while increasing link prediction performance, especially for graphs that are globally sparse but locally dense. In general, however, repulsion is needed, and dimension regularization provides an efficient alternative to SGNS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to set AdamW's weight decay as you scale model and dataset size</title>
<link>https://arxiv.org/abs/2405.13698</link>
<guid>https://arxiv.org/abs/2405.13698</guid>
<content:encoded><![CDATA[

arXiv:2405.13698v3 Announce Type: replace 
Abstract: The scaling of the optimal AdamW weight decay hyperparameter with model and dataset size is critical as we seek to build larger models, but is poorly understood. We show that weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates. This gives critical insights for how to set the weight decay in AdamW, and how the weight decay should scale with model and dataset size. In particular, the key hyperparameter for an exponential moving average is the EMA timescale. Intuitively, the EMA timescale can be understood as the number of recent iterations the EMA averages over. We find that the optimal timescale, measured in epochs, is roughly constant as we change model and dataset size. Moreover, given a learning rate, there is a one-to-one mapping from the EMA timescale to the weight decay hyperparameter. Thus, if the optimal EMA timescale is constant, that implies that as the dataset size increases, the optimal weight decay should fall and as the model size increases, the optimal weight decay should increase (if we follow the muP recommendation for scaling the learning rate). We validate these scaling rules on ResNet-18 and Vision Transformers trained on CIFAR-10 and ImageNet, and on NanoGPT pre-training on OpenWebText. Finally, we found that as training progresses, muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoAdaLer: Geometric Insights into Adaptive Stochastic Gradient Descent Algorithms</title>
<link>https://arxiv.org/abs/2405.16255</link>
<guid>https://arxiv.org/abs/2405.16255</guid>
<content:encoded><![CDATA[

arXiv:2405.16255v2 Announce Type: replace 
Abstract: The Adam optimization method has achieved remarkable success in addressing contemporary challenges in stochastic optimization. This method falls within the realm of adaptive sub-gradient techniques, yet the underlying geometric principles guiding its performance have remained shrouded in mystery, and have long confounded researchers. In this paper, we introduce GeoAdaLer (Geometric Adaptive Learner), a novel adaptive learning method for stochastic gradient descent optimization, which draws from the geometric properties of the optimization landscape. Beyond emerging as a formidable contender, the proposed method extends the concept of adaptive learning by introducing a geometrically inclined approach that enhances the interpretability and effectiveness in complex optimization scenarios
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-based Optimally Weighted Conformal Prediction Intervals</title>
<link>https://arxiv.org/abs/2405.16828</link>
<guid>https://arxiv.org/abs/2405.16828</guid>
<content:encoded><![CDATA[

arXiv:2405.16828v2 Announce Type: replace 
Abstract: In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real and synthetic time-series data against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models</title>
<link>https://arxiv.org/abs/2405.17829</link>
<guid>https://arxiv.org/abs/2405.17829</guid>
<content:encoded><![CDATA[

arXiv:2405.17829v3 Announce Type: replace 
Abstract: With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this, here we present a novel latent diffusion model dubbed LDMol for text-conditioned molecule generation. By recognizing that the suitable latent space design is the key to the diffusion model performance, we employ a contrastive learning strategy to extract novel feature space from text data that embeds the unique characteristics of the molecule structure. Experiments show that LDMol outperforms the existing autoregressive baselines on the text-to-molecule generation benchmark, being one of the first diffusion models that outperforms autoregressive models in textual data generation with a better choice of the latent domain. Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-guided molecule editing, demonstrating its versatility as a diffusion model.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic and statistical calibration of constitutive models from full-field data with parametric physics-informed neural networks</title>
<link>https://arxiv.org/abs/2405.18311</link>
<guid>https://arxiv.org/abs/2405.18311</guid>
<content:encoded><![CDATA[

arXiv:2405.18311v3 Announce Type: replace 
Abstract: The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities. In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest. However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches. Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated. In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation. In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration. We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data. We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty. A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid. Finally, we consider experimental data and show that the results are in good agreement with a finite element method-based calibration. Due to the fast evaluation of PINNs, calibration can be performed in near real-time. This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Policy Evaluation Uncovers Policies of Generative Flow Networks</title>
<link>https://arxiv.org/abs/2406.02213</link>
<guid>https://arxiv.org/abs/2406.02213</guid>
<content:encoded><![CDATA[

arXiv:2406.02213v3 Announce Type: replace 
Abstract: The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects proportionally to an unnormalized reward function. A number of recent works explored connections between GFlowNets and maximum entropy (MaxEnt) RL, which modifies the standard objective of RL agents by learning an entropy-regularized objective. However, the relationship between GFlowNets and standard RL remains largely unexplored, despite the inherent similarities in their sequential decision-making nature. While GFlowNets can discover diverse solutions through specialized flow-matching objectives, connecting them can simplify their implementation through established RL principles and improve RL's diverse solution discovery capabilities. In this paper, we bridge this gap by revealing a fundamental connection between GFlowNets and one RL's most basic components -- policy evaluation. Surprisingly, we find that the value function obtained from evaluating a uniform policy is closely associated with the flow functions in GFlowNets through the lens of flow iteration under certain structural conditions. Building upon these insights, we introduce a rectified random policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets based on simply evaluating a fixed random policy in these cases, offering a new perspective. Empirical results across extensive benchmarks demonstrate that RPE achieves competitive results compared to previous approaches, shedding light on the previously overlooked connection between (non-MaxEnt) RL and GFlowNets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2406.04328</link>
<guid>https://arxiv.org/abs/2406.04328</guid>
<content:encoded><![CDATA[

arXiv:2406.04328v5 Announce Type: replace 
Abstract: The past few years have seen remarkable progress in the decoding of speech from brain activity, primarily driven by large single-subject datasets. However, due to individual variation, such as anatomy, and differences in task design and scanning hardware, leveraging data across subjects and datasets remains challenging. In turn, the field has not benefited from the growing number of open neural data repositories to exploit large-scale deep learning. To address this, we develop neuroscience-informed self-supervised objectives, together with an architecture, for learning from heterogeneous brain recordings. Scaling to nearly 400 hours of MEG data and 900 subjects, our approach shows generalisation across participants, datasets, tasks, and even to novel subjects. It achieves improvements of 15-27% over state-of-the-art models and matches surgical decoding performance with non-invasive data. These advances unlock the potential for scaling speech decoding models beyond the current frontier.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Explainable Clustering with Differential Privacy</title>
<link>https://arxiv.org/abs/2406.04610</link>
<guid>https://arxiv.org/abs/2406.04610</guid>
<content:encoded><![CDATA[

arXiv:2406.04610v2 Announce Type: replace 
Abstract: This paper presents a novel approach to Explainable AI (XAI) that combines contrastive explanations with differential privacy for clustering algorithms. Focusing on k-median and k-means problems, we calculate contrastive explanations as the utility difference between original clustering and clustering with a centroid fixed to a specific data point. This method provides personalized insights into centroid placement. Our key contribution is demonstrating that these differentially private explanations achieve essentially the same utility bounds as non-private explanations. Experiments across various datasets show that our approach offers meaningful, privacy-preserving, and individually relevant explanations without significantly compromising clustering utility. This work advances privacy-aware machine learning by balancing data protection, explanation quality, and personalization in clustering tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
<link>https://arxiv.org/abs/2406.11569</link>
<guid>https://arxiv.org/abs/2406.11569</guid>
<content:encoded><![CDATA[

arXiv:2406.11569v4 Announce Type: replace 
Abstract: For modern artificial intelligence (AI) applications such as large language models (LLMs), the training paradigm has recently shifted to pre-training followed by fine-tuning. Furthermore, owing to dwindling open repositories of data and thanks to efforts to democratize access to AI models, pre-training is expected to increasingly migrate from the current centralized deployments to federated learning (FL) implementations. Meta-learning provides a general framework in which pre-training and fine-tuning can be formalized. Meta-learning-based personalized FL (meta-pFL) moves beyond basic personalization by targeting generalization to new agents and tasks. This paper studies the generalization performance of meta-pFL for a wireless setting in which the agents participating in the pre-training phase, i.e., meta-learning, are connected via a shared wireless channel to the server. Adopting over-the-air computing, we study the trade-off between generalization to new agents and tasks, on the one hand, and convergence, on the other hand. The trade-off arises from the fact that channel impairments may enhance generalization, while degrading convergence. Extensive numerical results validate the theory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Learning with Quality-Driven Data Selection</title>
<link>https://arxiv.org/abs/2407.00102</link>
<guid>https://arxiv.org/abs/2407.00102</guid>
<content:encoded><![CDATA[

arXiv:2407.00102v2 Announce Type: replace 
Abstract: The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2407.00490</link>
<guid>https://arxiv.org/abs/2407.00490</guid>
<content:encoded><![CDATA[

arXiv:2407.00490v2 Announce Type: replace 
Abstract: We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate $O(1/\sqrt{t})$. This is the first global convergence result for Gaussian mixtures with more than $2$ components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Conditional Probability for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2407.01171</link>
<guid>https://arxiv.org/abs/2407.01171</guid>
<content:encoded><![CDATA[

arXiv:2407.01171v2 Announce Type: replace 
Abstract: We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of com- plex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[

arXiv:2407.06533v2 Announce Type: replace 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden State Differential Private Mini-Batch Block Coordinate Descent for Multi-convexity Optimization</title>
<link>https://arxiv.org/abs/2407.08233</link>
<guid>https://arxiv.org/abs/2407.08233</guid>
<content:encoded><![CDATA[

arXiv:2407.08233v2 Announce Type: replace 
Abstract: We investigate the differential privacy (DP) guarantees under the hidden state assumption (HSA) for multi-convex problems. Recent analyses of privacy loss under the hidden state assumption have relied on strong assumptions such as convexity, thereby limiting their applicability to practical problems. In this paper, we introduce the Differential Privacy Mini-Batch Block Coordinate Descent (DP-MBCD) algorithm, accompanied by the privacy loss accounting methods under the hidden state assumption. Our proposed methods apply to a broad range of classical non-convex problems which are or can be converted to multi-convex problems, such as matrix factorization and neural network training. In addition to a tighter bound for privacy loss, our theoretical analysis is also compatible with proximal gradient descent and adaptive calibrated noise scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashNorm: fast normalization for LLMs</title>
<link>https://arxiv.org/abs/2407.09577</link>
<guid>https://arxiv.org/abs/2407.09577</guid>
<content:encoded><![CDATA[

arXiv:2407.09577v3 Announce Type: replace 
Abstract: This paper presents FlashNorm, which is an exact but faster implementation of RMSNorm followed by linear layers. RMSNorm is used by many LLMs such as Llama, Mistral, and OpenELM. FlashNorm also speeds up Layer Normalization and its recently proposed replacement Dynamic Tanh (DyT) arXiv:2503.10622. FlashNorm also reduces the number of parameter tensors by simply merging the normalization weights with the weights of the next linear layer. See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deflated Dynamics Value Iteration</title>
<link>https://arxiv.org/abs/2407.10454</link>
<guid>https://arxiv.org/abs/2407.10454</guid>
<content:encoded><![CDATA[

arXiv:2407.10454v2 Announce Type: replace 
Abstract: The Value Iteration (VI) algorithm is an iterative procedure to compute the value function of a Markov decision process, and is the basis of many reinforcement learning (RL) algorithms as well. As the error convergence rate of VI as a function of iteration $k$ is $O(\gamma^k)$, it is slow when the discount factor $\gamma$ is close to $1$. To accelerate the computation of the value function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses matrix splitting and matrix deflation techniques to effectively remove (deflate) the top $s$ dominant eigen-structure of the transition matrix $\mathcal{P}^{\pi}$. We prove that this leads to a $\tilde{O}(\gamma^k |\lambda_{s+1}|^k)$ convergence rate, where $\lambda_{s+1}$is $(s+1)$-th largest eigenvalue of the dynamics matrix. We then extend DDVI to the RL setting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We empirically show the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning</title>
<link>https://arxiv.org/abs/2408.03819</link>
<guid>https://arxiv.org/abs/2408.03819</guid>
<content:encoded><![CDATA[

arXiv:2408.03819v2 Announce Type: replace 
Abstract: Active Learning (AL) allows models to learn interactively from user feedback. This paper introduces a counterfactual data augmentation approach to AL, particularly addressing the selection of datapoints for user querying, a pivotal concern in enhancing data efficiency. Our approach is inspired by Variation Theory, a theory of human concept learning that emphasizes the essential features of a concept by focusing on what stays the same and what changes. Instead of just querying with existing datapoints, our approach synthesizes artificial datapoints that highlight potential key similarities and differences among labels using a neuro-symbolic pipeline combining large language models (LLMs) and rule-based models. Through an experiment in the example domain of text classification, we show that our approach achieves significantly higher performance when there are fewer annotated data. As the annotated training data gets larger the impact of the generated data starts to diminish showing its capability to address the cold start problem in AL. This research sheds light on integrating theories of human learning into the optimization of AL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On zero-shot learning in neural state estimation of power distribution systems</title>
<link>https://arxiv.org/abs/2408.05787</link>
<guid>https://arxiv.org/abs/2408.05787</guid>
<content:encoded><![CDATA[

arXiv:2408.05787v2 Announce Type: replace 
Abstract: This paper addresses the challenge of neural state estimation in power distribution systems. We identified a research gap in the current state of the art, which lies in the inability of models to adapt to changes in the power grid, such as loss of sensors and branch switching, in a zero-shot fashion. Based on the literature, we identified graph neural networks as the most promising class of models for this use case. Our experiments confirm their robustness to some grid changes and also show that a deeper network does not always perform better. We propose data augmentations to improve performance and conduct a comprehensive grid search of different model configurations for common zero-shot learning scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable k-Medoids Clustering via Whale Optimization Algorithm</title>
<link>https://arxiv.org/abs/2408.16993</link>
<guid>https://arxiv.org/abs/2408.16993</guid>
<content:encoded><![CDATA[

arXiv:2408.16993v2 Announce Type: replace 
Abstract: Unsupervised clustering has emerged as a critical tool for uncovering hidden patterns in vast, unlabeled datasets. However, traditional methods, such as Partitioning Around Medoids (PAM), struggle with scalability owing to their quadratic computational complexity. To address this limitation, we introduce WOA-kMedoids, a novel unsupervised clustering method that incorporates the Whale Optimization Algorithm (WOA), a nature-inspired metaheuristic inspired by the hunting strategies of humpback whales. By optimizing the centroid selection, WOA-kMedoids reduces the computational complexity from quadratic to near-linear with respect to the number of observations, enabling scalability to large datasets while maintaining high clustering accuracy. We evaluated WOA-kMedoids using 25 diverse time-series datasets from the UCR archive. Our empirical results show that WOA-kMedoids achieved a clustering performance comparable to PAM, with an average Rand Index (RI) of 0.731 compared to PAM's 0.739, outperforming PAM on 12 out of 25 datasets. While exhibiting a slightly higher runtime than PAM on small datasets (<300 observations), WOA-kMedoids outperformed PAM on larger datasets, with an average speedup of 1.7x and a maximum of 2.3x. The scalability of WOA-kMedoids, combined with its high accuracy, makes them a promising choice for unsupervised clustering in big data applications. This method has implications for efficient knowledge discovery in massive unlabeled datasets, particularly where traditional k-medoids methods are computationally infeasible, including IoT anomaly detection, biomedical signal analysis, and customer behavior clustering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELO-Rated Sequence Rewards: Advancing Reinforcement Learning Models</title>
<link>https://arxiv.org/abs/2409.03301</link>
<guid>https://arxiv.org/abs/2409.03301</guid>
<content:encoded><![CDATA[

arXiv:2409.03301v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) heavily relies on the careful design of the reward function. However, accurately assigning rewards to each state-action pair in Long-Term Reinforcement Learning (LTRL) tasks remains a significant challenge. As a result, RL agents are often trained under expert guidance. Inspired by the ordinal utility theory in economics, we propose a novel reward estimation algorithm: ELO-Rating based Reinforcement Learning (ERRL). This approach features two key contributions. First, it uses expert preferences over trajectories rather than cardinal rewards (utilities) to compute the ELO rating of each trajectory as its reward. Second, a new reward redistribution algorithm is introduced to alleviate training instability in the absence of a fixed anchor reward. In long-term scenarios (up to 5000 steps), where traditional RL algorithms struggle, our method outperforms several state-of-the-art baselines. Additionally, we conduct a comprehensive analysis of how expert preferences influence the results.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Deep Neural Operator Networks via Randomized Sampling</title>
<link>https://arxiv.org/abs/2409.13280</link>
<guid>https://arxiv.org/abs/2409.13280</guid>
<content:encoded><![CDATA[

arXiv:2409.13280v2 Announce Type: replace 
Abstract: Neural operators (NOs) employ deep neural networks to learn mappings between infinite-dimensional function spaces. Deep operator network (DeepONet), a popular NO architecture, has demonstrated success in the real-time prediction of complex dynamics across various scientific and engineering applications. In this work, we introduce a random sampling technique to be adopted during the training of DeepONet, aimed at improving the generalization ability of the model, while significantly reducing the computational time. The proposed approach targets the trunk network of the DeepONet model that outputs the basis functions corresponding to the spatiotemporal locations of the bounded domain on which the physical system is defined. While constructing the loss function, DeepONet training traditionally considers a uniform grid of spatiotemporal points at which all the output functions are evaluated for each iteration. This approach leads to a larger batch size, resulting in poor generalization and increased memory demands, due to the limitations of the stochastic gradient descent (SGD) optimizer. The proposed random sampling over the inputs of the trunk net mitigates these challenges, improving generalization and reducing memory requirements during training, resulting in significant computational gains. We validate our hypothesis through three benchmark examples, demonstrating substantial reductions in training time while achieving comparable or lower overall test errors relative to the traditional training approach. Our results indicate that incorporating randomization in the trunk network inputs during training enhances the efficiency and robustness of DeepONet, offering a promising avenue for improving the framework's performance in modeling complex physical systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Banking Dataset: Understanding Client Needs through Event Sequences</title>
<link>https://arxiv.org/abs/2409.17587</link>
<guid>https://arxiv.org/abs/2409.17587</guid>
<content:encoded><![CDATA[

arXiv:2409.17587v2 Announce Type: replace 
Abstract: Financial organizations collect a huge amount of temporal (sequential) data about clients, which is typically collected from multiple sources (modalities). Despite the urgent practical need, developing deep learning techniques suitable to handle such data is limited by the absence of large open-source multi-source real-world datasets of event sequences. To fill this gap, which is mainly caused by security reasons, we present the first industrial-scale publicly available multimodal banking dataset, MBD, that contains information on more than 2M corporate clients of a large bank. Clients are represented by several data sources: 950M bank transactions, 1B geo position events, 5M embeddings of dialogues with technical support, and monthly aggregated purchases of four bank products. All entries are properly anonymized from real proprietary bank data, and the experiments confirm that our anonymization still saves all significant information for introduced downstream tasks. Moreover, we introduce a novel multimodal benchmark suggesting several important practical tasks, such as future purchase prediction and modality matching. The benchmark incorporates our MBD and two public financial datasets. We provide numerical results for the state-of-the-art event sequence modeling techniques including large language models and demonstrate the superiority of fusion baselines over single-modal techniques for each task. Thus, MBD provides a valuable resource for future research in financial applications of multimodal event sequence analysis.
  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD
  Github Link: https://github.com/Dzhambo/MBD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data</title>
<link>https://arxiv.org/abs/2409.19078</link>
<guid>https://arxiv.org/abs/2409.19078</guid>
<content:encoded><![CDATA[

arXiv:2409.19078v3 Announce Type: replace 
Abstract: Speech pathology has impacts on communication abilities and quality of life. While deep learning-based models have shown potential in diagnosing these disorders, the use of sensitive data raises critical privacy concerns. Although differential privacy (DP) has been explored in the medical imaging domain, its application in pathological speech analysis remains largely unexplored despite the equally critical privacy concerns. To the best of our knowledge, this study is the first to investigate DP's impact on pathological speech data, focusing on the trade-offs between privacy, diagnostic accuracy, and fairness. Using a large, real-world dataset of 200 hours of recordings from 2,839 German-speaking participants, we observed a maximum accuracy reduction of 3.85% when training with DP with high privacy levels. To highlight real-world privacy risks, we demonstrated the vulnerability of non-private models to gradient inversion attacks, reconstructing identifiable speech samples and showcasing DP's effectiveness in mitigating these risks. To explore the potential generalizability across languages and disorders, we validated our approach on a dataset of Spanish-speaking Parkinson's disease patients, leveraging pretrained models from healthy English-speaking datasets, and demonstrated that careful pretraining on large-scale task-specific datasets can maintain favorable accuracy under DP constraints. A comprehensive fairness analysis revealed minimal gender bias at reasonable privacy levels but underscored the need for addressing age-related disparities. Our results establish that DP can balance privacy and utility in speech disorder detection, while highlighting unique challenges in privacy-fairness trade-offs for speech data. This provides a foundation for refining DP methodologies and improving fairness across diverse patient groups in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Practices for Multi-Fidelity Bayesian Optimization in Materials and Molecular Research</title>
<link>https://arxiv.org/abs/2410.00544</link>
<guid>https://arxiv.org/abs/2410.00544</guid>
<content:encoded><![CDATA[

arXiv:2410.00544v3 Announce Type: replace 
Abstract: Multi-fidelity Bayesian Optimization (MFBO) is a promising framework to speed up materials and molecular discovery as sources of information of different accuracies are at hand at increasing cost. Despite its potential use in chemical tasks, there is a lack of systematic evaluation of the many parameters playing a role in MFBO. In this work, we provide guidelines and recommendations to decide when to use MFBO in experimental settings. We investigate MFBO methods applied to molecules and materials problems. First, we test two different families of acquisition functions in two synthetic problems and study the effect of the informativeness and cost of the approximate function. We use our implementation and guidelines to benchmark three real discovery problems and compare them against their single-fidelity counterparts. Our results may help guide future efforts to implement MFBO as a routine tool in the chemical sciences.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softmax is not Enough (for Sharp Size Generalisation)</title>
<link>https://arxiv.org/abs/2410.01104</link>
<guid>https://arxiv.org/abs/2410.01104</guid>
<content:encoded><![CDATA[

arXiv:2410.01104v3 Announce Type: replace 
Abstract: A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions with increasing problem size, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization</title>
<link>https://arxiv.org/abs/2410.02628</link>
<guid>https://arxiv.org/abs/2410.02628</guid>
<content:encoded><![CDATA[

arXiv:2410.02628v3 Announce Type: replace 
Abstract: Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \sim \pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\textbf{end-to-end}$ learning algorithm to get $\pi^*(\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?</title>
<link>https://arxiv.org/abs/2410.02735</link>
<guid>https://arxiv.org/abs/2410.02735</guid>
<content:encoded><![CDATA[

arXiv:2410.02735v2 Announce Type: replace 
Abstract: Out-of-distribution (OOD) generalization is challenging because distribution shifts come in many forms. Numerous algorithms exist to address specific settings, but choosing the right training algorithm for the right dataset without trial and error is difficult. Indeed, real-world applications often involve multiple types and combinations of shifts that are hard to analyze theoretically.
  Method. This work explores the possibility of learning the selection of a training algorithm for OOD generalization. We propose a proof of concept (OOD-Chameleon) that formulates the selection as a multi-label classification over candidate algorithms, trained on a dataset of datasets representing a variety of shifts. We evaluate the ability of OOD-Chameleon to rank algorithms on unseen shifts and datasets based only on dataset characteristics, i.e., without training models first, unlike traditional model selection.
  Findings. Extensive experiments show that the learned selector identifies high-performing algorithms across synthetic, vision, and language tasks. Further inspection shows that it learns non-trivial decision rules, which provide new insights into the applicability of existing algorithms. Overall, this new approach opens the possibility of better exploiting and understanding the plethora of existing algorithms for OOD generalization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness</title>
<link>https://arxiv.org/abs/2410.03210</link>
<guid>https://arxiv.org/abs/2410.03210</guid>
<content:encoded><![CDATA[

arXiv:2410.03210v2 Announce Type: replace 
Abstract: Frameworks and domain-specific languages for auto-generating code have traditionally depended on human experts to implement rigorous methods ensuring the legality of code transformations. Recently, machine learning (ML) has gained traction for generating code optimized for specific hardware targets. However, ML approaches-particularly black-box neural networks-offer no guarantees on the correctness or legality of the transformations they produce. To address this gap, we introduce Tadashi, an end-to-end system that leverages the polyhedral model to support researchers in curating datasets critical for ML-based code generation. Tadashi provides an end-to-end system capable of applying, verifying, and evaluating candidate transformations on polyhedral schedules with both reliability and practicality. We formally prove that Tadashi guarantees the legality of generated transformations, demonstrate its low runtime overhead, and showcase its broad applicability. Tadashi available at https://github.com/vatai/tadashi/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation</title>
<link>https://arxiv.org/abs/2410.03960</link>
<guid>https://arxiv.org/abs/2410.03960</guid>
<content:encoded><![CDATA[

arXiv:2410.03960v3 Announce Type: replace 
Abstract: LLM inference for enterprise applications, such as summarization, RAG, and code-generation, typically observe much longer prompt than generations, leading to high prefill cost and response latency. We present SwiftKV, a novel model transformation and distillation procedure targeted at reducing the prefill compute (in FLOPs) of prompt tokens while preserving high generation quality. First, SwiftKV prefills later layers' KV cache using an earlier layer's output, allowing prompt tokens to skip those later layers. Second, SwiftKV employs a lightweight knowledge-preserving distillation procedure that can adapt existing LLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV cache compression to improve inference performance in low-memory scenarios. Our comprehensive experiments show that SwiftKV can effectively reduce prefill computation by 25-50% across several LLM families while incurring minimum quality degradation. In the end-to-end inference serving, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at https://github.com/snowflakedb/arctictraining.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Multi-task Policy Fine-tuning</title>
<link>https://arxiv.org/abs/2410.05026</link>
<guid>https://arxiv.org/abs/2410.05026</guid>
<content:encoded><![CDATA[

arXiv:2410.05026v2 Announce Type: replace 
Abstract: Pre-trained generalist policies are rapidly gaining relevance in robot learning due to their promise of fast adaptation to novel, in-domain tasks. This adaptation often relies on collecting new demonstrations for a specific task of interest and applying imitation learning algorithms, such as behavioral cloning. However, as soon as several tasks need to be learned, we must decide which tasks should be demonstrated and how often? We study this multi-task problem and explore an interactive framework in which the agent adaptively selects the tasks to be demonstrated. We propose AMF (Active Multi-task Fine-tuning), an algorithm to maximize multi-task policy performance under a limited demonstration budget by collecting demonstrations yielding the largest information gain on the expert policy. We derive performance guarantees for AMF under regularity assumptions and demonstrate its empirical effectiveness to efficiently fine-tune neural policies in complex and high-dimensional environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Dynamic Client Arrival and Departure: Convergence and Rapid Adaptation via Initial Model Construction</title>
<link>https://arxiv.org/abs/2410.05662</link>
<guid>https://arxiv.org/abs/2410.05662</guid>
<content:encoded><![CDATA[

arXiv:2410.05662v2 Announce Type: replace 
Abstract: Most federated learning (FL) approaches assume a fixed client set. However, real-world scenarios often involve clients dynamically joining or leaving the system based on their needs or interest in specific tasks. This dynamic setting introduces unique challenges: (1) the optimization objective evolves with the active client set, unlike traditional FL with a static objective; and (2) the current global model may no longer serve as an effective initialization for subsequent rounds, potentially hindering adaptation. To address these challenges, we first provide a convergence analysis under a non-convex loss with a dynamic client set, accounting for factors such as gradient noise, local training iterations, and data heterogeneity. Building on this analysis, we propose a model initialization algorithm that enables rapid adaptation to new client sets whenever clients join or leave the system. Our key idea is to compute a weighted average of previous global models, guided by gradient similarity, to prioritize models trained on data distributions that closely align with the current client set, thereby accelerating recovery from distribution shifts. This plug-and-play algorithm is designed to integrate seamlessly with existing FL methods, offering broad applicability in practice. Experimental results on diverse datasets including both image and text domains, varied label distributions, and multiple FL algorithms demonstrate the effectiveness of the proposed approach across a range of scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</title>
<link>https://arxiv.org/abs/2410.06820</link>
<guid>https://arxiv.org/abs/2410.06820</guid>
<content:encoded><![CDATA[

arXiv:2410.06820v3 Announce Type: replace 
Abstract: Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach extends to parametric PDEs. Specifically, we integrate the physical loss gradient with PDE parameters, allowing our method to solve over a distribution of PDE parameters, including coefficients, initial conditions, and boundary conditions. We demonstrate the effectiveness of our approach through empirical experiments on multiple datasets, comparing both training and test-time optimization performance. The code is available at https://github.com/2ailesB/neural-parametric-solver.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroplastic Expansion in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.07994</link>
<guid>https://arxiv.org/abs/2410.07994</guid>
<content:encoded><![CDATA[

arXiv:2410.07994v3 Announce Type: replace 
Abstract: The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, {\it Neuroplastic Expansion} (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (\textit{1}) elastic topology generation based on potential gradients, (\textit{2}) dormant neuron pruning to optimize network expressivity, and (\textit{3}) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</title>
<link>https://arxiv.org/abs/2410.09411</link>
<guid>https://arxiv.org/abs/2410.09411</guid>
<content:encoded><![CDATA[

arXiv:2410.09411v2 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency, and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviors in classification tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2410.11674</link>
<guid>https://arxiv.org/abs/2410.11674</guid>
<content:encoded><![CDATA[

arXiv:2410.11674v2 Announce Type: replace 
Abstract: Time series forecasting remains a challenging task, particularly in the context of complex multiscale temporal patterns. This study presents LLM-Mixer, a framework that improves forecasting accuracy through the combination of multiscale time-series decomposition with pre-trained LLMs (Large Language Models). LLM-Mixer captures both short-term fluctuations and long-term trends by decomposing the data into multiple temporal resolutions and processing them with a frozen LLM, guided by a textual prompt specifically designed for time-series data. Extensive experiments conducted on multivariate and univariate datasets demonstrate that LLM-Mixer achieves competitive performance, outperforming recent state-of-the-art models across various forecasting horizons. This work highlights the potential of combining multiscale analysis and LLMs for effective and scalable time-series forecasting.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FragNet: A Graph Neural Network for Molecular Property Prediction with Four Levels of Interpretability</title>
<link>https://arxiv.org/abs/2410.12156</link>
<guid>https://arxiv.org/abs/2410.12156</guid>
<content:encoded><![CDATA[

arXiv:2410.12156v2 Announce Type: replace 
Abstract: Molecular property prediction is essential in a variety of contemporary scientific fields, such as drug development and designing energy storage materials. Although there are many machine learning models available for this purpose, those that achieve high accuracy while also offering interpretability of predictions are uncommon. We present a graph neural network that not only matches the prediction accuracies of leading models but also provides insights on four levels of molecular substructures. This model helps identify which atoms, bonds, molecular fragments, and connections between fragments are significant for predicting a specific molecular property. Understanding the importance of connections between fragments is particularly valuable for molecules with substructures that do not connect through standard bonds. The model additionally can quantify the impact of specific fragments on the prediction, allowing the identification of fragments that may improve or degrade a property value. These interpretable features are essential for deriving scientific insights from the model's learned relationships between molecular structures and properties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2410.13097</link>
<guid>https://arxiv.org/abs/2410.13097</guid>
<content:encoded><![CDATA[

arXiv:2410.13097v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods typically assume that Large Language Models (LLMs) are trained on data from a single device or client. However, real-world scenarios often require fine-tuning these models on private data distributed across multiple devices. Federated Learning (FL) offers an appealing solution by preserving user privacy, as sensitive data remains on local devices during training. Nonetheless, integrating PEFT methods into FL introduces two main challenges: communication overhead and data heterogeneity. In this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by integrating tensorized adapters into client-side models' encoder/decoder blocks. FedTT is versatile and can be applied to both cross-silo FL and large-scale cross-device FL. FedTT+, an extension of FedTT tailored for cross-silo FL, enhances robustness against data heterogeneity by adaptively freezing portions of tensor factors, further reducing the number of trainable parameters. Experiments on BERT and LLaMA models demonstrate that our proposed methods successfully address data heterogeneity challenges and perform on par or even better than existing federated PEFT approaches while achieving up to 10$\times$ reduction in communication cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation</title>
<link>https://arxiv.org/abs/2410.13248</link>
<guid>https://arxiv.org/abs/2410.13248</guid>
<content:encoded><![CDATA[

arXiv:2410.13248v2 Announce Type: replace 
Abstract: Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. The datasets and benchmark implementation are available at: https://github.com/jchanxtarov/sent_xrec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2410.17933</link>
<guid>https://arxiv.org/abs/2410.17933</guid>
<content:encoded><![CDATA[

arXiv:2410.17933v2 Announce Type: replace 
Abstract: One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy preserved. Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation with Estimation of Source Reliability</title>
<link>https://arxiv.org/abs/2410.22954</link>
<guid>https://arxiv.org/abs/2410.22954</guid>
<content:encoded><![CDATA[

arXiv:2410.22954v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) addresses key limitations of large language models (LLMs), such as hallucinations and outdated knowledge, by incorporating external databases. These databases typically consult multiple sources to encompass up-to-date and various information. However, standard RAG methods often overlook the heterogeneous source reliability in the multi-source database and retrieve documents solely based on relevance, making them prone to propagating misinformation. To address this, we propose Reliability-Aware RAG (RA-RAG) which estimates the reliability of multiple sources and incorporates this information into both retrieval and aggregation processes. Specifically, it iteratively estimates source reliability and true answers for a set of queries with no labelling. Then, it selectively retrieves relevant documents from a few of reliable sources and aggregates them using weighted majority voting, where the selective retrieval ensures scalability while not compromising the performance. We also introduce a benchmark designed to reflect real-world scenarios with heterogeneous source reliability and demonstrate the effectiveness of RA-RAG compared to a set of baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison-based Active Preference Learning for Multi-dimensional Personalization</title>
<link>https://arxiv.org/abs/2411.00524</link>
<guid>https://arxiv.org/abs/2411.00524</guid>
<content:encoded><![CDATA[

arXiv:2411.00524v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable success, but aligning them with human preferences remains a core challenge. As individuals have their own, multi-dimensional preferences, recent studies have explored multi-dimensional personalization, which aims to enable models to generate responses personalized to explicit preferences. However, human preferences are often implicit and thus difficult to articulate, limiting the direct application of this approach. To bridge this gap, we propose Active Multi-dimensional Preference Learning (AMPLe), designed to capture implicit user preferences from interactively collected comparative feedback. Building on Bayesian inference, our work introduces a modified posterior update procedure to mitigate estimation bias and potential noise in comparisons. Also, inspired by generalized binary search, we employ an active query selection strategy to minimize the number of required comparisons by a user. Through theoretical analysis and experiments on language generation tasks, we demonstrate feedback efficiency and effectiveness of our framework in personalizing model responses. Our code is publicly available at https://github.com/ml-postech/AMPLe .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Filtering with RKHS Algebras</title>
<link>https://arxiv.org/abs/2411.01341</link>
<guid>https://arxiv.org/abs/2411.01341</guid>
<content:encoded><![CDATA[

arXiv:2411.01341v2 Announce Type: replace 
Abstract: In this paper, we develop a generalized theory of convolutional signal processing and neural networks for Reproducing Kernel Hilbert Spaces (RKHS). Leveraging the theory of algebraic signal processing (ASP), we show that any RKHS allows the formal definition of multiple algebraic convolutional models. We show that any RKHS induces algebras whose elements determine convolutional operators acting on RKHS elements. This approach allows us to achieve scalable filtering and learning as a byproduct of the convolutional model, and simultaneously take advantage of the well-known benefits of processing information in an RKHS. To emphasize the generality and usefulness of our approach, we show how algebraic RKHS can be used to define convolutional signal models on groups, graphons, and traditional Euclidean signal spaces. Furthermore, using algebraic RKHS models, we build convolutional networks, formally defining the notion of pointwise nonlinearities and deriving explicit expressions for the training. Such derivations are obtained in terms of the algebraic representation of the RKHS. We present a set of numerical experiments on real data in which wireless coverage is predicted from measurements captured by unmaned aerial vehicles. This particular real-life scenario emphasizes the benefits of the convolutional RKHS models in neural networks compared to fully connected and standard convolutional operators.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2411.04760</link>
<guid>https://arxiv.org/abs/2411.04760</guid>
<content:encoded><![CDATA[

arXiv:2411.04760v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices. However, SNN model parameters are sensitive to temporal resolution, leading to significant performance drops when the temporal resolution of target data at the edge is not the same with that of the pre-deployment source data used for training, especially when fine-tuning is not possible at the edge. To address this challenge, we propose three novel domain adaptation methods for adapting neuron parameters to account for the change in time resolution without re-training on target time-resolution. The proposed methods are based on a mapping between neuron dynamics in SNNs and State Space Models (SSMs); and are applicable to general neuron models. We evaluate the proposed methods under spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and MSWC as well as the image classification NMINST dataset. Our methods provide an alternative to - and in majority of the cases significantly outperform - the existing reference method that simply scales the time constant. Moreover, our results show that high accuracy on high temporal resolution data can be obtained by time efficient training on lower temporal resolution data and model adaptation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering</title>
<link>https://arxiv.org/abs/2411.09749</link>
<guid>https://arxiv.org/abs/2411.09749</guid>
<content:encoded><![CDATA[

arXiv:2411.09749v2 Announce Type: replace 
Abstract: Differentiable rendering techniques like Gaussian Splatting and Neural Radiance Fields have become powerful tools for generating high-fidelity models of 3D objects and scenes. Their ability to produce both physically plausible and differentiable models of scenes are key ingredient needed to produce physically plausible adversarial attacks on DNNs. However, the adversarial machine learning community has yet to fully explore these capabilities, partly due to differing attack goals (e.g., misclassification, misdetection) and a wide range of possible scene manipulations used to achieve them (e.g., alter texture, mesh). This survey contributes the first framework that unifies diverse goals and tasks, facilitating easy comparison of existing work, identifying research gaps, and highlighting future directions - ranging from expanding attack goals and tasks to account for new modalities, state-of-the-art models, tools, and pipelines, to underscoring the importance of studying real-world threats in complex scenes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models</title>
<link>https://arxiv.org/abs/2411.09837</link>
<guid>https://arxiv.org/abs/2411.09837</guid>
<content:encoded><![CDATA[

arXiv:2411.09837v2 Announce Type: replace 
Abstract: To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization</title>
<link>https://arxiv.org/abs/2411.10958</link>
<guid>https://arxiv.org/abs/2411.10958</guid>
<content:encoded><![CDATA[

arXiv:2411.10958v5 Announce Type: replace 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schemato - An LLM for Netlist-to-Schematic Conversion</title>
<link>https://arxiv.org/abs/2411.13899</link>
<guid>https://arxiv.org/abs/2411.13899</guid>
<content:encoded><![CDATA[

arXiv:2411.13899v2 Announce Type: replace 
Abstract: Machine learning models are advancing circuit design, particularly in analog circuits. They typically generate netlists that lack human interpretability. This is a problem as human designers heavily rely on the interpretability of circuit diagrams or schematics to intuitively understand, troubleshoot, and develop designs. Hence, to integrate domain knowledge effectively, it is crucial to translate ML-generated netlists into interpretable schematics quickly and accurately. We propose Schemato, a large language model (LLM) for netlist-to-schematic conversion. In particular, we consider our approach in converting netlists to .asc files, text-based schematic description used in LTSpice. Experiments on our circuit dataset show that Schemato achieves up to 76% compilation success rate, surpassing 63% scored by the state-of-the-art LLMs. Furthermore, our experiments show that Schemato generates schematics with an average graph edit distance score and mean structural similarity index measure, scaled by the compilation success rate that are 1.8x and 4.3x higher than the best performing LLMs respectively, demonstrating its ability to generate schematics that are more accurately connected and are closer to the reference human design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MC-NEST: Enhancing Mathematical Reasoning in Large Language Models leveraging a Monte Carlo Self-Refine Tree</title>
<link>https://arxiv.org/abs/2411.15645</link>
<guid>https://arxiv.org/abs/2411.15645</guid>
<content:encoded><![CDATA[

arXiv:2411.15645v2 Announce Type: replace 
Abstract: Mathematical reasoning presents significant challenges for large language models (LLMs). To enhance their capabilities, we propose Monte Carlo Self-Refine Tree (MC-NEST), an extension of Monte Carlo Tree Search that integrates LLM-based self-refinement and self-evaluation for improved decision-making in complex reasoning tasks. MC-NEST balances exploration and exploitation using Upper Confidence Bound (UCT) scores combined with diverse selection policies. Through iterative critique and refinement, LLMs learn to reason more strategically. Empirical results demonstrate that MC-NEST with an importance sampling policy substantially improves GPT-4o's performance, achieving state-of-the-art pass@1 scores on Olympiad-level benchmarks. Specifically, MC-NEST attains a pass@1 of 38.6 on AIME and 12.6 on MathOdyssey. The solution quality for MC-NEST using GPT-4o and Phi-3-mini reaches 84.0\% and 82.08\%, respectively, indicating robust consistency across different LLMs. MC-NEST performs strongly across Algebra, Geometry, and Number Theory, benefiting from its ability to handle abstraction, logical deduction, and multi-step reasoning -- core skills in mathematical problem solving.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization of Handwritten Text Recognition Models</title>
<link>https://arxiv.org/abs/2411.17332</link>
<guid>https://arxiv.org/abs/2411.17332</guid>
<content:encoded><![CDATA[

arXiv:2411.17332v2 Announce Type: replace 
Abstract: Recent advances in Handwritten Text Recognition (HTR) have led to significant reductions in transcription errors on standard benchmarks under the i.i.d. assumption, thus focusing on minimizing in-distribution (ID) errors. However, this assumption does not hold in real-world applications, which has motivated HTR research to explore Transfer Learning and Domain Adaptation techniques. In this work, we investigate the unaddressed limitations of HTR models in generalizing to out-of-distribution (OOD) data. We adopt the challenging setting of Domain Generalization, where models are expected to generalize to OOD data without any prior access. To this end, we analyze 336 OOD cases from eight state-of-the-art HTR models across seven widely used datasets, spanning five languages. Additionally, we study how HTR models leverage synthetic data to generalize. We reveal that the most significant factor for generalization lies in the textual divergence between domains, followed by visual divergence. We demonstrate that the error of HTR models in OOD scenarios can be reliably estimated, with discrepancies falling below 10 points in 70\% of cases. We identify the underlying limitations of HTR models, laying the foundation for future research to address this challenge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</title>
<link>https://arxiv.org/abs/2412.08174</link>
<guid>https://arxiv.org/abs/2412.08174</guid>
<content:encoded><![CDATA[

arXiv:2412.08174v3 Announce Type: replace 
Abstract: While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning</title>
<link>https://arxiv.org/abs/2412.08559</link>
<guid>https://arxiv.org/abs/2412.08559</guid>
<content:encoded><![CDATA[

arXiv:2412.08559v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2412.09059</link>
<guid>https://arxiv.org/abs/2412.09059</guid>
<content:encoded><![CDATA[

arXiv:2412.09059v5 Announce Type: replace 
Abstract: Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SB between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SB problems, and various other examples. We also test our approach on an Entropic Optimal Transport (EOT) benchmark problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Generative Modeling with Residual Vector Quantization-Based Tokens</title>
<link>https://arxiv.org/abs/2412.10208</link>
<guid>https://arxiv.org/abs/2412.10208</guid>
<content:encoded><![CDATA[

arXiv:2412.10208v3 Announce Type: replace 
Abstract: We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery</title>
<link>https://arxiv.org/abs/2412.13667</link>
<guid>https://arxiv.org/abs/2412.13667</guid>
<content:encoded><![CDATA[

arXiv:2412.13667v2 Announce Type: replace 
Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Policy Learning under Concept Drifts</title>
<link>https://arxiv.org/abs/2412.14297</link>
<guid>https://arxiv.org/abs/2412.14297</guid>
<content:encoded><![CDATA[

arXiv:2412.14297v2 Announce Type: replace 
Abstract: Distributionally robust policy learning aims to find a policy that performs well under the worst-case distributional shift, and yet most existing methods for robust policy learning consider the worst-case joint distribution of the covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative when we have more information on the source of distributional shifts. This paper studies a more nuanced problem -- robust policy learning under the concept drift, when only the conditional relationship between the outcome and the covariate changes. To this end, we first provide a doubly-robust estimator for evaluating the worst-case average reward of a given policy under a set of perturbed conditional distributions. We show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters are estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the estimated policy value within a given policy class $\Pi$, and show that the sub-optimality gap of the proposed algorithm is of the order $\kappa(\Pi)n^{-1/2}$, where $\kappa(\Pi)$ is the entropy integral of $\Pi$ under the Hamming distance and $n$ is the sample size. A matching lower bound is provided to show the optimality of the rate. The proposed methods are implemented and evaluated in numerical studies, demonstrating substantial improvement compared with existing benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principal-Agent Bandit Games with Self-Interested and Exploratory Learning Agents</title>
<link>https://arxiv.org/abs/2412.16318</link>
<guid>https://arxiv.org/abs/2412.16318</guid>
<content:encoded><![CDATA[

arXiv:2412.16318v2 Announce Type: replace 
Abstract: We study the repeated principal-agent bandit game, where the principal indirectly interacts with the unknown environment by proposing incentives for the agent to play arms. Most existing work assumes the agent has full knowledge of the reward means and always behaves greedily, but in many online marketplaces, the agent needs to learn the unknown environment and sometimes explore. Motivated by such settings, we model a self-interested learning agent with exploration behaviors who iteratively updates reward estimates and either selects an arm that maximizes the estimated reward plus incentive or explores arbitrarily with a certain probability. As a warm-up, we first consider a self-interested learning agent without exploration. We propose algorithms for both i.i.d. and linear reward settings with bandit feedback in a finite horizon $T$, achieving regret bounds of $\widetilde{O}(\sqrt{T})$ and $\widetilde{O}( T^{2/3} )$, respectively. Specifically, these algorithms are established upon a novel elimination framework coupled with newly-developed search algorithms which accommodate the uncertainty arising from the learning behavior of the agent. We then extend the framework to handle the exploratory learning agent and develop an algorithm to achieve a $\widetilde{O}(T^{2/3})$ regret bound in i.i.d. reward setup by enhancing the robustness of our elimination framework to the potential agent exploration. Finally, when reducing our agent behaviors to the one studied in (Dogan et al., 2023a), we propose an algorithm based on our robust framework, which achieves a $\widetilde{O}(\sqrt{T})$ regret bound, significantly improving upon their $\widetilde{O}(T^{11/12})$ bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating Flow Matching ODE Dynamics with Respect to Data Geometries</title>
<link>https://arxiv.org/abs/2412.18730</link>
<guid>https://arxiv.org/abs/2412.18730</guid>
<content:encoded><![CDATA[

arXiv:2412.18730v3 Announce Type: replace 
Abstract: Flow matching (FM) models extend ODE sampler based diffusion models into a general framework, significantly reducing sampling steps through learned vector fields. However, the theoretical understanding of FM models, particularly how their sample trajectories interact with underlying data geometry, remains underexplored. A rigorous theoretical analysis of FM ODE is essential for sample quality, stability, and broader applicability. In this paper, we advance the theory of FM models through a comprehensive analysis of sample trajectories. Central to our theory is the discovery that the denoiser, a key component of FM models, guides ODE dynamics through attracting and absorbing behaviors that adapt to the data geometry. We identify and analyze the three stages of ODE evolution: in the initial and intermediate stages, trajectories move toward the mean and local clusters of the data. At the terminal stage, we rigorously establish the convergence of FM ODE under weak assumptions, addressing scenarios where the data lie on a low-dimensional submanifold-cases that previous results could not handle. Our terminal stage analysis offers insights into the memorization phenomenon and establishes equivariance properties of FM ODEs. These findings bridge critical gaps in understanding flow matching models, with practical implications for optimizing sampling strategies and architectures guided by the intrinsic geometry of data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph</title>
<link>https://arxiv.org/abs/2501.00659</link>
<guid>https://arxiv.org/abs/2501.00659</guid>
<content:encoded><![CDATA[

arXiv:2501.00659v2 Announce Type: replace 
Abstract: Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is 'no' provided they have more than one layer -- they can distinguish sequences with permuted tokens without the need for explicit PEs. This follows from the fact that a cascade of (permutation invariant) set processors can collectively exhibit sequence-sensitive behavior in the autoregressive setting. This property has been known since early efforts (contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated, leading to recent rediscoveries. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2/3, but perhaps also due to the lack of a clear explanation in prior work, despite being commonly understood by practitioners in the past. Here we review the long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their inputs), as well as the origin of this result, and hope to re-establish it as a common knowledge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorGRaD: Tensor Gradient Robust Decomposition for Memory-Efficient Neural Operator Training</title>
<link>https://arxiv.org/abs/2501.02379</link>
<guid>https://arxiv.org/abs/2501.02379</guid>
<content:encoded><![CDATA[

arXiv:2501.02379v2 Announce Type: replace 
Abstract: Scientific problems require resolving multi-scale phenomena across different resolutions and learning solution operators in infinite-dimensional function spaces. Neural operators provide a powerful framework for this, using tensor-parameterized layers to capture complex, multi-dimensional relationships. However, scaling neural operators to high-resolution problems leads to significant computational demands, making the training of industrial-scale models prohibitive. In this work, we introduce \textbf{TensorGRaD}, a novel method that directly addresses the memory challenges associated with optimizing large tensor-structured weights. Our approach, based on a \texit{robust tensor decomposition}, factorizes gradients as the sum of a low-rank tensor and a sparse one to efficiently capture information within optimizer states, including outliers. Additionally, we provide a recipe for mixed precision training of TensorGRaD, achieving further memory savings without sacrificing accuracy. We showcase the effectiveness of TensorGRaD on Fourier Neural Operators, a class of models crucial for solving partial differential equations (PDE). We provide theoretical guarantees for TensorGRaD, demonstrating its fundamental advantage over matrix-based gradient compression methods. We empirically demonstrate large improvements across various PDE tasks, including the challenging turbulent Navier-Stokes case at a Reynolds number of $10^5$. TensorGRaD reduces total memory usage by over $50\%$ while maintaining and sometimes even improving accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?</title>
<link>https://arxiv.org/abs/2501.05000</link>
<guid>https://arxiv.org/abs/2501.05000</guid>
<content:encoded><![CDATA[

arXiv:2501.05000v4 Announce Type: replace 
Abstract: Energy communities (ECs) play a key role in enabling local demand shifting and enhancing self-sufficiency, as energy systems transition toward decentralized structures with high shares of renewable generation. To optimally operate them, accurate short-term load forecasting is essential, particularly for implementing demand-side management strategies. With the recent rise of deep learning methods, data-driven forecasting has gained significant attention, however, it remains insufficiently explored in many practical contexts. Therefore, this study evaluates the effectiveness of state-of-the-art deep learning models -- including LSTM, xLSTM, and Transformer architectures -- compared to traditional benchmarks such as K-Nearest Neighbors (KNN) and persistence forecasting, across varying community size, historical data availability, and model complexity. Additionally, we assess the benefits of transfer learning using publicly available synthetic load profiles. On average, transfer learning improves the normalized mean absolute error by 1.97%pt when only two months of training data are available. Interestingly, for less than six months of training data, simple persistence models outperform deep learning architectures in forecast accuracy. The practical value of improved forecasting is demonstrated using a mixed-integer linear programming optimization for ECs with a shared battery energy storage system. The most accurate deep learning model achieves an average reduction in financial energy costs of 8.06%. Notably, a simple KNN approach achieves average savings of 8.01%, making it a competitive and robust alternative. All implementations are publicly available to facilitate reproducibility. These findings offer actionable insights for ECs, and they highlight when the additional complexity of deep learning is warranted by performance gains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</title>
<link>https://arxiv.org/abs/2501.08219</link>
<guid>https://arxiv.org/abs/2501.08219</guid>
<content:encoded><![CDATA[

arXiv:2501.08219v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks, leading to widespread adoption in both research and industry. However, their inference workloads are computationally and energy intensive, raising concerns about sustainability and environmental impact. As LLMs continue to scale, it becomes essential to identify and optimize the factors that influence their runtime efficiency without compromising performance. In this work, we systematically investigate the energy-performance trade-offs of LLMs during inference. We benchmark models of varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1, LLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question answering, commonsense reasoning, and factual generation. We analyze the effect of input characteristics, such as sequence length, entropy, named entity density and so on. Furthermore, we examine the impact of hardware-level optimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring how different GPU clock settings affect latency and power consumption. Our empirical findings show that model architecture, input complexity, and clock configuration significantly influence inference efficiency. By correlating input features with energy metrics and evaluating DVFS behavior, we identify practical strategies that reduce energy consumption by up to 30% while preserving model quality. This study provides actionable insights for designing energy-efficient and sustainable LLM inference systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Seismic Artificial Intelligence with Uncertainty</title>
<link>https://arxiv.org/abs/2501.14809</link>
<guid>https://arxiv.org/abs/2501.14809</guid>
<content:encoded><![CDATA[

arXiv:2501.14809v2 Announce Type: replace 
Abstract: Artificial intelligence has transformed the seismic community with deep learning models (DLMs) that are trained to complete specific tasks within workflows. However, there is still lack of robust evaluation frameworks for evaluating and comparing DLMs. We address this gap by designing an evaluation framework that jointly incorporates two crucial aspects: performance uncertainty and learning efficiency. To target these aspects, we meticulously construct the training, validation, and test splits using a clustering method tailored to seismic data and enact an expansive training design to segregate performance uncertainty arising from stochastic training processes and random data sampling. The framework's ability to guard against misleading declarations of model superiority is demonstrated through evaluation of PhaseNet [1], a popular seismic phase picking DLM, under 3 training approaches. Our framework helps practitioners choose the best model for their problem and set performance expectations by explicitly analyzing model performance with uncertainty at varying budgets of training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIP: Perturbation-based Iterative Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2501.15278</link>
<guid>https://arxiv.org/abs/2501.15278</guid>
<content:encoded><![CDATA[

arXiv:2501.15278v2 Announce Type: replace 
Abstract: The rapid increase in the parameter counts of Large Language Models (LLMs), reaching billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To ease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in environments with constrained resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Low-Rank Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2501.15361</link>
<guid>https://arxiv.org/abs/2501.15361</guid>
<content:encoded><![CDATA[

arXiv:2501.15361v4 Announce Type: replace 
Abstract: While parameter-efficient fine-tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) offer computationally efficient adaptations of Large Language Models (LLMs), their practical deployment often assumes centralized data and training environments. However, real-world scenarios frequently involve distributed, privacy-sensitive datasets that require decentralized solutions. Federated learning (FL) addresses data privacy by coordinating model updates across clients, but it is typically based on centralized aggregation through a parameter server, which can introduce bottlenecks and communication constraints. Decentralized learning, in contrast, eliminates this dependency by enabling direct collaboration between clients, improving scalability and efficiency in distributed environments. Despite its advantages, decentralized LLM fine-tuning remains underexplored. In this work, we propose Dec-LoRA, a decentralized fine-tuning algorithm for LLMs based on LoRA. Through extensive experiments on BERT and LLaMA-2 models, we demonstrate that Dec-LoRA achieves performance comparable to centralized LoRA under various conditions, including data heterogeneity and quantization constraints. Additionally, we provide a rigorous theoretical guarantee proving the convergence of our algorithm to a stationary point for non-convex and smooth loss functions. These findings highlight the potential of Dec-LoRA for scalable LLM fine-tuning in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Machine Learning Potentials via Difference Vectors Based on Local Atomic Environments</title>
<link>https://arxiv.org/abs/2501.16398</link>
<guid>https://arxiv.org/abs/2501.16398</guid>
<content:encoded><![CDATA[

arXiv:2501.16398v2 Announce Type: replace 
Abstract: Constructing efficient and diverse datasets is essential for the development of accurate machine learning potentials (MLPs) in atomistic simulations. However, existing approaches often suffer from data redundancy and high computational costs. Herein, we propose a new method--Difference Vectors based on Local Atomic Environments (DV-LAE)--that encodes structural differences via histogram-based descriptors and enables visual analysis through t-SNE dimensionality reduction. This approach facilitates redundancy detection and dataset optimization while preserving structural diversity. We demonstrate that DV-LAE significantly reduces dataset size and training time across various materials systems, including high-pressure hydrogen, iron-hydrogen binaries, magnesium hydrides, and carbon allotropes, with minimal compromise in prediction accuracy. For instance, in the $\alpha$-Fe/H system, maintaining a highly similar MLP accuracy, the dataset size was reduced by 56%, and the training time per iteration dropped by over 50%. Moreover, we show how visualizing the DV-LAE representation aids in identifying out-of-distribution data by examining the spatial distribution of high-error prediction points, providing a robust reliability metric for new structures during simulations. Our results highlight the utility of local environment visualization not only as an interpretability tool but also as a practical means for accelerating MLP development and ensuring data efficiency in large-scale atomistic modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.17077</link>
<guid>https://arxiv.org/abs/2501.17077</guid>
<content:encoded><![CDATA[

arXiv:2501.17077v2 Announce Type: replace 
Abstract: Interpretability is crucial for ensuring RL systems align with human values. However, it remains challenging to achieve in complex decision making domains. Existing methods frequently attempt interpretability at the level of fundamental model units, such as neurons or decision nodes: an approach which scales poorly to large models. Here, we instead propose an approach to interpretability at the level of functional modularity. We show how encouraging sparsity and locality in network weights leads to the emergence of functional modules in RL policy networks. To detect these modules, we develop an extended Louvain algorithm which uses a novel `correlation alignment' metric to overcome the limitations of standard network analysis techniques when applied to neural network architectures. Applying these methods to 2D and 3D MiniGrid environments reveals the consistent emergence of distinct navigational modules for different axes, and we further demonstrate how these functions can be validated through direct interventions on network weights prior to inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Provably Improves the Convergence of Gradient Descent</title>
<link>https://arxiv.org/abs/2501.18092</link>
<guid>https://arxiv.org/abs/2501.18092</guid>
<content:encoded><![CDATA[

arXiv:2501.18092v4 Announce Type: replace 
Abstract: Learn to Optimize (L2O) trains deep neural network based solvers for optimization, achieving success in accelerating convex problems and improving non-convex solutions. However, L2O lacks rigorous theoretical backing for its own training convergence, as existing analyses often use unrealistic assumptions -- a gap this work highlights empirically. We bridge this gap by proving the training convergence of L2O models that learn Gradient Descent (GD) hyperparameters for quadratic programming, leveraging the Neural Tangent Kernel (NTK) theory. We propose a deterministic initialization strategy to support our theoretical results and promote stable training over extended optimization horizons by mitigating gradient explosion. Our L2O framework demonstrates over 50\% better optimality against GD and superior robustness over state-of-the-art L2O methods on synthetic datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algebra Unveils Deep Learning -- An Invitation to Neuroalgebraic Geometry</title>
<link>https://arxiv.org/abs/2501.18915</link>
<guid>https://arxiv.org/abs/2501.18915</guid>
<content:encoded><![CDATA[

arXiv:2501.18915v2 Announce Type: replace 
Abstract: In this position paper, we promote the study of function spaces parameterized by machine learning models through the lens of algebraic geometry. To this end, we focus on algebraic models, such as neural networks with polynomial activations, whose associated function spaces are semi-algebraic varieties. We outline a dictionary between algebro-geometric invariants of these varieties, such as dimension, degree, and singularities, and fundamental aspects of machine learning, such as sample complexity, expressivity, training dynamics, and implicit bias. Along the way, we review the literature and discuss ideas beyond the algebraic domain. This work lays the foundations of a research direction bridging algebraic geometry and deep learning, that we refer to as neuroalgebraic geometry.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabFSBench: Tabular Benchmark for Feature Shifts in Open Environments</title>
<link>https://arxiv.org/abs/2501.18935</link>
<guid>https://arxiv.org/abs/2501.18935</guid>
<content:encoded><![CDATA[

arXiv:2501.18935v3 Announce Type: replace 
Abstract: Tabular data is widely utilized in various machine learning tasks. Current tabular learning research predominantly focuses on closed environments, while in real-world applications, open environments are often encountered, where distribution and feature shifts occur, leading to significant degradation in model performance. Previous research has primarily concentrated on mitigating distribution shifts, whereas feature shifts, a distinctive and unexplored challenge of tabular data, have garnered limited attention. To this end, this paper conducts the first comprehensive study on feature shifts in tabular data and introduces the first tabular feature-shift benchmark (TabFSBench). TabFSBench evaluates impacts of four distinct feature-shift scenarios on four tabular model categories across various datasets and assesses the performance of large language models (LLMs) and tabular LLMs in the tabular benchmark for the first time. Our study demonstrates three main observations: (1) most tabular models have the limited applicability in feature-shift scenarios; (2) the shifted feature set importance has a linear relationship with model performance degradation; (3) model performance in closed environments correlates with feature-shift performance. Future research direction is also explored for each observation.
  Benchmark: https://github.com/LAMDASZ-ML/TabFSBench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected</title>
<link>https://arxiv.org/abs/2501.19107</link>
<guid>https://arxiv.org/abs/2501.19107</guid>
<content:encoded><![CDATA[

arXiv:2501.19107v2 Announce Type: replace 
Abstract: Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling and even exceed fully connected baselines in zero-shot tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking</title>
<link>https://arxiv.org/abs/2501.19358</link>
<guid>https://arxiv.org/abs/2501.19358</guid>
<content:encoded><![CDATA[

arXiv:2501.19358v3 Announce Type: replace 
Abstract: This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of Diffusion Models with Application to Schedule Design</title>
<link>https://arxiv.org/abs/2502.00180</link>
<guid>https://arxiv.org/abs/2502.00180</guid>
<content:encoded><![CDATA[

arXiv:2502.00180v2 Announce Type: replace 
Abstract: Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity assumption, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged to design a noise schedule that aligns effectively with the characteristics of the data. The spectral perspective also provides insights into the underlying dynamics and sheds light on the relationship between spectral properties and noise schedule structure. Our results lead to scheduling curves that are dependent on the spectral content of the data, offering a theoretical justification for some of the heuristics taken by practitioners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Abstraction Learning based on the Semantic Embedding Principle</title>
<link>https://arxiv.org/abs/2502.00407</link>
<guid>https://arxiv.org/abs/2502.00407</guid>
<content:encoded><![CDATA[

arXiv:2502.00407v2 Announce Type: replace 
Abstract: Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is semantic embedding, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the Stiefel manifold. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Over-parameterized Matrix Sensing from Noisy Measurements via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2502.00463</link>
<guid>https://arxiv.org/abs/2502.00463</guid>
<content:encoded><![CDATA[

arXiv:2502.00463v3 Announce Type: replace 
Abstract: We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$ of the target matrix $X_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorization $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r > r_\star $, with $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + \lambda I)^{-1} $ and $ (R^\top R + \lambda I)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $\lambda$ and are sensitive to step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter $\lambda$ and enabling faster convergence with larger step sizes. We theoretically prove that APGD convergences to a near-optimal error at a linear rate. We further show that APGD can be extended to deal with other low-rank matrix estimation tasks, also with a theoretical guarantee of linear convergence. To validate the effectiveness and scalability of the proposed APGD, we conduct simulated and real-world experiments on a wide range of low-rank estimation problems, including noisy matrix sensing, weighted PCA, 1-bit matrix completion, and matrix completion. The extensive results demonstrate that APGD consistently achieves the fastest convergence and the lowest computation time compared to the existing alternatives.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Safety Alignment is Divergence Estimation in Disguise</title>
<link>https://arxiv.org/abs/2502.00657</link>
<guid>https://arxiv.org/abs/2502.00657</guid>
<content:encoded><![CDATA[

arXiv:2502.00657v2 Announce Type: replace 
Abstract: We present a theoretical framework showing that popular LLM alignment methods, including RLHF and its variants, can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance-refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science</title>
<link>https://arxiv.org/abs/2502.01159</link>
<guid>https://arxiv.org/abs/2502.01159</guid>
<content:encoded><![CDATA[

arXiv:2502.01159v2 Announce Type: replace 
Abstract: The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Transformer World Models for Data-Efficient RL</title>
<link>https://arxiv.org/abs/2502.01591</link>
<guid>https://arxiv.org/abs/2502.01591</guid>
<content:encoded><![CDATA[

arXiv:2502.01591v2 Announce Type: replace 
Abstract: We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model</title>
<link>https://arxiv.org/abs/2502.01989</link>
<guid>https://arxiv.org/abs/2502.01989</guid>
<content:encoded><![CDATA[

arXiv:2502.01989v3 Announce Type: replace 
Abstract: Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel LRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\times15$, while standard diffusion model completely fails.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer Drug Response Prediction</title>
<link>https://arxiv.org/abs/2502.04034</link>
<guid>https://arxiv.org/abs/2502.04034</guid>
<content:encoded><![CDATA[

arXiv:2502.04034v2 Announce Type: replace 
Abstract: The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug--trained solely on in vitro cell line data without access to target-domain data--consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</title>
<link>https://arxiv.org/abs/2502.04420</link>
<guid>https://arxiv.org/abs/2502.04420</guid>
<content:encoded><![CDATA[

arXiv:2502.04420v4 Announce Type: replace 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans Coexist, So Must Embodied Artificial Agents</title>
<link>https://arxiv.org/abs/2502.04809</link>
<guid>https://arxiv.org/abs/2502.04809</guid>
<content:encoded><![CDATA[

arXiv:2502.04809v3 Announce Type: replace 
Abstract: This paper introduces the concept of coexistence for embodied artificial agents and argues that it is a prerequisite for long-term, in-the-wild interaction with humans. Contemporary embodied artificial agents excel in static, predefined tasks but fall short in dynamic and long-term interactions with humans. On the other hand, humans can adapt and evolve continuously, exploiting the situated knowledge embedded in their environment and other agents, thus contributing to meaningful interactions. We take an interdisciplinary approach at different levels of organization, drawing from biology and design theory, to understand how human and non-human organisms foster entities that coexist within their specific environments. Finally, we propose key research directions for the artificial intelligence community to develop coexisting embodied agents, focusing on the principles, hardware and learning methods responsible for shaping them.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</title>
<link>https://arxiv.org/abs/2502.06051</link>
<guid>https://arxiv.org/abs/2502.06051</guid>
<content:encoded><![CDATA[

arXiv:2502.06051v2 Announce Type: replace 
Abstract: Although many popular reinforcement learning algorithms are underpinned by $f$-divergence regularization, their sample complexity with respect to the \emph{regularized objective} still lacks a tight characterization. In this paper, we analyze $f$-divergence-regularized offline policy learning. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we give the first $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for contextual bandits, surpassing existing $\tilde{O}(\epsilon^{-1})$ bound under all-policy concentrability and $\tilde{O}(\epsilon^{-2})$ bound under single-policy concentrability. Our analysis for general function approximation leverages the principle of pessimism in the face of uncertainty to refine a mean-value-type argument to its extreme. This in turn leads to a novel moment-based technique, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. We further propose a lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the strong convexity of reverse KL. In addition, for $f$-divergences with strongly convex $f$, to which reverse KL \emph{does not} belong, we show that the sharp sample complexity $\tilde{\Theta}(\epsilon^{-1})$ is achievable even without single-policy concentrability. In this case, the algorithm design can get rid of pessimistic estimators. We further extend our analysis to dueling bandits, and we believe these results take a significant step toward a comprehensive understanding of $f$-divergence-regularized policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes a good feedforward computational graph?</title>
<link>https://arxiv.org/abs/2502.06751</link>
<guid>https://arxiv.org/abs/2502.06751</guid>
<content:encoded><![CDATA[

arXiv:2502.06751v2 Announce Type: replace 
Abstract: As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logarithmic Regret for Online KL-Regularized Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.07460</link>
<guid>https://arxiv.org/abs/2502.07460</guid>
<content:encoded><![CDATA[

arXiv:2502.07460v5 Announce Type: replace 
Abstract: Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy amplification by random allocation</title>
<link>https://arxiv.org/abs/2502.08202</link>
<guid>https://arxiv.org/abs/2502.08202</guid>
<content:encoded><![CDATA[

arXiv:2502.08202v3 Announce Type: replace 
Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization [Chua et al., 2024a, Choquette-Choo et al., 2024] and is also motivated by communication-efficient high-dimensional private aggregation [Asi et al., 2025]. Existing analyses of this scheme either rely on privacy amplification by shuffling which leads to overly conservative bounds or require Monte Carlo simulations that are computationally prohibitive in most practical scenarios.
  We give the first theoretical guarantees and numerical estimation algorithms for this sampling scheme. In particular, we demonstrate that the privacy guarantees of random $k$-out-of-$t$ allocation can be upper bounded by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+o(1))k/t$. Further, we provide two additional analysis techniques that lead to numerical improvements in several parameter regimes. Altogether, our bounds give efficiently-computable and nearly tight numerical results for random allocation applied to Gaussian noise addition.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Total-Variance and Signal-to-Noise-Ratio Improves Diffusion Models</title>
<link>https://arxiv.org/abs/2502.08598</link>
<guid>https://arxiv.org/abs/2502.08598</guid>
<content:encoded><![CDATA[

arXiv:2502.08598v2 Announce Type: replace 
Abstract: The long sampling time of diffusion models remains a significant bottleneck, which can be mitigated by reducing the number of diffusion time steps. However, the quality of samples with fewer steps is highly dependent on the noise schedule, i.e., the specific manner in which noise is introduced and the signal is reduced at each step. Although prior work has improved upon the original variance-preserving and variance-exploding schedules, these approaches $\textit{passively}$ adjust the total variance, without direct control over it. In this work, we propose a novel total-variance/signal-to-noise-ratio disentangled (TV/SNR) framework, where TV and SNR can be controlled independently. Our approach reveals that schedules where the TV explodes exponentially can often be improved by adopting a constant TV schedule while preserving the same SNR schedule. Furthermore, generalizing the SNR schedule of the optimal transport flow matching significantly improves the generation performance. Our findings hold across various reverse diffusion solvers and diverse applications, including molecular structure and image generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative</title>
<link>https://arxiv.org/abs/2502.08942</link>
<guid>https://arxiv.org/abs/2502.08942</guid>
<content:encoded><![CDATA[

arXiv:2502.08942v2 Announce Type: replace 
Abstract: While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. With recent progress in large language models and time series learning, we revisit the integration of paired texts with time series through the Platonic Representation Hypothesis, which posits that representations of different modalities converge to shared spaces. In this context, we identify that time-series-paired texts may naturally exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance without modifying model architectures. Code available at https://github.com/iDEA-iSAIL-Lab-UIUC/TaTS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2502.09254</link>
<guid>https://arxiv.org/abs/2502.09254</guid>
<content:encoded><![CDATA[

arXiv:2502.09254v2 Announce Type: replace 
Abstract: Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Path Structural Encoding for Graph Transformers</title>
<link>https://arxiv.org/abs/2502.09365</link>
<guid>https://arxiv.org/abs/2502.09365</guid>
<content:encoded><![CDATA[

arXiv:2502.09365v2 Announce Type: replace 
Abstract: Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation</title>
<link>https://arxiv.org/abs/2502.10762</link>
<guid>https://arxiv.org/abs/2502.10762</guid>
<content:encoded><![CDATA[

arXiv:2502.10762v2 Announce Type: replace 
Abstract: User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[

arXiv:2502.11196v2 Announce Type: replace 
Abstract: Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Selection to Generation: A Survey of LLM-based Active Learning</title>
<link>https://arxiv.org/abs/2502.11767</link>
<guid>https://arxiv.org/abs/2502.11767</guid>
<content:encoded><![CDATA[

arXiv:2502.11767v2 Announce Type: replace 
Abstract: Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation</title>
<link>https://arxiv.org/abs/2502.13576</link>
<guid>https://arxiv.org/abs/2502.13576</guid>
<content:encoded><![CDATA[

arXiv:2502.13576v2 Announce Type: replace 
Abstract: Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning</title>
<link>https://arxiv.org/abs/2502.15296</link>
<guid>https://arxiv.org/abs/2502.15296</guid>
<content:encoded><![CDATA[

arXiv:2502.15296v2 Announce Type: replace 
Abstract: Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. We introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. We benchmark EVTSF performance using three real-world datasets and compare it against three potential solutions employing SOTA MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, particularly on expanding variables. Notably, STEV, with only 5% of observations from the expanding period, is on par with SOTA MTSF models trained with complete observations. Further exploration of various expanding strategies underscores the generalizability of STEV in real-world applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning</title>
<link>https://arxiv.org/abs/2502.15455</link>
<guid>https://arxiv.org/abs/2502.15455</guid>
<content:encoded><![CDATA[

arXiv:2502.15455v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) is computationally expensive, and Low-Rank Adaptation (LoRA) provides a cost-effective solution by approximating weight updates through low-rank matrices. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Dropout and Multi-Head Random Initialization, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Our approach not only improves performance in MTL but also reduces GPU memory usage and training time. Experiments show that R-LoRA's gains stem from increased diversity in the head matrices, demonstrating its effectiveness for multi-task learning. The code is available at https://github.com/jinda-liu/R-LoRA
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title>
<link>https://arxiv.org/abs/2502.15798</link>
<guid>https://arxiv.org/abs/2502.15798</guid>
<content:encoded><![CDATA[

arXiv:2502.15798v2 Announce Type: replace 
Abstract: Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS, consistently reducing overconfidence while preserving richer feature representations. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems</title>
<link>https://arxiv.org/abs/2502.17019</link>
<guid>https://arxiv.org/abs/2502.17019</guid>
<content:encoded><![CDATA[

arXiv:2502.17019v2 Announce Type: replace 
Abstract: Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, PDE solving, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[

arXiv:2502.18137v3 Announce Type: replace 
Abstract: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training</title>
<link>https://arxiv.org/abs/2502.19726</link>
<guid>https://arxiv.org/abs/2502.19726</guid>
<content:encoded><![CDATA[

arXiv:2502.19726v2 Announce Type: replace 
Abstract: Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose \methodname, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases</title>
<link>https://arxiv.org/abs/2502.20317</link>
<guid>https://arxiv.org/abs/2502.20317</guid>
<content:encoded><![CDATA[

arXiv:2502.20317v4 Announce Type: replace 
Abstract: Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach</title>
<link>https://arxiv.org/abs/2503.00206</link>
<guid>https://arxiv.org/abs/2503.00206</guid>
<content:encoded><![CDATA[

arXiv:2503.00206v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) methods frequently assume that each new observation completely reflects the environment's state, thereby guaranteeing Markovian (one-step) transitions. In practice, partial observability or sensor/actuator noise often invalidates this assumption. This paper proposes a systematic methodology for detecting such violations, combining a partial correlation-based causal discovery process (PCMCI) with a novel Markov Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property.
  Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even substantial observation noise sometimes fails to induce strong multi-lag dependencies in certain domains (e.g., Acrobot). In contrast, dimension-dropping investigations show that excluding some state variables (e.g., angular velocities in CartPole and Pendulum) significantly reduces returns and increases MVS, while removing other dimensions has minimal impact.
  These findings emphasize the importance of locating and safeguarding the most causally essential dimensions in order to preserve effective single-step learning. By integrating partial correlation tests with RL performance outcomes, the proposed approach precisely identifies when and where the Markov assumption is violated. This framework offers a principled mechanism for developing robust policies, informing representation learning, and addressing partial observability in real-world RL scenarios. All code and experimental logs are accessible for reproducibility (https://github.com/ucsb/markovianess).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[

arXiv:2503.01450v4 Announce Type: replace 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models</title>
<link>https://arxiv.org/abs/2503.01461</link>
<guid>https://arxiv.org/abs/2503.01461</guid>
<content:encoded><![CDATA[

arXiv:2503.01461v2 Announce Type: replace 
Abstract: Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems</title>
<link>https://arxiv.org/abs/2503.01891</link>
<guid>https://arxiv.org/abs/2503.01891</guid>
<content:encoded><![CDATA[

arXiv:2503.01891v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \textbf{63.77\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Randomness in Model and Data Partitioning for Privacy Amplification</title>
<link>https://arxiv.org/abs/2503.03043</link>
<guid>https://arxiv.org/abs/2503.03043</guid>
<content:encoded><![CDATA[

arXiv:2503.03043v2 Announce Type: replace 
Abstract: We study how inherent randomness in the training process -- where each sample (or client in federated learning) contributes only to a randomly selected portion of training -- can be leveraged for privacy amplification. This includes (1) data partitioning, where a sample participates in only a subset of training iterations, and (2) model partitioning, where a sample updates only a subset of the model parameters. We apply our framework to model parallelism in federated learning, where each client updates a randomly selected subnetwork to reduce memory and computational overhead, and show that existing methods, e.g. model splitting or dropout, provide a significant privacy amplification gain not captured by previous privacy analysis techniques. Additionally, we introduce Balanced Iteration Subsampling, a new data partitioning method where each sample (or client) participates in a fixed number of training iterations. We show that this method yields stronger privacy amplification than Poisson (i.i.d.) sampling of data (or clients). Our results demonstrate that randomness in the training process, which is structured rather than i.i.d. and interacts with data in complex ways, can be systematically leveraged for significant privacy amplification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Reliable Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2503.04363</link>
<guid>https://arxiv.org/abs/2503.04363</guid>
<content:encoded><![CDATA[

arXiv:2503.04363v2 Announce Type: replace 
Abstract: Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[

arXiv:2503.04992v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRACode: LoRA Adapters for Code Embeddings</title>
<link>https://arxiv.org/abs/2503.05315</link>
<guid>https://arxiv.org/abs/2503.05315</guid>
<content:encoded><![CDATA[

arXiv:2503.05315v2 Announce Type: replace 
Abstract: Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations. To foster research in this area, we make our code and pre-trained models publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOPO: Neural Combinatorial Optimization via Best-anchored and Objective-guided Preference Optimization</title>
<link>https://arxiv.org/abs/2503.07580</link>
<guid>https://arxiv.org/abs/2503.07580</guid>
<content:encoded><![CDATA[

arXiv:2503.07580v3 Announce Type: replace 
Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising approach for NP-hard problems. However, prevailing RL-based methods suffer from low sample efficiency due to sparse rewards and underused solutions. We propose Best-anchored and Objective-guided Preference Optimization (BOPO), a training paradigm that leverages solution preferences via objective values. It introduces: (1) a best-anchored preference pair construction for better explore and exploit solutions, and (2) an objective-guided pairwise loss function that adaptively scales gradients via objective differences, removing reliance on reward models or reference policies. Experiments on Job-shop Scheduling Problem (JSP), Traveling Salesman Problem (TSP), and Flexible Job-shop Scheduling Problem (FJSP) show BOPO outperforms state-of-the-art neural methods, reducing optimality gaps impressively with efficient inference. BOPO is architecture-agnostic, enabling seamless integration with existing NCO models, and establishes preference optimization as a principled framework for combinatorial optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability</title>
<link>https://arxiv.org/abs/2503.09532</link>
<guid>https://arxiv.org/abs/2503.09532</guid>
<content:encoded><![CDATA[

arXiv:2503.09532v3 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across eight diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across eight recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance. For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at: www.neuronpedia.org/sae-bench
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance</title>
<link>https://arxiv.org/abs/2503.13509</link>
<guid>https://arxiv.org/abs/2503.13509</guid>
<content:encoded><![CDATA[

arXiv:2503.13509v2 Announce Type: replace 
Abstract: We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection</title>
<link>https://arxiv.org/abs/2503.16072</link>
<guid>https://arxiv.org/abs/2503.16072</guid>
<content:encoded><![CDATA[

arXiv:2503.16072v2 Announce Type: replace 
Abstract: Most toxicity detection models treat toxicity as an intrinsic property of text, overlooking the role of context in shaping its impact. Drawing on interdisciplinary research, we reconceptualise toxicity as a socially emergent stress signal. We introduce a new framework for toxicity detection, including a formal definition and metric, and validate our approach on a novel dataset, demonstrating improved contextual sensitivity and adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</title>
<link>https://arxiv.org/abs/2504.00010</link>
<guid>https://arxiv.org/abs/2504.00010</guid>
<content:encoded><![CDATA[

arXiv:2504.00010v2 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present $\textbf{LayerCraft}$, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) $\textit{structured generation}$ from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) $\textit{layered object integration}$, allowing users to insert and customize objects -- such as characters or props -- across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the $\textbf{ChainArchitect}$ for CoT-driven layout planning, and the $\textbf{Object Integration Network (OIN)}$ for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released at https://github.com/PeterYYZhang/LayerCraft.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-guided Representation Learning for Multi-Label Recognition</title>
<link>https://arxiv.org/abs/2504.03801</link>
<guid>https://arxiv.org/abs/2504.03801</guid>
<content:encoded><![CDATA[

arXiv:2504.03801v2 Announce Type: replace 
Abstract: Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at https://github.com/MVL-Lab/SigRL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2504.04799</link>
<guid>https://arxiv.org/abs/2504.04799</guid>
<content:encoded><![CDATA[

arXiv:2504.04799v3 Announce Type: replace 
Abstract: Given two boundary distributions, the Schr\"odinger Bridge (SB) problem seeks the ``most likely`` random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the Topological Schr\"odinger Bridge problem (TSBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable topology-aware stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a closed-form topological SB (TSB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop TSB-based models for matching topological signals by parameterizing the unknowns in the optimal process as (topological) neural networks and learning them through likelihood training. We validate the theoretical results and demonstrate the practical applications of TSB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of TSB-based models to other emerging models, and outline future directions for topological signal matching.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An All-Atom Generative Model for Designing Protein Complexes</title>
<link>https://arxiv.org/abs/2504.13075</link>
<guid>https://arxiv.org/abs/2504.13075</guid>
<content:encoded><![CDATA[

arXiv:2504.13075v2 Announce Type: replace 
Abstract: Proteins typically exist in complexes, interacting with other proteins or biomolecules to perform their specific biological roles. Research on single-chain protein modeling has been extensively and deeply explored, with advancements seen in models like the series of ESM and AlphaFold2. Despite these developments, the study and modeling of multi-chain proteins remain largely uncharted, though they are vital for understanding biological functions. Recognizing the importance of these interactions, we introduce APM (All-Atom Protein Generative Model), a model specifically designed for modeling multi-chain proteins. By integrating atom-level information and leveraging data on multi-chain proteins, APM is capable of precisely modeling inter-chain interactions and designing protein complexes with binding capabilities from scratch. It also performs folding and inverse-folding tasks for multi-chain proteins. Moreover, APM demonstrates versatility in downstream applications: it achieves enhanced performance through supervised fine-tuning (SFT) while also supporting zero-shot sampling in certain tasks, achieving state-of-the-art results. We released our code at https://github.com/bytedance/apm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research</title>
<link>https://arxiv.org/abs/2504.13101</link>
<guid>https://arxiv.org/abs/2504.13101</guid>
<content:encoded><![CDATA[

arXiv:2504.13101v2 Announce Type: replace 
Abstract: Self-Supervised Learning (SSL) powers many current AI systems. As research interest and investment grow, the SSL design space continues to expand. The Platonic view of SSL, following the Platonic Representation Hypothesis (PRH), suggests that despite different methods and engineering approaches, all representations converge to the same Platonic ideal. However, this phenomenon lacks precise theoretical explanation. By synthesizing evidence from Identifiability Theory (IT), we show that the PRH can emerge in SSL. However, current IT cannot explain SSL's empirical success. To bridge the gap between theory and practice, we propose expanding IT into what we term Singular Identifiability Theory (SITh), a broader theoretical framework encompassing the entire SSL pipeline. SITh would allow deeper insights into the implicit data assumptions in SSL and advance the field towards learning more interpretable and generalizable representations. We highlight three critical directions for future research: 1) training dynamics and convergence properties of SSL; 2) the impact of finite samples, batch size, and data diversity; and 3) the role of inductive biases in architecture, augmentations, initialization schemes, and optimizers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17004</link>
<guid>https://arxiv.org/abs/2504.17004</guid>
<content:encoded><![CDATA[

arXiv:2504.17004v2 Announce Type: replace 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19583</link>
<guid>https://arxiv.org/abs/2504.19583</guid>
<content:encoded><![CDATA[

arXiv:2504.19583v2 Announce Type: replace 
Abstract: This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[

arXiv:2505.00812v3 Announce Type: replace 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2505.01199</link>
<guid>https://arxiv.org/abs/2505.01199</guid>
<content:encoded><![CDATA[

arXiv:2505.01199v2 Announce Type: replace 
Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in clinical diagnosis. However, analyzing these signals remains challenging: traditional methods rely on handcrafted features or supervised deep learning models that demand extensive labeled datasets, limiting their scalability and applicability. To address these issues, we propose CaReAQA, an audio-language model that integrates a foundation audio model with the reasoning capabilities of large language models, enabling clinically relevant, open-ended diagnostic responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of annotated medical audio recordings enriched with metadata and paired question-answer examples, intended to drive progress in diagnostic reasoning research. Evaluation results show that CaReAQA achieves 86.2% accuracy on open-ended diagnostic reasoning tasks, outperforming baseline models. It also generalizes well to closed-ended classification tasks, achieving an average accuracy of 56.9% on unseen datasets. Our findings show how audio-language integration and reasoning advances medical diagnostics, enabling efficient AI systems for clinical decision support.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[

arXiv:2505.01892v2 Announce Type: replace 
Abstract: With over 700 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.02515</link>
<guid>https://arxiv.org/abs/2505.02515</guid>
<content:encoded><![CDATA[

arXiv:2505.02515v2 Announce Type: replace 
Abstract: Traditional domain generalization approaches predominantly focus on leveraging target domain-aware features while overlooking the critical role of source domain-specific characteristics, particularly in federated settings with inherent data isolation. To address this gap, we propose the Federated Source Domain Awareness Framework (FedSDAF), the first method to systematically exploit source domain-aware features for enhanced federated domain generalization (FedDG). The FedSDAF framework consists of two synergistic components: the Domain-Invariant Adapter, which preserves critical domain-invariant features, and the Domain-Aware Adapter, which extracts and integrates source domain-specific knowledge using a Multihead Self-Attention mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge distillation mechanism that fosters knowledge sharing among clients while safeguarding privacy. Our approach represents the first systematic exploitation of source domain-aware features, resulting in significant advancements in model generalization capability.Extensive experiments on four standard benchmarks (OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently surpasses state-of-the-art federated domain generalization approaches, with accuracy gains of 5.2-13.8%. The source code is available at https://github.com/pizzareapers/FedSDAF.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales</title>
<link>https://arxiv.org/abs/2505.04608</link>
<guid>https://arxiv.org/abs/2505.04608</guid>
<content:encoded><![CDATA[

arXiv:2505.04608v3 Announce Type: replace 
Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but also continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Methods for nonparametric sequential testing -- especially conformal test martingales (CTMs) and anytime-valid inference -- offer promising tools for this monitoring task. However, existing approaches are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (e.g., detecting data shifts that violate certain exchangeability or IID assumptions), do not allow for online adaptation in response to shifts, and/or cannot diagnose the cause of degradation or alarm. In this paper, we address these limitations by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution), quickly detect harmful shifts, and diagnose those harmful shifts as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning</title>
<link>https://arxiv.org/abs/2505.05702</link>
<guid>https://arxiv.org/abs/2505.05702</guid>
<content:encoded><![CDATA[

arXiv:2505.05702v2 Announce Type: replace 
Abstract: The absence of intrinsic adjacency relations and orientation systems in hypergraphs creates fundamental challenges for constructing sheaf Laplacians of arbitrary degrees. We resolve these limitations through symmetric simplicial sets derived directly from hypergraphs, which encode all possible oriented subrelations within each hyperedge as ordered tuples. This construction canonically defines adjacency via facet maps while inherently preserving hyperedge provenance. We establish that the normalized degree zero sheaf Laplacian on our induced symmetric simplicial set reduces exactly to the traditional graph normalized sheaf Laplacian when restricted to graphs, validating its mathematical consistency with prior graph-based sheaf theory. Furthermore, the induced structure preserves all structural information from the original hypergraph, ensuring that every multi-way relational detail is faithfully retained. Leveraging this framework, we introduce Hypergraph Neural Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf Laplacians over symmetric simplicial sets, resolving orientation ambiguity and adjacency sparsity inherent to hypergraph learning. Experimental evaluations demonstrate HNSD's competitive performance across established benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance</title>
<link>https://arxiv.org/abs/2505.07004</link>
<guid>https://arxiv.org/abs/2505.07004</guid>
<content:encoded><![CDATA[

arXiv:2505.07004v2 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</title>
<link>https://arxiv.org/abs/2505.08740</link>
<guid>https://arxiv.org/abs/2505.08740</guid>
<content:encoded><![CDATA[

arXiv:2505.08740v3 Announce Type: replace 
Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</title>
<link>https://arxiv.org/abs/2505.10930</link>
<guid>https://arxiv.org/abs/2505.10930</guid>
<content:encoded><![CDATA[

arXiv:2505.10930v2 Announce Type: replace 
Abstract: Auto-regressive partial differential equation (PDE) foundation models have shown great potential in handling time-dependent data. However, these models suffer from the shortcut problem deeply rooted in auto-regressive prediction, causing error accumulation. The challenge becomes particularly evident for out-of-distribution data, as the pretraining performance may approach random model initialization for downstream tasks with long-term dynamics. To deal with this problem, we propose physics-informed temporal alignment (PITA), a self-supervised learning framework inspired by inverse problem solving. Specifically, PITA aligns the physical dynamics discovered at different time steps on each given PDE trajectory by integrating physics-informed constraints into the self-supervision signal. The alignment is derived from observation data without relying on known physics priors, indicating strong generalization ability to the out-of-distribution data. Extensive experiments show that PITA significantly enhances the accuracy and robustness of existing foundation models on diverse time-dependent PDE data. The code is available at https://github.com/SCAILab-USTC/PITA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2505.11197</link>
<guid>https://arxiv.org/abs/2505.11197</guid>
<content:encoded><![CDATA[

arXiv:2505.11197v2 Announce Type: replace 
Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial for understanding complex cellular processes and behavior. Existing methods leverage optimal transport, Schr\"odinger bridge theory, or their variants to simultaneously infer stochastic, unbalanced dynamics from snapshot data. However, these approaches remain limited in their ability to account for cell-cell interactions. This integration is essential in real-world scenarios since intercellular communications are fundamental life processes and can influence cell state-transition dynamics. To address this challenge, we formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to model unbalanced stochastic interaction dynamics from snapshot data. Inspired by this framework, we further propose CytoBridge, a deep learning algorithm designed to approximate the UMFSB problem. By explicitly modeling cellular transitions, proliferation, and interactions through neural networks, CytoBridge offers the flexibility to learn these processes directly from data. The effectiveness of our method has been extensively validated using both synthetic gene regulatory data and real scRNA-seq datasets. Compared to existing methods, CytoBridge identifies growth, transition, and interaction patterns, eliminates false transitions, and reconstructs the developmental landscape with greater accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Counterfactual Evidences for Node Classification</title>
<link>https://arxiv.org/abs/2505.11396</link>
<guid>https://arxiv.org/abs/2505.11396</guid>
<content:encoded><![CDATA[

arXiv:2505.11396v2 Announce Type: replace 
Abstract: Counterfactual learning is emerging as an important paradigm, rooted in causality, which promises to alleviate common issues of graph neural networks (GNNs), such as fairness and interpretability. However, as in many real-world application domains where conducting randomized controlled trials is impractical, one has to rely on available observational (factual) data to detect counterfactuals. In this paper, we introduce and tackle the problem of searching for counterfactual evidences for the GNN-based node classification task. A counterfactual evidence is a pair of nodes such that, regardless they exhibit great similarity both in the features and in their neighborhood subgraph structures, they are classified differently by the GNN. We develop effective and efficient search algorithms and a novel indexing solution that leverages both node features and structural information to identify counterfactual evidences, and generalizes beyond any specific GNN. Through various downstream applications, we demonstrate the potential of counterfactual evidences to enhance fairness and accuracy of GNNs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games</title>
<link>https://arxiv.org/abs/2505.16401</link>
<guid>https://arxiv.org/abs/2505.16401</guid>
<content:encoded><![CDATA[

arXiv:2505.16401v3 Announce Type: replace 
Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.17155</link>
<guid>https://arxiv.org/abs/2505.17155</guid>
<content:encoded><![CDATA[

arXiv:2505.17155v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu Pro MoE, Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[

arXiv:2505.17370v3 Announce Type: replace 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Approach for Child Reading Assessment in the Xhosa Language</title>
<link>https://arxiv.org/abs/2505.17371</link>
<guid>https://arxiv.org/abs/2505.17371</guid>
<content:encoded><![CDATA[

arXiv:2505.17371v2 Announce Type: replace 
Abstract: Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</title>
<link>https://arxiv.org/abs/2505.17662</link>
<guid>https://arxiv.org/abs/2505.17662</guid>
<content:encoded><![CDATA[

arXiv:2505.17662v2 Announce Type: replace 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Electron Cloud in Protein-Ligand Interaction Learning</title>
<link>https://arxiv.org/abs/2505.19014</link>
<guid>https://arxiv.org/abs/2505.19014</guid>
<content:encoded><![CDATA[

arXiv:2505.19014v2 Announce Type: replace 
Abstract: The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulation and signal transduction. Most deep-learning-based prediction approaches focus on structures of atoms or fragments. However, quantum chemical properties, such as electronic structures, are the key to unveiling interaction patterns but remain largely underexplored. To bridge this gap, we propose ECBind, a method for tokenizing electron cloud signals into quantized embeddings, enabling their integration into downstream tasks such as binding affinity prediction. By incorporating electron densities, ECBind helps uncover binding modes that cannot be fully represented by atom-level models. Specifically, to remove the redundancy inherent in electron cloud signals, a structure-aware transformer and hierarchical codebooks encode 3D binding sites enriched with electron structures into tokens. These tokenized codes are then used for specific tasks with labels. To extend its applicability to a wider range of scenarios, we utilize knowledge distillation to develop an electron-cloud-agnostic prediction model. Experimentally, ECBind demonstrates state-of-the-art performance across multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure Pearson and Spearman correlation coefficients, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Graph Learning Over Sets of Temporally-Sparse Data</title>
<link>https://arxiv.org/abs/2505.19193</link>
<guid>https://arxiv.org/abs/2505.19193</guid>
<content:encoded><![CDATA[

arXiv:2505.19193v2 Announce Type: replace 
Abstract: Real-world medical data often includes measurements from multiple signals that are collected at irregular and asynchronous time intervals. For example, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling of different attributes occur in other domains, such as monitoring of large systems using event log files or the spread of fake news on social networks. Effectively learning from such data requires models that can handle sets of temporally sparse and heterogeneous signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design model for learning over irregular sets of temporal signals. Our method achieves state-of-the-art performance in real-world medical tasks, including a 4-point increase in the AUROC score of in-hospital mortality prediction, compared to existing methods. We further showcase GMAN's flexibility by applying it to a fake news detection task. We demonstrate how its interpretability capabilities, including node-level, graph-level, and subset-level importance, allow for transition phases detection and gaining medical insights with real-world high-stakes implications. Finally, we provide theoretical insights on GMAN expressive power.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title>
<link>https://arxiv.org/abs/2505.19433</link>
<guid>https://arxiv.org/abs/2505.19433</guid>
<content:encoded><![CDATA[

arXiv:2505.19433v2 Announce Type: replace 
Abstract: Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On scalable and efficient training of diffusion samplers</title>
<link>https://arxiv.org/abs/2505.19552</link>
<guid>https://arxiv.org/abs/2505.19552</guid>
<content:encoded><![CDATA[

arXiv:2505.19552v2 Announce Type: replace 
Abstract: We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and the sampling space is high-dimensional. To address this limitation, we propose a scalable and sample-efficient framework that properly harmonizes the powerful classical sampling method and the diffusion sampler. Specifically, we utilize Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy as a Searcher to collect off-policy samples, using an auxiliary energy function to compensate for exploring modes the diffusion sampler rarely visits. These off-policy samples are then combined with on-policy data to train the diffusion sampler, thereby expanding its coverage of the energy landscape. Furthermore, we identify primacy bias, i.e., the preference of samplers for early experience during training, as the main cause of mode collapse during training, and introduce a periodic re-initialization trick to resolve this issue. Our method significantly improves sample efficiency on standard benchmarks for diffusion samplers and also excels at higher-dimensional problems and real-world molecular conformer generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling</title>
<link>https://arxiv.org/abs/2505.19669</link>
<guid>https://arxiv.org/abs/2505.19669</guid>
<content:encoded><![CDATA[

arXiv:2505.19669v2 Announce Type: replace 
Abstract: Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^\pi$-Realizable MDPs</title>
<link>https://arxiv.org/abs/2505.19946</link>
<guid>https://arxiv.org/abs/2505.19946</guid>
<content:encoded><![CDATA[

arXiv:2505.19946v2 Announce Type: replace 
Abstract: We study the problem of offline imitation learning in Markov decision processes (MDPs), where the goal is to learn a well-performing policy given a dataset of state-action pairs generated by an expert policy. Complementing a recent line of work on this topic that assumes the expert belongs to a tractable class of known policies, we approach this problem from a new angle and leverage a different type of structural assumption about the environment. Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a new algorithm called saddle-point offline imitation learning (\SPOIL), which is guaranteed to match the performance of any expert up to an additive error $\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover, we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$. Finally, our analysis suggests a new loss function for training critic networks from expert data in deep imitation learning. Empirical evaluations on standard benchmarks demonstrate that the neural net implementation of \SPOIL is superior to behavior cloning and competitive with state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[

arXiv:2505.20734v2 Announce Type: replace 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Transformers Learn Variable Binding in Symbolic Programs?</title>
<link>https://arxiv.org/abs/2505.20896</link>
<guid>https://arxiv.org/abs/2505.20896</guid>
<content:encoded><![CDATA[

arXiv:2505.20896v2 Announce Type: replace 
Abstract: Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches. To facilitate reproducible research, we developed Variable Scope, an interactive web platform for exploring our findings at https://variablescope.org
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Value-Function Uncertainties</title>
<link>https://arxiv.org/abs/2505.21119</link>
<guid>https://arxiv.org/abs/2505.21119</guid>
<content:encoded><![CDATA[

arXiv:2505.21119v2 Announce Type: replace 
Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution</title>
<link>https://arxiv.org/abs/2207.06418</link>
<guid>https://arxiv.org/abs/2207.06418</guid>
<content:encoded><![CDATA[

arXiv:2207.06418v2 Announce Type: replace-cross 
Abstract: Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the WorldStrat dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels and Sentinel2 imagery are CC BY, and the source code and pre-trained models under BSD. The dataset is available at https://zenodo.org/record/6810791 and the software package at https://github.com/worldstrat/worldstrat .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining marginal properness in the external validation of survival models with squared and logarithmic losses</title>
<link>https://arxiv.org/abs/2212.05260</link>
<guid>https://arxiv.org/abs/2212.05260</guid>
<content:encoded><![CDATA[

arXiv:2212.05260v3 Announce Type: replace-cross 
Abstract: Scoring rules promote rational and honest decision-making, which is important for model evaluation and becoming increasingly important for automated procedures such as `AutoML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis, with a focus on their theoretical and empirical properness. We introduce a marginal definition of properness and show that both the Integrated Survival Brier Score (ISBS) and the Right-Censored Log-Likelihood (RCLL) are theoretically improper under this definition. We also investigate a new class of losses that may inform future survival scoring rules. Simulation experiments reveal that both the ISBS and RCLL behave as proper scoring rules in practice. The RCLL showed no violations across all settings, while ISBS exhibited only minor, negligible violations at extremely small sample sizes, suggesting one can trust results from historical experiments. As such we advocate for both the RCLL and ISBS in external validation of models, including in automated procedures. However, we note practical challenges in estimating these losses including estimation of censoring distributions and densities; as such further research is required to advance development of robust and honest evaluation in survival analysis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel $\epsilon$-Greedy for Multi-Armed Bandits with Covariates</title>
<link>https://arxiv.org/abs/2306.17329</link>
<guid>https://arxiv.org/abs/2306.17329</guid>
<content:encoded><![CDATA[

arXiv:2306.17329v2 Announce Type: replace-cross 
Abstract: We consider the $\epsilon$-greedy strategy for the multi-arm bandit with covariates (MABC) problem, where the mean reward functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). We propose to estimate the unknown mean reward functions using an online weighted kernel ridge regression estimator, and show the resultant estimator to be consistent under appropriate decay rates of the exploration probability sequence, $\{\epsilon_t\}_t$, and regularization parameter, $\{\lambda_t\}_t$. Moreover, we show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Streaming of Large-Scale Medical Image Datasets for Deep Learning</title>
<link>https://arxiv.org/abs/2307.00438</link>
<guid>https://arxiv.org/abs/2307.00438</guid>
<content:encoded><![CDATA[

arXiv:2307.00438v3 Announce Type: replace-cross 
Abstract: Large-scale medical imaging datasets have accelerated deep learning (DL) for medical image analysis. However, the large scale of these datasets poses a challenge for researchers, resulting in increased storage and bandwidth requirements for hosting and accessing them. Since different researchers have different use cases and require different resolutions or formats for DL, it is neither feasible to anticipate every researcher's needs nor practical to store data in multiple resolutions and formats. To that end, we propose the Medical Image Streaming Toolkit (MIST), a format-agnostic database that enables streaming of medical images at different resolutions and formats from a single high-resolution copy. We evaluated MIST across eight popular, large-scale medical imaging datasets spanning different body parts, modalities, and formats. Our results showed that our framework reduced the storage and bandwidth requirements for hosting and downloading datasets without impacting image quality. We demonstrate that MIST addresses the challenges posed by large-scale medical imaging datasets by building a data-efficient and format-agnostic database to meet the diverse needs of researchers and reduce barriers to DL research in medical imaging.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Checking of AI-Generated Reports</title>
<link>https://arxiv.org/abs/2307.14634</link>
<guid>https://arxiv.org/abs/2307.14634</guid>
<content:encoded><![CDATA[

arXiv:2307.14634v2 Announce Type: replace-cross 
Abstract: With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resampled Confidence Regions with Exponential Shrinkage for the Regression Function of Binary Classification</title>
<link>https://arxiv.org/abs/2308.01835</link>
<guid>https://arxiv.org/abs/2308.01835</guid>
<content:encoded><![CDATA[

arXiv:2308.01835v2 Announce Type: replace-cross 
Abstract: The regression function is one of the key objects of binary classification, since it not only determines a Bayes optimal classifier, hence, defines an optimal decision boundary, but also encodes the conditional distribution of the output given the input. In this paper we build distribution-free confidence regions for the regression function for any user-chosen confidence level and any finite sample size based on a resampling test. These regions are abstract, as the model class can be almost arbitrary, e.g., it does not have to be finitely parameterized. We prove the strong uniform consistency of a new empirical risk minimization based approach for model classes with finite pseudo-dimensions and inverse Lipschitz parameterizations. We provide exponential probably approximately correct bounds on the $L_2$ sizes of these regions, and demonstrate the ideas on specific models. Additionally, we also consider a k-nearest neighbors based method, for which we prove strong pointwise bounds on the probability of exclusion. Finally, the constructions are illustrated on a logistic model class and compared to the asymptotic ellipsoids of the maximum likelihood estimator.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Differential Operators for Hybrid Neural Fields</title>
<link>https://arxiv.org/abs/2312.05984</link>
<guid>https://arxiv.org/abs/2312.05984</guid>
<content:encoded><![CDATA[

arXiv:2312.05984v2 Announce Type: replace-cross 
Abstract: Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield accurate spatial derivatives needed for these downstream applications. In this work, we propose two ways to circumvent these challenges. Our first approach is a post hoc operator that uses local polynomial fitting to obtain more accurate derivatives from pre-trained hybrid neural fields. Additionally, we also propose a self-supervised fine-tuning approach that refines the hybrid neural field to yield accurate derivatives directly while preserving the initial signal. We show applications of our method to rendering, collision simulation, and solving PDEs. We observe that using our approach yields more accurate derivatives, reducing artifacts and leading to more accurate simulations in downstream applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Meta-Prompting</title>
<link>https://arxiv.org/abs/2312.06562</link>
<guid>https://arxiv.org/abs/2312.06562</guid>
<content:encoded><![CDATA[

arXiv:2312.06562v3 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You</title>
<link>https://arxiv.org/abs/2401.16092</link>
<guid>https://arxiv.org/abs/2401.16092</guid>
<content:encoded><![CDATA[

arXiv:2401.16092v4 Announce Type: replace-cross 
Abstract: Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment, and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this technology. However, our results show that multilingual models suffer from significant gender biases just as monolingual models do. Furthermore, the natural expectation that multilingual models will provide similar results across languages does not hold up. Instead, there are important differences between languages. We propose a novel benchmark, MAGBIG, intended to foster research on gender bias in multilingual models. We use MAGBIG to investigate the effect of multilingualism on gender bias in T2I models. To this end, we construct multilingual prompts requesting portraits of people with a certain occupation or trait. Our results show that not only do models exhibit strong gender biases but they also behave differently across languages. Furthermore, we investigate prompt engineering strategies, such as indirect, neutral formulations, to mitigate these biases. Unfortunately, these approaches have limited success and result in worse text-to-image alignment. Consequently, we call for more research into diverse representations across languages in image generators, as well as into steerability to address biased model behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation</title>
<link>https://arxiv.org/abs/2402.07723</link>
<guid>https://arxiv.org/abs/2402.07723</guid>
<content:encoded><![CDATA[

arXiv:2402.07723v3 Announce Type: replace-cross 
Abstract: Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. We support our theory with experiments conducted in a variety of settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbative partial moment matching and gradient-flow adaptive importance sampling transformations for Bayesian leave one out cross-validation</title>
<link>https://arxiv.org/abs/2402.08151</link>
<guid>https://arxiv.org/abs/2402.08151</guid>
<content:encoded><![CDATA[

arXiv:2402.08151v3 Announce Type: replace-cross 
Abstract: Importance sampling (IS) allows one to approximate leave one out (LOO) cross-validation for a Bayesian model, without refitting, by inverting the Bayesian update equation to subtract a given data point from a model posterior. For each data point, one computes expectations under the corresponding LOO posterior by weighted averaging over the full data posterior. This task sometimes requires weight stabilization in the form of adapting the posterior distribution via transformation. So long as one is successful in finding a suitable transformation, one avoids refitting. To this end, we motivate the use of bijective perturbative transformations of the form $T(\boldsymbol{\theta})=\boldsymbol{\theta} + h Q(\boldsymbol{\theta}),$ for $0<h\ll 1,$ and introduce two classes of such transformations: 1) partial moment matching and 2) gradient flow evolution. The former extends prior literature on moment-matching under the recognition that adaptation for LOO is a small perturbation on the full data posterior. The latter class of methods define transformations based on relaxing various statistical objectives: in our case the variance of the IS estimator and the KL divergence between the transformed distribution and the statistics of the LOO fold. Being model-specific, the gradient flow transformations require evaluating Jacobian determinants. While these quantities are generally readily available through auto-differentiation, we derive closed-form expressions in the case of logistic regression and shallow ReLU activated neural networks. We tested the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Control of Linear Systems under Unbounded Noise</title>
<link>https://arxiv.org/abs/2402.10252</link>
<guid>https://arxiv.org/abs/2402.10252</guid>
<content:encoded><![CDATA[

arXiv:2402.10252v2 Announce Type: replace-cross 
Abstract: This paper investigates the problem of controlling a linear system under possibly unbounded stochastic noise with unknown convex cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we show that an $ \tilde{O}(\sqrt{T}) $ high-probability regret can be achieved under unbounded noise, where $ T $ denotes the time horizon. Notably, the noise is only required to have a finite fourth moment. Moreover, when the costs are strongly convex and the noise is sub-Gaussian, we establish an $ O({\rm poly} (\log T)) $ regret bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Robustness to Clean-Label Poisoning Using Diffusion Denoising</title>
<link>https://arxiv.org/abs/2403.11981</link>
<guid>https://arxiv.org/abs/2403.11981</guid>
<content:encoded><![CDATA[

arXiv:2403.11981v2 Announce Type: replace-cross 
Abstract: We present a certified defense to clean-label poisoning attacks under $\ell_2$-norm. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $randomized$ $smoothing$, we show how an off-the-shelf diffusion denoising model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks in both $\ell_2$ and $\ell_{\infty}$-norms and reduce their attack success to 0-16% with only a negligible drop in the test accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs</title>
<link>https://arxiv.org/abs/2404.10304</link>
<guid>https://arxiv.org/abs/2404.10304</guid>
<content:encoded><![CDATA[

arXiv:2404.10304v2 Announce Type: replace-cross 
Abstract: Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in software testing. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80x, 2.65x, and 1.66x those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions</title>
<link>https://arxiv.org/abs/2405.03205</link>
<guid>https://arxiv.org/abs/2405.03205</guid>
<content:encoded><![CDATA[

arXiv:2405.03205v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the "logit lens" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Accuracy in Generative Models via Knowledge Transfer</title>
<link>https://arxiv.org/abs/2405.16837</link>
<guid>https://arxiv.org/abs/2405.16837</guid>
<content:encoded><![CDATA[

arXiv:2405.16837v3 Announce Type: replace-cross 
Abstract: This paper investigates the accuracy of generative models and the impact of knowledge transfer on their generation precision. Specifically, we examine a generative model for a target task, fine-tuned using a pre-trained model from a source task. Building on the "Shared Embedding" concept, which bridges the source and target tasks, we introduce a novel framework for transfer learning under distribution metrics such as the Kullback-Leibler divergence. This framework underscores the importance of leveraging inherent similarities between diverse tasks despite their distinct data distributions. Our theory suggests that the shared structures can augment the generation accuracy for a target task, reliant on the capability of a source model to identify shared structures and effective knowledge transfer from source to target learning. To demonstrate the practical utility of this framework, we explore the theoretical implications for two specific generative models: diffusion and normalizing flows. The results show enhanced performance in both models over their non-transfer counterparts, indicating advancements for diffusion models and providing fresh insights into normalizing flows in transfer and non-transfer settings. These results highlight the significant contribution of knowledge transfer in boosting the generation capabilities of these models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine</title>
<link>https://arxiv.org/abs/2406.02394</link>
<guid>https://arxiv.org/abs/2406.02394</guid>
<content:encoded><![CDATA[

arXiv:2406.02394v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Low Rank Neural Representation of Entropy Solutions</title>
<link>https://arxiv.org/abs/2406.05694</link>
<guid>https://arxiv.org/abs/2406.05694</guid>
<content:encoded><![CDATA[

arXiv:2406.05694v2 Announce Type: replace-cross 
Abstract: We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension. The representation is a generalization of the method of characteristics and posseses a compositional form. While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear. This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure. Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2406.11753</link>
<guid>https://arxiv.org/abs/2406.11753</guid>
<content:encoded><![CDATA[

arXiv:2406.11753v3 Announce Type: replace-cross 
Abstract: Finetuning language models (LMs) is crucial for adapting the models to downstream data and tasks. However, full finetuning is usually costly. Existing work, such as parameter-efficient finetuning (PEFT), often focuses on \textit{how to finetune} but neglects the issue of \textit{where to finetune}. As a pioneering work on reducing the cost of backpropagation (at the layer level) by answering where to finetune, we conduct a semantic analysis of the LM inference process. We first propose using transition traces of the latent representation to compute deviations (or loss). Then, using a derived formula of scaling law, we estimate the gain of each layer in reducing deviation (or loss). Further, we narrow down the scope for finetuning, and also, study the cost-benefit balance of LM finetuning. We perform extensive experiments across well-known LMs and datasets. The results show that our approach is effective and efficient, and outperforms the existing baselines. Our approach is orthogonal to other techniques for improving finetuning efficiency, such as PEFT methods, offering practical values on LM finetuning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks</title>
<link>https://arxiv.org/abs/2406.13945</link>
<guid>https://arxiv.org/abs/2406.13945</guid>
<content:encoded><![CDATA[

arXiv:2406.13945v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \textit{CityData} to integrate the diverse urban data and \textit{CitySimu} to simulate fine-grained urban dynamics. Based on \textit{CityData} and \textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityGPT: Empowering Urban Spatial Cognition of Large Language Models</title>
<link>https://arxiv.org/abs/2406.13948</link>
<guid>https://arxiv.org/abs/2406.13948</guid>
<content:encoded><![CDATA[

arXiv:2406.13948v2 Announce Type: replace-cross 
Abstract: Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit{CityInstruction} by \textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit{CityEval}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Foundation Models with Black-Box Visual Prompting</title>
<link>https://arxiv.org/abs/2407.17491</link>
<guid>https://arxiv.org/abs/2407.17491</guid>
<content:encoded><![CDATA[

arXiv:2407.17491v2 Announce Type: replace-cross 
Abstract: With a surge of large-scale pre-trained models, parameter-efficient transfer learning (PETL) of large models has garnered significant attention. While promising, they commonly rely on two optimistic assumptions: 1) full access to the parameters of a PTM, and 2) sufficient memory capacity to cache all intermediate activations for gradient computation. However, in most real-world applications, PTMs serve as black-box APIs or proprietary software without full parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. This work proposes black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge of their architectures or parameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs input-dependent visual prompts, which allow the target PTM to adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to update Coordinator. Besides, we introduce a variant, BlackVIP-SE, which significantly reduces the runtime and computational cost of BlackVIP. Extensive experiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation to diverse domains and tasks with minimal memory requirements. We further provide a theoretical analysis on the generalization of visual prompting methods by presenting their connection to the certified robustness of randomized smoothing, and presenting an empirical support for improved robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and cross validation in ridge regression with correlated samples</title>
<link>https://arxiv.org/abs/2408.04607</link>
<guid>https://arxiv.org/abs/2408.04607</guid>
<content:encoded><![CDATA[

arXiv:2408.04607v4 Announce Type: replace-cross 
Abstract: Recent years have seen substantial advances in our understanding of high-dimensional ridge regression, but existing theories assume that training examples are independent. By leveraging techniques from random matrix theory and free probability, we provide sharp asymptotics for the in- and out-of-sample risks of ridge regression when the data points have arbitrary correlations. We demonstrate that in this setting, the generalized cross validation estimator (GCV) fails to correctly predict the out-of-sample risk. However, in the case where the noise residuals have the same correlations as the data points, one can modify the GCV to yield an efficiently-computable unbiased estimator that concentrates in the high-dimensional limit, which we dub CorrGCV. We further extend our asymptotic analysis to the case where the test point has nontrivial correlations with the training set, a setting often encountered in time series forecasting. Assuming knowledge of the correlation structure of the time series, this again yields an extension of the GCV estimator, and sharply characterizes the degree to which such test points yield an overly optimistic prediction of long-time risk. We validate the predictions of our theory across a variety of high dimensional data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADetBench: Towards Benchmarking Physical Attacks against Object Detection</title>
<link>https://arxiv.org/abs/2408.09181</link>
<guid>https://arxiv.org/abs/2408.09181</guid>
<content:encoded><![CDATA[

arXiv:2408.09181v3 Announce Type: replace-cross 
Abstract: Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing realistic simulation to thoroughly and rigorously benchmark physical attacks with fairness under controlled physical dynamics and cross-domain transformation. This resolves the problem of capturing identical adversarial images that cannot be achieved in the real world. Our benchmark includes 20 physical attack methods, 48 object detectors, comprehensive physical dynamics, and evaluation metrics. We also provide end-to-end pipelines for dataset generation, detection, evaluation, and further analysis. In addition, we perform 8064 groups of evaluation based on our benchmark, which includes both overall evaluation and further detailed ablation studies for controlled physical dynamics. Through these experiments, we provide in-depth analyses of physical attack performance and physical adversarial robustness, draw valuable observations, and discuss potential directions for future research.
  Codebase: https://github.com/JiaweiLian/Benchmarking_Physical_Attack
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANVIL: Anomaly-based Vulnerability Identification without Labelled Training Data</title>
<link>https://arxiv.org/abs/2408.16028</link>
<guid>https://arxiv.org/abs/2408.16028</guid>
<content:encoded><![CDATA[

arXiv:2408.16028v3 Announce Type: replace-cross 
Abstract: Supervised-learning-based vulnerability detectors often fall short due to limited labelled training data. In contrast, Large Language Models (LLMs) like GPT-4 are trained on vast unlabelled code corpora, yet perform only marginally better than coin flips when directly prompted to detect vulnerabilities. In this paper, we reframe vulnerability detection as anomaly detection, based on the premise that vulnerable code is rare and thus anomalous relative to patterns learned by LLMs. We introduce ANVIL, which performs a masked code reconstruction task: the LLM reconstructs a masked line of code, and deviations from the original are scored as anomalies. We propose a hybrid anomaly score that combines exact match, cross-entropy loss, prediction confidence, and structural complexity. We evaluate our approach across multiple LLM families, scoring methods, and context sizes, and against vulnerabilities after the LLM's training cut-off. On the PrimeVul dataset, ANVIL outperforms state-of-the-art supervised detectors-LineVul, LineVD, and LLMAO-achieving up to 2x higher Top-3 accuracy, 75% better Normalized MFR, and a significant improvement on ROC-AUC. Finally, by integrating ANVIL with fuzzers, we uncover two previously unknown vulnerabilities, demonstrating the practical utility of anomaly-guided detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Privacy Leakage in Dimensionality Reduction Methods via Reconstruction Attack</title>
<link>https://arxiv.org/abs/2408.17151</link>
<guid>https://arxiv.org/abs/2408.17151</guid>
<content:encoded><![CDATA[

arXiv:2408.17151v3 Announce Type: replace-cross 
Abstract: This study investigates privacy leakage in dimensionality reduction methods through a novel machine learning-based reconstruction attack. Employing an informed adversary threat model, we develop a neural network capable of reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: principal component analysis (PCA), sparse random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-distributed stochastic neighbor embedding ($t$-SNE), and uniform manifold approximation and projection (UMAP). Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative analysis to identify key factors affecting reconstruction quality. Furthermore, we assess the effectiveness of an additive noise mechanism in mitigating these reconstruction attacks. Our experimental results on both datasets reveal that the attack is effective against deterministic methods (PCA and Isomap). but ineffective against methods that employ random initialization (SRP, MDS, $t$-SNE and UMAP). The experimental results also show that, for PCA and Isomap, our reconstruction network produces higher quality outputs compared to a previously proposed network.
  We also study the effect of additive noise mechanism to prevent the reconstruction attack. Our experiment shows that, when adding the images with large noises before performing PCA or Isomap, the attack produced severely distorted reconstructions. In contrast, for the other four methods, the reconstructions still show some recognizable features, though they bear little resemblance to the original images. The code is available at https://github.com/Chayadon/Reconstruction_attack_on_DR
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content Moderation by LLM: From Accuracy to Legitimacy</title>
<link>https://arxiv.org/abs/2409.03219</link>
<guid>https://arxiv.org/abs/2409.03219</guid>
<content:encoded><![CDATA[

arXiv:2409.03219v2 Announce Type: replace-cross 
Abstract: One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy -- the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs' real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs' role in content moderation and redirect relevant research in this field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>View-Invariant Policy Learning via Zero-Shot Novel View Synthesis</title>
<link>https://arxiv.org/abs/2409.03685</link>
<guid>https://arxiv.org/abs/2409.03685</guid>
<content:encoded><![CDATA[

arXiv:2409.03685v3 Announce Type: replace-cross 
Abstract: Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks</title>
<link>https://arxiv.org/abs/2409.04459</link>
<guid>https://arxiv.org/abs/2409.04459</guid>
<content:encoded><![CDATA[

arXiv:2409.04459v2 Announce Type: replace-cross 
Abstract: Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs</title>
<link>https://arxiv.org/abs/2409.05806</link>
<guid>https://arxiv.org/abs/2409.05806</guid>
<content:encoded><![CDATA[

arXiv:2409.05806v4 Announce Type: replace-cross 
Abstract: Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title>
<link>https://arxiv.org/abs/2409.10289</link>
<guid>https://arxiv.org/abs/2409.10289</guid>
<content:encoded><![CDATA[

arXiv:2409.10289v4 Announce Type: replace-cross 
Abstract: Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach</title>
<link>https://arxiv.org/abs/2409.19458</link>
<guid>https://arxiv.org/abs/2409.19458</guid>
<content:encoded><![CDATA[

arXiv:2409.19458v3 Announce Type: replace-cross 
Abstract: We study the problem of fine-tuning a language model (LM) for a target task by optimally using the information from $n$ auxiliary tasks. This problem has broad applications in NLP, such as targeted instruction tuning and data selection in chain-of-thought fine-tuning. The key challenge of this problem is that not all auxiliary tasks are beneficial in improving the performance of the target task. Thus, selecting the right subset of auxiliary tasks is crucial. Conventional subset selection methods, such as forward and backward stepwise selection, are unsuitable for LM fine-tuning because they require repeated training on subsets of auxiliary tasks. This paper introduces a new algorithm for estimating model fine-tuning performance without requiring repeated training. Our algorithm first performs multitask training using data from all tasks to obtain a meta initialization. Then, we approximate the model fine-tuning loss of a subset using functional values and gradients from the meta initialization. Empirically, we find that this gradient-based approximation holds with remarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate fine-tuning performances on CPUs within a few seconds. Finally, we fine-tune the pretrained base model once on the selected subset of tasks. We conduct extensive experiments to validate this approach, delivering a speedup of $30\times$ over conventional subset selection while incurring only $1\%$ error of the true fine-tuning performances. In downstream evaluations involving both instruction tuning and chain-of-thought fine-tuning, this loss-based selection approach improves over prior gradient or representation similarity-based methods for subset selection by up to $3.8\%$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title>
<link>https://arxiv.org/abs/2410.03026</link>
<guid>https://arxiv.org/abs/2410.03026</guid>
<content:encoded><![CDATA[

arXiv:2410.03026v2 Announce Type: replace-cross 
Abstract: Language models (LMs) rely on their parametric knowledge augmented with relevant contextual knowledge for certain tasks, such as question answering. However, the contextual knowledge can contain private information that may be leaked when answering queries, and estimating this privacy leakage is not well understood. A straightforward approach of directly comparing an LM's output to the contexts can overestimate the privacy risk, since the LM's parametric knowledge might already contain the augmented contextual knowledge. To this end, we introduce $\emph{context influence}$, a metric that builds on differential privacy, a widely-adopted privacy notion, to estimate the privacy leakage of contextual knowledge during decoding. Our approach effectively measures how each subset of the context influences an LM's response while separating the specific parametric knowledge of the LM. Using our context influence metric, we demonstrate that context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge. Moreover, we experimentally demonstrate how context influence properly attributes the privacy leakage to augmented contexts, and we evaluate how factors-- such as model size, context size, generation position, etc.-- affect context privacy leakage. The practical implications of our results will inform practitioners of the privacy risk associated with augmented contextual knowledge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title>
<link>https://arxiv.org/abs/2410.07176</link>
<guid>https://arxiv.org/abs/2410.07176</guid>
<content:encoded><![CDATA[

arXiv:2410.07176v2 Announce Type: replace-cross 
Abstract: Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees</title>
<link>https://arxiv.org/abs/2410.08934</link>
<guid>https://arxiv.org/abs/2410.08934</guid>
<content:encoded><![CDATA[

arXiv:2410.08934v4 Announce Type: replace-cross 
Abstract: Personalized federated learning (PFL) offers a flexible framework for aggregating information across distributed clients with heterogeneous data. This work considers a personalized federated learning setting that simultaneously learns global and local models. While purely local training has no communication cost, collaborative learning among the clients can leverage shared knowledge to improve statistical accuracy, presenting an accuracy-communication trade-off in personalized federated learning. However, the theoretical analysis of how personalization quantitatively influences sample and algorithmic efficiency and their inherent trade-off is largely unexplored. This paper makes a contribution towards filling this gap, by providing a quantitative characterization of the personalization degree on the tradeoff. The results further offers theoretical insights for choosing the personalization degree. As a side contribution, we establish the minimax optimality in terms of statistical accuracy for a widely studied PFL formulation. The theoretical result is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-light Uncertainty Set Merging with Admissibility: Synthetics, Aggregation, and Test Inversion</title>
<link>https://arxiv.org/abs/2410.12201</link>
<guid>https://arxiv.org/abs/2410.12201</guid>
<content:encoded><![CDATA[

arXiv:2410.12201v2 Announce Type: replace-cross 
Abstract: This article introduces a Synthetics, Aggregation, and Test inversion (SAT) approach for merging diverse and potentially dependent uncertainty sets into a single unified set. The procedure is data-light, relying only on initial sets and control levels, and it adapts to any user-specified initial uncertainty sets, accommodating potentially varying coverage levels. SAT is motivated by the challenge of integrating uncertainty sets when only the initial sets and their control levels are available - for example, when merging confidence sets from distributed sites under communication constraints or combining conformal prediction sets generated by different algorithms or data splits. To address this, SAT constructs and aggregates novel synthetic test statistics, and then derive merged sets through test inversion. Our method leverages the duality between set estimation and hypothesis testing, ensuring reliable coverage in dependent scenarios. A key theoretical contribution is a rigorous analysis of SAT's properties, including a proof of its admissibility in the context of deterministic set merging. Both theoretical analyses and empirical results confirm the method's finite-sample coverage validity and desirable set sizes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Model Kinship for Merging Large Language Models</title>
<link>https://arxiv.org/abs/2410.12613</link>
<guid>https://arxiv.org/abs/2410.12613</guid>
<content:encoded><![CDATA[

arXiv:2410.12613v2 Announce Type: replace-cross 
Abstract: Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[

arXiv:2410.13098v2 Announce Type: replace-cross 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised contrastive learning performs non-linear system identification</title>
<link>https://arxiv.org/abs/2410.14673</link>
<guid>https://arxiv.org/abs/2410.14673</guid>
<content:encoded><![CDATA[

arXiv:2410.14673v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models</title>
<link>https://arxiv.org/abs/2410.20445</link>
<guid>https://arxiv.org/abs/2410.20445</guid>
<content:encoded><![CDATA[

arXiv:2410.20445v2 Announce Type: replace-cross 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning</title>
<link>https://arxiv.org/abs/2410.21119</link>
<guid>https://arxiv.org/abs/2410.21119</guid>
<content:encoded><![CDATA[

arXiv:2410.21119v3 Announce Type: replace-cross 
Abstract: One-Shot Federated Learning (OSFL) restricts communication between the server and clients to a single round, significantly reducing communication costs and minimizing privacy leakage risks compared to traditional Federated Learning (FL), which requires multiple rounds of communication. However, existing OSFL frameworks remain vulnerable to distributional heterogeneity, as they primarily focus on model heterogeneity while neglecting data heterogeneity. To bridge this gap, we propose FedHydra, a unified, data-free, OSFL framework designed to effectively address both model and data heterogeneity. Unlike existing OSFL approaches, FedHydra introduces a novel two-stage learning mechanism. Specifically, it incorporates model stratification and heterogeneity-aware stratified aggregation to mitigate the challenges posed by both model and data heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with five SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous OSFL methods in both homogeneous and heterogeneous settings. The code is available at https://github.com/Jun-B0518/FedHydra.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</title>
<link>https://arxiv.org/abs/2411.00355</link>
<guid>https://arxiv.org/abs/2411.00355</guid>
<content:encoded><![CDATA[

arXiv:2411.00355v2 Announce Type: replace-cross 
Abstract: In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowCoder-X: Boosting Multilingual Information Extraction via Code</title>
<link>https://arxiv.org/abs/2411.04794</link>
<guid>https://arxiv.org/abs/2411.04794</guid>
<content:encoded><![CDATA[

arXiv:2411.04794v3 Announce Type: replace-cross 
Abstract: Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactLens: Benchmarking Fine-Grained Fact Verification</title>
<link>https://arxiv.org/abs/2411.05980</link>
<guid>https://arxiv.org/abs/2411.05980</guid>
<content:encoded><![CDATA[

arXiv:2411.05980v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2411.18688</link>
<guid>https://arxiv.org/abs/2411.18688</guid>
<content:encoded><![CDATA[

arXiv:2411.18688v4 Announce Type: replace-cross 
Abstract: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Operators for Predictor Feedback Control of Nonlinear Delay Systems</title>
<link>https://arxiv.org/abs/2411.18964</link>
<guid>https://arxiv.org/abs/2411.18964</guid>
<content:encoded><![CDATA[

arXiv:2411.18964v3 Announce Type: replace-cross 
Abstract: Predictor feedback designs are critical for delay-compensating controllers in nonlinear systems. However, these designs are limited in practical applications as predictors cannot be directly implemented, but require numerical approximation schemes, which become computationally prohibitive when system dynamics are expensive to compute. To address this challenge, we recast the predictor design as an operator learning problem, and learn the predictor mapping via a neural operator. We prove the existence of an arbitrarily accurate neural operator approximation of the predictor operator. Under the approximated predictor, we achieve semiglobal practical stability of the closed-loop nonlinear delay system. The estimate is semiglobal in a unique sense - one can enlarge the set of initial states as desired, though this increases the difficulty of training a neural operator, which appears practically in the stability estimate. Furthermore, our analysis holds for any black-box predictor satisfying the universal approximation error bound. We demonstrate the approach by controlling a 5-link robotic manipulator with different neural operator models, achieving significant speedups compared to classic predictor feedback schemes while maintaining closed-loop stability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Consistent $k$-Center Clustering with Optimal Recourse</title>
<link>https://arxiv.org/abs/2412.03238</link>
<guid>https://arxiv.org/abs/2412.03238</guid>
<content:encoded><![CDATA[

arXiv:2412.03238v2 Announce Type: replace-cross 
Abstract: Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem? This question has received attention in recent years under the name consistent clustering.
  Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson [SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions. In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion. Before our work, {\L}\k{a}cki, Haeupler, Grunau, Rozho\v{n}, and Jayaram [SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.
  In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update. Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure. Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update. Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani [STOC '97]. Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Samudra: An AI Global Ocean Emulator for Climate</title>
<link>https://arxiv.org/abs/2412.03795</link>
<guid>https://arxiv.org/abs/2412.03795</guid>
<content:encoded><![CDATA[

arXiv:2412.03795v4 Announce Type: replace-cross 
Abstract: AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long climate simulations with skill across a range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multi-depth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remain stable, requiring further work.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Sample Generation of Diffusion Models using Noise Level Correction</title>
<link>https://arxiv.org/abs/2412.05488</link>
<guid>https://arxiv.org/abs/2412.05488</guid>
<content:encoded><![CDATA[

arXiv:2412.05488v3 Announce Type: replace-cross 
Abstract: The denoising process of diffusion models can be interpreted as an approximate projection of noisy samples onto the data manifold. Moreover, the noise level in these samples approximates their distance to the underlying manifold. Building on this insight, we propose a novel method to enhance sample generation by aligning the estimated noise level with the true distance of noisy samples to the manifold. Specifically, we introduce a noise level correction network, leveraging a pre-trained denoising network, to refine noise level estimates during the denoising process. Additionally, we extend this approach to various image restoration tasks by integrating task-specific constraints, including inpainting, deblurring, super-resolution, colorization, and compressed sensing. Experimental results demonstrate that our method significantly improves sample quality in both unconstrained and constrained generation scenarios. Notably, the proposed noise level correction framework is compatible with existing denoising schedulers (e.g., DDIM), offering additional performance improvements.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLZero: Direct Policy Inference from Language Without In-Domain Supervision</title>
<link>https://arxiv.org/abs/2412.05718</link>
<guid>https://arxiv.org/abs/2412.05718</guid>
<content:encoded><![CDATA[

arXiv:2412.05718v2 Announce Type: replace-cross 
Abstract: The reward hypothesis states that all goals and purposes can be understood as the maximization of a received scalar reward signal. However, in practice, defining such a reward signal is notoriously difficult, as humans are often unable to predict the optimal behavior corresponding to a reward function. Natural language offers an intuitive alternative for instructing reinforcement learning (RL) agents, yet previous language-conditioned approaches either require costly supervision or test-time training given a language instruction. In this work, we present a new approach that uses a pretrained RL agent trained using only unlabeled, offline interactions--without task-specific supervision or labeled trajectories--to get zero-shot test-time policy inference from arbitrary natural language instructions. We introduce a framework comprising three steps: imagine, project, and imitate. First, the agent imagines a sequence of observations corresponding to the provided language description using video generative models. Next, these imagined observations are projected into the target environment domain. Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution. To the best of our knowledge, our method, RLZero, is the first approach to show direct language-to-behavior generation abilities on a variety of tasks and environments without any in-domain supervision. We further show that components of RLZero can be used to generate policies zero-shot from cross-embodied videos, such as those available on YouTube, even for complex embodiments like humanoids.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dark side of the forces: assessing non-conservative force models for atomistic machine learning</title>
<link>https://arxiv.org/abs/2412.11569</link>
<guid>https://arxiv.org/abs/2412.11569</guid>
<content:encoded><![CDATA[

arXiv:2412.11569v4 Announce Type: replace-cross 
Abstract: The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, has revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training. This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for. The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Bayesian deep reinforcement learning</title>
<link>https://arxiv.org/abs/2412.11743</link>
<guid>https://arxiv.org/abs/2412.11743</guid>
<content:encoded><![CDATA[

arXiv:2412.11743v2 Announce Type: replace-cross 
Abstract: Bayesian reinforcement learning (BRL) is a method that merges principles from Bayesian statistics and reinforcement learning to make optimal decisions in uncertain environments. As a model-based RL method, it has two key components: (1) inferring the posterior distribution of the model for the data-generating process (DGP) and (2) policy learning using the learned posterior. We propose to model the dynamics of the unknown environment through deep generative models, assuming Markov dependence. In the absence of likelihood functions for these models, we train them by learning a generalized predictive-sequential (or prequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC) samplers to draw samples from this generalized Bayesian posterior distribution. In conjunction, to achieve scalability in the high-dimensional parameter space of the neural networks, we use the gradient-based Markov kernels within SMC. To justify the use of the prequential scoring rule posterior, we prove a Bernstein-von Mises-type theorem. For policy learning, we propose expected Thompson sampling (ETS) to learn the optimal policy by maximising the expected value function with respect to the posterior distribution. This improves upon traditional Thompson sampling (TS) and its extensions, which utilize only one sample drawn from the posterior distribution. This improvement is studied both theoretically and using simulation studies, assuming a discrete action space. Finally, we successfully extended our setup for a challenging problem with a continuous action space without theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective</title>
<link>https://arxiv.org/abs/2412.12276</link>
<guid>https://arxiv.org/abs/2412.12276</guid>
<content:encoded><![CDATA[

arXiv:2412.12276v3 Announce Type: replace-cross 
Abstract: Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., "Finding the first noun in a sentence.") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic interior-point methods for smooth conic optimization with applications</title>
<link>https://arxiv.org/abs/2412.12987</link>
<guid>https://arxiv.org/abs/2412.12987</guid>
<content:encoded><![CDATA[

arXiv:2412.12987v2 Announce Type: replace-cross 
Abstract: Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the iteration complexity of our proposed SIPMs, which, up to a polylogarithmic factor, match the best-known results in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Modality Generalization: A Benchmark and Prospective Analysis</title>
<link>https://arxiv.org/abs/2412.18277</link>
<guid>https://arxiv.org/abs/2412.18277</guid>
<content:encoded><![CDATA[

arXiv:2412.18277v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has achieved remarkable success by integrating information from various modalities, achieving superior performance in tasks like recognition and retrieval compared to uni-modal approaches. However, real-world scenarios often present novel modalities that are unseen during training due to resource and privacy constraints, a challenge current methods struggle to address. This paper introduces Modality Generalization (MG), which focuses on enabling models to generalize to unseen modalities. We define two cases: Weak MG, where both seen and unseen modalities can be mapped into a joint embedding space via existing perceptors, and Strong MG, where no such mappings exist. To facilitate progress, we propose a comprehensive benchmark featuring multi-modal algorithms and adapt existing methods that focus on generalization. Extensive experiments highlight the complexity of MG, exposing the limitations of existing methods and identifying key directions for future research. Our work provides a foundation for advancing robust and adaptable multi-modal models, enabling them to handle unseen modalities in realistic scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Budget-Aware LLM Reasoning</title>
<link>https://arxiv.org/abs/2412.18547</link>
<guid>https://arxiv.org/abs/2412.18547</guid>
<content:encoded><![CDATA[

arXiv:2412.18547v5 Announce Type: replace-cross 
Abstract: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging</title>
<link>https://arxiv.org/abs/2412.20070</link>
<guid>https://arxiv.org/abs/2412.20070</guid>
<content:encoded><![CDATA[

arXiv:2412.20070v2 Announce Type: replace-cross 
Abstract: Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback</title>
<link>https://arxiv.org/abs/2501.01377</link>
<guid>https://arxiv.org/abs/2501.01377</guid>
<content:encoded><![CDATA[

arXiv:2501.01377v2 Announce Type: replace-cross 
Abstract: Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Vision Model in Street Scene Semantic Understanding through Leveraging Posterior Optimization Trajectory</title>
<link>https://arxiv.org/abs/2501.01710</link>
<guid>https://arxiv.org/abs/2501.01710</guid>
<content:encoded><![CDATA[

arXiv:2501.01710v2 Announce Type: replace-cross 
Abstract: To improve the generalization of the autonomous driving (AD) perception model, vehicles need to update the model over time based on the continuously collected data. As time progresses, the amount of data fitted by the AD model expands, which helps to improve the AD model generalization substantially. However, such ever-expanding data is a double-edged sword for the AD model. Specifically, as the fitted data volume grows to exceed the the AD model's fitting capacities, the AD model is prone to under-fitting. To address this issue, we propose to use a pretrained Large Vision Models (LVMs) as backbone coupled with downstream perception head to understand AD semantic information. This design can not only surmount the aforementioned under-fitting problem due to LVMs' powerful fitting capabilities, but also enhance the perception generalization thanks to LVMs' vast and diverse training data. On the other hand, to mitigate vehicles' computational burden of training the perception head while running LVM backbone, we introduce a Posterior Optimization Trajectory (POT)-Guided optimization scheme (POTGui) to accelerate the convergence. Concretely, we propose a POT Generator (POTGen) to generate posterior (future) optimization direction in advance to guide the current optimization iteration, through which the model can generally converge within 10 epochs. Extensive experiments demonstrate that the proposed method improves the performance by over 66.48\% and converges faster over 6 times, compared to the existing state-of-the-art approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Equivariance and Symmetry Breaking in Convolutional Networks</title>
<link>https://arxiv.org/abs/2501.01999</link>
<guid>https://arxiv.org/abs/2501.01999</guid>
<content:encoded><![CDATA[

arXiv:2501.01999v3 Announce Type: replace-cross 
Abstract: In this work, we explore the trade-offs of explicit structural priors, particularly group equivariance. We address this through theoretical analysis and a comprehensive empirical study. To enable controlled and fair comparisons, we introduce \texttt{Rapidash}, a unified group convolutional architecture that allows for different variants of equivariant and non-equivariant models. Our results suggest that more constrained equivariant models outperform less constrained alternatives when aligned with the geometry of the task, and increasing representation capacity does not fully eliminate performance gaps. We see improved performance of models with equivariance and symmetry-breaking through tasks like segmentation, regression, and generation across diverse datasets. Explicit \textit{symmetry breaking} via geometric reference frames consistently improves performance, while \textit{breaking equivariance} through geometric input features can be helpful when aligned with task geometry. Our results provide task-specific performance trends that offer a more nuanced way for model selection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?</title>
<link>https://arxiv.org/abs/2501.02669</link>
<guid>https://arxiv.org/abs/2501.02669</guid>
<content:encoded><![CDATA[

arXiv:2501.02669v2 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) are impressive at visual question answering and image captioning. But they underperform on multi-step visual reasoning -- even compared to LLMs on the same tasks presented in text form -- giving rise to perceptions of modality imbalance or brittleness. Towards a systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning, comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We propose strategies for training on the SIMPLE version of tasks that improve performance on the corresponding HARD task, i.e., simple-to-hard (S2H) generalization. This controlled setup, where each task also has an equivalent text-only version, allows a quantification of the modality imbalance and how it is impacted by training strategy. We show that 1) explicit image-to-text conversion is important in promoting S2H generalization on images, by transferring reasoning from text; 2) conversion can be internalized at test time. We also report results of mechanistic study of this phenomenon. We identify measures of gradient alignment that can identify training strategies that promote better S2H generalization. Ablations highlight the importance of chain-of-thought.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge</title>
<link>https://arxiv.org/abs/2501.04292</link>
<guid>https://arxiv.org/abs/2501.04292</guid>
<content:encoded><![CDATA[

arXiv:2501.04292v3 Announce Type: replace-cross 
Abstract: The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Early Prediction of Self-Supervised Speech Model Performance</title>
<link>https://arxiv.org/abs/2501.05966</link>
<guid>https://arxiv.org/abs/2501.05966</guid>
<content:encoded><![CDATA[

arXiv:2501.05966v2 Announce Type: replace-cross 
Abstract: In Self-Supervised Learning (SSL), pre-training and evaluation are resource intensive. In the speech domain, current indicators of the quality of SSL models during pre-training, such as the loss, do not correlate well with downstream performance. Consequently, it is often difficult to gauge the final downstream performance in a cost efficient manner during pre-training. In this work, we propose unsupervised efficient methods that give insights into the quality of the pre-training of SSL speech models, namely, measuring the cluster quality and rank of the embeddings of the SSL model. Results show that measures of cluster quality and rank correlate better with downstream performance than the pre-training loss with only one hour of unlabeled audio, reducing the need for GPU hours and labeled data in SSL model evaluation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing</title>
<link>https://arxiv.org/abs/2501.07985</link>
<guid>https://arxiv.org/abs/2501.07985</guid>
<content:encoded><![CDATA[

arXiv:2501.07985v2 Announce Type: replace-cross 
Abstract: Robotic systems are increasingly employed for industrial automation, with contact-rich tasks like polishing requiring dexterity and compliant behaviour. These tasks are difficult to model, making classical control challenging. Deep reinforcement learning (RL) offers a promising solution by enabling the learning of models and control policies directly from data. However, its application to real-world problems is limited by data inefficiency and unsafe exploration. Adaptive hybrid RL methods blend classical control and RL adaptively, combining the strengths of both: structure from control and learning from RL. This has led to improvements in data efficiency and exploration safety. However, their potential for hardware applications remains underexplored, with no evaluations on physical systems to date. Such evaluations are critical to fully assess the practicality and effectiveness of these methods in real-world settings. This work presents an experimental demonstration of the hybrid RL algorithm CHEQ for robotic polishing with variable impedance, a task requiring precise force and velocity tracking. In simulation, we show that variable impedance enhances polishing performance. We compare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves effective learning while adhering to safety constraints. On hardware, CHEQ achieves effective polishing behaviour, requiring only eight hours of training and incurring just five failures. These results highlight the potential of adaptive hybrid RL for real-world, contact-rich tasks trained directly on hardware.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction</title>
<link>https://arxiv.org/abs/2501.13125</link>
<guid>https://arxiv.org/abs/2501.13125</guid>
<content:encoded><![CDATA[

arXiv:2501.13125v3 Announce Type: replace-cross 
Abstract: In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior</title>
<link>https://arxiv.org/abs/2501.13134</link>
<guid>https://arxiv.org/abs/2501.13134</guid>
<content:encoded><![CDATA[

arXiv:2501.13134v2 Announce Type: replace-cross 
Abstract: Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models</title>
<link>https://arxiv.org/abs/2501.13772</link>
<guid>https://arxiv.org/abs/2501.13772</guid>
<content:encoded><![CDATA[

arXiv:2501.13772v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant security risks, as models can be exploited to generate harmful or inappropriate content through jailbreak attack. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce \textbf{Jailbreak-AudioBench}, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations</title>
<link>https://arxiv.org/abs/2501.15056</link>
<guid>https://arxiv.org/abs/2501.15056</guid>
<content:encoded><![CDATA[

arXiv:2501.15056v2 Announce Type: replace-cross 
Abstract: Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multimodal Learning via Cross-Modal Proxy Tokens</title>
<link>https://arxiv.org/abs/2501.17823</link>
<guid>https://arxiv.org/abs/2501.17823</guid>
<content:encoded><![CDATA[

arXiv:2501.17823v3 Announce Type: replace-cross 
Abstract: Multimodal models often experience a significant performance drop when one or more modalities are missing during inference. To address this challenge, we propose a simple yet effective approach that enhances robustness to missing modalities while maintaining strong performance when all modalities are available. Our method introduces cross-modal proxy tokens (CMPTs), which approximate the class token of a missing modality by attending only to the tokens of the available modality without requiring explicit modality generation or auxiliary networks. To efficiently learn these approximations with minimal computational overhead, we employ low-rank adapters in frozen unimodal encoders and jointly optimize an alignment loss with a task-specific loss. Extensive experiments on five multimodal datasets show that our method outperforms state-of-the-art baselines across various missing rates while achieving competitive results in complete-modality settings. Overall, our method offers a flexible and efficient solution for robust multimodal learning. The code and pretrained models will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization</title>
<link>https://arxiv.org/abs/2501.18756</link>
<guid>https://arxiv.org/abs/2501.18756</guid>
<content:encoded><![CDATA[

arXiv:2501.18756v2 Announce Type: replace-cross 
Abstract: Bayesian optimization is a widely used method for optimizing expensive black-box functions, with Expected Improvement being one of the most commonly used acquisition functions. In contrast, information-theoretic acquisition functions aim to reduce uncertainty about the function's optimum and are often considered fundamentally distinct from EI. In this work, we challenge this prevailing perspective by introducing a unified theoretical framework, Variational Entropy Search, which reveals that EI and information-theoretic acquisition functions are more closely related than previously recognized. We demonstrate that EI can be interpreted as a variational inference approximation of the popular information-theoretic acquisition function, named Max-value Entropy Search. Building on this insight, we propose VES-Gamma, a novel acquisition function that balances the strengths of EI and MES. Extensive empirical evaluations across both low- and high-dimensional synthetic and real-world benchmarks demonstrate that VES-Gamma is competitive with state-of-the-art acquisition functions and in many cases outperforms EI and MES.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Generative Model Comparison</title>
<link>https://arxiv.org/abs/2501.18897</link>
<guid>https://arxiv.org/abs/2501.18897</guid>
<content:encoded><![CDATA[

arXiv:2501.18897v2 Announce Type: replace-cross 
Abstract: Generative models have recently achieved remarkable empirical performance in various applications, however, their evaluations yet lack uncertainty quantification. In this paper, we propose a method to compare two generative models with statistical confidence based on an unbiased estimator of their relative performance gap. Theoretically, our estimator achieves parametric convergence rates and admits asymptotic normality, which enables valid inference. Empirically, on simulated datasets, our approach effectively controls type I error without compromising its power. In addition, on real image and language datasets, we demonstrate our method's performance in comparing generative models with statistical guarantees.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection-Window Decoding: Text Generation with Selective Refinement</title>
<link>https://arxiv.org/abs/2502.03678</link>
<guid>https://arxiv.org/abs/2502.03678</guid>
<content:encoded><![CDATA[

arXiv:2502.03678v3 Announce Type: replace-cross 
Abstract: The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Deepening Sampling as Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.05449</link>
<guid>https://arxiv.org/abs/2502.05449</guid>
<content:encoded><![CDATA[

arXiv:2502.05449v2 Announce Type: replace-cross 
Abstract: Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms</title>
<link>https://arxiv.org/abs/2502.06231</link>
<guid>https://arxiv.org/abs/2502.06231</guid>
<content:encoded><![CDATA[

arXiv:2502.06231v2 Announce Type: replace-cross 
Abstract: A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and semi-synthetic data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-oriented Data Augmentation with Large Language Models</title>
<link>https://arxiv.org/abs/2502.11671</link>
<guid>https://arxiv.org/abs/2502.11671</guid>
<content:encoded><![CDATA[

arXiv:2502.11671v2 Announce Type: replace-cross 
Abstract: Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data \textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). % \(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</title>
<link>https://arxiv.org/abs/2502.13775</link>
<guid>https://arxiv.org/abs/2502.13775</guid>
<content:encoded><![CDATA[

arXiv:2502.13775v2 Announce Type: replace-cross 
Abstract: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[

arXiv:2502.13943v2 Announce Type: replace-cross 
Abstract: Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Lower Bounds for Stochastic Non-Convex Optimization through Divergence Decomposition</title>
<link>https://arxiv.org/abs/2502.14060</link>
<guid>https://arxiv.org/abs/2502.14060</guid>
<content:encoded><![CDATA[

arXiv:2502.14060v2 Announce Type: replace-cross 
Abstract: We study fundamental limits of first-order stochastic optimization in a range of nonconvex settings, including L-smooth functions satisfying Quasar-Convexity (QC), Quadratic Growth (QG), and Restricted Secant Inequalities (RSI). While the convergence properties of standard algorithms are well-understood in deterministic regimes, significantly fewer results address the stochastic case, where only unbiased and noisy gradients are available. We establish new lower bounds on the number of noisy gradient queries to minimize these classes of functions, also showing that they are tight (up to a logarithmic factor) in all the relevant quantities characterizing each class. Our approach reformulates the optimization task as a function identification problem, leveraging divergence decomposition arguments to construct a challenging subclass that leads to sharp lower bounds. Furthermore, we present a specialized algorithm in the one-dimensional setting that achieves faster rates, suggesting that certain dimensional thresholds are intrinsic to the complexity of non-convex stochastic optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIS-CO: Discovering Copyrighted Content in VLMs Training Data</title>
<link>https://arxiv.org/abs/2502.17358</link>
<guid>https://arxiv.org/abs/2502.17358</guid>
<content:encoded><![CDATA[

arXiv:2502.17358v3 Announce Type: replace-cross 
Abstract: How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs</title>
<link>https://arxiv.org/abs/2502.17701</link>
<guid>https://arxiv.org/abs/2502.17701</guid>
<content:encoded><![CDATA[

arXiv:2502.17701v2 Announce Type: replace-cross 
Abstract: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Experts</title>
<link>https://arxiv.org/abs/2502.18530</link>
<guid>https://arxiv.org/abs/2502.18530</guid>
<content:encoded><![CDATA[

arXiv:2502.18530v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents have emerged as a promising solution to automate the workflow of machine learning, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves overall model performance. We also provide some theoretical edvience of the superior properties of this Iterative Refinement. Further, we implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency</title>
<link>https://arxiv.org/abs/2502.19163</link>
<guid>https://arxiv.org/abs/2502.19163</guid>
<content:encoded><![CDATA[

arXiv:2502.19163v2 Announce Type: replace-cross 
Abstract: Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2502.19765</link>
<guid>https://arxiv.org/abs/2502.19765</guid>
<content:encoded><![CDATA[

arXiv:2502.19765v2 Announce Type: replace-cross 
Abstract: We propose EdiText, a controllable text editing method that modifies the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at a broad range of levels across various tasks, including toxicity control and sentiment control.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Collaborative Anti-Money Laundering Among Financial Institutions</title>
<link>https://arxiv.org/abs/2502.19952</link>
<guid>https://arxiv.org/abs/2502.19952</guid>
<content:encoded><![CDATA[

arXiv:2502.19952v2 Announce Type: replace-cross 
Abstract: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[

arXiv:2502.20727v4 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization</title>
<link>https://arxiv.org/abs/2503.05750</link>
<guid>https://arxiv.org/abs/2503.05750</guid>
<content:encoded><![CDATA[

arXiv:2503.05750v2 Announce Type: replace-cross 
Abstract: A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists' workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification</title>
<link>https://arxiv.org/abs/2503.05763</link>
<guid>https://arxiv.org/abs/2503.05763</guid>
<content:encoded><![CDATA[

arXiv:2503.05763v3 Announce Type: replace-cross 
Abstract: Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \textbf{Graph Masked Language Model (GMLM)}, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \textbf{dynamic active node selection} strategy for scalable PLM text processing; (ii) a GNN-specific \textbf{contrastive pretraining stage} using soft masking with a learnable graph \texttt{[MASK]} token for robust structural representations; and (iii) a \textbf{dedicated fusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \& DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \textbf{4.7\%} on Cornell and over \textbf{2.0\%} on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization</title>
<link>https://arxiv.org/abs/2503.15704</link>
<guid>https://arxiv.org/abs/2503.15704</guid>
<content:encoded><![CDATA[

arXiv:2503.15704v3 Announce Type: replace-cross 
Abstract: The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization has been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our implementations are able to obtain a full schedule of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning For Repairable Hardware Systems With Partial Coverage</title>
<link>https://arxiv.org/abs/2503.16315</link>
<guid>https://arxiv.org/abs/2503.16315</guid>
<content:encoded><![CDATA[

arXiv:2503.16315v2 Announce Type: replace-cross 
Abstract: Identifying the optimal diagnostic test and hardware system instance to infer reliability characteristics using field data is challenging, especially when constrained by fixed budgets and minimal maintenance cycles. Active Learning (AL) has shown promise for parameter inference with limited data and budget constraints in machine learning/deep learning tasks. However, AL for reliability model parameter inference remains underexplored for repairable hardware systems. It requires specialized AL Acquisition Functions (AFs) that consider hardware aging and the fact that a hardware system consists of multiple sub-systems, which may undergo only partial testing during a given diagnostic test. To address these challenges, we propose a relaxed Mixed Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing budgets. Furthermore, we design empirical-based simulation experiments focusing on two diagnostic testing scenarios: (1) partial tests of a hardware system with overlapping subsystem coverage, and (2) partial tests where one diagnostic test fully subsumes the subsystem coverage of another. We evaluate our proposed approach against the most widely used AL AF in the literature (entropy), as well as several intuitive AL AFs tailored for reliability model parameter inference. Our proposed AF ranked best on average among the alternative AFs across 6,000 experimental configurations, with respect to Area Under the Curve (AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error (MSE) curves, with statistical significance calculated at a 0.05 alpha level using a Friedman hypothesis test.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of Frontier Data Governance With Synthetic Data</title>
<link>https://arxiv.org/abs/2503.17414</link>
<guid>https://arxiv.org/abs/2503.17414</guid>
<content:encoded><![CDATA[

arXiv:2503.17414v2 Announce Type: replace-cross 
Abstract: Synthetic data, or data generated by machine learning models, is increasingly emerging as a solution to the data access problem. However, its use introduces significant governance and accountability challenges, and potentially debases existing governance paradigms, such as compute and data governance. In this paper, we identify 3 key governance and accountability challenges that synthetic data poses - it can enable the increased emergence of malicious actors, spontaneous biases and value drift. We thus craft 3 technical mechanisms to address these specific challenges, finding applications for synthetic data towards adversarial training, bias mitigation and value reinforcement. These could not only counteract the risks of synthetic data, but serve as critical levers for governance of the frontier in the future.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[

arXiv:2503.18938v4 Announce Type: replace-cross 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations</title>
<link>https://arxiv.org/abs/2503.19449</link>
<guid>https://arxiv.org/abs/2503.19449</guid>
<content:encoded><![CDATA[

arXiv:2503.19449v2 Announce Type: replace-cross 
Abstract: Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RxRx3-core: Benchmarking drug-target interactions in High-Content Microscopy</title>
<link>https://arxiv.org/abs/2503.20158</link>
<guid>https://arxiv.org/abs/2503.20158</guid>
<content:encoded><![CDATA[

arXiv:2503.20158v2 Announce Type: replace-cross 
Abstract: High Content Screening (HCS) microscopy datasets have transformed the ability to profile cellular responses to genetic and chemical perturbations, enabling cell-based inference of drug-target interactions (DTI). However, the adoption of representation learning methods for HCS data has been hindered by the lack of accessible datasets and robust benchmarks. To address this gap, we present RxRx3-core, a curated and compressed subset of the RxRx3 dataset, and an associated DTI benchmarking task. At just 18GB, RxRx3-core significantly reduces the size barrier associated with large-scale HCS datasets while preserving critical data necessary for benchmarking representation learning models against a zero-shot DTI prediction task. RxRx3-core includes 222,601 microscopy images spanning 736 CRISPR knockouts and 1,674 compounds at 8 concentrations. RxRx3-core is available on HuggingFace and Polaris, along with pre-trained embeddings and benchmarking code, ensuring accessibility for the research community. By providing a compact dataset and robust benchmarks, we aim to accelerate innovation in representation learning methods for HCS data and support the discovery of novel biological insights.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[

arXiv:2503.20756v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced simulation paradigm of human behaviour unveils complex financial systemic projection</title>
<link>https://arxiv.org/abs/2503.20787</link>
<guid>https://arxiv.org/abs/2503.20787</guid>
<content:encoded><![CDATA[

arXiv:2503.20787v2 Announce Type: replace-cross 
Abstract: The high-order complexity of human behaviour is likely the root cause of extreme difficulty in financial market projections. We consider that behavioural simulation can unveil systemic dynamics to support analysis. Simulating diverse human groups must account for the behavioural heterogeneity, especially in finance. To address the fidelity of simulated agents, on the basis of agent-based modeling, we propose a new paradigm of behavioural simulation where each agent is supported and driven by a hierarchical knowledge architecture. This architecture, integrating language and professional models, imitates behavioural processes in specific scenarios. Evaluated on futures markets, our simulator achieves a 13.29% deviation in simulating crisis scenarios whose price increase rate reaches 285.34%. Under normal conditions, our simulator also exhibits lower mean square error in predicting futures price of specific commodities. This technique bridges non-quantitative information with diverse market behaviour, offering a promising platform to simulate investor behaviour and its impact on market dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language and Reasoning Models are Shallow Disjunctive Reasoners</title>
<link>https://arxiv.org/abs/2503.23487</link>
<guid>https://arxiv.org/abs/2503.23487</guid>
<content:encoded><![CDATA[

arXiv:2503.23487v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[

arXiv:2504.03561v3 Announce Type: replace-cross 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Adaptive Self-Attention for Quantum Transformer Models</title>
<link>https://arxiv.org/abs/2504.05336</link>
<guid>https://arxiv.org/abs/2504.05336</guid>
<content:encoded><![CDATA[

arXiv:2504.05336v2 Announce Type: replace-cross 
Abstract: Transformer models have revolutionized sequential learning across various domains, yet their self-attention mechanism incurs quadratic computational cost, posing limitations for real-time and resource-constrained tasks. To address this, we propose Quantum Adaptive Self-Attention (QASA), a novel hybrid architecture that enhances classical Transformer models with a quantum attention mechanism. QASA replaces dot-product attention with a parameterized quantum circuit (PQC) that adaptively captures inter-token relationships in the quantum Hilbert space. Additionally, a residual quantum projection module is introduced before the feedforward network to further refine temporal features. Our design retains classical efficiency in earlier layers while injecting quantum expressiveness in the final encoder block, ensuring compatibility with current NISQ hardware. Experiments on synthetic time-series tasks demonstrate that QASA achieves faster convergence and superior generalization compared to both standard Transformers and reduced classical variants. Preliminary complexity analysis suggests potential quantum advantages in gradient computation, opening new avenues for efficient quantum deep learning models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories</title>
<link>https://arxiv.org/abs/2504.16449</link>
<guid>https://arxiv.org/abs/2504.16449</guid>
<content:encoded><![CDATA[

arXiv:2504.16449v2 Announce Type: replace-cross 
Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage.This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</title>
<link>https://arxiv.org/abs/2504.19475</link>
<guid>https://arxiv.org/abs/2504.19475</guid>
<content:encoded><![CDATA[

arXiv:2504.19475v2 Announce Type: replace-cross 
Abstract: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.00763</link>
<guid>https://arxiv.org/abs/2505.00763</guid>
<content:encoded><![CDATA[

arXiv:2505.00763v2 Announce Type: replace-cross 
Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to understand the structure of dark matter halos. However, the kinematic information of these stars is often limited to celestial positions and line-of-sight velocities, making full phase space analysis challenging. Conventional methods rely on projected analytic phase space density models with several parameters and infer dark matter halo structures by solving the spherical Jeans equation. In this paper, we introduce an unsupervised machine learning method for solving the spherical Jeans equation in a model-independent way as a first step toward model-independent analysis of dwarf spheroidal galaxies. Using equivariant continuous normalizing flows, we demonstrate that spherically symmetric stellar phase space densities and velocity dispersions can be estimated without model assumptions. As a proof of concept, we apply our method to Gaia challenge datasets for spherical models and measure dark matter mass densities for given velocity anisotropy profiles. Our method can identify halo structures accurately, even with a small number of tracer stars.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Descriptor: C++ Self-Admitted Technical Debt Dataset (CppSATD)</title>
<link>https://arxiv.org/abs/2505.01136</link>
<guid>https://arxiv.org/abs/2505.01136</guid>
<content:encoded><![CDATA[

arXiv:2505.01136v2 Announce Type: replace-cross 
Abstract: In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Integrated Gradients for Feature Attribution</title>
<link>https://arxiv.org/abs/2505.03201</link>
<guid>https://arxiv.org/abs/2505.03201</guid>
<content:encoded><![CDATA[

arXiv:2505.03201v2 Announce Type: replace-cross 
Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique for assessing the significance of feature attributes of the input on model outputs by evaluating contributions from a baseline input to the current input. The choice of the baseline input significantly influences the resulting explanation. While the traditional Expected Gradients (EG) method assumes baselines can be uniformly sampled and averaged with equal weights, this study argues that baselines should not be treated equivalently. We introduce Weighted Integrated Gradients (WG), a novel approach that unsupervisedly evaluates baseline suitability and incorporates a strategy for selecting effective baselines. Theoretical analysis demonstrates that WG satisfies essential explanation method criteria and offers greater stability than prior approaches. Experimental results further confirm that WG outperforms EG across diverse scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by evaluating baselines, our method can filter a subset of effective baselines for each input to calculate explanations, maintaining high accuracy while reducing computational cost. The code is available at: https://github.com/tamnt240904/weighted_ig.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors</title>
<link>https://arxiv.org/abs/2505.04792</link>
<guid>https://arxiv.org/abs/2505.04792</guid>
<content:encoded><![CDATA[

arXiv:2505.04792v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence has advanced significantly in recent years thanks to innovations in the design and training of artificial neural networks (ANNs). Despite these advancements, we still understand relatively little about how elementary forms of ANNs learn, fail to learn, and generate false information without the intent to deceive, a phenomenon known as `confabulation'. To provide some foundational insight, in this paper we analyse how confabulation occurs in reservoir computers (RCs): a dynamical system in the form of an ANN. RCs are particularly useful to study as they are known to confabulate in a well-defined way: when RCs are trained to reconstruct the dynamics of a given attractor, they sometimes construct an attractor that they were not trained to construct, a so-called `untrained attractor' (UA). This paper sheds light on the role played by UAs when reconstruction fails and their influence when modelling transitions between reconstructed attractors. Based on our results, we conclude that UAs are an intrinsic feature of learning systems whose state spaces are bounded, and that this means of confabulation may be present in systems beyond RCs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Algorithms for Boolean Satisfiability</title>
<link>https://arxiv.org/abs/2505.06146</link>
<guid>https://arxiv.org/abs/2505.06146</guid>
<content:encoded><![CDATA[

arXiv:2505.06146v2 Announce Type: replace-cross 
Abstract: Learning-augmented algorithms are a prominent recent development in beyond worst-case analysis. In this framework, a problem instance is provided with a prediction (``advice'') from a machine-learning oracle, which provides partial information about an optimal solution, and the goal is to design algorithms that leverage this advice to improve worst-case performance. We study the classic Boolean satisfiability (SAT) decision and optimization problems within this framework using two forms of advice. ``Subset advice" provides a random $\epsilon$ fraction of the variables from an optimal assignment, whereas ``label advice" provides noisy predictions for all variables in an optimal assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate the exponential running time of the PPSZ family of algorithms due to Paturi, Pudlak, Saks and Zane, which currently represent the state of the art in the worst case. We accelerate the running time by a multiplicative factor of $2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and $k$. For the optimization problem, we show how to incorporate subset advice in a black-box fashion with any $\alpha$-approximation algorithm, improving the approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 + \Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT. Moreover, for label advice, we obtain near-optimal approximation for instances with large average degree, thereby generalizing recent results on MAX-CUT and MAX-$2$-LIN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measurement to Meaning: A Validity-Centered Framework for AI Evaluation</title>
<link>https://arxiv.org/abs/2505.10573</link>
<guid>https://arxiv.org/abs/2505.10573</guid>
<content:encoded><![CDATA[

arXiv:2505.10573v2 Announce Type: replace-cross 
Abstract: While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions, which provide a limited and potentially misleading assessment. We provide a structured approach for reasoning about the types of evaluative claims that can be made given the available evidence. For instance, our framework helps determine whether performance on a mathematical benchmark is an indication of the ability to solve problems on math tests or instead indicates a broader ability to reason. Our framework is well-suited for the contemporary paradigm in machine learning, where various stakeholders provide measurements and evaluations that downstream users use to validate their claims and decisions. At the same time, our framework also informs the construction of evaluations designed to speak to the validity of the relevant claims. By leveraging psychometrics' breakdown of validity, evaluations can prioritize the most critical facets for a given claim, improving empirical utility and decision-making efficacy. We illustrate our framework through detailed case studies of vision and language model evaluations, highlighting how explicitly considering validity strengthens the connection between evaluation evidence and the claims being made.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective</title>
<link>https://arxiv.org/abs/2505.12185</link>
<guid>https://arxiv.org/abs/2505.12185</guid>
<content:encoded><![CDATA[

arXiv:2505.12185v2 Announce Type: replace-cross 
Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A3 : an Analytical Low-Rank Approximation Framework for Attention</title>
<link>https://arxiv.org/abs/2505.12942</link>
<guid>https://arxiv.org/abs/2505.12942</guid>
<content:encoded><![CDATA[

arXiv:2505.12942v2 Announce Type: replace-cross 
Abstract: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL</title>
<link>https://arxiv.org/abs/2505.15791</link>
<guid>https://arxiv.org/abs/2505.15791</guid>
<content:encoded><![CDATA[

arXiv:2505.15791v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.16415</link>
<guid>https://arxiv.org/abs/2505.16415</guid>
<content:encoded><![CDATA[

arXiv:2505.16415v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models. Our code is available at https://github.com/ruizheliUOA/ARC_JSD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective</title>
<link>https://arxiv.org/abs/2505.16900</link>
<guid>https://arxiv.org/abs/2505.16900</guid>
<content:encoded><![CDATA[

arXiv:2505.16900v4 Announce Type: replace-cross 
Abstract: During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[

arXiv:2505.18174v2 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
<link>https://arxiv.org/abs/2505.18361</link>
<guid>https://arxiv.org/abs/2505.18361</guid>
<content:encoded><![CDATA[

arXiv:2505.18361v3 Announce Type: replace-cross 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM $\times$ DATA</title>
<link>https://arxiv.org/abs/2505.18458</link>
<guid>https://arxiv.org/abs/2505.18458</guid>
<content:encoded><![CDATA[

arXiv:2505.18458v3 Announce Type: replace-cross 
Abstract: The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPCAHUS: Subspace Clustering for Heteroscedastic Data</title>
<link>https://arxiv.org/abs/2505.18918</link>
<guid>https://arxiv.org/abs/2505.18918</guid>
<content:encoded><![CDATA[

arXiv:2505.18918v2 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. Various methods have been proposed to extend PCA to the union of subspace (UoS) setting for clustering data that come from multiple subspaces like K-Subspaces (KSS). However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a heteroscedastic-focused subspace clustering method, named ALPCAHUS, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace bases associated with the low-rank structure of the data. This clustering algorithm builds on K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS setting. Simulations and real-data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing clustering algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Representation Learning Approach for Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2505.19110</link>
<guid>https://arxiv.org/abs/2505.19110</guid>
<content:encoded><![CDATA[

arXiv:2505.19110v2 Announce Type: replace-cross 
Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models. In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74% and shows a better disentanglement than the 3D representation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Articulatory Inversion Models for Inter-Speaker Consistency</title>
<link>https://arxiv.org/abs/2505.20529</link>
<guid>https://arxiv.org/abs/2505.20529</guid>
<content:encoded><![CDATA[

arXiv:2505.20529v2 Announce Type: replace-cross 
Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse mapping from speech to articulation. Exact articulatory prediction from speech alone may be impossible, as speakers can choose different forms of articulation seemingly without reference to their vocal tract structure. However, once a speaker has selected an articulatory form, their productions vary minimally. Recent works in AAI have proposed adapting Self-Supervised Learning (SSL) models to single-speaker datasets, claiming that these single-speaker models provide a universal articulatory template. In this paper, we investigate whether SSL-adapted models trained on single and multi-speaker data produce articulatory targets which are consistent across speaker identities for English and Russian. We do this through the use of a novel evaluation method which extracts articulatory targets using minimal pair sets. We also present a training method which can improve interspeaker consistency using only speech data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[

arXiv:2505.22232v2 Announce Type: replace-cross 
Abstract: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval</title>
<link>https://arxiv.org/abs/2505.22238</link>
<guid>https://arxiv.org/abs/2505.22238</guid>
<content:encoded><![CDATA[

arXiv:2505.22238v2 Announce Type: replace-cross 
Abstract: We present Yambda-5B, a large-scale open dataset sourced from the Yandex Music streaming platform. Yambda-5B contains 4.79 billion user-item interactions from 1 million users across 9.39 million tracks. The dataset includes two primary types of interactions: implicit feedback (listening events) and explicit feedback (likes, dislikes, unlikes and undislikes). In addition, we provide audio embeddings for most tracks, generated by a convolutional neural network trained on audio spectrograms. A key distinguishing feature of Yambda-5B is the inclusion of the is_organic flag, which separates organic user actions from recommendation-driven events. This distinction is critical for developing and evaluating machine learning algorithms, as Yandex Music relies on recommender systems to personalize track selection for users. To support rigorous benchmarking, we introduce an evaluation protocol based on a Global Temporal Split, allowing recommendation algorithms to be assessed in conditions that closely mirror real-world use. We report benchmark results for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using a variety of evaluation metrics. By releasing Yambda-5B to the community, we aim to provide a readily accessible, industrial-scale resource to advance research, foster innovation, and promote reproducible results in recommender systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[

arXiv:2505.22642v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
<link>https://arxiv.org/abs/2502.18807</link>
<guid>https://arxiv.org/abs/2502.18807</guid>
<content:encoded><![CDATA[
<div> Dataset, battery life prediction, time series data, benchmark, neural networks  
Summary:  
The research addresses challenges in Battery Life Prediction (BLP) by proposing BatteryLife, a comprehensive dataset and benchmark for battery degradation tests. BatteryLife integrates 16 datasets, offering a larger sample size and more diverse battery life data compared to existing resources. The dataset includes batteries from various formats, chemical systems, operating temperatures, and charge/discharge protocols. Additionally, BatteryLife includes data from zinc-ion and sodium-ion batteries, as well as industry-tested large-capacity lithium-ion batteries. The study evaluates the effectiveness of popular baselines in both the BLP field and other time series fields, highlighting the need for specialized models. A plug-in technique called CyclePatch is introduced to enhance neural network performance for BLP. Benchmarking of 18 methods demonstrates the potential for improving model performance with CyclePatch across different aging conditions and domains. The dataset and research findings are available at the provided GitHub link.  
<br /><br />Summary: <div>
arXiv:2502.18807v5 Announce Type: replace 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.5 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 59 chemical systems, 9 operating temperatures, and 421 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in various neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
<div> interaction, data heterogeneity, federated learning, decision boundary, forgetting<br />
<br />Summary: 
This paper addresses the challenges of data heterogeneity in federated learning and its impact on the global decision boundary. Existing methods often suffer from forgetting, where clients only learn local decision boundaries and forget the global one. Even with pre-trained optimal weights, this issue persists. The proposed FedProj framework aims to robustly learn and retain the global decision boundary during local training. It introduces a server-side ensemble knowledge transfer loss for better ensemble knowledge fusion and utilizes an episodic memory of average ensemble logits from a public unlabeled dataset to regulate gradient updates. Experimental results demonstrate that FedProj significantly outperforms state-of-the-art methods in learning and retaining the global decision boundary, highlighting its effectiveness in addressing the challenges of data heterogeneity in federated learning. <div>
arXiv:2505.20485v2 Announce Type: replace 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, informational complexity, algorithmic information theory, Binarized Neural Networks, Block Decomposition Method

Summary:
Neural networks' informational complexity is a crucial challenge in machine learning, affecting generalization and optimization. Current approaches often use entropy-based metrics that do not capture underlying algorithmic regularities. This study proposes utilizing algorithmic information theory, specifically with Binarized Neural Networks (BNNs), to analyze learning dynamics. By applying the Block Decomposition Method (BDM) based on algorithmic complexity, the study shows that it better reflects structural changes during training than entropy. The BDM has stronger correlations with training loss for various model sizes and training runs, indicating that learning involves algorithmic compression of structured regularities. This work provides a method to estimate learning progression and suggests a framework for complexity-aware learning and regularization based on information theory and computability principles.

Summary: <div>
arXiv:2505.20646v2 Announce Type: replace 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control</title>
<link>https://arxiv.org/abs/2505.23857</link>
<guid>https://arxiv.org/abs/2505.23857</guid>
<content:encoded><![CDATA[
<div> MDP, reinforcement learning, output-feedback, observation histories, policy expressiveness <br />
<br />
Reinforcement learning often involves scenarios where the agent only receives partial state information, known as output-feedback settings. To address this challenge, the Output-Feedback Markov Decision Process (OPMDP) is proposed, extending the standard MDP formulation to incorporate decision-making based on observation histories. A new algorithm, Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), is introduced to encode historical observations using depthwise separable convolution and multi-head attention, maintaining policy expressiveness without the instability of recurrent models. Experimental results on continuous control tasks show that DATD3 outperforms existing memory-based and recurrent methods in both partial and full observability scenarios.<br /><br />Summary: <div>
arXiv:2505.23857v1 Announce Type: new 
Abstract: Reinforcement learning in real-world applications often involves output-feedback settings, where the agent receives only partial state information. To address this challenge, we propose the Output-Feedback Markov Decision Process (OPMDP), which extends the standard MDP formulation to accommodate decision-making based on observation histories. Building on this framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise separable convolution and multi-head attention to encode historical observations. DATD3 maintains policy expressiveness while avoiding the instability of recurrent models. Extensive experiments on continuous control tasks demonstrate that DATD3 outperforms existing memory-based and recurrent baselines under both partial and full observability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration</title>
<link>https://arxiv.org/abs/2505.23859</link>
<guid>https://arxiv.org/abs/2505.23859</guid>
<content:encoded><![CDATA[
<div> model merging, multi-task, feature drift, LOT Merging, optimization<br />
<br />
Summary:<br />
The article introduces Layer-wise Optimal Task Vector Merging (LOT Merging), a novel technique for consolidating knowledge from multiple fine-tuned task-specific experts into a unified model. LOT Merging aims to minimize feature drift between task-specific experts and the unified model, which is identified as a key factor in performance degradation during model merging. This technique can be formulated as a convex quadratic optimization problem, allowing for efficient model consolidation through basic matrix operations. Experimental results across vision and vision-language benchmarks show that LOT Merging outperforms existing methods, achieving improvements of up to 4.4% over state-of-the-art approaches. LOT Merging provides a promising approach for multi-task model merging without the performance gaps seen in parameter-level methods or the costly secondary training procedures of task-loss approaches. <div>
arXiv:2505.23859v1 Announce Type: new 
Abstract: Multi-task model merging aims to consolidate knowledge from multiple fine-tuned task-specific experts into a unified model while minimizing performance degradation. Existing methods primarily approach this by minimizing differences between task-specific experts and the unified model, either from a parameter-level or a task-loss perspective. However, parameter-level methods exhibit a significant performance gap compared to the upper bound, while task-loss approaches entail costly secondary training procedures. In contrast, we observe that performance degradation closely correlates with feature drift, i.e., differences in feature representations of the same sample caused by model merging. Motivated by this observation, we propose Layer-wise Optimal Task Vector Merging (LOT Merging), a technique that explicitly minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner. LOT Merging can be formulated as a convex quadratic optimization problem, enabling us to analytically derive closed-form solutions for the parameters of linear and normalization layers. Consequently, LOT Merging achieves efficient model consolidation through basic matrix operations. Extensive experiments across vision and vision-language benchmarks demonstrate that LOT Merging significantly outperforms baseline methods, achieving improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiBLDR: Bidirectional Behavior Learning for Drug Repositioning</title>
<link>https://arxiv.org/abs/2505.23861</link>
<guid>https://arxiv.org/abs/2505.23861</guid>
<content:encoded><![CDATA[
<div> Drug repositioning, deep learning, bidirectional behavior learning, BiBLDR, state-of-the-art performance <br />
Summary: <br />
Drug repositioning aims to find new uses for existing drugs efficiently. Existing deep learning methods struggle with novel drugs in cold-start scenarios. BiBLDR proposes a bidirectional behavior learning strategy by constructing bidirectional behavioral sequences and utilizing a two-stage strategy. The model captures drug-disease interaction patterns more robustly, achieving state-of-the-art performance and outperforming previous methods in cold-start scenarios. The code is available on GitHub. <div>
arXiv:2505.23861v1 Announce Type: new 
Abstract: Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in https://github.com/Renyeeah/BiBLDR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting</title>
<link>https://arxiv.org/abs/2505.23863</link>
<guid>https://arxiv.org/abs/2505.23863</guid>
<content:encoded><![CDATA[
<div> framework, chaotic systems, forecasting, physics-informed, PhyxMamba
Summary:
PhyxMamba introduces a novel framework for long-term forecasting of chaotic systems using a Mamba-based state-space model and physics-informed principles. By reconstructing attractor manifolds from short-term observations, the framework captures essential dynamical features for accurate forecasting. A generative training scheme allows Mamba to replicate the physical process with multi-token prediction and attractor geometry regularization, ensuring prediction accuracy and maintaining key statistical invariants. Extensive evaluations on simulated and real-world chaotic systems demonstrate PhyxMamba's superior forecasting capabilities and ability to capture essential dynamical invariants from limited data. This framework has broad implications in fields such as climate science, neuroscience, and epidemiology, providing a reliable means of predicting chaotic systems under observation-scarce conditions. The open-source code for PhyxMamba is available at https://github.com/tsinghua-fib-lab/PhyxMamba. 

<br /><br />Summary: <div>
arXiv:2505.23863v1 Announce Type: new 
Abstract: Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections</title>
<link>https://arxiv.org/abs/2505.23864</link>
<guid>https://arxiv.org/abs/2505.23864</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, graph-structured data, personalized subgraph, auxiliary projections, node embeddings 
Summary: 
FedAux is a novel framework for federated learning on graph-structured data, addressing challenges of non-IID scenarios where clients have distinct subgraphs. It incorporates personalized models through the use of auxiliary projection vectors (APVs) that capture client-specific information without sharing raw data or embeddings. These APVs are trained alongside local graph neural networks (GNNs), enabling inter-client similarities to be computed based on the APVs for knowledge transfer and aggregation of models at the server. The design of FedAux is supported by rigorous theoretical analysis, ensuring convergence and rationality. Empirical evaluations on various graph benchmarks demonstrate significant improvements in accuracy and personalization compared to existing methods. FedAux represents a promising approach for achieving effective federated learning on graph data while maintaining individual client customization. 
<br /><br />Summary: <div>
arXiv:2505.23864v1 Announce Type: new 
Abstract: Federated learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration</title>
<link>https://arxiv.org/abs/2505.23865</link>
<guid>https://arxiv.org/abs/2505.23865</guid>
<content:encoded><![CDATA[
<div> Keywords: Precision agriculture, Autonomous systems, Deep learning, Exploration efficiency, Crop monitoring

Summary: 
This study focuses on the development of efficient autonomous systems for crop monitoring in precision agriculture. The researchers propose a two-stage deep learning framework that incorporates a belief model to update a probabilistic map of the environment and prioritize informative regions. Three agent architectures were evaluated, with the Double-CNN DQN agent demonstrating superior exploration efficiency, especially in larger environments. The study highlights the importance of uncertainty-aware policies that leverage entropy, belief states, and visibility tracking for robust and scalable exploration. Future work will explore curriculum learning, multi-agent cooperation, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization. <br /><br />Summary: <div>
arXiv:2505.23865v1 Announce Type: new 
Abstract: Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments.
  Results show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2505.23866</link>
<guid>https://arxiv.org/abs/2505.23866</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, calibration, overconfidence, sharpness-aware minimization (SAM), CSAM 

Summary: 
Deep neural networks are commonly used in critical applications like medical diagnosis and autonomous driving, but they are often poorly calibrated and tend to be overconfident, leading to potential risks. Traditional training methods like stochastic gradient descent may exacerbate this issue. However, the sharpness-aware minimization (SAM) technique has been shown to mitigate overconfidence by implicitly maximizing the entropy of the predictive distribution. Building on SAM, a new approach called CSAM is proposed to further enhance model calibration. Experimental results on various datasets, including ImageNet-1K, demonstrate the effectiveness of SAM in reducing calibration error. CSAM outperforms SAM and other methods consistently in reducing calibration error, highlighting its potential for improving the reliability of deep neural networks in safety-critical applications.<br /><br />Summary: <div>
arXiv:2505.23866v1 Announce Type: new 
Abstract: Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title>
<link>https://arxiv.org/abs/2505.23868</link>
<guid>https://arxiv.org/abs/2505.23868</guid>
<content:encoded><![CDATA[
<div> Keywords: parameter-efficient fine-tuning, pre-trained language models, noise-robust adaptation, LoRA poisoning experts, noisy data <br />
Summary: 
The article introduces a new method, LoRA poisoning experts (LoPE), for adapting pre-trained language models to downstream tasks while improving robustness to noisy data. The conventional noise-handling approaches often require laborious data pre-processing or model architecture modifications susceptible to error accumulation. In contrast, LoPE enhances model robustness solely through generated noisy data. By incorporating a dedicated poisoning expert in an asymmetric LoRA configuration, LoPE strategically injects noise during fine-tuning to improve noise discrimination and processing. During inference, the dedicated poisoning expert is selectively masked to utilize purified knowledge from normal experts, resulting in noise-robust output. Extensive experiments show that LoPE achieves strong performance without the need for data cleaning, demonstrating its effectiveness in handling noisy data. <br /><br />Summary: <div>
arXiv:2505.23868v1 Announce Type: new 
Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2505.23870</link>
<guid>https://arxiv.org/abs/2505.23870</guid>
<content:encoded><![CDATA[
<div> adaptation, MaCP, cosine projection, model efficiency, accuracy

Summary: 
MaCP is a new adaptation method that utilizes cosine projection to achieve outstanding performance with minimal parameters and memory requirements for fine-tuning large foundation models. By projecting weight changes from low-rank adaptation into discrete cosine space, MaCP effectively enhances energy compaction and decorrelation properties. The weight change is then segmented across different levels of the discrete cosine spectrum, selecting the most crucial frequency components from each partition. Various experiments across single and multi-modality tasks, including natural language understanding, generation, text summarization, image classification, and video understanding, demonstrate MaCP's superior accuracy, reduced computational complexity, and lower memory demands compared to existing methods. MaCP proves to be a promising approach for enhancing model efficiency and accuracy in various tasks. 

<br /><br />Summary: <div>
arXiv:2505.23870v1 Announce Type: new 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23871</link>
<guid>https://arxiv.org/abs/2505.23871</guid>
<content:encoded><![CDATA[
<div> Keywords: offline reinforcement learning, data corruption, diffusion models, denoising, dataset recovery<br />
<br />
Summary: <br />
The article introduces a novel approach called Ambient Diffusion-Guided Dataset Recovery (ADG) to address data corruption in offline reinforcement learning (RL). It leverages Ambient Denoising Diffusion Probabilistic Models (DDPM) to handle noise in high-dimensional state spaces and multiple corrupted elements simultaneously. The ADG method first uses Ambient DDPM to distinguish between clean and corrupted data and then trains a standard DDPM on the clean subset. This trained standard DDPM is then used to refine the previously identified corrupted data, improving data quality for offline RL training. ADG is versatile and can be seamlessly integrated with any offline RL algorithm, offering strong denoising capabilities and theoretical guarantees. Experiments on various benchmarks show that ADG effectively mitigates the impact of corrupted data and enhances the robustness of offline RL, achieving state-of-the-art results. <br /> <div>
arXiv:2505.23871v1 Announce Type: new 
Abstract: Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants</title>
<link>https://arxiv.org/abs/2505.23875</link>
<guid>https://arxiv.org/abs/2505.23875</guid>
<content:encoded><![CDATA[
<div> benchmark, graph regression, program graphs, RelSC, graph neural network architectures

Summary:
RelSC is a new graph-regression dataset created from program graphs, offering a diverse and challenging benchmark for graph regression models. The dataset includes two variants, RelSC-H with rich node features and RelSC-M with multi-relational structures. The labels are execution-time costs of programs, different from existing benchmarks. Various graph neural network architectures are evaluated on both variants, showing consistent performance differences based on the structural representation chosen. This highlights the significance of representation choice in model behavior and indicates the value of RelSC in advancing graph regression methods.<br /><br />Summary: <div>
arXiv:2505.23875v1 Announce Type: new 
Abstract: Graph-level regression underpins many real-world applications, yet public benchmarks remain heavily skewed toward molecular graphs and citation networks. This limited diversity hinders progress on models that must generalize across both homogeneous and heterogeneous graph structures. We introduce RelSC, a new graph-regression dataset built from program graphs that combine syntactic and semantic information extracted from source code. Each graph is labelled with the execution-time cost of the corresponding program, providing a continuous target variable that differs markedly from those found in existing benchmarks. RelSC is released in two complementary variants. RelSC-H supplies rich node features under a single (homogeneous) edge type, while RelSC-M preserves the original multi-relational structure, connecting nodes through multiple edge types that encode distinct semantic relationships. Together, these variants let researchers probe how representation choice influences model behaviour. We evaluate a diverse set of graph neural network architectures on both variants of RelSC. The results reveal consistent performance differences between the homogeneous and multi-relational settings, emphasising the importance of structural representation. These findings demonstrate RelSC's value as a challenging and versatile benchmark for advancing graph regression methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size</title>
<link>https://arxiv.org/abs/2505.23876</link>
<guid>https://arxiv.org/abs/2505.23876</guid>
<content:encoded><![CDATA[
<div> neural networks, multilayer perceptron, metric recognition methods, MNIST dataset, training 

Summary:
The paper explores the effectiveness of utilizing multilayer perceptron neural networks with pre-calculated weights in metric recognition tasks. The study compares the performance of neural networks trained with pre-calculated weights against those with randomly initialized weights on various MNIST dataset sizes. Results indicate that neural networks with pre-calculated weights exhibit quicker training times and greater robustness when faced with reduced training datasets. This suggests that the analytical calculation of weights can significantly enhance the efficiency and reliability of multilayer perceptrons for metric recognition applications. <div>
arXiv:2505.23876v1 Announce Type: new 
Abstract: The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actor-Critic based Online Data Mixing For Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2505.23878</link>
<guid>https://arxiv.org/abs/2505.23878</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data mixing methods, online data mixing, actor-critic network, pretraining

Summary:<br /><br />
The paper introduces a new method called actor-critic based online data mixing (AC-ODM) to improve the pretraining process of Large Language Models (LLMs). By leveraging auxiliary actor-critic networks, AC-ODM dynamically adjusts domain weights and considers intra-domain interactions during data mixing. The method transfers the sampling strategy from a small proxy LLM to a larger target LLM, increasing efficiency and accelerating convergence. Numerical results show that AC-ODM-410M, using a proxy LLM with 410M parameters, achieves optimal validation perplexity 71% faster and improves performance on benchmarks like zero-shot MMLU and HumanEval. This approach addresses the limitations of previous data mixing techniques by adapting to training dynamics and optimizing data composition for better generalization in LLMs. <div>
arXiv:2505.23878v1 Announce Type: new 
Abstract: The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data</title>
<link>https://arxiv.org/abs/2505.23879</link>
<guid>https://arxiv.org/abs/2505.23879</guid>
<content:encoded><![CDATA[
<div> CNN-LSTM, deep learning, COVID-19 severity, spike protein sequences, clinical data
<br />
Summary:
A hybrid CNN-LSTM deep learning model was developed to predict COVID-19 severity using spike protein sequences and clinical metadata from South American patients. The model achieved high performance metrics, with an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%. Training stability was achieved at 85% accuracy with minimal overfitting. The prevalent viral lineages and clades aligned with regional epidemiological trends, indicating potential associations between viral genetics and clinical outcomes. The study demonstrates the effectiveness of AI in predicting disease severity and highlights the importance of genomic surveillance for precision public health interventions. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
<br />  <div>
arXiv:2505.23879v1 Announce Type: new 
Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Training Done Right</title>
<link>https://arxiv.org/abs/2505.23884</link>
<guid>https://arxiv.org/abs/2505.23884</guid>
<content:encoded><![CDATA[
<div> Large Chunk Test-Time Training, Context dependencies, Fast weights, State capacity, Scaling, 

Summary:
The paper introduces a new approach called Large Chunk Test-Time Training (LaCT) to handle context dependencies in deep learning models. LaCT adapts part of the model's weights (fast weights) during inference, similar to recurrent states in RNNs, but with larger chunk sizes ranging from 2K to 1M tokens. This approach significantly improves hardware utilization and allows for scaling of nonlinear state sizes without requiring complex kernel implementations. By using large chunk updates, LaCT enhances state capacity and enables efficient handling of long-context data in various tasks such as language models, auto-regressive video diffusion, and novel view synthesis. The method showcases successful experiments on tasks with sequences ranging up to 56K tokens, including novel view synthesis with a context length of 1 million. LaCT opens new possibilities for long-context modeling and test-time training in deep learning research. 

<br /><br />Summary: <div>
arXiv:2505.23884v1 Announce Type: new 
Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Website: https://tianyuanzhang.com/projects/ttt-done-right
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling</title>
<link>https://arxiv.org/abs/2505.23913</link>
<guid>https://arxiv.org/abs/2505.23913</guid>
<content:encoded><![CDATA[
<div> surrogate model, acquisition function, Bayesian optimization, deep generative model, efficiency<br />
<br />
Summary: 
This article presents a new approach to optimizing expensive black-box functions using Bayesian optimization (BO). The traditional BO process involves fitting a surrogate model and optimizing an acquisition function at each iteration. However, this study proposes a zero-shot solution that eliminates the need for surrogate fitting or acquisition function optimization. By using a pre-trained deep generative model to sample from the posterior over the optimum point, equivalent to Thompson sampling, this approach achieves significant efficiency gains. The method outperforms Gaussian process-based BO, offering a more cost-effective and time-efficient solution for high-throughput optimization tasks. This innovative approach enables efficient parallel and distributed BO, making it a promising advancement in the field of optimization. <div>
arXiv:2505.23913v1 Announce Type: new 
Abstract: The optimization of expensive black-box functions is ubiquitous in science and engineering. A common solution to this problem is Bayesian optimization (BO), which is generally comprised of two components: (i) a surrogate model and (ii) an acquisition function, which generally require expensive re-training and optimization steps at each iteration, respectively. Although recent work enabled in-context surrogate models that do not require re-training, virtually all existing BO methods still require acquisition function maximization to select the next observation, which introduces many knobs to tune, such as Monte Carlo samplers and multi-start optimizers. In this work, we propose a completely in-context, zero-shot solution for BO that does not require surrogate fitting or acquisition function optimization. This is done by using a pre-trained deep generative model to directly sample from the posterior over the optimum point. We show that this process is equivalent to Thompson sampling and demonstrate the capabilities and cost-effectiveness of our foundation model on a suite of real-world benchmarks. We achieve an efficiency gain of more than 35x in terms of wall-clock time when compared with Gaussian process-based BO, enabling efficient parallel and distributed BO, e.g., for high-throughput optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thompson Sampling in Online RLHF with General Function Approximation</title>
<link>https://arxiv.org/abs/2505.23927</link>
<guid>https://arxiv.org/abs/2505.23927</guid>
<content:encoded><![CDATA[
<div> Thompson sampling, reinforcement learning, human feedback, language models, regret bound  
Summary:  
- The study focuses on the theoretical efficiency of reinforcement learning from human feedback (RLHF) algorithms in aligning large language models with human preference.  
- A model-free posterior sampling algorithm for online RLHF is proposed, inspired by Thompson sampling, with an O(sqrt{T}) regret bound.  
- The Bellman eluder dimension is used as the complexity measure, and the algorithm's performance depends on the horizon, BE dimension, and log-bracketing number of the function class.  
- The concentration-type inequality of the squared Bellman error bound is established based on the maximum likelihood estimator generalization bound, crucial for obtaining the eluder-type regret bound.  
- The theoretical analysis provides insights into the statistical efficiency of RLHF algorithms in the online setting, contributing to the understanding of reinforcement learning with human feedback.  
<br /><br />Summary: <div>
arXiv:2505.23927v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has achieved great empirical success in aligning large language models (LLMs) with human preference, and it is of great importance to study the statistical efficiency of RLHF algorithms from a theoretical perspective. In this work, we consider the online RLHF setting where the preference data is revealed during the learning process and study action value function approximation. We design a model-free posterior sampling algorithm for online RLHF inspired by Thompson sampling and provide its theoretical guarantee. Specifically, we adopt Bellman eluder (BE) dimension as the complexity measure of the function class and establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other multiplicative factor depending on the horizon, BE dimension and the $log$-bracketing number of the function class. Further, in the analysis, we first establish the concentration-type inequality of the squared Bellman error bound based on the maximum likelihood estimator (MLE) generalization bound, which plays the crucial rules in obtaining the eluder-type regret bound and may be of independent interest.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIRD: Behavior Induction via Representation-structure Distillation</title>
<link>https://arxiv.org/abs/2505.23933</link>
<guid>https://arxiv.org/abs/2505.23933</guid>
<content:encoded><![CDATA[
<div> Keywords: Behavior Induction, Deep Learning, Robustness, Transfer Learning, Representation Structure

Summary:
BIRD (Behavior Induction via Representation-structure Distillation) is a framework designed to transfer human-aligned behavioral properties, such as robustness, fairness, and honesty, from a teacher model to a student model. This framework aims to address the challenge of forgetting aligned behavior during fine-tuning and the high cost of collecting task-specific data. BIRD has shown improved performance in out-of-distribution robustness in image classification, outperforming traditional methods like fine-tuning and transfer learning. It is effective even when the teacher model is simpler and significantly smaller than the student model. By analyzing the teacher's internal representation structure, BIRD can identify key properties that contribute to successful transfer of aligned behavior, offering practical guidance for teacher selection and design. This framework transforms small, well-aligned models into scalable alignment seeds, removing a bottleneck in deploying safe AI systems in real-world scenarios.
<br /><br />Summary: <div>
arXiv:2505.23933v1 Announce Type: new 
Abstract: Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searching Neural Architectures for Sensor Nodes on IoT Gateways</title>
<link>https://arxiv.org/abs/2505.23939</link>
<guid>https://arxiv.org/abs/2505.23939</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Networks, Edge Computing, Internet of Things, Privacy, Machine Learning

Summary: 
Neural Networks designing method at the edge for IoT applications, enabling ML access without data sharing. The approach runs on IoT gateways, designing custom NNs for sensor nodes on the local network, suitable for HIoT and IIoT. This method facilitates personalized healthcare and industrial services like quality control and predictive maintenance. The process prevents data disclosure to protect sensitive information, maintaining industrial secrets and personal data privacy. Experimental results on the Visual Wake Words dataset demonstrate state-of-the-art performance achieved in less than 10 hours on the Raspberry Pi Zero 2. 
<br /><br />Summary: <div>
arXiv:2505.23939v1 Announce Type: new 
Abstract: This paper presents an automatic method for the design of Neural Networks (NNs) at the edge, enabling Machine Learning (ML) access even in privacy-sensitive Internet of Things (IoT) applications. The proposed method runs on IoT gateways and designs NNs for connected sensor nodes without sharing the collected data outside the local network, keeping the data in the site of collection. This approach has the potential to enable ML for Healthcare Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing hardware-friendly and custom NNs at the edge for personalized healthcare and advanced industrial services such as quality control, predictive maintenance, or fault diagnosis. By preventing data from being disclosed to cloud services, this method safeguards sensitive information, including industrial secrets and personal data. The outcomes of a thorough experimental session confirm that -- on the Visual Wake Words dataset -- the proposed approach can achieve state-of-the-art results by exploiting a search procedure that runs in less than 10 hours on the Raspberry Pi Zero 2.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Language Models are Biased</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, vision language models, biases, counting accuracy, automated framework

Summary:
State-of-the-art vision language models (VLMs) are found to be strongly biased, impacting their accuracy on visual tasks such as counting and identification. These biases are evident across various domains including animals, logos, chess, board games, optical illusions, and patterned grids. Even when instructed to double-check results or rely solely on image details, VLMs only show a slight improvement in counting accuracy. The study showcases how prior knowledge encoded in large language models can lead to inaccurate responses in visual tasks. An automated framework for testing biases in VLMs is introduced, highlighting the need to address and mitigate these biases for better model performance. The research sheds light on a significant failure mode in VLMs and emphasizes the importance of assessing and rectifying biases in model development. <div>
arXiv:2505.23941v1 Announce Type: new 
Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations</title>
<link>https://arxiv.org/abs/2505.23942</link>
<guid>https://arxiv.org/abs/2505.23942</guid>
<content:encoded><![CDATA[
<div> Keywords: SG-Blend, activation functions, deep neural networks, SSwish, GELU

Summary:
SG-Blend is a novel activation function designed to optimize deep neural networks by blending SSwish, a first-order symmetric variant of Swish, and the established GELU through dynamic interpolation. The goal is to combine the controlled non-monotonicity and symmetry of SSwish with the smooth, probabilistic profile of GELU to achieve a more universally robust balance between model expressivity and gradient stability. Empirical evaluations across various natural language and computer vision tasks and models show performance improvements with negligible computational overhead. SG-Blend serves as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code for SG-Blend is available for reference. 

<br /><br />Summary: <div>
arXiv:2505.23942v1 Announce Type: new 
Abstract: The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at https://anonymous.4open.science/r/SGBlend-6CBC.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Future of Bayesian Prediction Is Prior-Fitted</title>
<link>https://arxiv.org/abs/2505.23947</link>
<guid>https://arxiv.org/abs/2505.23947</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, prior-data fitted networks, Bayesian models, amortized inference, data-scarce problems <br />
Summary: 
Prior-Data Fitted Networks (PFNs) are neural network methods trained on artificial datasets to capture prior distributions. They offer efficient pre-training in low-data scenarios and are increasingly used in complex domains. PFNs and amortized inference are seen as the future of Bayesian inference due to their ability to tackle data-scarce problems. The paper argues for the importance of PFNs in research and explores their potential and limitations. With computational resources expanding and real-world data generation stagnant, PFNs are highlighted as a crucial tool in various applications. The field has evolved from small tasks to handling larger datasets, indicating their versatility and relevance in modern Bayesian modeling. The paper advocates for continued exploration and development of PFNs to address the evolving challenges in Bayesian inference. 

<br /><br />Summary: <div>
arXiv:2505.23947v1 Announce Type: new 
Abstract: Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks</title>
<link>https://arxiv.org/abs/2505.23949</link>
<guid>https://arxiv.org/abs/2505.23949</guid>
<content:encoded><![CDATA[
<div> Pruning, Neural Networks, N:M Sparsity, Transposable Masks, Efficient Solver <br />
<br />
Summary: Network pruning reduces computational requirements in neural networks by utilizing N:M sparsity. However, this sparsity pattern does not efficiently accelerate both forward and backward passes during training. A new method is introduced to efficiently generate transposable N:M sparse masks that scale to large models. The approach formulates mask generation as optimal transport problems, utilizing entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Implementing this approach using tensors leverages GPU parallelism, achieving significant speedup with minimal error compared to existing methods. The generated transposable masks can be integrated with various pruning frameworks to produce models with arbitrary N:M values. Experimental results show that models with transposable 16:32 sparsity maintain performance close to their standard N:M counterparts and outperform standard 2:4 sparse models, demonstrating the practical value of the proposed approach. <br /> <div>
arXiv:2505.23949v1 Announce Type: new 
Abstract: Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective</title>
<link>https://arxiv.org/abs/2505.23954</link>
<guid>https://arxiv.org/abs/2505.23954</guid>
<content:encoded><![CDATA[
<div> allocation, resources, ML models, misreporting, causally-motivated

Summary:
- ML models are used for resource allocation, where affected agents may strategically modify their features.
- Distinguishing between deceptive changes and genuine modifications is challenging.
- A causally-motivated approach is proposed to identify and quantify misreporting by analyzing causal effects.
- Misreported features do not causally affect downstream variables, allowing for the detection of deception.
- The approach is validated using semi-synthetic and real Medicare datasets, proving its effectiveness in identifying misreporting in real-world scenarios.

<br /><br />Summary: <div>
arXiv:2505.23954v1 Announce Type: new 
Abstract: In settings where ML models are used to inform the allocation of resources, agents affected by the allocation decisions might have an incentive to strategically change their features to secure better outcomes. While prior work has studied strategic responses broadly, disentangling misreporting from genuine modification remains a fundamental challenge. In this paper, we propose a causally-motivated approach to identify and quantify how much an agent misreports on average by distinguishing deceptive changes in their features from genuine modification. Our key insight is that, unlike genuine modification, misreported features do not causally affect downstream variables (i.e., causal descendants). We exploit this asymmetry by comparing the causal effect of misreported features on their causal descendants as derived from manipulated datasets against those from unmanipulated datasets. We formally prove identifiability of the misreporting rate and characterize the variance of our estimator. We empirically validate our theoretical results using a semi-synthetic and real Medicare dataset with misreported data, demonstrating that our approach can be employed to identify misreporting in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</title>
<link>https://arxiv.org/abs/2505.23960</link>
<guid>https://arxiv.org/abs/2505.23960</guid>
<content:encoded><![CDATA[
<div> quantitative methods, representational spaces, systematic structure, deep-learning models, multi-agent reinforcement learning models <br />
Summary: This thesis introduces quantitative methods to analyze representational spaces in large-scale neural networks. It identifies structural primitives and information theoretic quantifications to understand how deep-learning models learn to represent information and what structures drive generalization. The research focuses on multi-agent reinforcement learning models, sequence-to-sequence models, and Large Language Models, shedding light on how large-scale distributed models of cognition learn. The experiments demonstrate parallels between the structures of language and those driving contemporary neural networks' performance. Additionally, a novel approach to estimating vector space entropy is introduced, allowing for analysis across models of varying parameter sizes from one million to 12 billion. This analysis aims to enhance our understanding of how neural networks learn and their similarities to human cognitive systems. <br /><br />Summary: <div>
arXiv:2505.23960v1 Announce Type: new 
Abstract: Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Approximations for Hard Graph Problems using Predictions</title>
<link>https://arxiv.org/abs/2505.23967</link>
<guid>https://arxiv.org/abs/2505.23967</guid>
<content:encoded><![CDATA[
<div> prediction model, graph problems, approximation algorithms, MaxCut, Vertex Cover, Set Cover<br />
Summary: <br />
The article introduces improved approximation algorithms for NP-hard graph problems by incorporating predictions based on past data. Building upon the $\varepsilon$-prediction framework, the edge-based model uses two bits of information per edge to make predictions about optimal solutions. Even with weak correlations, these predictions help break approximation barriers in MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems. The algorithms developed improve approximation ratios by satisfying constraints related to high degree vertices using predictions and low-degree vertices without predictions, and combining the results effectively. <div>
arXiv:2505.23967v1 Announce Type: new 
Abstract: We design improved approximation algorithms for NP-hard graph problems by incorporating predictions (e.g., learned from past data). Our prediction model builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad, d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based version of this model, where each edge provides two bits of information, corresponding to predictions about whether each of its endpoints belong to an optimal solution. Even with weak predictions where each bit is only $\varepsilon$-correlated with the true solution, this information allows us to break approximation barriers in the standard setting. We develop algorithms with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems (among others). Across these problems, our algorithms share a unifying theme, where we separately satisfy constraints related to high degree vertices (using predictions) and low-degree vertices (without using predictions) and carefully combine the answers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training</title>
<link>https://arxiv.org/abs/2505.23971</link>
<guid>https://arxiv.org/abs/2505.23971</guid>
<content:encoded><![CDATA[
<div> keywords: batch size, language models, training, gradient noise, CBS <br />
Summary: 
The paper discusses the importance of determining the right batch size for training language models at scale. McCandlish et al. proposed a method to estimate the critical batch size (CBS) based on gradient noise scale, but this method has limitations. The authors introduce an empirical approach to directly measure the CBS and observe how it evolves during training. They find that the CBS starts near 0, increases rapidly initially, then plateaus as training progresses. This trend is consistent across different model sizes (1B and 7B), suggesting that insights from smaller training runs can inform larger-scale training runs. The study advocates for batch size warmup as a method to train language models reliably at large batch sizes. By implementing batch size warmup, they successfully train a model to achieve better loss with fewer gradient steps. This approach demonstrates the potential to enhance data parallelism without sacrificing performance. <br /> <br />Summary: <div>
arXiv:2505.23971v1 Announce Type: new 
Abstract: The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is too large will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a critical batch size (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to directly measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate batch size warmup as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Deadline and Batch Layered Synchronized Federated Learning</title>
<link>https://arxiv.org/abs/2505.23973</link>
<guid>https://arxiv.org/abs/2505.23973</guid>
<content:encoded><![CDATA[
<div> Framework, Federated Learning, Optimization, Convergence Analysis, Heterogeneous Conditions
Summary:
ADEL-FL introduces a novel framework for Federated Learning that optimally adjusts per-round deadlines and user-specific batch sizes. By jointly optimizing these parameters, the approach minimizes latency bottlenecks caused by slower devices (stragglers) in synchronous FL settings. A constrained optimization problem is formulated to minimize the expected L2 distance to the global optimum under total training time and global rounds. Convergence analysis under exponential compute models shows that ADEL-FL provides unbiased updates with bounded variance. Extensive experiments demonstrate ADEL-FL's superior performance in convergence rate and final accuracy compared to alternative methods in the presence of device heterogeneity. The approach effectively addresses the challenges posed by strict time constraints in Federated Learning settings, offering a promising solution for collaborative model training across distributed edge devices while preserving data privacy.<br /><br />Summary: <div>
arXiv:2505.23973v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed edge devices while preserving data privacy, and typically operates in a round-based synchronous manner. However, synchronous FL suffers from latency bottlenecks due to device heterogeneity, where slower clients (stragglers) delay or degrade global updates. Prior solutions, such as fixed deadlines, client selection, and layer-wise partial aggregation, alleviate the effect of stragglers, but treat round timing and local workload as static parameters, limiting their effectiveness under strict time constraints. We propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines and user-specific batch sizes for layer-wise aggregation. Our approach formulates a constrained optimization problem minimizing the expected L2 distance to the global optimum under total training time and global rounds. We provide a convergence analysis under exponential compute models and prove that ADEL-FL yields unbiased updates with bounded variance. Extensive experiments demonstrate that ADEL-FL outperforms alternative methods in both convergence rate and final accuracy under heterogeneous conditions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization</title>
<link>https://arxiv.org/abs/2505.23987</link>
<guid>https://arxiv.org/abs/2505.23987</guid>
<content:encoded><![CDATA[
<div> Keywords: molecule optimization, drug design, instruction-tuning, property-specific objectives, LLMs

Summary:
The study introduces C-MuMOInstruct, a dataset aimed at multi-property optimization in drug design with explicit, property-specific objectives. Using this dataset, the researchers developed GeLLMO-Cs, a series of instruction-tuned language models (LLMs) capable of targeted property-specific optimization. The experiments conducted across various tasks demonstrated that GeLLMO-Cs outperform strong baselines consistently, achieving success rates up to 126% higher. Moreover, the GeLLMO-Cs exhibit exceptional 0-shot generalization to new optimization tasks and unseen instructions, indicating their potential to support diverse and realistic optimizations with property-specific goals. The dataset C-MuMOInstruct and the code for GeLLMO-Cs are publicly available for further research and applications. The study signifies a significant advancement in addressing the complex requirements of molecule optimization in drug design, offering a promising solution for achieving pharmaceutically relevant molecular properties efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2505.23987v1 Announce Type: new 
Abstract: In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24003</link>
<guid>https://arxiv.org/abs/2505.24003</guid>
<content:encoded><![CDATA[
<div> Keywords: time series, multi-modal views, long-term forecasting, large vision models, trend-seasonal decomposition

Summary:
DMMV introduces a novel approach to enhance long-term time series forecasting by leveraging multi-modal views of numerical sequences in the form of images and texts. By integrating trend-seasonal decomposition and a unique backcast residual method, DMMV outperforms existing models in predicting future trends across diverse datasets. The use of large vision models (LVMs) for forecasting introduces a bias towards forecasting periods, which DMMV addresses effectively. Comparative evaluations show that DMMV achieves the best mean squared error on the majority of benchmark datasets, highlighting its superior performance in long-term time series forecasting. This innovative framework presents a promising direction for utilizing multi-modal views to improve forecasting accuracy. <br /><br />Summary: <div>
arXiv:2505.24003v1 Announce Type: new 
Abstract: Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, applying LVMs to LTSF poses an inductive bias towards "forecasting periods". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art (SOTA) models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How far away are truly hyperparameter-free learning algorithms?</title>
<link>https://arxiv.org/abs/2505.24005</link>
<guid>https://arxiv.org/abs/2505.24005</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperparameter tuning, machine learning systems, deep neural networks, optimization methods, learning-rate-free methods 

Summary: 
Despite advances in methodology, hyperparameter tuning remains crucial for machine learning system development. Deep neural networks have numerous optimization and regularization hyperparameters requiring careful tuning per workload. The ideal scenario would involve training algorithms with default settings that perform well across various workloads without the need for workload-specific tuning. Recent research focuses on reducing hyperparameters, particularly the learning rate and its schedule. This paper evaluates the potential of learning-rate-free methods as components of hyperparameter-free methods. Testing against the AlgoPerf benchmark revealed that default settings performed poorly, necessitating a search for configurations that achieved high performance across all workloads simultaneously. While the best learning-rate-free methods showed improved performance, they still slightly lagged behind a NadamW baseline. This study highlights the need for further enhancement of learning-rate-free methods and underscores the importance of benchmarking against strong, workload-agnostic baselines. 

<br /><br />Summary: <div>
arXiv:2505.24005v1 Announce Type: new 
Abstract: Despite major advances in methodology, hyperparameter tuning remains a crucial (and expensive) part of the development of machine learning systems. Even ignoring architectural choices, deep neural networks have a large number of optimization and regularization hyperparameters that need to be tuned carefully per workload in order to obtain the best results. In a perfect world, training algorithms would not require workload-specific hyperparameter tuning, but would instead have default settings that performed well across many workloads. Recently, there has been a growing literature on optimization methods which attempt to reduce the number of hyperparameters -- particularly the learning rate and its accompanying schedule. Given these developments, how far away is the dream of neural network training algorithms that completely obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as components of hyperparameter-free methods. We freeze their (non-learning rate) hyperparameters to default values, and score their performance using the recently-proposed AlgoPerf: Training Algorithms benchmark. We found that literature-supplied default settings performed poorly on the benchmark, so we performed a search for hyperparameter configurations that performed well across all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free methods had much improved performance but still lagged slightly behind a similarly calibrated NadamW baseline in overall benchmark score. Our results suggest that there is still much room for improvement for learning-rate-free methods, and that testing against a strong, workload-agnostic baseline is important to improve hyperparameter reduction techniques.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rich and the Simple: On the Implicit Bias of Adam and SGD</title>
<link>https://arxiv.org/abs/2505.24022</link>
<guid>https://arxiv.org/abs/2505.24022</guid>
<content:encoded><![CDATA[
<div> Adam, optimization algorithm, deep learning, implicit bias, gradient descent<br />
<br />
Summary: <br />
The paper explores the implicit bias of Adam, a popular optimization algorithm in deep learning, compared to standard first-order methods like gradient descent (GD). While neural networks trained with SGD tend to favor simple solutions, Adam is shown to be more resistant to simplicity bias. By analyzing the biases of Adam and GD when training two-layer ReLU neural networks, the study reveals that GD exhibits a simplicity bias, leading to suboptimal linear decision boundaries, while Adam creates richer, nonlinear boundaries closer to optimal solutions. Theoretical analysis and empirical results demonstrate that Adam's diversity in features leads to higher test accuracy and better generalization across datasets with spurious correlations, where networks trained with SGD struggle. <div>
arXiv:2505.24022v1 Announce Type: new 
Abstract: Adam is the de facto optimization algorithm for several deep learning applications, but an understanding of its implicit bias and how it differs from other algorithms, particularly standard first-order methods such as (stochastic) gradient descent (GD), remains limited. In practice, neural networks trained with SGD are known to exhibit simplicity bias -- a tendency to find simple solutions. In contrast, we show that Adam is more resistant to such simplicity bias. To demystify this phenomenon, in this paper, we investigate the differences in the implicit biases of Adam and GD when training two-layer ReLU neural networks on a binary classification task involving synthetic data with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting in a linear decision boundary with a suboptimal margin, whereas Adam leads to much richer and more diverse features, producing a nonlinear boundary that is closer to the Bayes' optimal predictor. This richer decision boundary also allows Adam to achieve higher test accuracy both in-distribution and under certain distribution shifts. We theoretically prove these results by analyzing the population gradients. To corroborate our theoretical findings, we present empirical results showing that this property of Adam leads to superior generalization across datasets with spurious correlations where neural networks trained with SGD are known to show simplicity bias and don't generalize well under certain distributional shifts.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</title>
<link>https://arxiv.org/abs/2505.24030</link>
<guid>https://arxiv.org/abs/2505.24030</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, Large Language Models (LLMs), Large Vision Models (LVMs), time series analysis, multi-modality <br />
<br />Summary:
The study investigates the utility of Large Vision Models (LVMs) in time series analysis, comparing their performance on classification and forecasting tasks using various datasets and baselines. The findings suggest that LVMs are effective for time series classification but face challenges in forecasting. While some LVMs show promise as forecasters, they are limited in their ability to utilize long look-back windows and exhibit a bias toward forecasting periods. The study highlights the need for further research to enhance LVMs and explore multimodal solutions for diverse time series tasks. <div>
arXiv:2505.24030v1 Announce Type: new 
Abstract: Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin</title>
<link>https://arxiv.org/abs/2505.24034</link>
<guid>https://arxiv.org/abs/2505.24034</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Distributed Training, Asynchronous RL Framework, GPU Clusters

Summary:
LlamaRL is a new asynchronous RL framework designed for efficiently training large-scale Language Models (LLMs) on GPU clusters. It is optimized for models with billions of parameters, ranging from 8B to 405B. The framework is built on PyTorch and features a streamlined, single-controller architecture for easy scalability and modularity. The asynchronous design of LlamaRL ensures strict RL speed-up, leading to efficiency gains of up to 10.7x compared to existing systems like DeepSpeed-Chat for a 405B-parameter model. By utilizing techniques such as colocated model offloading and distributed weight synchronization, LlamaRL demonstrates significant efficiency advantages that increase with model scale. This makes it well-suited for future large-scale RL training endeavors.

<br /><br />Summary: LlamaRL is an asynchronous RL framework optimized for training large-scale LLMs on GPU clusters. Built on PyTorch, it offers modularity and scalability, achieving up to 10.7x speed-up compared to existing systems. Through techniques like colocated model offloading and distributed weight synchronization, LlamaRL demonstrates superior efficiency gains that grow with model size, proving to be a promising solution for large-scale RL training. <div>
arXiv:2505.24034v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronTune: Towards Self-Guided Spurious Bias Mitigation</title>
<link>https://arxiv.org/abs/2505.24048</link>
<guid>https://arxiv.org/abs/2505.24048</guid>
<content:encoded><![CDATA[
<div> NeuronTune, spurious bias, deep neural networks, post hoc method, self-guided mitigation<br />
<br />
Summary:<br />
Deep neural networks often exhibit spurious bias, leading to reliance on non-essential features for predictions. Existing methods to mitigate this bias require external annotations, which may not be practical. NeuronTune is a post hoc method that intervenes in a model's decision process to regulate neurons causing spurious behaviors, without needing correlation annotations. Theoretical justification and experiments across various architectures and data modalities show that NeuronTune effectively reduces spurious bias in a self-guided manner, improving model robustness. <div>
arXiv:2505.24048v1 Announce Type: new 
Abstract: Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Gated Self-Attention</title>
<link>https://arxiv.org/abs/2505.24054</link>
<guid>https://arxiv.org/abs/2505.24054</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Multihead Differential Gated Self-Attention, noise resilience, contrast enhancement, lateral inhibition

Summary: 
The study introduces Multihead Differential Gated Self-Attention (M-DGSA) as a solution to address the vulnerability of Transformers to corrupted inputs. Inspired by biological neural circuits, M-DGSA incorporates an input-dependent gating mechanism grounded in lateral inhibition. The model includes excitatory and inhibitory branches per head that dynamically suppress attention noise. By fusing dual softmax maps with a sigmoid gate predicted from token embeddings, M-DGSA achieves context-aware contrast enhancement. This innovative approach seamlessly integrates into existing Transformer stacks with minimal computational overhead. Experimental results on vision and language benchmarks showcase the superior robustness of M-DGSA compared to vanilla Transformer, Vision Transformer, and Differential Transformer baselines. The study contributes a novel method for noise resilience in self-attention, combining principles of contrast enhancement and self-attention theory, backed by comprehensive cross-domain experiments. 

<br /><br />Summary: <div>
arXiv:2505.24054v1 Announce Type: new 
Abstract: Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self-attention treats all query-key interactions uniformly. Inspired by lateral inhibition in biological neural circuits and building on the recent use by the Differential Transformer's use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self-attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast-enhancement and self-attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs</title>
<link>https://arxiv.org/abs/2505.24055</link>
<guid>https://arxiv.org/abs/2505.24055</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, unsupervised domain adaptation, link prediction, message-passing mechanism, identity-preserving learning<br />
<br />Summary:
This paper introduces a novel framework for unsupervised domain adaptation in graph neural networks. It addresses the challenge of domain shift by leveraging link prediction to connect nodes between source and target graphs. This facilitates message-passing between the graphs and ensures that target nodes have "in-distribution" neighborhoods with the source domain. The framework modifies the target graph on the input level to reduce deviation from the source domain in the embedding space. It also includes an identity-preserving learning objective to prevent the loss of discriminative information in the target graph. Experimental results on real-world datasets demonstrate the effectiveness of this approach. <div>
arXiv:2505.24055v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have shown great ability for node classification on graphs. However, the success of GNNs relies on abundant labeled data, while obtaining high-quality labels is costly and challenging, especially for newly emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a classifier on the labeled source graph and adapts it to the unlabeled target graph, is attracting increasing attention. Various approaches have been proposed to alleviate the distribution shift between the source and target graphs to facilitate the classifier adaptation. However, most of them simply adopt existing UDA techniques developed for independent and identically distributed data to gain domain-invariant node embeddings for graphs, which do not fully consider the graph structure and message-passing mechanism of GNNs during the adaptation and will fail when label distribution shift exists among domains. In this paper, we proposed a novel framework that adopts link prediction to connect nodes between source and target graphs, which can facilitate message-passing between the source and target graphs and augment the target nodes to have ``in-distribution'' neighborhoods with the source domain. This strategy modified the target graph on the input level to reduce its deviation from the source domain in the embedding space and is insensitive to disproportional label distributions across domains. To prevent the loss of discriminative information in the target graph, we further design a novel identity-preserving learning objective, which guides the learning of the edge insertion module together with reconstruction and adaptation losses. Experimental results on real-world datasets demonstrate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition</title>
<link>https://arxiv.org/abs/2505.24059</link>
<guid>https://arxiv.org/abs/2505.24059</guid>
<content:encoded><![CDATA[
<div> Audio, video, phoneme recognition, multimodal learning, MRI corpus  
Summary:  
- Study developed models for phoneme recognition using single-speaker MRI data, aiming to understand the relationship between acoustics and articulation in speech.  
- Unimodal audio and video models as well as multimodal models were compared, showing similar performance in phonetic manner classes but differences in places of articulation.  
- Latent space analysis revealed similar encoding of phonetic space across audio and multimodal models.  
- Attention weights of the models highlighted differences in acoustic and articulatory timing for certain phonemes.  
- Previous multi-speaker corpus studies were limited in understanding detailed relationships between acoustics and articulation due to cross-speaker variability, emphasizing the importance of single-speaker data in multimodal learning research.  

<br /><br />Summary: <div>
arXiv:2505.24059v1 Announce Type: new 
Abstract: Although many previous studies have carried out multimodal learning with real-time MRI data that captures the audio-visual kinematics of the vocal tract during speech, these studies have been limited by their reliance on multi-speaker corpora. This prevents such models from learning a detailed relationship between acoustics and articulation due to considerable cross-speaker variability. In this study, we develop unimodal audio and video models as well as multimodal models for phoneme recognition using a long-form single-speaker MRI corpus, with the goal of disentangling and interpreting the contributions of each modality. Audio and multimodal models show similar performance on different phonetic manner classes but diverge on places of articulation. Interpretation of the models' latent space shows similar encoding of the phonetic space across audio and multimodal models, while the models' attention weights highlight differences in acoustic and articulatory timing for certain phonemes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterising the Inductive Biases of Neural Networks on Boolean Data</title>
<link>https://arxiv.org/abs/2505.24060</link>
<guid>https://arxiv.org/abs/2505.24060</guid>
<content:encoded><![CDATA[
<div> Neural networks, overparameterization, generalization, NTK task-model alignment, feature learning <br />
<br />
Summary: This article explores the ability of deep neural networks to generalize well across various tasks despite being heavily overparameterized. By studying the training dynamics of depth-2 discrete fully connected networks on Boolean functions, the authors establish a connection between the network's inductive prior, training dynamics, feature learning, and eventual generalization. Through a Monte Carlo learning algorithm, predictable training dynamics and interpretable features emerge, shedding light on how inductive bias and feature formation drive generalization. This end-to-end case study offers insights into the mechanisms underlying deep neural network generalization and highlights the importance of understanding the interplay between inductive priors, training dynamics, and feature learning in achieving robust generalization performance. <br /><br /> <div>
arXiv:2505.24060v1 Announce Type: new 
Abstract: Deep neural networks are renowned for their ability to generalise well across diverse tasks, even when heavily overparameterized. Existing works offer only partial explanations (for example, the NTK-based task-model alignment explanation neglects feature learning). Here, we provide an end-to-end, analytically tractable case study that links a network's inductive prior, its training dynamics including feature learning, and its eventual generalisation. Specifically, we exploit the one-to-one correspondence between depth-2 discrete fully connected networks and disjunctive normal form (DNF) formulas by training on Boolean functions. Under a Monte Carlo learning algorithm, our model exhibits predictable training dynamics and the emergence of interpretable features. This framework allows us to trace, in detail, how inductive bias and feature formation drive generalisation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning</title>
<link>https://arxiv.org/abs/2505.24061</link>
<guid>https://arxiv.org/abs/2505.24061</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, neuronal activity loss, tau-dormant neuron ratio, GraMa, ReGraMa <br />
Summary: <br />
1. Deep reinforcement learning agents often experience neuronal activity loss hindering their ability to adapt and learn continuously. <br />
2. The traditional method to address this issue, tau-dormant neuron ratio, lacks effectiveness in complex architectures. <br />
3. Maintaining a neuron's learning capacity is crucial for advanced RL agents, leading to the introduction of GraMa, a metric focusing on gradient-based adaptability. <br />
4. GraMa successfully identifies inactive neurons in diverse architectures and facilitates improved learning through ReGraMa, a reset process guided by GraMa. <br />
5. The application of ReGraMa enhances learning performance across various deep RL algorithms and benchmarks, including MuJoCo and the DeepMind Control Suite. <br /> <div>
arXiv:2505.24061v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical power in more complex architectures. To address this, we argue that in advanced RL agents, maintaining a neuron's learning capacity, its ability to adapt via gradient updates, is more critical than preserving its expressive ability. Based on this insight, we shift the statistical objective from activations to gradients, and introduce GraMa (Gradient Magnitude Neural Activity Metric), a lightweight, architecture-agnostic metric for quantifying neuron-level learning capacity. We show that GraMa effectively reveals persistent neuron inactivity across diverse architectures, including residual networks, diffusion models, and agents with varied activation functions. Moreover, resetting neurons guided by GraMa (ReGraMa) consistently improves learning performance across multiple deep RL algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Primal-Dual Neural Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2505.24067</link>
<guid>https://arxiv.org/abs/2505.24067</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Algorithmic Reasoning, primal-dual paradigm, Graph Neural Networks, approximation algorithms, real-world datasets

Summary: 
Neural Algorithmic Reasoning (NAR) is a framework that trains neural networks to simulate classical algorithms, allowing for structured and interpretable reasoning on complex data. This study introduces a general NAR framework based on the primal-dual paradigm, a method for developing efficient approximation algorithms. By connecting primal and dual variables in a bipartite representation, the model aligns primal-dual algorithms with Graph Neural Networks. Additionally, optimal solutions from small instances are incorporated to improve the model's reasoning capabilities. Empirical results show that the model not only simulates but also surpasses approximation algorithms for various tasks, demonstrating robust generalization to larger and out-of-distribution graphs. The framework's practical utility is exemplified by integrating it with commercial solvers and applying it to real-world datasets. <br /><br />Summary: <div>
arXiv:2505.24067v1 Announce Type: new 
Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures</title>
<link>https://arxiv.org/abs/2505.24069</link>
<guid>https://arxiv.org/abs/2505.24069</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, structural reasoning, data structures, benchmark, evaluation

Summary:
Large language models (LLMs) are increasingly used in real-world tasks that require data manipulation. A new benchmark called DSR-Bench has been introduced to evaluate LLMs' structural reasoning abilities through data structures, providing interpretable representations of data relationships. This benchmark includes various data structures, operations, and problem instances for detailed analysis. The evaluation process is automated and deterministic, ensuring objectivity and scalability. Results show that instruction-tuned models struggle with basic reasoning tasks, while reasoning-oriented models perform better but still face challenges with complex structures. Models often struggle with multi-dimensional data and natural language tasks, indicating a gap for real-world applications. These findings highlight the need for further improvement in LLMs' reasoning capabilities for practical deployment.

<br /><br />Summary: <div>
arXiv:2505.24069v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals</title>
<link>https://arxiv.org/abs/2505.24085</link>
<guid>https://arxiv.org/abs/2505.24085</guid>
<content:encoded><![CDATA[
<div> deep learning, atrial fibrillation, gradient boosting, autoencoder, AF detection

Summary:
The study introduces a novel hybrid methodology for atrial fibrillation (AF) detection, combining unsupervised deep learning with boosting models. A deep convolutional autoencoder (DCAE) is integrated with three boosting classifiers - AdaBoost, XGBoost, and LightGBM - to improve AF identification without manual feature extraction. The DCAE-LGBM model achieves impressive performance metrics, including an F1-score of 95.20% and a sensitivity of 99.99%, with a low inference latency of four seconds. This hybrid approach outperforms existing methods and meets the requirements for clinical deployment. By combining DCAE with gradient boosting, the system provides a reliable tool for automated AF detection in healthcare settings. <div>
arXiv:2505.24085v1 Announce Type: new 
Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with elevated health risks, where timely detection is pivotal for mitigating stroke-related morbidity. This study introduces an innovative hybrid methodology integrating unsupervised deep learning and gradient boosting models to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM (LGBM)-to harness their complementary advantages while addressing individual limitations. The proposed framework uniquely combines DCAE with gradient boosting, enabling end-to-end AF identification devoid of manual feature extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of 99.99%, and inference latency of four seconds, outperforming existing methods and aligning with clinical deployment requirements. The DCAE integration significantly enhances boosting models, positioning this hybrid system as a reliable tool for automated AF detection in clinical settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting</title>
<link>https://arxiv.org/abs/2505.24088</link>
<guid>https://arxiv.org/abs/2505.24088</guid>
<content:encoded><![CDATA[
<div> Proxy-FDA, fine-tuning, knowledge preservation, feature space, concept forgetting <br />
Summary: <br />
Proxy-FDA is a novel regularization method that aims to mitigate concept forgetting during fine-tuning of vision foundation models. It preserves structural knowledge in feature space through Feature Distribution Alignment using nearest neighbor graphs. By incorporating informative proxies to increase data diversity, Proxy-FDA significantly reduces forgetting and aligns feature distributions between pre-trained and fine-tuned models. The method demonstrates benefits across various fine-tuning settings and tasks like image classification, captioning, and VQA. A strong correlation between forgetting and distributional distance metrics is observed, highlighting the effectiveness of Proxy-FDA in maintaining knowledge while adapting models to new tasks. <div>
arXiv:2505.24088v1 Announce Type: new 
Abstract: Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. Experiments show that Proxy-FDA significantly reduces concept forgetting during fine-tuning, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end, few-shot and continual tuning) and across different tasks like image classification, captioning and VQA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Bayes-Optimal Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2505.24089</link>
<guid>https://arxiv.org/abs/2505.24089</guid>
<content:encoded><![CDATA[
<div> Bayes-optimal, membership inference attacks, i.i.d. data, graph-structured data, node-level MIAs, graph neural networks, BASE, G-BASE, classifier-based attacks, computational efficiency, optimal query strategies, RMIA, LiRA, Bayesian decision-theoretic framework<br />
Summary:<br />
The study presents membership inference attacks (MIAs) for both i.i.d. data and graph-structured data. By leveraging the Bayesian decision-theoretic framework, the researchers derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks. They introduce BASE and G-BASE, efficient approximations of the Bayes-optimal attack, with G-BASE demonstrating superior performance compared to existing classifier-based attacks. BASE, applicable to non-graph data as well, achieves performance on par or better than prior state-of-the-art MIAs like LiRA and RMIA at a lower computational cost. The study also establishes the equivalence between BASE and RMIA under a specific hyperparameter setting, offering a principled, Bayes-optimal rationale for the RMIA attack. <div>
arXiv:2505.24089v1 Announce Type: new 
Abstract: We develop practical and theoretically grounded membership inference attacks (MIAs) against both independent and identically distributed (i.i.d.) data and graph-structured data. Building on the Bayesian decision-theoretic framework of Sablayrolles et al., we derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks, addressing key open questions about optimal query strategies in the graph setting. We introduce BASE and G-BASE, computationally efficient approximations of the Bayes-optimal attack. G-BASE achieves superior performance compared to previously proposed classifier-based node-level MIA attacks. BASE, which is also applicable to non-graph data, matches or exceeds the performance of prior state-of-the-art MIAs, such as LiRA and RMIA, at a significantly lower computational cost. Finally, we show that BASE and RMIA are equivalent under a specific hyperparameter setting, providing a principled, Bayes-optimal justification for the RMIA attack.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke</title>
<link>https://arxiv.org/abs/2505.24101</link>
<guid>https://arxiv.org/abs/2505.24101</guid>
<content:encoded><![CDATA[
<div> Keywords: LOS prediction, acute stroke, machine learning, interpretable model, feature selection <br />
Summary: 
An interpretable multi-level stacking ensemble model was developed to predict length of stay (LOS) in acute stroke patients, using data from the Stroke Foundation Acute Audit in Australia. The model, developed separately for ischaemic and haemorrhagic stroke, outperformed logistic regression in predicting prolonged LOS in ischaemic stroke but not in haemorrhagic stroke. Candidate predictors were categorized into patient, clinical, and system domains, with key predictors refined using correlation-based approaches. Shared predictors for both types of stroke included rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and involvement of stroke care coordinators. The model achieved superior performance in ischaemic stroke, with further validation needed for haemorrhagic stroke in larger cohorts. SHAP analysis provided insights into the importance of these predictors in predicting LOS.<br /><br /> <div>
arXiv:2505.24101v1 Announce Type: new 
Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving care planning. Existing machine learning models have shown suboptimal predictive performance, limited generalisability, and have overlooked system-level factors. We aimed to enhance model efficiency, performance, and interpretability by refining predictors and developing an interpretable multi-level stacking ensemble model. Data were accessed from the biennial Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models were developed for ischaemic and haemorrhagic stroke separately. The outcome was prolonged LOS (the LOS above the 75th percentile). Candidate predictors (ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical, and system domains. Feature selection with correlation-based approaches was used to refine key predictors. The evaluation of models included discrimination (AUC), calibration curves, and interpretability (SHAP plots). In ischaemic stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance [AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However, the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for haemorrhagic. SHAP analysis identified shared predictors for both types of stroke: rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and stroke care coordinators involvement. An explainable ensemble model effectively predicted the prolonged LOS in ischaemic stroke. Further validation in larger cohorts is needed for haemorrhagic stroke.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs</title>
<link>https://arxiv.org/abs/2505.24110</link>
<guid>https://arxiv.org/abs/2505.24110</guid>
<content:encoded><![CDATA[
<div> Nondeterministic finite automata, ReLU neural networks, equivalence, symbolic simulation, regular language <br />
<br />
Summary: 
This study establishes the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks. It shows that ReLU activations can mimic nondeterministic branching, subset construction, and ε-closures precisely. The research proves that a three-layer ReLU network can accurately recognize any regular language accepted by an NFA of n states. Additionally, it demonstrates that gradient descent over these networks preserves symbolic semantics and acceptance behavior. Through extensive experiments, including tasks like parallel path tracking and acceptance classification, the study achieves high alignment with ground-truth automata. This work provides a complete symbolic simulation of NFAs within deep learning architectures, bridging automata theory with neural computation using ReLU dynamics. <div>
arXiv:2505.24110v1 Announce Type: new 
Abstract: We present a formal and constructive framework establishing the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks. By encoding automaton states as binary vectors and transitions as sparse linear layers, we show that ReLU activations simulate nondeterministic branching, subset construction, and $\epsilon$-closures in a mathematically precise manner. Our core theoretical results prove that a three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any regular language accepted by an $n$-state NFA-without recurrence, memory, or approximation. Furthermore, we show that gradient descent over structure-preserving networks preserves symbolic semantics and acceptance behavior. Extensive experiments across multiple validation tasks-including parallel path tracking, symbolic subset construction, $\epsilon$-closure convergence, acceptance classification, structural training invariants, and functional equivalence-achieve perfect or near-perfect empirical alignment with ground-truth automata. This work provides the first provably complete symbolic simulation of NFAs within standard deep learning architectures, uniting automata theory with neural computation through ReLU dynamics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</title>
<link>https://arxiv.org/abs/2505.24138</link>
<guid>https://arxiv.org/abs/2505.24138</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog/Mixed-Signal circuits, Multi-modal Large Language Models, AMSbench, circuit design, benchmark

Summary: 

Analog/Mixed-Signal (AMS) circuits are crucial in the IC industry, but automating their design has been challenging. Multi-modal Large Language Models (MLLMs) show promise in supporting AMS circuit analysis and design. To evaluate MLLM performance across AMS tasks, a benchmark suite called AMSbench was introduced. It includes 8000 test questions across circuit schematic perception, analysis, and design, assessing eight models. Results indicate limitations in complex reasoning and design tasks for current MLLMs. Advancements are needed to narrow the performance gap relative to human expertise and move towards fully automated AMS circuit design workflows. The data from this evaluation is available at the provided link. <br /><br />Summary: <div>
arXiv:2505.24138v1 Announce Type: new 
Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at https://huggingface.co/datasets/wwhhyy/AMSBench
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction</title>
<link>https://arxiv.org/abs/2505.24145</link>
<guid>https://arxiv.org/abs/2505.24145</guid>
<content:encoded><![CDATA[
<div> predictive modeling, computational fluid dynamics, machine learning, turbulent flows, stochastic differential equation

Summary:
The article introduces a novel conditional score-based diffusion model for multi-scenario fluid flow prediction. This model integrates an energy constraint based on statistical properties of turbulent flows, improving prediction quality with minimal training. The model has a simple and versatile architecture that does not require specific design, supporting plug-and-play enhancements for efficient solution generation. It features an efficient conditioning mechanism for training across different scenarios without model redesign. Various stochastic differential equation formulations are explored to demonstrate performance enhancements. Extensive experiments on complex fluid dynamics datasets show that the model consistently delivers stable, robust, and physically accurate predictions, even under turbulent conditions. With tuned parameters, it achieves accuracy across multiple scenarios while preserving key physical and statistical properties. The study provides a thorough analysis of the impact of stochastic differential equations and discusses the methodology across a range of fluid mechanics tasks. 

Summary: <br /><br /> <div>
arXiv:2505.24145v1 Announce Type: new 
Abstract: Building on recent advances in scientific machine learning and generative modeling for computational fluid dynamics, we propose a conditional score-based diffusion model designed for multi-scenarios fluid flow prediction. Our model integrates an energy constraint rooted in the statistical properties of turbulent flows, improving prediction quality with minimal training, while enabling efficient sampling at low cost. The method features a simple and general architecture that requires no problem-specific design, supports plug-and-play enhancements, and enables fast and flexible solution generation. It also demonstrates an efficient conditioning mechanism that simplifies training across different scenarios without demanding a redesign of existing models. We further explore various stochastic differential equation formulations to demonstrate how thoughtful design choices enhance performance. We validate the proposed methodology through extensive experiments on complex fluid dynamics datasets encompassing a variety of flow regimes and configurations. Results demonstrate that our model consistently achieves stable, robust, and physically faithful predictions, even under challenging turbulent conditions. With properly tuned parameters, it achieves accurate results across multiple scenarios while preserving key physical and statistical properties. We present a comprehensive analysis of stochastic differential equation impact and discuss our approach across diverse fluid mechanics tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</title>
<link>https://arxiv.org/abs/2505.24149</link>
<guid>https://arxiv.org/abs/2505.24149</guid>
<content:encoded><![CDATA[
<div> dynamic model update, concept drift, resource constraints, Lyapunov drift-plus-penalty framework, real-time ML deployments

Summary: 
RCCDA is proposed as a dynamic model update policy for machine learning algorithms facing concept drift in real-world environments. The policy optimizes training dynamics while ensuring strict adherence to predefined resource constraints. By utilizing past loss information and a tunable drift threshold, RCCDA minimizes computational overhead and provides theoretical performance assurances. The policy is developed within a Lyapunov drift-plus-penalty framework that limits update frequency and cost based on accumulated loss thresholds. Experimental results on domain generalization datasets show that RCCDA outperforms baseline methods in inference accuracy while meeting resource constraints under various concept drift scenarios. This lightweight policy is well-suited for real-time ML deployments, offering a practical and effective solution for maintaining model performance in changing environments. 

<br /><br />Summary: <div>
arXiv:2505.24149v1 Announce Type: new 
Abstract: Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24155</link>
<guid>https://arxiv.org/abs/2505.24155</guid>
<content:encoded><![CDATA[
arXiv:2505.24155v1 Announce Type: new 
Abstract: Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents</title>
<link>https://arxiv.org/abs/2505.24157</link>
<guid>https://arxiv.org/abs/2505.24157</guid>
<content:encoded><![CDATA[
arXiv:2505.24157v1 Announce Type: new 
Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem</title>
<link>https://arxiv.org/abs/2505.24178</link>
<guid>https://arxiv.org/abs/2505.24178</guid>
<content:encoded><![CDATA[
arXiv:2505.24178v1 Announce Type: new 
Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at https://github.com/kthrn22/OOD-Linker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</title>
<link>https://arxiv.org/abs/2505.24179</link>
<guid>https://arxiv.org/abs/2505.24179</guid>
<content:encoded><![CDATA[
arXiv:2505.24179v1 Announce Type: new 
Abstract: Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeV-R1: Reasoning-Enhanced Verilog Generation</title>
<link>https://arxiv.org/abs/2505.24183</link>
<guid>https://arxiv.org/abs/2505.24183</guid>
<content:encoded><![CDATA[
arXiv:2505.24183v1 Announce Type: new 
Abstract: Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling</title>
<link>https://arxiv.org/abs/2505.24185</link>
<guid>https://arxiv.org/abs/2505.24185</guid>
<content:encoded><![CDATA[
arXiv:2505.24185v1 Announce Type: new 
Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
arXiv:2505.24189v1 Announce Type: new 
Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Improving Generalization of Few-Shot Models with Synthetic Data</title>
<link>https://arxiv.org/abs/2505.24190</link>
<guid>https://arxiv.org/abs/2505.24190</guid>
<content:encoded><![CDATA[
arXiv:2505.24190v1 Announce Type: new 
Abstract: Few-shot image classification remains challenging due to the scarcity of labeled training examples. Augmenting them with synthetic data has emerged as a promising way to alleviate this issue, but models trained on synthetic samples often face performance degradation due to the inherent gap between real and synthetic distributions. To address this limitation, we develop a theoretical framework that quantifies the impact of such distribution discrepancies on supervised learning, specifically in the context of image classification. More importantly, our framework suggests practical ways to generate good synthetic samples and to train a predictor with high generalization ability. Building upon this framework, we propose a novel theoretical-based algorithm that integrates prototype learning to optimize both data partitioning and model training, effectively bridging the gap between real few-shot data and synthetic data. Extensive experiments results show that our approach demonstrates superior performance compared to state-of-the-art methods, outperforming them across multiple datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback</title>
<link>https://arxiv.org/abs/2505.24193</link>
<guid>https://arxiv.org/abs/2505.24193</guid>
<content:encoded><![CDATA[
arXiv:2505.24193v1 Announce Type: new 
Abstract: We study the multi-armed bandit problem with adversarially chosen delays in the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal performance in both stochastic and adversarial environments. While prior work has made progress toward this goal, existing algorithms suffer from significant gaps to the known lower bounds, especially in the stochastic settings. Our main contribution is a new algorithm that, up to logarithmic factors, matches the known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of $\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is the cumulative delay. In the stochastic case, we provide a regret bound which scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum \Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$ and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to simultaneously match the lower bounds in both stochastic and adversarial regimes in delayed environment. Moreover, even beyond the BoBW setting, our stochastic regret bound is the first to match the known lower bound under adversarial delays, improving the second term over the best known result by a factor of $K$.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks</title>
<link>https://arxiv.org/abs/2505.24205</link>
<guid>https://arxiv.org/abs/2505.24205</guid>
<content:encoded><![CDATA[
arXiv:2505.24205v1 Announce Type: new 
Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that $\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion</title>
<link>https://arxiv.org/abs/2505.24231</link>
<guid>https://arxiv.org/abs/2505.24231</guid>
<content:encoded><![CDATA[
arXiv:2505.24231v1 Announce Type: new 
Abstract: Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Informed Flows for Bayesian Inference of Probabilistic Programs</title>
<link>https://arxiv.org/abs/2505.24243</link>
<guid>https://arxiv.org/abs/2505.24243</guid>
<content:encoded><![CDATA[
arXiv:2505.24243v1 Announce Type: new 
Abstract: Variational inference often struggles with the posterior geometry exhibited by complex hierarchical Bayesian models. Recent advances in flow-based variational families and Variationally Inferred Parameters (VIP) each address aspects of this challenge, but their formal relationship is unexplored. Here, we prove that the combination of VIP and a full-rank Gaussian can be represented exactly as a forward autoregressive flow augmented with a translation term and input from the model's prior. Guided by this theoretical insight, we introduce the Model-Informed Flow (MIF) architecture, which adds the necessary translation mechanism, prior information, and hierarchical ordering. Empirically, MIF delivers tighter posterior approximations and matches or exceeds state-of-the-art performance across a suite of hierarchical and non-hierarchical benchmarks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Continual Learning with Progressive Neural Collapse</title>
<link>https://arxiv.org/abs/2505.24254</link>
<guid>https://arxiv.org/abs/2505.24254</guid>
<content:encoded><![CDATA[
arXiv:2505.24254v1 Announce Type: new 
Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a sequence of tasks, where a key challenge, namely Catastrophic Forgetting, persists due to the potential knowledge interference among different tasks. On the other hand, deep neural networks (DNNs) are shown to converge to a terminal state termed Neural Collapse during training, where all class prototypes geometrically form a static simplex equiangular tight frame (ETF). These maximally and equally separated class prototypes make the ETF an ideal target for model learning in CL to mitigate knowledge interference. Thus inspired, several studies have emerged very recently to leverage a fixed global ETF in CL, which however suffers from key drawbacks, such as impracticability and limited performance.To address these challenges and fully unlock the potential of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework that completely removes the need of a fixed global ETF in CL. Specifically, ProNC progressively expands the ETF target in a principled way by adding new class prototypes as vertices for new tasks, ensuring maximal separability across all encountered classes with minimal shifts from the previous ETF. We next develop a new CL framework by plugging ProNC into commonly used CL algorithm designs, where distillation is further leveraged to balance between target shifting for old classes and target aligning for new classes. Extensive experiments show that our approach significantly outperforms related baselines while maintaining superior flexibility, simplicity, and efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining</title>
<link>https://arxiv.org/abs/2505.24261</link>
<guid>https://arxiv.org/abs/2505.24261</guid>
<content:encoded><![CDATA[
arXiv:2505.24261v1 Announce Type: new 
Abstract: Data attribution methods, which quantify the influence of individual training data points on a machine learning model, have gained increasing popularity in data-centric applications in modern AI. Despite a recent surge of new methods developed in this space, the impact of hyperparameter tuning in these methods remains under-explored. In this work, we present the first large-scale empirical study to understand the hyperparameter sensitivity of common data attribution methods. Our results show that most methods are indeed sensitive to certain key hyperparameters. However, unlike typical machine learning algorithms -- whose hyperparameters can be tuned using computationally-cheap validation metrics -- evaluating data attribution performance often requires retraining models on subsets of training data, making such metrics prohibitively costly for hyperparameter tuning. This poses a critical open challenge for the practical application of data attribution methods. To address this challenge, we advocate for better theoretical understandings of hyperparameter behavior to inform efficient tuning strategies. As a case study, we provide a theoretical analysis of the regularization term that is critical in many variants of influence function methods. Building on this analysis, we propose a lightweight procedure for selecting the regularization value without model retraining, and validate its effectiveness across a range of standard data attribution benchmarks. Overall, our study identifies a fundamental yet overlooked challenge in the practical application of data attribution, and highlights the importance of careful discussion on hyperparameter selection in future method development.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fairness of Task Arithmetic: The Role of Task Vectors</title>
<link>https://arxiv.org/abs/2505.24262</link>
<guid>https://arxiv.org/abs/2505.24262</guid>
<content:encoded><![CDATA[
arXiv:2505.24262v1 Announce Type: new 
Abstract: Model editing techniques, particularly task arithmetic using task vectors, have shown promise in efficiently modifying pre-trained models through arithmetic operations like task addition and negation. Despite computational advantages, these methods may inadvertently affect model fairness, creating risks in sensitive applications like hate speech detection. However, the fairness implications of task arithmetic remain largely unexplored, presenting a critical gap in the existing literature. We systematically examine how manipulating task vectors affects fairness metrics, including Demographic Parity and Equalized Odds. To rigorously assess these effects, we benchmark task arithmetic against full fine-tuning, a costly but widely used baseline, and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning method. Additionally, we explore merging task vectors from models fine-tuned on demographic subgroups vulnerable to hate speech, investigating whether fairness outcomes can be controlled by adjusting task vector coefficients, potentially enabling tailored model behavior. Our results offer novel insights into the fairness implications of model editing and establish a foundation for fairness-aware and responsible model editing practices.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradPower: Powering Gradients for Faster Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2505.24275</link>
<guid>https://arxiv.org/abs/2505.24275</guid>
<content:encoded><![CDATA[
arXiv:2505.24275v1 Announce Type: new 
Abstract: We propose GradPower, a lightweight gradient-transformation technique for accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$, GradPower first applies the elementwise sign-power transformation: $\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds the transformed gradient into a base optimizer. Notably, GradPower requires only a single-line code change and no modifications to the base optimizer's internal logic, including the hyperparameters. When applied to Adam (termed AdamPower), GradPower consistently achieves lower terminal loss across diverse architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements. Finally, we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlights the influence of gradient noise.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Locally Linear Mappings</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[
arXiv:2505.24293v1 Announce Type: new 
Abstract: We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
arXiv:2505.24298v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective</title>
<link>https://arxiv.org/abs/2505.24313</link>
<guid>https://arxiv.org/abs/2505.24313</guid>
<content:encoded><![CDATA[
arXiv:2505.24313v1 Announce Type: new 
Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong student model, trained on a dataset labeled by a weak teacher, ultimately outperforms the teacher on the target task. Recent studies attribute this performance gain to the prediction misfit between the student and teacher models. In this work, we theoretically investigate the emergence of W2SG through a generalized bias-variance decomposition of Bregman divergence. Specifically, we show that the expected population risk gap between the student and teacher is quantified by the expected misfit between the two models. While this aligns with previous results, our analysis removes several restrictive assumptions, most notably, the convexity of the student's hypothesis class, required in earlier works. Moreover, we show that W2SG is more likely to emerge when the student model approximates its posterior mean teacher, rather than mimicking an individual teacher. Using a concrete example, we demonstrate that if the student model has significantly larger capacity than the teacher, it can indeed converge to this posterior mean. Our analysis also suggests that avoiding overfitting to the teacher's supervision and reducing the entropy of student's prediction further facilitate W2SG. In addition, we show that the reverse cross-entropy loss, unlike the standard forward cross-entropy, is less sensitive to the predictive uncertainty of the teacher. Finally, we empirically verify our theoretical insights and demonstrate that incorporating the reverse cross-entropy loss consistently improves student performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.24317</link>
<guid>https://arxiv.org/abs/2505.24317</guid>
<content:encoded><![CDATA[
arXiv:2505.24317v1 Announce Type: new 
Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation</title>
<link>https://arxiv.org/abs/2505.24324</link>
<guid>https://arxiv.org/abs/2505.24324</guid>
<content:encoded><![CDATA[
arXiv:2505.24324v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cartan Networks: Group theoretical Hyperbolic Deep Learning</title>
<link>https://arxiv.org/abs/2505.24353</link>
<guid>https://arxiv.org/abs/2505.24353</guid>
<content:encoded><![CDATA[
arXiv:2505.24353v1 Announce Type: new 
Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces to develop efficient and informative embeddings of hierarchical data. Here, we focus on the solvable group structure of hyperbolic spaces, which follows naturally from their construction as symmetric spaces. This dual nature of Lie group and Riemannian manifold allows us to propose a new class of hyperbolic deep learning algorithms where group homomorphisms are interleaved with metric-preserving diffeomorphisms. The resulting algorithms, which we call Cartan networks, show promising results on various benchmark data sets and open the way to a novel class of hyperbolic deep learning architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration</title>
<link>https://arxiv.org/abs/2505.24357</link>
<guid>https://arxiv.org/abs/2505.24357</guid>
<content:encoded><![CDATA[
arXiv:2505.24357v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: https://github.com/XIANGLONGYAN/ReCalKV.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning</title>
<link>https://arxiv.org/abs/2505.24360</link>
<guid>https://arxiv.org/abs/2505.24360</guid>
<content:encoded><![CDATA[
arXiv:2505.24360v1 Announce Type: new 
Abstract: Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm</title>
<link>https://arxiv.org/abs/2505.24365</link>
<guid>https://arxiv.org/abs/2505.24365</guid>
<content:encoded><![CDATA[
arXiv:2505.24365v1 Announce Type: new 
Abstract: This paper introduces a unified approach to cluster refinement and anomaly detection in datasets. We propose a novel algorithm that iteratively reduces the intra-cluster variance of N clusters until a global minimum is reached, yielding tighter clusters than the standard k-means algorithm. We evaluate the method using intrinsic measures for unsupervised learning, including the silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and extend it to anomaly detection by identifying points whose assignment causes a significant variance increase. External validation on synthetic data and the UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity score, V-measure, and F1 score. Results show variance reductions of 18.7% and 88.1% on the synthetic and Wine Quality datasets, respectively, along with accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Preference Learning for Robust LLM Alignment</title>
<link>https://arxiv.org/abs/2505.24369</link>
<guid>https://arxiv.org/abs/2505.24369</guid>
<content:encoded><![CDATA[
arXiv:2505.24369v1 Announce Type: new 
Abstract: Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</title>
<link>https://arxiv.org/abs/2505.24378</link>
<guid>https://arxiv.org/abs/2505.24378</guid>
<content:encoded><![CDATA[
arXiv:2505.24378v1 Announce Type: new 
Abstract: Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.24379</link>
<guid>https://arxiv.org/abs/2505.24379</guid>
<content:encoded><![CDATA[
arXiv:2505.24379v1 Announce Type: new 
Abstract: Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightSAM: Parameter-Agnostic Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2505.24399</link>
<guid>https://arxiv.org/abs/2505.24399</guid>
<content:encoded><![CDATA[
arXiv:2505.24399v1 Announce Type: new 
Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization ability of the machine learning model by exploring the flat minima landscape through weight perturbations. Despite its empirical success, SAM introduces an additional hyper-parameter, the perturbation radius, which causes the sensitivity of SAM to it. Moreover, it has been proved that the perturbation radius and learning rate of SAM are constrained by problem-dependent parameters to guarantee convergence. These limitations indicate the requirement of parameter-tuning in practical applications. In this paper, we propose the algorithm LightSAM which sets the perturbation radius and learning rate of SAM adaptively, thus extending the application scope of SAM. LightSAM employs three popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to replace the SGD optimizer for weight perturbation and model updating, reducing sensitivity to parameters. Theoretical results show that under weak assumptions, LightSAM could converge ideally with any choices of perturbation radius and learning rate, thus achieving parameter-agnostic. We conduct preliminary experiments on several deep learning tasks, which together with the theoretical findings validate the the effectiveness of LightSAM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets</title>
<link>https://arxiv.org/abs/2505.24403</link>
<guid>https://arxiv.org/abs/2505.24403</guid>
<content:encoded><![CDATA[
arXiv:2505.24403v1 Announce Type: new 
Abstract: The Lipschitz constant of a neural network is connected to several important properties of the network such as its robustness and generalization. It is thus useful in many settings to estimate the Lipschitz constant of a model. Prior work has focused mainly on estimating the Lipschitz constant of multi-layer perceptrons and convolutional neural networks. Here we focus on data modeled as sets or multisets of vectors and on neural networks that can handle such data. These models typically apply some permutation invariant aggregation function, such as the sum, mean or max operator, to the input multisets to produce a single vector for each input sample. In this paper, we investigate whether these aggregation functions are Lipschitz continuous with respect to three distance functions for unordered multisets, and we compute their Lipschitz constants. In the general case, we find that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Then, we build on these results to derive upper bounds on the Lipschitz constant of neural networks that can process multisets of vectors, while we also study their stability to perturbations and generalization under distribution shifts. To empirically verify our theoretical analysis, we conduct a series of experiments on datasets from different domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data</title>
<link>https://arxiv.org/abs/2505.24413</link>
<guid>https://arxiv.org/abs/2505.24413</guid>
<content:encoded><![CDATA[
arXiv:2505.24413v1 Announce Type: new 
Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool to solve multiple learning tasks simultaneously and has been successfully applied to healthcare, marketing, and biomedical fields. However, in order to borrow information across different tasks effectively, it is essential to utilize both homogeneous and heterogeneous information. Among the extensive literature on MTL, various forms of heterogeneity are presented in MTL problems, such as block-wise, distribution, and posterior heterogeneity. Existing methods, however, struggle to tackle these forms of heterogeneity simultaneously in a unified framework. In this paper, we propose a two-step learning strategy for MTL which addresses the aforementioned heterogeneity. First, we impute the missing blocks using shared representations extracted from homogeneous source across different tasks. Next, we disentangle the mappings between input features and responses into a shared component and a task-specific component, respectively, thereby enabling information borrowing through the shared component. Our numerical experiments and real-data analysis from the ADNI database demonstrate the superior MTL performance of the proposed method compared to other competing methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation</title>
<link>https://arxiv.org/abs/2505.24415</link>
<guid>https://arxiv.org/abs/2505.24415</guid>
<content:encoded><![CDATA[
arXiv:2505.24415v1 Announce Type: new 
Abstract: Automated evaluation of movement quality holds significant potential for enhancing physiotherapeutic treatments and sports training by providing objective, real-time feedback. However, the effectiveness of deep learning models in assessing movements captured by inertial measurement units (IMUs) is often hampered by limited data availability, class imbalance, and label ambiguity. In this work, we present a novel data augmentation method that generates realistic IMU data using musculoskeletal simulations integrated with systematic modifications of movement trajectories. Crucially, our approach ensures biomechanical plausibility and allows for automatic, reliable labeling by combining inverse kinematic parameters with a knowledge-based evaluation strategy. Extensive evaluations demonstrate that augmented variants closely resembles real-world data, significantly improving the classification accuracy and generalization capability of neural network models. Additionally, we highlight the benefits of augmented data for patient-specific fine-tuning scenarios, particularly when only limited subject-specific training examples are available. Our findings underline the practicality and efficacy of this augmentation method in overcoming common challenges faced by deep learning applications in physiotherapeutic exercise evaluation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.24424</link>
<guid>https://arxiv.org/abs/2505.24424</guid>
<content:encoded><![CDATA[
arXiv:2505.24424v1 Announce Type: new 
Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot capabilities in classification and retrieval. However, these models often struggle with compositional reasoning - the ability to understand the relationships between concepts. A recent benchmark, SugarCrepe++, reveals that previous works on improving compositionality have mainly improved lexical sensitivity but neglected semantic understanding. In addition, downstream retrieval performance often deteriorates, although one would expect that improving compositionality should enhance retrieval. In this work, we introduce CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a novel training technique combining multiple images and their associated captions. CLIC improves compositionality across architectures as well as differently pre-trained CLIP models, both in terms of lexical and semantic understanding, and achieves consistent gains in retrieval performance. This even applies to the recent CLIPS, which achieves SOTA retrieval performance. Nevertheless, the short fine-tuning with CLIC leads to an improvement in retrieval and to the best compositional CLIP model on SugarCrepe++. All our models and code are available at https://clic-compositional-clip.github.io
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields</title>
<link>https://arxiv.org/abs/2505.24434</link>
<guid>https://arxiv.org/abs/2505.24434</guid>
<content:encoded><![CDATA[
arXiv:2505.24434v1 Announce Type: new 
Abstract: Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves Fr\'echet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs</title>
<link>https://arxiv.org/abs/2505.24438</link>
<guid>https://arxiv.org/abs/2505.24438</guid>
<content:encoded><![CDATA[
arXiv:2505.24438v1 Announce Type: new 
Abstract: An important characteristic of temporal graphs is how the directed arrow of time influences their causal topology, i.e., which nodes can possibly influence each other causally via time-respecting paths. The resulting patterns are often neglected by temporal graph neural networks (TGNNs). To formally analyze the expressive power of TGNNs, we lack a generalization of graph isomorphism to temporal graphs that fully captures their causal topology. Addressing this gap, we introduce the notion of consistent event graph isomorphism, which utilizes a time-unfolded representation of time-respecting paths in temporal graphs. We compare this definition with existing notions of temporal graph isomorphisms. We illustrate and highlight the advantages of our approach and develop a temporal generalization of the Weisfeiler-Leman algorithm to heuristically distinguish non-isomorphic temporal graphs. Building on this theoretical foundation, we derive a novel message passing scheme for temporal graph neural networks that operates on the event graph representation of temporal graphs. An experimental evaluation shows that our approach performs well in a temporal graph classification experiment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Safety Constraints for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24445</link>
<guid>https://arxiv.org/abs/2505.24445</guid>
<content:encoded><![CDATA[
arXiv:2505.24445v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepsize anything: A unified learning rate schedule for budgeted-iteration training</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
arXiv:2505.24452v1 Announce Type: new 
Abstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between $\varphi$ and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of $\varphi$.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA \textit{consistently surpasses} the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logits-Based Finetuning</title>
<link>https://arxiv.org/abs/2505.24461</link>
<guid>https://arxiv.org/abs/2505.24461</guid>
<content:encoded><![CDATA[
arXiv:2505.24461v1 Announce Type: new 
Abstract: The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at https://github.com/JulietLJY/MOOD.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Model Compression without Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.24469</link>
<guid>https://arxiv.org/abs/2505.24469</guid>
<content:encoded><![CDATA[
arXiv:2505.24469v1 Announce Type: new 
Abstract: Compressing and pruning large machine learning models has become a critical step towards their deployment in real-world applications. Standard pruning and compression techniques are typically designed without taking the structure of the network's weights into account, limiting their effectiveness. We explore the impact of smooth regularization on neural network training and model compression. By applying nuclear norm, first- and second-order derivative penalties of the weights during training, we encourage structured smoothness while preserving predictive performance on par with non-smooth models. We find that standard pruning methods often perform better when applied to these smooth models. Building on this observation, we apply a Singular-Value-Decomposition-based compression method that exploits the underlying smooth structure and approximates the model's weight tensors by smaller low-rank tensors. Our approach enables state-of-the-art compression without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18 on CIFAR-10 with $70\%$ fewer parameters.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy</title>
<link>https://arxiv.org/abs/2505.24473</link>
<guid>https://arxiv.org/abs/2505.24473</guid>
<content:encoded><![CDATA[
arXiv:2505.24473v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Centric Concept Bottlenecks</title>
<link>https://arxiv.org/abs/2505.24492</link>
<guid>https://arxiv.org/abs/2505.24492</guid>
<content:encoded><![CDATA[
arXiv:2505.24492v1 Announce Type: new 
Abstract: Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem</title>
<link>https://arxiv.org/abs/2505.24498</link>
<guid>https://arxiv.org/abs/2505.24498</guid>
<content:encoded><![CDATA[
arXiv:2505.24498v1 Announce Type: new 
Abstract: Recent work in online speech spectrogram inversion effectively combines Deep Learning with the Gradient Theorem to predict phase derivatives directly from magnitudes. Then, phases are estimated from their derivatives via least squares, resulting in a high quality reconstruction. In this work, we introduce three innovations that drastically reduce computational cost, while maintaining high quality: Firstly, we introduce a novel neural network architecture with just 8k parameters, 30 times smaller than previous state of the art. Secondly, increasing latency by 1 hop size allows us to further halve the cost of the neural inference step. Thirdly, we we observe that the least squares problem features a tridiagonal matrix and propose a linear-complexity solver for the least squares step that leverages tridiagonality and positive-semidefiniteness, achieving a speedup of several orders of magnitude. We release samples online.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset</title>
<link>https://arxiv.org/abs/2505.24505</link>
<guid>https://arxiv.org/abs/2505.24505</guid>
<content:encoded><![CDATA[
arXiv:2505.24505v1 Announce Type: new 
Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in power system operations, ensuring voltage stability and minimizing power losses. Recent advances in machine learning, particularly within the ``learning to optimize'' framework, have enabled fast and efficient approximations of ORPD solutions, typically by training models on precomputed optimization results. While these approaches have demonstrated promising performance on synthetic datasets, their effectiveness under real-world grid conditions remains largely unexplored. This paper makes two key contributions. First, we introduce a publicly available power system dataset that includes both the structural characteristics of Uruguay's electrical grid and nearly two years of real-world operational data, encompassing actual demand and generation profiles. Given Uruguay's high penetration of renewable energy, the ORPD problem has become the primary optimization challenge in its power network. Second, we assess the impact of real-world data on learning-based ORPD solutions, revealing a significant increase in prediction errors when transitioning from synthetic to actual demand and generation inputs. Our results highlight the limitations of existing models in learning under the complex statistical properties of real grid conditions and emphasize the need for more expressive architectures. By providing this dataset, we aim to facilitate further research into robust learning-based optimization techniques for power system management.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v1 Announce Type: new 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airborne Neural Network</title>
<link>https://arxiv.org/abs/2505.24513</link>
<guid>https://arxiv.org/abs/2505.24513</guid>
<content:encoded><![CDATA[
arXiv:2505.24513v1 Announce Type: new 
Abstract: Deep Learning, driven by neural networks, has led to groundbreaking advancements in Artificial Intelligence by enabling systems to learn and adapt like the human brain. These models have achieved remarkable results, particularly in data-intensive domains, supported by massive computational infrastructure. However, deploying such systems in Aerospace, where real time data processing and ultra low latency are critical, remains a challenge due to infrastructure limitations. This paper proposes a novel concept: the Airborne Neural Network a distributed architecture where multiple airborne devices each host a subset of neural network neurons. These devices compute collaboratively, guided by an airborne network controller and layer specific controllers, enabling real-time learning and inference during flight. This approach has the potential to revolutionize Aerospace applications, including airborne air traffic control, real-time weather and geographical predictions, and dynamic geospatial data processing. By enabling large-scale neural network operations in airborne environments, this work lays the foundation for the next generation of AI powered Aerospace systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Are Universally Consistent</title>
<link>https://arxiv.org/abs/2505.24531</link>
<guid>https://arxiv.org/abs/2505.24531</guid>
<content:encoded><![CDATA[
arXiv:2505.24531v1 Announce Type: new 
Abstract: Despite their central role in the success of foundational models and large-scale language modeling, the theoretical foundations governing the operation of Transformers remain only partially understood. Contemporary research has largely focused on their representational capacity for language comprehension and their prowess in in-context learning, frequently under idealized assumptions such as linearized attention mechanisms. Initially conceived to model sequence-to-sequence transformations, a fundamental and unresolved question is whether Transformers can robustly perform functional regression over sequences of input tokens. This question assumes heightened importance given the inherently non-Euclidean geometry underlying real-world data distributions. In this work, we establish that Transformers equipped with softmax-based nonlinear attention are uniformly consistent when tasked with executing Ordinary Least Squares (OLS) regression, provided both the inputs and outputs are embedded in hyperbolic space. We derive deterministic upper bounds on the empirical error which, in the asymptotic regime, decay at a provable rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens and $d$ the embedding dimensionality. Notably, our analysis subsumes the Euclidean setting as a special case, recovering analogous convergence guarantees parameterized by the intrinsic dimensionality of the data manifold. These theoretical insights are corroborated through empirical evaluations on real-world datasets involving both continuous and categorical response variables.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators</title>
<link>https://arxiv.org/abs/2505.24533</link>
<guid>https://arxiv.org/abs/2505.24533</guid>
<content:encoded><![CDATA[
arXiv:2505.24533v1 Announce Type: new 
Abstract: We introduce a novel framework consisting of a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions by defining per-axis composition operators subject to non-commutativity and a global interchange law. These structures, defined recursively from a base case of vector-matrix pairs, model directional composition in multiple dimensions while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms in signal processing and data analysis. In this framework, data indices are embedded into a composite structure that decomposes into simpler components. We show that classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform are special cases of our algebraic structure. The framework provides a systematic way to derive these transforms by appropriately choosing vector and matrix pairs. By subsuming classical transforms within a common structure, the framework also enables the development of learnable transformations tailored to specific data modalities and tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.24534</link>
<guid>https://arxiv.org/abs/2505.24534</guid>
<content:encoded><![CDATA[
arXiv:2505.24534v1 Announce Type: new 
Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in time-evolving simplicial complexes. While traditional graph anomaly detection techniques have been extensively studied, they often fail to capture changes in higher-order interactions that are crucial for identifying complex structural anomalies. These higher-order interactions can arise either directly from the underlying data itself or through graph lifting techniques. Our approach leverages the spectral properties of Hodge Laplacians of simplicial complexes to effectively model multi-way interactions among data points. By incorporating higher-dimensional simplicial structures into our method, our method enhances both detection accuracy and computational efficiency. Through comprehensive experiments on both synthetic and real-world datasets, we demonstrate that our approach outperforms existing graph methods in detecting both events and change points.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Linear Steering: Unified Multi-Attribute Control for Language Models</title>
<link>https://arxiv.org/abs/2505.24535</link>
<guid>https://arxiv.org/abs/2505.24535</guid>
<content:encoded><![CDATA[
arXiv:2505.24535v1 Announce Type: new 
Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems</title>
<link>https://arxiv.org/abs/2505.24578</link>
<guid>https://arxiv.org/abs/2505.24578</guid>
<content:encoded><![CDATA[
arXiv:2505.24578v1 Announce Type: new 
Abstract: Complex piezoelectric systems are foundational in industrial applications. Their performance, however, is challenged by the nonlinear voltage-displacement hysteretic relationships. Efficient characterization methods are, therefore, essential for reliable design, monitoring, and maintenance. Recently proposed neural operator methods serve as surrogates for system characterization but face two pressing issues: interpretability and generalizability. State-of-the-art (SOTA) neural operators are black-boxes, providing little insight into the learned operator. Additionally, generalizing them to novel voltages and predicting displacement profiles beyond the training domain is challenging, limiting their practical use. To address these limitations, this paper proposes a neuro-symbolic operator (NSO) framework that derives the analytical operators governing hysteretic relationships. NSO first learns a Fourier neural operator mapping voltage fields to displacement profiles, followed by a library-based sparse model discovery method, generating white-box parsimonious models governing the underlying hysteresis. These models enable accurate and interpretable prediction of displacement profiles across varying and out-of-distribution voltage fields, facilitating generalizability. The potential of NSO is demonstrated by accurately predicting voltage-displacement hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts displacement profiles even for noisy and low-fidelity voltage data, emphasizing its robustness. The results highlight the advantages of NSO compared to SOTA neural operators and model discovery methods on several evaluation metrics. Consequently, NSO contributes to characterizing complex piezoelectric systems while improving the interpretability and generalizability of neural operators, essential for design, monitoring, maintenance, and other real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservation-preserved Fourier Neural Operator through Adaptive Correction</title>
<link>https://arxiv.org/abs/2505.24579</link>
<guid>https://arxiv.org/abs/2505.24579</guid>
<content:encoded><![CDATA[
arXiv:2505.24579v1 Announce Type: new 
Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and efficient approach for learning the numerical solutions to partial differential equations (PDEs) from data. However, standard FNO often fails to preserve key conservation laws, such as mass conservation, momentum conservation, norm conservation, etc., which are crucial for accurately modeling physical systems. Existing methods for incorporating these conservation laws into Fourier neural operators are achieved by designing related loss function or incorporating post-processing method at the training time. None of them can both exactly and adaptively correct the outputs to satisfy conservation laws, and our experiments show that these methods can lead to inferior performance while preserving conservation laws. In this work, we propose a novel adaptive correction approach to ensure the conservation of fundamental quantities. Our method introduces a learnable matrix to adaptively adjust the solution to satisfy the conservation law during training. It ensures that the outputs exactly satisfy the goal conservation law and allow for more flexibility and adaptivity for the model to correct the outputs. We theoretically show that applying our adaptive correction to an unconstrained FNO yields a solution with data loss no worse than that of the best conservation-satisfying FNO. We compare our approach with existing methods on a range of representative PDEs. Experiment results show that our method consistently outperform other methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams</title>
<link>https://arxiv.org/abs/2505.24584</link>
<guid>https://arxiv.org/abs/2505.24584</guid>
<content:encoded><![CDATA[
arXiv:2505.24584v1 Announce Type: new 
Abstract: Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[
arXiv:2505.24592v1 Announce Type: new 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Cumulative Encoding meets Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24595</link>
<guid>https://arxiv.org/abs/2505.24595</guid>
<content:encoded><![CDATA[
arXiv:2505.24595v1 Announce Type: new 
Abstract: Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches</title>
<link>https://arxiv.org/abs/2505.24603</link>
<guid>https://arxiv.org/abs/2505.24603</guid>
<content:encoded><![CDATA[
arXiv:2505.24603v1 Announce Type: new 
Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random Gaussian matrix, is a widely used technique for multiple problems in data science and machine learning, with applications spanning computationally efficient optimization, coded computing, and federated learning. This operation also provides differential privacy guarantees due to its inherent randomness. In this work, we revisit this operation through the lens of Renyi Differential Privacy (RDP), providing a refined privacy analysis that yields significantly tighter bounds than prior results. We then demonstrate how this improved analysis leads to performance improvement in different linear regression settings, establishing theoretical utility guarantees. Empirically, our methods improve performance across multiple datasets and, in several cases, reduce runtime.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-criteria Rank-based Aggregation for Explainable AI</title>
<link>https://arxiv.org/abs/2505.24612</link>
<guid>https://arxiv.org/abs/2505.24612</guid>
<content:encoded><![CDATA[
arXiv:2505.24612v1 Announce Type: new 
Abstract: Explainability is crucial for improving the transparency of black-box machine learning models. With the advancement of explanation methods such as LIME and SHAP, various XAI performance metrics have been developed to evaluate the quality of explanations. However, different explainers can provide contrasting explanations for the same prediction, introducing trade-offs across conflicting quality metrics. Although available aggregation approaches improve robustness, reducing explanations' variability, very limited research employed a multi-criteria decision-making approach. To address this gap, this paper introduces a multi-criteria rank-based weighted aggregation method that balances multiple quality metrics simultaneously to produce an ensemble of explanation models. Furthermore, we propose rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations. Extensive experiments on publicly available datasets demonstrate the robustness of the proposed model across these metrics. Comparative analyses of various multi-criteria decision-making and rank aggregation algorithms showed that TOPSIS and WSUM are the best candidates for this use case.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.24623</link>
<guid>https://arxiv.org/abs/2505.24623</guid>
<content:encoded><![CDATA[
arXiv:2505.24623v1 Announce Type: new 
Abstract: To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. Notably, HDD is seamlessly compatible with most existing DM methods, and extensive experiments on different datasets validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees</title>
<link>https://arxiv.org/abs/2505.24627</link>
<guid>https://arxiv.org/abs/2505.24627</guid>
<content:encoded><![CDATA[
arXiv:2505.24627v1 Announce Type: new 
Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and proposes a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performances on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks</title>
<link>https://arxiv.org/abs/2505.24629</link>
<guid>https://arxiv.org/abs/2505.24629</guid>
<content:encoded><![CDATA[
arXiv:2505.24629v1 Announce Type: new 
Abstract: Penalties are fraught and game-changing moments in soccer games that teams explicitly prepare for. Consequently, there has been substantial interest in analyzing them in order to provide advice to practitioners. From a data science perspective, such analyses suffer from a significant limitation: they make the unrealistic simplifying assumption that goalkeepers and takers select their action -- where to dive and where to the place the kick -- independently of each other. In reality, the choices that some goalkeepers make depend on the taker's movements and vice-versa. This adds substantial complexity to the problem because not all players have the same action capacities, that is, only some players are capable of basing their decisions on their opponent's movements. However, the small sample sizes on the player level mean that one may have limited insights into a specific opponent's capacities. We address these challenges by developing a player-agnostic simulation framework that can evaluate the efficacy of different goalkeeper strategies. It considers a rich set of choices and incorporates information about a goalkeeper's skills. Our work is grounded in a large dataset of penalties that were annotated by penalty experts and include aspects of both kicker and goalkeeper strategies. We show how our framework can be used to optimize goalkeeper policies in real-world situations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WILTing Trees: Interpreting the Distance Between MPNN Embeddings</title>
<link>https://arxiv.org/abs/2505.24642</link>
<guid>https://arxiv.org/abs/2505.24642</guid>
<content:encoded><![CDATA[
arXiv:2505.24642v1 Announce Type: new 
Abstract: We investigate the distance function learned by message passing neural networks (MPNNs) in specific tasks, aiming to capture the functional distance between prediction targets that MPNNs implicitly learn. This contrasts with previous work, which links MPNN distances on arbitrary tasks to structural distances on graphs that ignore task-specific information. To address this gap, we distill the distance between MPNN embeddings into an interpretable graph distance. Our method uses optimal transport on the Weisfeiler Leman Labeling Tree (WILT), where the edge weights reveal subgraphs that strongly influence the distance between embeddings. This approach generalizes two well-known graph kernels and can be computed in linear time. Through extensive experiments, we demonstrate that MPNNs define the relative position of embeddings by focusing on a small set of subgraphs that are known to be functionally important in the domain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Distributions over Permutations and Rankings with Factorized Representations</title>
<link>https://arxiv.org/abs/2505.24664</link>
<guid>https://arxiv.org/abs/2505.24664</guid>
<content:encoded><![CDATA[
arXiv:2505.24664v1 Announce Type: new 
Abstract: Learning distributions over permutations is a fundamental problem in machine learning, with applications in ranking, combinatorial optimization, structured prediction, and data association. Existing methods rely on mixtures of parametric families or neural networks with expensive variational inference procedures. In this work, we propose a novel approach that leverages alternative representations for permutations, including Lehmer codes, Fisher-Yates draws, and Insertion-Vectors. These representations form a bijection with the symmetric group, allowing for unconstrained learning using conventional deep learning techniques, and can represent any probability distribution over permutations. Our approach enables a trade-off between expressivity of the model family and computational requirements. In the least expressive and most computationally efficient case, our method subsumes previous families of well established probabilistic models over permutations, including Mallow's and the Repeated Insertion Model. Experiments indicate our method significantly outperforms current approaches on the jigsaw puzzle benchmark, a common task for permutation learning. However, we argue this benchmark is limited in its ability to assess learning probability distributions, as the target is a delta distribution (i.e., a single correct solution exists). We therefore propose two additional benchmarks: learning cyclic permutations and re-ranking movies based on user preference. We show that our method learns non-trivial distributions even in the least expressive mode, while traditional models fail to even generate valid permutations in this setting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning geometry and topology via multi-chart flows</title>
<link>https://arxiv.org/abs/2505.24665</link>
<guid>https://arxiv.org/abs/2505.24665</guid>
<content:encoded><![CDATA[
arXiv:2505.24665v1 Announce Type: new 
Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in high-dimensional spaces. This motivates learning degenerate normalizing flows that map between the ambient space and a low-dimensional latent space. However, if the manifold has a non-trivial topology, it can never be correctly learned using a single flow. Instead multiple flows must be `glued together'. In this paper, we first propose the general training scheme for learning such a collection of flows, and secondly we develop the first numerical algorithms for computing geodesics on such manifolds. Empirically, we demonstrate that this leads to highly significant improvements in topology estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning</title>
<link>https://arxiv.org/abs/2505.24676</link>
<guid>https://arxiv.org/abs/2505.24676</guid>
<content:encoded><![CDATA[
arXiv:2505.24676v1 Announce Type: new 
Abstract: Despite well-documented consequences of the U.S. government's 1930s housing policies on racial wealth disparities, scholars have struggled to quantify its precise financial effects due to the inaccessibility of historical property appraisal records. Many counties still store these records in physical formats, making large-scale quantitative analysis difficult. We present an approach scholars can use to digitize historical housing assessment data, applying it to build and release a dataset for one county. Starting from publicly available scanned documents, we manually annotated property cards for over 12,000 properties to train and validate our methods. We use OCR to label data for an additional 50,000 properties, based on our two-stage approach combining classical computer vision techniques with deep learning-based OCR. For cases where OCR cannot be applied, such as when scanned documents are not available, we show how a regression model based on building feature data can estimate the historical values, and test the generalizability of this model to other counties. With these cost-effective tools, scholars, community activists, and policy makers can better analyze and understand the historical impacts of redlining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs</title>
<link>https://arxiv.org/abs/2505.24684</link>
<guid>https://arxiv.org/abs/2505.24684</guid>
<content:encoded><![CDATA[
arXiv:2505.24684v1 Announce Type: new 
Abstract: Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting "V"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms</title>
<link>https://arxiv.org/abs/2505.24692</link>
<guid>https://arxiv.org/abs/2505.24692</guid>
<content:encoded><![CDATA[
arXiv:2505.24692v1 Announce Type: new 
Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary reward environment where the size of the action space (number of arms) is small. More recently developed methods typically relax only one of these assumptions: existing non-stationary bandit policies are designed for a small number of arms, while Lipschitz, linear, and Gaussian process bandit policies are designed to handle a large (or infinite) number of arms in stationary reward environments under constraints on the reward function. In this manuscript, we propose a novel policy to learn reward environments over a continuous space using Gaussian interpolation. We show that our method efficiently learns continuous Lipschitz reward functions with $\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally extends to non-stationary problems with a simple modification. We finally demonstrate that our method is computationally favorable (100-10000x faster) and experimentally outperforms sliding Gaussian process policies on datasets with non-stationarity and an extremely large number of arms.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Symmetric Losses for Robust Policy Optimization with Noisy Preferences</title>
<link>https://arxiv.org/abs/2505.24709</link>
<guid>https://arxiv.org/abs/2505.24709</guid>
<content:encoded><![CDATA[
arXiv:2505.24709v1 Announce Type: new 
Abstract: Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting</title>
<link>https://arxiv.org/abs/2505.24710</link>
<guid>https://arxiv.org/abs/2505.24710</guid>
<content:encoded><![CDATA[
arXiv:2505.24710v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRet: Improved Retriever for Code Editing</title>
<link>https://arxiv.org/abs/2505.24715</link>
<guid>https://arxiv.org/abs/2505.24715</guid>
<content:encoded><![CDATA[
arXiv:2505.24715v1 Announce Type: new 
Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations</title>
<link>https://arxiv.org/abs/2505.24717</link>
<guid>https://arxiv.org/abs/2505.24717</guid>
<content:encoded><![CDATA[
arXiv:2505.24717v1 Announce Type: new 
Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach</title>
<link>https://arxiv.org/abs/2505.24721</link>
<guid>https://arxiv.org/abs/2505.24721</guid>
<content:encoded><![CDATA[
arXiv:2505.24721v1 Announce Type: new 
Abstract: Memristor-based hardware offers new possibilities for energy-efficient machine learning (ML) by providing analog in-memory matrix multiplication. Current hardware prototypes cannot fit large neural networks, and related literature covers only small ML models for tasks like MNIST or single word recognition. Simulation can be used to explore how hardware properties affect larger models, but existing software assumes simplified hardware. We propose a PyTorch-based library based on "Synaptogen" to simulate neural network execution with accurately captured memristor hardware properties. For the first time, we show how an ML system with millions of parameters would behave on memristor hardware, using a Conformer trained on the speech recognition task TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the relative degradation in word error rate to 25% when using a 3-bit weight precision to execute linear operations via simulated analog computation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</title>
<link>https://arxiv.org/abs/2505.24722</link>
<guid>https://arxiv.org/abs/2505.24722</guid>
<content:encoded><![CDATA[
arXiv:2505.24722v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Federated Learning against Model Perturbation in Edge Networks</title>
<link>https://arxiv.org/abs/2505.24728</link>
<guid>https://arxiv.org/abs/2505.24728</guid>
<content:encoded><![CDATA[
arXiv:2505.24728v1 Announce Type: new 
Abstract: Federated Learning (FL) is a promising paradigm for realizing edge intelligence, allowing collaborative learning among distributed edge devices by sharing models instead of raw data. However, the shared models are often assumed to be ideal, which would be inevitably violated in practice due to various perturbations, leading to significant performance degradation. To overcome this challenge, we propose a novel method, termed Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL), which aims to improve model robustness against perturbations by exploring the geometrical property of the model landscape. Specifically, SMRFL solves a min-max optimization problem that promotes model convergence towards a flat minimum by minimizing the maximum loss within a neighborhood of the model parameters. In this way, model sensitivity to perturbations is reduced, and robustness is enhanced since models in the neighborhood of the flat minimum also enjoy low loss values. The theoretical result proves that SMRFL can converge at the same rate as FL without perturbations. Extensive experimental results show that SMRFL significantly enhances robustness against perturbations compared to three baseline methods on two real-world datasets under three perturbation scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Attribution from First Principles</title>
<link>https://arxiv.org/abs/2505.24729</link>
<guid>https://arxiv.org/abs/2505.24729</guid>
<content:encoded><![CDATA[
arXiv:2505.24729v1 Announce Type: new 
Abstract: Feature attribution methods are a popular approach to explain the behavior of machine learning models. They assign importance scores to each input feature, quantifying their influence on the model's prediction. However, evaluating these methods empirically remains a significant challenge. To bypass this shortcoming, several prior works have proposed axiomatic frameworks that any feature attribution method should satisfy. In this work, we argue that such axioms are often too restrictive, and propose in response a new feature attribution framework, built from the ground up. Rather than imposing axioms, we start by defining attributions for the simplest possible models, i.e., indicator functions, and use these as building blocks for more complex models. We then show that one recovers several existing attribution methods, depending on the choice of atomic attribution. Subsequently, we derive closed-form expressions for attribution of deep ReLU networks, and take a step toward the optimization of evaluation metrics with respect to feature attributions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning</title>
<link>https://arxiv.org/abs/2505.24737</link>
<guid>https://arxiv.org/abs/2505.24737</guid>
<content:encoded><![CDATA[
arXiv:2505.24737v1 Announce Type: new 
Abstract: This paper studies the problem of differentially private empirical risk minimization (DP-ERM) for binary linear classification. We obtain an efficient $(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of $\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} + \frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and $\gamma$ is the margin of linear separation of the remaining data points (after $S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic terms. In the agnostic case, we improve the existing results when the number of outliers is small. Our algorithm is highly adaptive because it does not require knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We also derive a utility bound for the advanced private hyperparameter tuning algorithm.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training</title>
<link>https://arxiv.org/abs/2505.24749</link>
<guid>https://arxiv.org/abs/2505.24749</guid>
<content:encoded><![CDATA[
arXiv:2505.24749v1 Announce Type: new 
Abstract: Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.24760</link>
<guid>https://arxiv.org/abs/2505.24760</guid>
<content:encoded><![CDATA[
arXiv:2505.24760v1 Announce Type: new 
Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption</title>
<link>https://arxiv.org/abs/2505.24773</link>
<guid>https://arxiv.org/abs/2505.24773</guid>
<content:encoded><![CDATA[
arXiv:2505.24773v1 Announce Type: new 
Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Symbolic Regression</title>
<link>https://arxiv.org/abs/2505.24776</link>
<guid>https://arxiv.org/abs/2505.24776</guid>
<content:encoded><![CDATA[
arXiv:2505.24776v1 Announce Type: new 
Abstract: Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlightened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative processes with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation</title>
<link>https://arxiv.org/abs/2505.24779</link>
<guid>https://arxiv.org/abs/2505.24779</guid>
<content:encoded><![CDATA[
arXiv:2505.24779v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features -- particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QGAN-based data augmentation for hybrid quantum-classical neural networks</title>
<link>https://arxiv.org/abs/2505.24780</link>
<guid>https://arxiv.org/abs/2505.24780</guid>
<content:encoded><![CDATA[
arXiv:2505.24780v1 Announce Type: new 
Abstract: Quantum neural networks converge faster and achieve higher accuracy than classical models. However, data augmentation in quantum machine learning remains underexplored. To tackle data scarcity, we integrate quantum generative adversarial networks (QGANs) with hybrid quantum-classical neural networks (HQCNNs) to develop an augmentation framework. We propose two strategies: a general approach to enhance data processing and classification across HQCNNs, and a customized strategy that dynamically generates samples tailored to the HQCNN's performance on specific data categories, improving its ability to learn from complex datasets. Simulation experiments on the MNIST dataset demonstrate that QGAN outperforms traditional data augmentation methods and classical GANs. Compared to baseline DCGAN, QGAN achieves comparable performance with half the parameters, balancing efficiency and effectiveness. This suggests that QGANs can simplify models and generate high-quality data, enhancing HQCNN accuracy and performance. These findings pave the way for applying quantum data augmentation techniques in machine learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding</title>
<link>https://arxiv.org/abs/2505.24791</link>
<guid>https://arxiv.org/abs/2505.24791</guid>
<content:encoded><![CDATA[
arXiv:2505.24791v1 Announce Type: new 
Abstract: Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByzFL: Research Framework for Robust Federated Learning</title>
<link>https://arxiv.org/abs/2505.24802</link>
<guid>https://arxiv.org/abs/2505.24802</guid>
<content:encoded><![CDATA[
arXiv:2505.24802v1 Announce Type: new 
Abstract: We present ByzFL, an open-source Python library for developing and benchmarking robust federated learning (FL) algorithms. ByzFL provides a unified and extensible framework that includes implementations of state-of-the-art robust aggregators, a suite of configurable attacks, and tools for simulating a variety of FL scenarios, including heterogeneous data distributions, multiple training algorithms, and adversarial threat models. The library enables systematic experimentation via a single JSON-based configuration file and includes built-in utilities for result visualization. Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to facilitate reproducible research and rapid prototyping of robust FL solutions. ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on GitHub: https://github.com/LPD-EPFL/byzfl.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24823</link>
<guid>https://arxiv.org/abs/2505.24823</guid>
<content:encoded><![CDATA[
arXiv:2505.24823v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24835</link>
<guid>https://arxiv.org/abs/2505.24835</guid>
<content:encoded><![CDATA[
arXiv:2505.24835v1 Announce Type: new 
Abstract: Fund allocation has been an increasingly important problem in the financial domain. In reality, we aim to allocate the funds to buy certain assets within a certain future period. Naive solutions such as prediction-only or Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the introduction of the SOTA time series forecasting model inevitably introduces additional uncertainty in the predicted result. To solve both problems mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate (RTS-PnO) framework, which holds no prior assumption on the forecasting models. Such a framework contains three features: (i) end-to-end training with objective alignment measurement, (ii) adaptive forecasting uncertainty calibration, and (iii) agnostic towards forecasting models. The evaluation of RTS-PnO is conducted over both online and offline experiments. For offline experiments, eight datasets from three categories of financial applications are used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other competitive baselines. The online experiment is conducted on the Cross-Border Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed when compared with the product-line approach. The code for the offline experiment is available at https://github.com/fuyuanlyu/RTS-PnO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascading Adversarial Bias from Injection to Distillation in Language Models</title>
<link>https://arxiv.org/abs/2505.24842</link>
<guid>https://arxiv.org/abs/2505.24842</guid>
<content:encoded><![CDATA[
arXiv:2505.24842v1 Announce Type: new 
Abstract: Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching</title>
<link>https://arxiv.org/abs/2505.24843</link>
<guid>https://arxiv.org/abs/2505.24843</guid>
<content:encoded><![CDATA[
arXiv:2505.24843v1 Announce Type: new 
Abstract: Spurious correlations can cause model performance to degrade in new environments. Prior causality-inspired works aim to learn invariant representations (e.g., IRM) but typically underperform empirical risk minimization (ERM). Recent alternatives improve robustness by leveraging test-time data, but such data may be unavailable in practice. To address these issues, we take a data-centric approach by leveraging invariant data pairs, pairs of samples that would have the same prediction with the optimally robust classifier. We prove that certain counterfactual pairs will naturally satisfy this invariance property and introduce noisy counterfactual matching (NCM), a simple constraint-based method for leveraging invariant pairs for enhanced robustness, even with a small set of noisy pairs-in the ideal case, each pair can eliminate one spurious feature. For linear causal models, we prove that the test domain error can be upper bounded by the in-domain error and a term that depends on the counterfactuals' diversity and quality. We validate on a synthetic dataset and demonstrate on real-world benchmarks that linear probing on a pretrained backbone improves robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning</title>
<link>https://arxiv.org/abs/2505.24844</link>
<guid>https://arxiv.org/abs/2505.24844</guid>
<content:encoded><![CDATA[
arXiv:2505.24844v1 Announce Type: new 
Abstract: Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at https://github.com/LIONS-EPFL/Chameleon.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
arXiv:2505.24850v1 Announce Type: new 
Abstract: Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking</title>
<link>https://arxiv.org/abs/2505.24857</link>
<guid>https://arxiv.org/abs/2505.24857</guid>
<content:encoded><![CDATA[
arXiv:2505.24857v1 Announce Type: new 
Abstract: Recent masked diffusion models (MDMs) have shown competitive performance compared to autoregressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization</title>
<link>https://arxiv.org/abs/2505.24859</link>
<guid>https://arxiv.org/abs/2505.24859</guid>
<content:encoded><![CDATA[
arXiv:2505.24859v1 Announce Type: new 
Abstract: Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. So far, steering vectors have predominantly been evaluated in multiple-choice settings, while their effectiveness in free-form generation tasks remains understudied. Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries of the NEWTS dataset. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Compared to steering, prompting offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results underscore the practical trade-off between control strength and text quality preservation when applying steering vectors to free-form generation tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models</title>
<link>https://arxiv.org/abs/2505.24874</link>
<guid>https://arxiv.org/abs/2505.24874</guid>
<content:encoded><![CDATA[
arXiv:2505.24874v1 Announce Type: new 
Abstract: Neuro-symbolic learning was proposed to address challenges with training neural networks for complex reasoning tasks with the added benefits of interpretability, reliability, and efficiency. Neuro-symbolic learning methods traditionally train neural models in conjunction with symbolic programs, but they face significant challenges that limit them to simplistic problems. On the other hand, purely-neural foundation models now reach state-of-the-art performance through prompting rather than training, but they are often unreliable and lack interpretability. Supplementing foundation models with symbolic programs, which we call neuro-symbolic prompting, provides a way to use these models for complex reasoning tasks. Doing so raises the question: What role does specialized model training as part of neuro-symbolic learning have in the age of foundation models? To explore this question, we highlight three pitfalls of traditional neuro-symbolic learning with respect to the compute, data, and programs leading to generalization problems. This position paper argues that foundation models enable generalizable neuro-symbolic solutions, offering a path towards achieving the original goals of neuro-symbolic learning without the downsides of training from scratch.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset</title>
<link>https://arxiv.org/abs/2411.14207</link>
<guid>https://arxiv.org/abs/2411.14207</guid>
<content:encoded><![CDATA[
arXiv:2411.14207v2 Announce Type: cross 
Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse Responses (HOA-RIRs), created using the Image Source Method. By employing higher-order Ambisonics, our dataset enables precise spatial audio reproduction, a critical requirement for realistic immersive audio applications. Leveraging the virtual simulation, we present a unique microphone configuration, based on the superposition principle, designed to optimize sound field coverage while addressing the limitations of traditional microphone arrays. The presented 64-microphone configuration allows us to capture RIRs directly in the Spherical Harmonics domain. The dataset features a wide range of room configurations, encompassing variations in room geometry, acoustic absorption materials, and source-receiver distances. A detailed description of the simulation setup is provided alongside for an accurate reproduction. The dataset serves as a vital resource for researchers working on spatial audio, particularly in applications involving machine learning to improve room acoustics modeling and sound field synthesis. It further provides a very high level of spatial resolution and realism crucial for tasks such as source localization, reverberation prediction, and immersive sound reproduction.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams</title>
<link>https://arxiv.org/abs/2505.05880</link>
<guid>https://arxiv.org/abs/2505.05880</guid>
<content:encoded><![CDATA[
arXiv:2505.05880v1 Announce Type: cross 
Abstract: Monitoring and analyzing process traces is a critical task for modern companies and organizations. In scenarios where there is a gap between trace events and reference business activities, this entails an interpretation problem, amounting to translating each event of any ongoing trace into the corresponding step of the activity instance. Building on a recent approach that frames the interpretation problem as an acceptance problem within an Abstract Argumentation Framework (AAF), one can elegantly analyze plausible event interpretations (possibly in an aggregated form), as well as offer explanations for those that conflict with prior process knowledge. Since, in settings where event-to-activity mapping is highly uncertain (or simply under-specified) this reasoning-based approach may yield lowly-informative results and heavy computation, one can think of discovering a sequencetagging model, trained to suggest highly-probable candidate event interpretations in a context-aware way. However, training such a model optimally may require using a large amount of manually-annotated example traces. Considering the urgent need of developing Green AI solutions enabling environmental and societal sustainability (with reduced labor/computational costs and carbon footprint), we propose a data/computation-efficient neuro-symbolic approach to the problem, where the candidate interpretations returned by the example-driven sequence tagger is refined by the AAF-based reasoner. This allows us to also leverage prior knowledge to compensate for the scarcity of example data, as confirmed by experimental results; clearly, this property is particularly useful in settings where data annotation and model optimization costs are subject to stringent constraints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection</title>
<link>https://arxiv.org/abs/2505.18754</link>
<guid>https://arxiv.org/abs/2505.18754</guid>
<content:encoded><![CDATA[
arXiv:2505.18754v1 Announce Type: cross 
Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection (59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Parallels Between Evolutionary Theory and the State of AI</title>
<link>https://arxiv.org/abs/2505.23774</link>
<guid>https://arxiv.org/abs/2505.23774</guid>
<content:encoded><![CDATA[
arXiv:2505.23774v1 Announce Type: cross 
Abstract: This article critically examines the foundational principles of contemporary AI methods, exploring the limitations that hinder its potential. We draw parallels between the modern AI landscape and the 20th-century Modern Synthesis in evolutionary biology, and highlight how advancements in evolutionary theory that augmented the Modern Synthesis, particularly those of Evolutionary Developmental Biology, offer insights that can inform a new design paradigm for AI. By synthesizing findings across AI and evolutionary theory, we propose a pathway to overcome existing limitations, enabling AI to achieve its aspirational goals.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified AI for Accurate Audio Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.23781</link>
<guid>https://arxiv.org/abs/2505.23781</guid>
<content:encoded><![CDATA[
arXiv:2505.23781v1 Announce Type: cross 
Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly detection by integrating advanced noise reduction, feature extraction, and machine learning modeling techniques. The approach combines spectral subtraction and adaptive filtering to enhance audio quality, followed by feature extraction using traditional methods like MFCCs and deep embeddings from pre-trained models such as OpenL3. The modeling pipeline incorporates classical models (SVM, Random Forest), deep learning architectures (CNNs), and ensemble methods to boost robustness and accuracy. Evaluated on benchmark datasets including TORGO and LibriSpeech, the proposed framework demonstrates superior performance in precision, recall, and classification of slurred vs. normal speech. This work addresses challenges in noisy environments and real-time applications and provides a scalable solution for audio-based anomaly detection.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning</title>
<link>https://arxiv.org/abs/2505.23783</link>
<guid>https://arxiv.org/abs/2505.23783</guid>
<content:encoded><![CDATA[
arXiv:2505.23783v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases, but also enables the ability to alter and even completely reverse the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques: context-invariance and directional trust-region. The former is designed to tackle the instability issue in ICL, while the latter controls the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Normal Patterns in Musical Loops</title>
<link>https://arxiv.org/abs/2505.23784</link>
<guid>https://arxiv.org/abs/2505.23784</guid>
<content:encoded><![CDATA[
arXiv:2505.23784v1 Announce Type: cross 
Abstract: This paper introduces an unsupervised framework for detecting audio patterns in musical samples (loops) through anomaly detection techniques, addressing challenges in music information retrieval (MIR). Existing methods are often constrained by reliance on handcrafted features, domain-specific limitations, or dependence on iterative user interaction. We address these limitations through an architecture combining deep feature extraction with unsupervised anomaly detection. Our approach leverages a pre-trained Hierarchical Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion Mechanism (FFM), to generate representations from variable-length audio loops. These embeddings are processed using one-class Deep Support Vector Data Description (Deep SVDD), which learns normative audio patterns by mapping them to a compact latent hypersphere. Evaluations on curated bass and guitar datasets compare standard and residual autoencoder variants against baselines like Isolation Forest (IF) and and principle component analysis (PCA) methods. Results show our Deep SVDD models, especially the residual autoencoder variant, deliver improved anomaly separation, particularly for larger variations. This research contributes a flexible, fully unsupervised solution for processing diverse audio samples, overcoming previous structural and input limitations while enabling effective pattern identification through distance-based latent space scoring.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: A Practical Attack on GGUF Quantization</title>
<link>https://arxiv.org/abs/2505.23786</link>
<guid>https://arxiv.org/abs/2505.23786</guid>
<content:encoded><![CDATA[
arXiv:2505.23786v1 Announce Type: cross 
Abstract: With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Exploration of Literature Landscape with LitChat</title>
<link>https://arxiv.org/abs/2505.23789</link>
<guid>https://arxiv.org/abs/2505.23789</guid>
<content:encoded><![CDATA[
arXiv:2505.23789v1 Announce Type: cross 
Abstract: We are living in an era of "big literature", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering "comprehensive, objective, open and transparent" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.23791</link>
<guid>https://arxiv.org/abs/2505.23791</guid>
<content:encoded><![CDATA[
arXiv:2505.23791v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Suicidal Risk on Social Media: A Hybrid Model</title>
<link>https://arxiv.org/abs/2505.23797</link>
<guid>https://arxiv.org/abs/2505.23797</guid>
<content:encoded><![CDATA[
arXiv:2505.23797v1 Announce Type: cross 
Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</title>
<link>https://arxiv.org/abs/2505.23799</link>
<guid>https://arxiv.org/abs/2505.23799</guid>
<content:encoded><![CDATA[
arXiv:2505.23799v1 Announce Type: cross 
Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks</title>
<link>https://arxiv.org/abs/2505.23801</link>
<guid>https://arxiv.org/abs/2505.23801</guid>
<content:encoded><![CDATA[
arXiv:2505.23801v1 Announce Type: cross 
Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies</title>
<link>https://arxiv.org/abs/2505.23804</link>
<guid>https://arxiv.org/abs/2505.23804</guid>
<content:encoded><![CDATA[
arXiv:2505.23804v1 Announce Type: cross 
Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions</title>
<link>https://arxiv.org/abs/2505.23811</link>
<guid>https://arxiv.org/abs/2505.23811</guid>
<content:encoded><![CDATA[
arXiv:2505.23811v1 Announce Type: cross 
Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance.It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content</title>
<link>https://arxiv.org/abs/2505.23812</link>
<guid>https://arxiv.org/abs/2505.23812</guid>
<content:encoded><![CDATA[
arXiv:2505.23812v1 Announce Type: cross 
Abstract: The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and 17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and 11.18\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLMs by Predicting Preferences from User Writing Samples</title>
<link>https://arxiv.org/abs/2505.23815</link>
<guid>https://arxiv.org/abs/2505.23815</guid>
<content:encoded><![CDATA[
arXiv:2505.23815v1 Announce Type: cross 
Abstract: Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\% improvement over ICL alone.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs</title>
<link>https://arxiv.org/abs/2505.23816</link>
<guid>https://arxiv.org/abs/2505.23816</guid>
<content:encoded><![CDATA[
arXiv:2505.23816v1 Announce Type: cross 
Abstract: Despite advances in large language models (LLMs) on reasoning and instruction-following benchmarks, it remains unclear whether they can reliably produce outputs aligned with a broad variety of user goals, a concept we refer to as steerability. The abundance of methods proposed to modify LLM behavior makes it unclear whether current LLMs are already steerable, or require further intervention. In particular, LLMs may exhibit (i) poor coverage, where rare user goals are underrepresented; (ii) miscalibration, where models overshoot requests; and (iii) side effects, where changes to one dimension of text inadvertently affect others. To systematically evaluate these failures, we introduce a framework based on a multi-dimensional goal space that models user goals and LLM outputs as vectors with dimensions corresponding to text attributes (e.g., reading difficulty). Applied to a text-rewriting task, we find that current LLMs struggle with steerability, as side effects are persistent. Interventions to improve steerability, such as prompt engineering, best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying effectiveness, yet side effects remain problematic. Our findings suggest that even strong LLMs struggle with steerability, and existing alignment strategies may be insufficient. We open-source our steerability evaluation framework at https://github.com/MLD3/steerability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses</title>
<link>https://arxiv.org/abs/2505.23834</link>
<guid>https://arxiv.org/abs/2505.23834</guid>
<content:encoded><![CDATA[
arXiv:2505.23834v1 Announce Type: cross 
Abstract: Lung sound classification is vital for early diagnosis of respiratory diseases. However, biomedical signals often exhibit inter-patient variability even among patients with the same symptoms, requiring a learning approach that considers individual differences. We propose a Patient-Aware Feature Alignment (PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL) and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same patient while separating those from other patients to capture patient variability, whereas GPAL draws each patient's centroid toward a global center, preventing feature space fragmentation. Our method achieves outstanding results on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for two-class classification. These findings highlight PAFA's ability to capture individualized patterns and demonstrate performance gains in distinct patient clusters, offering broader applications for patient-centered healthcare.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks</title>
<link>https://arxiv.org/abs/2505.23843</link>
<guid>https://arxiv.org/abs/2505.23843</guid>
<content:encoded><![CDATA[
arXiv:2505.23843v1 Announce Type: cross 
Abstract: Multi-round incomplete information tasks are crucial for evaluating the lateral thinking capabilities of large language models (LLMs). Currently, research primarily relies on multiple benchmarks and automated evaluation metrics to assess these abilities. However, our study reveals novel insights into the limitations of existing methods, as they often yield misleading results that fail to uncover key issues, such as shortcut-taking behaviors, rigid patterns, and premature task termination. These issues obscure the true reasoning capabilities of LLMs and undermine the reliability of evaluations. To address these limitations, we propose a refined set of evaluation standards, including inspection of reasoning paths, diversified assessment metrics, and comparative analyses with human performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models</title>
<link>https://arxiv.org/abs/2505.23848</link>
<guid>https://arxiv.org/abs/2505.23848</guid>
<content:encoded><![CDATA[
arXiv:2505.23848v1 Announce Type: cross 
Abstract: We introduce a method to reduce refusal rates of large language models (LLMs) on sensitive content without modifying model weights or prompts. Motivated by the observation that refusals in certain models were often preceded by the specific token sequence of a token marking the beginning of the chain-of-thought (CoT) block () followed by a double newline token (\n\n), we investigate the impact of two simple formatting adjustments during generation: suppressing \n\n after  and suppressing the end-of-sequence token after the end of the CoT block (). Our method requires no datasets, parameter changes, or training, relying solely on modifying token probabilities during generation. In our experiments with official DeepSeek-R1 distillations, these interventions increased the proportion of substantive answers to sensitive prompts without affecting performance on standard benchmarks. Our findings suggest that refusal behaviors can be circumvented by blocking refusal subspaces at specific points in the generation process.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2505.23849</link>
<guid>https://arxiv.org/abs/2505.23849</guid>
<content:encoded><![CDATA[
arXiv:2505.23849v1 Announce Type: cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves privacy and security of the client's data by not exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a novel framework that allows users to define custom data readiness (DR) standards, metrics, rules, and remedies tailored to specific FL tasks. Our framework generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are optimally prepared for FL while preserving privacy. We demonstrate the framework's practical application by integrating it into an existing PPFL framework. We conducted experiments across six diverse datasets, addressing seven different DR issues. The results illustrate the framework's versatility and effectiveness in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources by identifying and addressing data-related issues before the training phase.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Uncertainty Estimation and Calibration of Large Language Models</title>
<link>https://arxiv.org/abs/2505.23854</link>
<guid>https://arxiv.org/abs/2505.23854</guid>
<content:encoded><![CDATA[
arXiv:2505.23854v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
arXiv:2505.23856v1 Announce Type: cross 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE</title>
<link>https://arxiv.org/abs/2505.23858</link>
<guid>https://arxiv.org/abs/2505.23858</guid>
<content:encoded><![CDATA[
arXiv:2505.23858v1 Announce Type: cross 
Abstract: With the increasing brightness of Light sources, including the Diffraction-Limited brightness upgrade of APS and the high-repetition-rate upgrade of LCLS, the proposed experiments therein are becoming increasingly complex. For instance, experiments at LCLS-II-HE will require the X-ray beam to be within a fraction of a micron in diameter, with pointing stability of a few nanoradians, at the end of a kilometer-long electron accelerator, a hundred-meter-long undulator section, and tens of meters long X-ray optics. This enhancement of brightness will increase the data production rate to rival the largest data generators in the world. Without real-time active feedback control and an optimized pipeline to transform measurements to scientific information and insights, researchers will drown in a deluge of mostly useless data, and fail to extract the highly sophisticated insights that the recent brightness upgrades promise.
  In this article, we outline the strategy we are developing at SLAC to implement Machine Learning driven optimization, automation and real-time knowledge extraction from the electron-injector at the start of the electron accelerator, to the multidimensional X-ray optical systems, and till the experimental endstations and the high readout rate, multi-megapixel detectors at LCLS to deliver the design performance to the users. This is illustrated via examples from Accelerator, Optics and End User applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum computing and artificial intelligence: status and perspectives</title>
<link>https://arxiv.org/abs/2505.23860</link>
<guid>https://arxiv.org/abs/2505.23860</guid>
<content:encoded><![CDATA[
arXiv:2505.23860v1 Announce Type: cross 
Abstract: This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors</title>
<link>https://arxiv.org/abs/2505.23862</link>
<guid>https://arxiv.org/abs/2505.23862</guid>
<content:encoded><![CDATA[
arXiv:2505.23862v1 Announce Type: cross 
Abstract: The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at https://github.com/HudenJear/RPLoss.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gibbs randomness-compression proposition: An efficient deep learning</title>
<link>https://arxiv.org/abs/2505.23869</link>
<guid>https://arxiv.org/abs/2505.23869</guid>
<content:encoded><![CDATA[
arXiv:2505.23869v1 Announce Type: cross 
Abstract: A proposition that connects randomness and compression put forward via Gibbs entropy over set of measurement vectors associated with a compression process. The proposition states that a lossy compression process is equivalent to {\it directed randomness} that preserves information content. The proposition originated from the observed behaviour in newly proposed {\it Dual Tomographic Compression} (DTC) compress-train framework. This is akin to tomographic reconstruction of layer weight matrices via building compressed sensed projections, so called {\it weight rays}. This tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning. This novel model compress-train scheme appear in iterative fashion and act as smart neural architecture search, Experiments demonstrated utility of this dual-tomography producing state-of-the-art performance with efficient compression during training, accelerating and supporting lottery ticket hypothesis. However, random compress-train iterations having similar performance demonstrated the connection between randomness and compression from statistical physics perspective, we formulated so called {\it Gibbs randomness-compression proposition}, signifying randomness-compression relationship via Gibbs entropy. Practically, DTC framework provides a promising approach for massively energy and resource efficient deep learning training approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.23883</link>
<guid>https://arxiv.org/abs/2505.23883</guid>
<content:encoded><![CDATA[
arXiv:2505.23883v1 Announce Type: cross 
Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representational Difference Explanations</title>
<link>https://arxiv.org/abs/2505.23917</link>
<guid>https://arxiv.org/abs/2505.23917</guid>
<content:encoded><![CDATA[
arXiv:2505.23917v1 Announce Type: cross 
Abstract: We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve</title>
<link>https://arxiv.org/abs/2505.23946</link>
<guid>https://arxiv.org/abs/2505.23946</guid>
<content:encoded><![CDATA[
arXiv:2505.23946v1 Announce Type: cross 
Abstract: Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Emotion Fool Anti-spoofing?</title>
<link>https://arxiv.org/abs/2505.23962</link>
<guid>https://arxiv.org/abs/2505.23962</guid>
<content:encoded><![CDATA[
arXiv:2505.23962v1 Announce Type: cross 
Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset: https://emospoof-tts.github.io/Dataset/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic Classification of Maritime Vessels using Learnable Filterbanks</title>
<link>https://arxiv.org/abs/2505.23964</link>
<guid>https://arxiv.org/abs/2505.23964</guid>
<content:encoded><![CDATA[
arXiv:2505.23964v1 Announce Type: cross 
Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic signatures is complicated by the variability of different recording scenarios. A robust classification framework must be able to generalize across diverse acoustic environments and variable source-sensor distances. To this end, we present a deep learning model with robust performance across different recording scenarios. Using a trainable spectral front-end and temporal feature encoder to learn a Gabor filterbank, the model can dynamically emphasize different frequency components. Trained on the VTUAD hydrophone recordings from the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 % percent test accuracy across varying source-sensor distances, surpassing the previous benchmark by over 12 percentage points. We present the model, justify our architectural choices, analyze the learned Gabor filters, and perform ablation studies on sensor data fusion and attention-based pooling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention</title>
<link>https://arxiv.org/abs/2505.23968</link>
<guid>https://arxiv.org/abs/2505.23968</guid>
<content:encoded><![CDATA[
arXiv:2505.23968v1 Announce Type: cross 
Abstract: Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL</title>
<link>https://arxiv.org/abs/2505.23977</link>
<guid>https://arxiv.org/abs/2505.23977</guid>
<content:encoded><![CDATA[
arXiv:2505.23977v1 Announce Type: cross 
Abstract: Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets</title>
<link>https://arxiv.org/abs/2505.23980</link>
<guid>https://arxiv.org/abs/2505.23980</guid>
<content:encoded><![CDATA[
arXiv:2505.23980v1 Announce Type: cross 
Abstract: Understanding Greenland's subglacial topography is critical for projecting the future mass loss of the ice sheet and its contribution to global sea-level rise. However, the complex and sparse nature of observational data, particularly information about the bed topography under the ice sheet, significantly increases the uncertainty in model projections. Bed topography is traditionally measured by airborne ice-penetrating radar that measures the ice thickness directly underneath the aircraft, leaving data gap of tens of kilometers in between flight lines. This study introduces a deep learning framework, which we call as DeepTopoNet, that integrates radar-derived ice thickness observations and BedMachine Greenland data through a novel dynamic loss-balancing mechanism. Among all efforts to reconstruct bed topography, BedMachine has emerged as one of the most widely used datasets, combining mass conservation principles and ice thickness measurements to generate high-resolution bed elevation estimates. The proposed loss function adaptively adjusts the weighting between radar and BedMachine data, ensuring robustness in areas with limited radar coverage while leveraging the high spatial resolution of BedMachine predictions i.e. bed estimates. Our approach incorporates gradient-based and trend surface features to enhance model performance and utilizes a CNN architecture designed for subgrid-scale predictions. By systematically testing on the Upernavik Isstr{\o}m) region, the model achieves high accuracy, outperforming baseline methods in reconstructing subglacial terrain. This work demonstrates the potential of deep learning in bridging observational gaps, providing a scalable and efficient solution to inferring subglacial topography.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs</title>
<link>https://arxiv.org/abs/2505.23996</link>
<guid>https://arxiv.org/abs/2505.23996</guid>
<content:encoded><![CDATA[
arXiv:2505.23996v1 Announce Type: cross 
Abstract: The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study</title>
<link>https://arxiv.org/abs/2505.24006</link>
<guid>https://arxiv.org/abs/2505.24006</guid>
<content:encoded><![CDATA[
arXiv:2505.24006v1 Announce Type: cross 
Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network (A2-SBNN), a predictive spatial model designed to map coordinates to continuous fields while capturing both typical spatial patterns and extreme dependencies. By embedding the dual-tail novel Archimedean copula viz. A2 directly into the network's weight initialization, A2-SBNN naturally models complex spatial relationships, including rare co-movements in the data. The model is trained through a calibration-driven process combining Wasserstein loss, moment matching, and correlation penalties to refine predictions and manage uncertainty. Simulation results show that A2-SBNN consistently delivers high accuracy across a wide range of dependency strengths, offering a new, effective solution for spatial data modeling beyond traditional Gaussian-based approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</title>
<link>https://arxiv.org/abs/2505.24009</link>
<guid>https://arxiv.org/abs/2505.24009</guid>
<content:encoded><![CDATA[
arXiv:2505.24009v1 Announce Type: cross 
Abstract: Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System</title>
<link>https://arxiv.org/abs/2505.24016</link>
<guid>https://arxiv.org/abs/2505.24016</guid>
<content:encoded><![CDATA[
arXiv:2505.24016v1 Announce Type: cross 
Abstract: This paper discusses the construction, fine-tuning, and deployment of BeaverTalk, a cascaded system for speech-to-text translation as part of the IWSLT 2025 simultaneous translation task. The system architecture employs a VAD segmenter for breaking a speech stream into segments, Whisper Large V2 for automatic speech recognition (ASR), and Gemma 3 12B for simultaneous translation. Regarding the simultaneous translation LLM, it is fine-tuned via low-rank adaptors (LoRAs) for a conversational prompting strategy that leverages a single prior-sentence memory bank from the source language as context. The cascaded system participated in the English$\rightarrow$German and English$\rightarrow$Chinese language directions for both the low and high latency regimes. In particular, on the English$\rightarrow$German task, the system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and 3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and 3521.35, respectively.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging machine learning features for linear optical interferometer control</title>
<link>https://arxiv.org/abs/2505.24032</link>
<guid>https://arxiv.org/abs/2505.24032</guid>
<content:encoded><![CDATA[
arXiv:2505.24032v1 Announce Type: cross 
Abstract: We have developed an algorithm that constructs a model of a reconfigurable optical interferometer, independent of specific architectural constraints. The programming of unitary transformations on the interferometer's optical modes relies on either an analytical method for deriving the unitary matrix from a set of phase shifts or an optimization routine when such decomposition is not available. Our algorithm employs a supervised learning approach, aligning the interferometer model with a training set derived from the device being studied. A straightforward optimization procedure leverages this trained model to determine the phase shifts of the interferometer with a specific architecture, obtaining the required unitary transformation. This approach enables the effective tuning of interferometers without requiring a precise analytical solution, paving the way for the exploration of new interferometric circuit architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Object Detection by Sequential Risk Control</title>
<link>https://arxiv.org/abs/2505.24038</link>
<guid>https://arxiv.org/abs/2505.24038</guid>
<content:encoded><![CDATA[
arXiv:2505.24038v1 Announce Type: cross 
Abstract: Recent advances in object detectors have led to their adoption for industrial uses. However, their deployment in critical applications is hindered by the inherent lack of reliability of neural networks and the complex structure of object detection models. To address these challenges, we turn to Conformal Prediction, a post-hoc procedure which offers statistical guarantees that are valid for any dataset size, without requiring prior knowledge on the model or data distribution. Our contribution is manifold: first, we formally define the problem of Conformal Object Detection (COD) and introduce a novel method, Sequential Conformal Risk Control (SeqCRC), that extends the statistical guarantees of Conformal Risk Control (CRC) to two sequential tasks with two parameters, as required in the COD setting. Then, we propose loss functions and prediction sets suited to applying CRC to different applications and certification requirements. Finally, we present a conformal toolkit, enabling replication and further exploration of our methods. Using this toolkit, we perform extensive experiments, yielding a benchmark that validates the investigated methods and emphasizes trade-offs and other practical consequences.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM</title>
<link>https://arxiv.org/abs/2505.24062</link>
<guid>https://arxiv.org/abs/2505.24062</guid>
<content:encoded><![CDATA[
arXiv:2505.24062v1 Announce Type: cross 
Abstract: Domain-wall dynamics in ferroelectric materials are strongly position-dependent since each polar interface is locked into a unique local microstructure. This necessitates spatially resolved studies of the wall-pinning using scanning-probe microscopy techniques. The pinning centers and preexisting domain walls are usually sparse within image plane, precluding the use of dense hyperspectral imaging modes and requiring time-consuming human experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$ were investigated to quantify the electric field driven dynamics of the polar-strain domain structures using ML-controlled automated Piezoresponse Force Microscopy. Analysis of 1500 switching events reveals that domain wall displacement depends not only on field parameters but also on the local ferroelectric-ferroelastic configuration. For example, twin boundaries in polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up to a certain level of bias magnitude and change only marginally as the bias increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics on the possible ferroelectric and ferroelastic wall orientations, together with the automated, high-throughput AFM workflow, can be distilled into a predictive map that links domain configurations to pulse parameters. This microstructure-specific rule set forms the foundation for designing ferroelectric memories.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models</title>
<link>https://arxiv.org/abs/2505.24065</link>
<guid>https://arxiv.org/abs/2505.24065</guid>
<content:encoded><![CDATA[
arXiv:2505.24065v1 Announce Type: cross 
Abstract: The growth and characterization of materials using empirical optimization typically requires a significant amount of expert time, experience, and resources. Several complementary characterization methods are routinely performed to determine the quality and properties of a grown sample. Machine learning (ML) can support the conventional approaches by using historical data to guide and provide speed and efficiency to the growth and characterization of materials. Specifically, ML can provide quantitative information from characterization data that is typically obtained from a different modality. In this study, we have investigated the feasibility of projecting the quantitative metric from microscopy measurements, such as atomic force microscopy (AFM), using data obtained from spectroscopy measurements, like Raman spectroscopy. Generative models were also trained to generate the full and specific features of the Raman and photoluminescence spectra from each other and the AFM images of the thin film MoS$_2$. The results are promising and have provided a foundational guide for the use of ML for the cross-modal characterization of materials for their accelerated, efficient, and cost-effective discovery.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity</title>
<link>https://arxiv.org/abs/2505.24097</link>
<guid>https://arxiv.org/abs/2505.24097</guid>
<content:encoded><![CDATA[
arXiv:2505.24097v1 Announce Type: cross 
Abstract: Calibrating blackbox machine learning models to achieve risk control is crucial to ensure reliable decision-making. A rich line of literature has been studying how to calibrate a model so that its predictions satisfy explicit finite-sample statistical guarantees under a fixed, static, and unknown data-generating distribution. However, prediction-supported decisions may influence the outcome they aim to predict, a phenomenon named performativity of predictions, which is commonly seen in social science and economics. In this paper, we introduce Performative Risk Control, a framework to calibrate models to achieve risk control under performativity with provable theoretical guarantees. Specifically, we provide an iteratively refined calibration process, where we ensure the predictions are improved and risk-controlled throughout the process. We also study different types of risk measures and choices of tail bounds. Lastly, we demonstrate the effectiveness of our framework by numerical experiments on the task of predicting credit default risk. To the best of our knowledge, this work is the first one to study statistically rigorous risk control under performativity, which will serve as an important safeguard against a wide range of strategic manipulation in decision-making processes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v1 Announce Type: cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel methodology that integrates ESNs with transfer learning, aiming to enhance predictive performance across various parameter regimes of the gKS model. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Foundation Model for GI Endoscopy Images</title>
<link>https://arxiv.org/abs/2505.24108</link>
<guid>https://arxiv.org/abs/2505.24108</guid>
<content:encoded><![CDATA[
arXiv:2505.24108v1 Announce Type: cross 
Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract abnormalities in order to detect diseases in their early stages and improve patient outcomes. Although deep learning has shown success in supporting GI diagnostics and decision-making, these models require curated datasets with labels that are expensive to acquire. Foundation models offer a promising solution by learning general-purpose representations, which can be finetuned for specific tasks, overcoming data scarcity. Developing foundation models for medical imaging holds significant potential, but the sensitive and protected nature of medical data presents unique challenges. Foundation model training typically requires extensive datasets, and while hospitals generate large volumes of data, privacy restrictions prevent direct data sharing, making foundation model training infeasible in most scenarios. In this work, we propose a FL framework for training foundation models for gastroendoscopy imaging, enabling data to remain within local hospital environments while contributing to a shared model. We explore several established FL algorithms, assessing their suitability for training foundation models without relying on task-specific labels, conducting experiments in both homogeneous and heterogeneous settings. We evaluate the trained foundation model on three critical downstream tasks--classification, detection, and segmentation--and demonstrate that it achieves improved performance across all tasks, highlighting the effectiveness of our approach in a federated, privacy-preserving setting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures</title>
<link>https://arxiv.org/abs/2505.24117</link>
<guid>https://arxiv.org/abs/2505.24117</guid>
<content:encoded><![CDATA[
arXiv:2505.24117v1 Announce Type: cross 
Abstract: Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov chain in that order (i.e., $Y \to X \to Z$), we derive upper bounds on the excess minimum risk using generalized information divergence measures. Here, $Y$ is a target vector to be estimated from an observed feature vector $X$ or its stochastically degraded version $Z$. The excess minimum risk is defined as the difference between the minimum expected loss in estimating $Y$ from $X$ and from $Z$. We present a family of bounds that generalize the mutual information based bound of Gy\"orfi et al. (2023), using the R\'enyi and $\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information. Our bounds are similar to those developed by Modak et al. (2021) and Aminian et al. (2024) for the generalization error of learning algorithms. However, unlike these works, our bounds do not require the sub-Gaussian parameter to be constant and therefore apply to a broader class of joint distributions over $Y$, $X$, and $Z$. We also provide numerical examples under both constant and non-constant sub-Gaussianity assumptions, illustrating that our generalized divergence based bounds can be tighter than the one based on mutual information for certain regimes of the parameter $\alpha$.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes</title>
<link>https://arxiv.org/abs/2505.24132</link>
<guid>https://arxiv.org/abs/2505.24132</guid>
<content:encoded><![CDATA[
arXiv:2505.24132v1 Announce Type: cross 
Abstract: We perform an information-theoretic mode decomposition for separated wakes around a wing. The current data-driven approach based on a neural network referred to as deep sigmoidal flow enables the extraction of an informative component from a given flow field snapshot with respect to a target variable at a future time stamp, thereby capturing the causality as a time-varying modal structure. We consider three examples of separated flows around a NACA0012 airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2. strong vortex-airfoil interactions, and 3. a turbulent wake in a spanwise-periodic domain. The present approach reveals informative vortical structures associated with a time-varying lift response. For the periodic shedding cases, the informative structures vary in time corresponding to the fluctuation level from their mean values. With the second example of vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in the lift response over time is identified in an interpretable manner. Furthermore, for the case of turbulent wake, the present model highlights structures near the wing and vortex cores as informative components based solely on the information metric without any prior knowledge of aerodynamics and length scales. This study provides causality-based insights into a range of unsteady aerodynamic problems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mathematical Perspective On Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.24134</link>
<guid>https://arxiv.org/abs/2505.24134</guid>
<content:encoded><![CDATA[
arXiv:2505.24134v1 Announce Type: cross 
Abstract: Multimodal contrastive learning is a methodology for linking different data modalities; the canonical example is linking image and text data. The methodology is typically framed as the identification of a set of encoders, one for each modality, that align representations within a common latent space. In this work, we focus on the bimodal setting and interpret contrastive learning as the optimization of (parameterized) encoders that define conditional probability distributions, for each modality conditioned on the other, consistent with the available data. This provides a framework for multimodal algorithms such as crossmodal retrieval, which identifies the mode of one of these conditional distributions, and crossmodal classification, which is similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This probabilistic perspective suggests two natural generalizations of contrastive learning: the introduction of novel probabilistic loss functions, and the use of alternative metrics for measuring alignment in the common latent space. We study these generalizations of the classical approach in the multivariate Gaussian setting. In this context we view the latent space identification as a low-rank matrix approximation problem. This allows us to characterize the capabilities of loss functions and alignment metrics to approximate natural statistics, such as conditional means and covariances; doing so yields novel variants on contrastive learning algorithms for specific mode-seeking and for generative tasks. The framework we introduce is also studied through numerical experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data assimilation application arising in oceanography.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction</title>
<link>https://arxiv.org/abs/2505.24136</link>
<guid>https://arxiv.org/abs/2505.24136</guid>
<content:encoded><![CDATA[
arXiv:2505.24136v1 Announce Type: cross 
Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the model's ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control</title>
<link>https://arxiv.org/abs/2505.24161</link>
<guid>https://arxiv.org/abs/2505.24161</guid>
<content:encoded><![CDATA[
arXiv:2505.24161v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making through neuromorphic hardware, making them compelling for Reinforcement Learning (RL) in resource-constrained edge devices. Recent studies in this field directly replace Artificial Neural Networks (ANNs) by SNNs in existing RL frameworks, overlooking whether the RL algorithm is suitable for SNNs. However, most RL algorithms in continuous control are designed tailored to ANNs, including the target network soft updates mechanism, which conflict with the discrete, non-differentiable dynamics of SNN spikes. We identify that this mismatch destabilizes SNN training in continuous control tasks. To bridge this gap between discrete SNN and continuous control, we propose a novel proxy target framework. The continuous and differentiable dynamics of the proxy target enable smooth updates, bypassing the incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy network operates only during training, the SNN retains its energy efficiency during deployment without inference overhead. Extensive experiments on continuous control benchmarks demonstrate that compared to vanilla SNNs, the proxy target framework enables SNNs to achieve up to 32% higher performance across different spiking neurons. Notably, we are the first to surpass ANN performance in continuous control with simple Leaky-Integrate-and-Fire (LIF) neurons. This work motivates a new class of SNN-friendly RL algorithms tailored to SNN's characteristics, paving the way for neuromorphic agents that combine high performance with low power consumption.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation</title>
<link>https://arxiv.org/abs/2505.24174</link>
<guid>https://arxiv.org/abs/2505.24174</guid>
<content:encoded><![CDATA[
arXiv:2505.24174v1 Announce Type: cross 
Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Protein Conformation Ensemble Generation with Physical Feedback</title>
<link>https://arxiv.org/abs/2505.24203</link>
<guid>https://arxiv.org/abs/2505.24203</guid>
<content:encoded><![CDATA[
arXiv:2505.24203v1 Announce Type: cross 
Abstract: Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin</title>
<link>https://arxiv.org/abs/2505.24222</link>
<guid>https://arxiv.org/abs/2505.24222</guid>
<content:encoded><![CDATA[
arXiv:2505.24222v1 Announce Type: cross 
Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of generating images via learning the noised score function of data distribution. Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level, with efforts concentrated on refining inter-level denoising strategies. While leveraging additional second-order Hessian geometry to enhance the sampling quality of Langevin is a common practice in Markov chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in high-dimensional DMs lead to quadratic-complexity computational costs, rendering them non-scalable. In this work, we introduce a novel Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion Hessian geometry in a training-free manner, drawing inspiration from the celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces two key innovations: (1) A low-rank approximation of the diffusion Hessian, leveraging the DMs' inherent structure and circumventing explicit quadratic-complexity computations; (2) A damping mechanism to stabilize the approximated Hessian. This LML approximated Hessian geometry enables the diffusion sampling to execute more accurate steps and improve the image generation quality. We further conduct a theoretical analysis to substantiate the approximation error bound of low-rank approximation and the convergence property of the damping mechanism. Extensive experiments across multiple pretrained DMs validate that the LML method significantly improves image generation quality, with negligible computational overhead.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM</title>
<link>https://arxiv.org/abs/2505.24238</link>
<guid>https://arxiv.org/abs/2505.24238</guid>
<content:encoded><![CDATA[
arXiv:2505.24238v1 Announce Type: cross 
Abstract: Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the {\dataset} benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. {\dataset} introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals that (1) the model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations; (2) current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities; and (3) question types correlate with distinct hallucination patterns, highlighting targeted challenges and potential mitigation strategies. To address these challenges, we propose {\method}, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. {\method} establishes a baseline on {\dataset}, and reduces the logical hallucinations in original base models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring</title>
<link>https://arxiv.org/abs/2505.24239</link>
<guid>https://arxiv.org/abs/2505.24239</guid>
<content:encoded><![CDATA[
arXiv:2505.24239v1 Announce Type: cross 
Abstract: While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Knockout for Unraveling Factual Information Flow</title>
<link>https://arxiv.org/abs/2505.24244</link>
<guid>https://arxiv.org/abs/2505.24244</guid>
<content:encoded><![CDATA[
arXiv:2505.24244v1 Announce Type: cross 
Abstract: This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct "features" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings</title>
<link>https://arxiv.org/abs/2505.24281</link>
<guid>https://arxiv.org/abs/2505.24281</guid>
<content:encoded><![CDATA[
arXiv:2505.24281v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) has become an essential machine learning tool for addressing multiple learning tasks simultaneously and has been effectively applied across fields such as healthcare, marketing, and biomedical research. However, to enable efficient information sharing across tasks, it is crucial to leverage both shared and heterogeneous information. Despite extensive research on MTL, various forms of heterogeneity, including distribution and posterior heterogeneity, present significant challenges. Existing methods often fail to address these forms of heterogeneity within a unified framework. In this paper, we propose a dual-encoder framework to construct a heterogeneous latent factor space for each task, incorporating a task-shared encoder to capture common information across tasks and a task-specific encoder to preserve unique task characteristics. Additionally, we explore the intrinsic similarity structure of the coefficients corresponding to learned latent factors, allowing for adaptive integration across tasks to manage posterior heterogeneity. We introduce a unified algorithm that alternately learns the task-specific and task-shared encoders and coefficients. In theory, we investigate the excess risk bound for the proposed MTL method using local Rademacher complexity and apply it to a new but related task. Through simulation studies, we demonstrate that the proposed method outperforms existing data integration methods across various settings. Furthermore, the proposed method achieves superior predictive performance for time to tumor doubling across five distinct cancer types in PDX data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Fusion for Partial Identification of Causal Effects</title>
<link>https://arxiv.org/abs/2505.24296</link>
<guid>https://arxiv.org/abs/2505.24296</guid>
<content:encoded><![CDATA[
arXiv:2505.24296v1 Announce Type: cross 
Abstract: Data fusion techniques integrate information from heterogeneous data sources to improve learning, generalization, and decision making across data sciences. In causal inference, these methods leverage rich observational data to improve causal effect estimation, while maintaining the trustworthiness of randomized controlled trials. Existing approaches often relax the strong no unobserved confounding assumption by instead assuming exchangeability of counterfactual outcomes across data sources. However, when both assumptions simultaneously fail - a common scenario in practice - current methods cannot identify or estimate causal effects. We address this limitation by proposing a novel partial identification framework that enables researchers to answer key questions such as: Is the causal effect positive or negative? and How severe must assumption violations be to overturn this conclusion? Our approach introduces interpretable sensitivity parameters that quantify assumption violations and derives corresponding causal effect bounds. We develop doubly robust estimators for these bounds and operationalize breakdown frontier analysis to understand how causal conclusions change as assumption violations increase. We apply our framework to the Project STAR study, which investigates the effect of classroom size on students' third-grade standardized test performance. Our analysis reveals that the Project STAR results are robust to simultaneous violations of key assumptions, both on average and across various subgroups of interest. This strengthens confidence in the study's conclusions despite potential unmeasured biases in the data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels</title>
<link>https://arxiv.org/abs/2505.24311</link>
<guid>https://arxiv.org/abs/2505.24311</guid>
<content:encoded><![CDATA[
arXiv:2505.24311v1 Announce Type: cross 
Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm for visualizing high-dimensional data by finding low-dimensional representations. In this paper, we study the convergence of t-SNE with generalized kernels and extend the results of Auffinger and Fletcher in 2023. Our work starts by giving a concrete formulation of generalized input and output kernels. Then we prove that under certain conditions, the t-SNE algorithm converges to an equilibrium distribution for a wide range of input and output kernels as the number of data points diverges.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising</title>
<link>https://arxiv.org/abs/2505.24327</link>
<guid>https://arxiv.org/abs/2505.24327</guid>
<content:encoded><![CDATA[
arXiv:2505.24327v1 Announce Type: cross 
Abstract: Remote sensing image (RSI) denoising is an important topic in the field of remote sensing. Despite the impressive denoising performance of RSI denoising methods, most current deep learning-based approaches function as black boxes and lack integration with physical information models, leading to limited interpretability. Additionally, many methods may struggle with insufficient attention to non-local self-similarity in RSI and require tedious tuning of regularization parameters to achieve optimal performance, particularly in conventional iterative optimization approaches. In this paper, we first propose a novel RSI denoising method named sparse tensor-aided representation network (STAR-Net), which leverages a low-rank prior to effectively capture the non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a sparse variant called STAR-Net-S to deal with the interference caused by non-Gaussian noise in original RSI for the purpose of improving robustness. Different from conventional iterative optimization, we develop an alternating direction method of multipliers (ADMM)-guided deep unrolling network, in which all regularization parameters can be automatically learned, thus inheriting the advantages of both model-based and deep learning-based approaches and successfully addressing the above-mentioned shortcomings. Comprehensive experiments on synthetic and real-world datasets demonstrate that STAR-Net and STAR-Net-S outperform state-of-the-art RSI denoising methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation</title>
<link>https://arxiv.org/abs/2505.24333</link>
<guid>https://arxiv.org/abs/2505.24333</guid>
<content:encoded><![CDATA[
arXiv:2505.24333v1 Announce Type: cross 
Abstract: Finding the right initialisation for neural networks is crucial to ensure smooth training and good performance. In transformers, the wrong initialisation can lead to one of two failure modes of self-attention layers: rank collapse, where all tokens collapse into similar representations, and entropy collapse, where highly concentrated attention scores lead to training instability. While the right initialisation has been extensively studied in feed-forward networks, an exact description of signal propagation through a full transformer block has so far been lacking. Here, we provide an analytical theory of signal propagation through vanilla transformer blocks with self-attention layers, layer normalisation, skip connections and ReLU MLP. To treat the self-attention layer, we draw on a formal parallel with the Random Energy Model from statistical physics. We identify and characterise two regimes governed by the variance of the query and key initialisations: a low-variance regime, where we recover the known rank collapse behaviour; and a previously unexplored high-variance regime, where signal is preserved but \textit{entropy collapse} occurs. In the low-variance regime, we calculate the critical strength for the residual connection to ensure signal propagation. Our theory yields trainability diagrams that identify the correct choice of initialisation hyper-parameters for a given architecture. Experiments with BERT-style models trained on TinyStories validate our predictions. Our theoretical framework gives a unified perspective on the two failure modes of self-attention and gives quantitative predictions on the scale of both weights and residual connections that guarantees smooth training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds</title>
<link>https://arxiv.org/abs/2505.24336</link>
<guid>https://arxiv.org/abs/2505.24336</guid>
<content:encoded><![CDATA[
arXiv:2505.24336v1 Announce Type: cross 
Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres. Demo samples are available at https://nc-ai.github.io/speech/publications/nonhuman-vc/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models</title>
<link>https://arxiv.org/abs/2505.24340</link>
<guid>https://arxiv.org/abs/2505.24340</guid>
<content:encoded><![CDATA[
arXiv:2505.24340v1 Announce Type: cross 
Abstract: Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Unlearning via Sparse Autoencoder Subspace Guided Projections</title>
<link>https://arxiv.org/abs/2505.24428</link>
<guid>https://arxiv.org/abs/2505.24428</guid>
<content:encoded><![CDATA[
arXiv:2505.24428v1 Announce Type: cross 
Abstract: Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an "irrelevant" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System</title>
<link>https://arxiv.org/abs/2505.24429</link>
<guid>https://arxiv.org/abs/2505.24429</guid>
<content:encoded><![CDATA[
arXiv:2505.24429v1 Announce Type: cross 
Abstract: Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers</title>
<link>https://arxiv.org/abs/2505.24443</link>
<guid>https://arxiv.org/abs/2505.24443</guid>
<content:encoded><![CDATA[
arXiv:2505.24443v1 Announce Type: cross 
Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and unlabeled data share an identical class distribution, however in practice, this assumption is easily violated, as unlabeled data often includes unknown class data, i.e., outliers. The outliers are treated as noise, considerably degrading the performance of SSL models. To address this drawback, we propose a novel framework, Diversify and Conquer (DAC), to enhance SSL robustness in the context of open-set semi-supervised learning. In particular, we note that existing open-set SSL methods rely on prediction discrepancies between inliers and outliers from a single model trained on labeled data. This approach can be easily failed when the labeled data is insufficient, leading to performance degradation that is worse than naive SSL that do not account for outliers. In contrast, our approach exploits prediction disagreements among multiple models that are differently biased towards the unlabeled distribution. By leveraging the discrepancies arising from training on unlabeled data, our method enables robust outlier detection even when the labeled data is underspecified. Our key contribution is constructing a collection of differently biased models through a single training process. By encouraging divergent heads to be differently biased towards outliers while making consistent predictions for inliers, we exploit the disagreement among these heads as a measure to identify unknown concepts. Our code is available at https://github.com/heejokong/DivCon.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed gradient methods under heavy-tailed communication noise</title>
<link>https://arxiv.org/abs/2505.24464</link>
<guid>https://arxiv.org/abs/2505.24464</guid>
<content:encoded><![CDATA[
arXiv:2505.24464v1 Announce Type: cross 
Abstract: We consider a standard distributed optimization problem in which networked nodes collaboratively minimize the sum of their locally known convex costs. For this setting, we address for the first time the fundamental problem of design and analysis of distributed methods to solve the above problem when inter-node communication is subject to \emph{heavy-tailed} noise. Heavy-tailed noise is highly relevant and frequently arises in densely deployed wireless sensor and Internet of Things (IoT) networks. Specifically, we design a distributed gradient-type method that features a carefully balanced mixed time-scale time-varying consensus and gradient contribution step sizes and a bounded nonlinear operator on the consensus update to limit the effect of heavy-tailed noise. Assuming heterogeneous strongly convex local costs with mutually different minimizers that are arbitrarily far apart, we show that the proposed method converges to a neighborhood of the network-wide problem solution in the mean squared error (MSE) sense, and we also characterize the corresponding convergence rate. We further show that the asymptotic MSE can be made arbitrarily small through consensus step-size tuning, possibly at the cost of slowing down the transient error decay. Numerical experiments corroborate our findings and demonstrate the resilience of the proposed method to heavy-tailed (and infinite variance) communication noise. They also show that existing distributed methods, designed for finite-communication-noise-variance settings, fail in the presence of infinite variance noise.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation</title>
<link>https://arxiv.org/abs/2505.24472</link>
<guid>https://arxiv.org/abs/2505.24472</guid>
<content:encoded><![CDATA[
arXiv:2505.24472v1 Announce Type: cross 
Abstract: Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Gemini in an arena for learning</title>
<link>https://arxiv.org/abs/2505.24477</link>
<guid>https://arxiv.org/abs/2505.24477</guid>
<content:encoded><![CDATA[
arXiv:2505.24477v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an "arena for learning" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Code-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2505.24480</link>
<guid>https://arxiv.org/abs/2505.24480</guid>
<content:encoded><![CDATA[
arXiv:2505.24480v1 Announce Type: cross 
Abstract: In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.24486</link>
<guid>https://arxiv.org/abs/2505.24486</guid>
<content:encoded><![CDATA[
arXiv:2505.24486v1 Announce Type: cross 
Abstract: The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don't effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953 % across five experiences. The code is available at: https://github.com/falihgoz/RAIS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Fall Prevention system for the Next-generation of Workers</title>
<link>https://arxiv.org/abs/2505.24487</link>
<guid>https://arxiv.org/abs/2505.24487</guid>
<content:encoded><![CDATA[
arXiv:2505.24487v1 Announce Type: cross 
Abstract: Developing a general-purpose wearable real-time fall-detection system is still a challenging task, especially for healthy and strong subjects, such as industrial workers that work in harsh environments. In this work, we present a hybrid approach for fall detection and prevention, which uses the dynamic model of an inverted pendulum to generate simulations of falling that are then fed to a deep learning framework. The output is a signal to activate a fall mitigation mechanism when the subject is at risk of harm. The advantage of this approach is that abstracted models can be used to efficiently generate training data for thousands of different subjects with different falling initial conditions, something that is practically impossible with real experiments. This approach is suitable for a specific type of fall, where the subjects fall without changing their initial configuration significantly, and it is the first step toward a general-purpose wearable device, with the aim of reducing fall-associated injuries in industrial environments, which can improve the safety of workers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Density Functions for Weighted Convolution in Learning Models</title>
<link>https://arxiv.org/abs/2505.24527</link>
<guid>https://arxiv.org/abs/2505.24527</guid>
<content:encoded><![CDATA[
arXiv:2505.24527v1 Announce Type: cross 
Abstract: The paper introduces the weighted convolution, a novel approach to the convolution for signals defined on regular grids (e.g., 2D images) through the application of an optimal density function to scale the contribution of neighbouring pixels based on their distance from the central pixel. This choice differs from the traditional uniform convolution, which treats all neighbouring pixels equally. Our weighted convolution can be applied to convolutional neural network problems to improve the approximation accuracy. Given a convolutional network, we define a framework to compute the optimal density function through a minimisation model. The framework separates the optimisation of the convolutional kernel weights (using stochastic gradient descent) from the optimisation of the density function (using DIRECT-L). Experimental results on a learning model for an image-to-image task (e.g., image denoising) show that the weighted convolution significantly reduces the loss (up to 53% improvement) and increases the test accuracy compared to standard convolution. While this method increases execution time by 11%, it is robust across several hyperparameters of the learning model. Future work will apply the weighted convolution to real-case 2D and 3D image convolutional learning problems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial Foundation Models to Enable Progress on Sustainable Development Goals</title>
<link>https://arxiv.org/abs/2505.24528</link>
<guid>https://arxiv.org/abs/2505.24528</guid>
<content:encoded><![CDATA[
arXiv:2505.24528v1 Announce Type: cross 
Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data</title>
<link>https://arxiv.org/abs/2505.24556</link>
<guid>https://arxiv.org/abs/2505.24556</guid>
<content:encoded><![CDATA[
arXiv:2505.24556v1 Announce Type: cross 
Abstract: Bayesian models based on Gaussian processes (GPs) offer a flexible framework to predict spatially distributed variables with uncertainty. But the use of nonstationary priors, often necessary for capturing complex spatial patterns, makes sampling from the predictive posterior distribution (PPD) computationally intractable. In this paper, we propose a two-step approach based on diffusion generative models (DGMs) to mimic PPDs associated with non-stationary GP priors: we replace the GP prior by a DGM surrogate, and leverage recent advances on training-free guidance algorithms for DGMs to sample from the desired posterior distribution. We apply our approach to a rich non-stationary GP prior from which exact posterior sampling is untractable and validate that the issuing distributions are close to their GP counterpart using several statistical metrics. We also demonstrate how one can fine-tune the trained DGMs to target specific parts of the GP prior. Finally we apply the proposed approach to solve inverse problems arising in environmental sciences, thus yielding state-of-the-art predictions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</title>
<link>https://arxiv.org/abs/2505.24593</link>
<guid>https://arxiv.org/abs/2505.24593</guid>
<content:encoded><![CDATA[
arXiv:2505.24593v1 Announce Type: cross 
Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable phenotyping of Heart Failure patients with Dutch discharge letters</title>
<link>https://arxiv.org/abs/2505.24619</link>
<guid>https://arxiv.org/abs/2505.24619</guid>
<content:encoded><![CDATA[
arXiv:2505.24619v1 Announce Type: cross 
Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes affecting treatment and prognosis. This study evaluates models for phenotyping HF patients based on left ventricular ejection fraction (LVEF) classes, using structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105 hospitalizations, 16,334 patients). Data from AMC were used for model training, and from VUmc for external validation. The dataset was unlabelled and included tabular clinical measurements and discharge letters. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for 300 patients for testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. To evaluate interpretability, two clinicians annotated 20 discharge letters by highlighting information they considered relevant for LVEF classification. These were compared to SHAP and LIME explanations from black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone, achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation), outperforming baselines. Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for phenotyping HF patients. Aug-Linear models matched black-box performance while providing clinician-aligned interpretability, supporting their use in transparent clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success</title>
<link>https://arxiv.org/abs/2505.24622</link>
<guid>https://arxiv.org/abs/2505.24622</guid>
<content:encoded><![CDATA[
arXiv:2505.24622v1 Announce Type: cross 
Abstract: Predicting startup success requires models that are both accurate and interpretable. We present a lightweight ensemble framework that combines YES/NO questions generated by large language models (LLMs), forming a transparent decision-making system. Each question acts as a weak heuristic, and by filtering, ranking, and aggregating them through a threshold-based voting mechanism, we construct a strong ensemble predictor. On a test set where 10% of startups are classified as successful, our approach achieves a precision rate of 50%, representing a 5x improvement over random selection, while remaining fully transparent. When we incorporate expert-guided heuristics into the generation process, performance improves further to 54% precision. These results highlight the value of combining LLM reasoning with human insight and demonstrate that simple, interpretable ensembles can support high-stakes decisions in domains such as venture capital (VC).
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
arXiv:2505.24650v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models</title>
<link>https://arxiv.org/abs/2505.24655</link>
<guid>https://arxiv.org/abs/2505.24655</guid>
<content:encoded><![CDATA[
arXiv:2505.24655v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) risk prediction models are essential for identifying high-risk individuals and guiding preventive actions. However, existing models struggle with the challenges of real-world clinical practice as they oversimplify patient profiles, rely on rigid input schemas, and are sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk prediction framework built on large language models extensively fine-tuned on over half a million participants from the UK Biobank. In benchmark comparisons, AdaCVD surpasses established risk scores and standard machine learning approaches, achieving state-of-the-art performance. Crucially, for the first time, it addresses key clinical challenges across three dimensions: it flexibly incorporates comprehensive yet variable patient information; it seamlessly integrates both structured data and unstructured text; and it rapidly adapts to new patient populations using minimal additional data. In stratified analyses, it demonstrates robust performance across demographic, socioeconomic, and clinical subgroups, including underrepresented cohorts. AdaCVD offers a promising path toward more flexible, AI-driven clinical decision support tools suited to the realities of heterogeneous and dynamic healthcare environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders</title>
<link>https://arxiv.org/abs/2505.24668</link>
<guid>https://arxiv.org/abs/2505.24668</guid>
<content:encoded><![CDATA[
arXiv:2505.24668v1 Announce Type: cross 
Abstract: Modern deep neural networks exhibit strong generalization even in highly overparameterized regimes. Significant progress has been made to understand this phenomenon in the context of supervised learning, but for unsupervised tasks such as denoising, several open questions remain. While some recent works have successfully characterized the test error of the linear denoising problem, they are limited to linear models (one-layer network). In this work, we focus on two-layer linear denoising autoencoders trained under gradient flow, incorporating two key ingredients of modern deep learning architectures: A low-dimensional bottleneck layer that effectively enforces a rank constraint on the learned solution, as well as the possibility of a skip connection that bypasses the bottleneck. We derive closed-form expressions for all critical points of this model under product regularization, and in particular describe its global minimizer under the minimum-norm principle. From there, we derive the test risk formula in the overparameterized regime, both for models with and without skip connections. Our analysis reveals two interesting phenomena: Firstly, the bottleneck layer introduces an additional complexity measure akin to the classical bias-variance trade-off -- increasing the bottleneck width reduces bias but introduces variance, and vice versa. Secondly, skip connection can mitigate the variance in denoising autoencoders -- especially when the model is mildly overparameterized. We further analyze the impact of skip connections in denoising autoencoder using random matrix theory and support our claims with numerical evidence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches</title>
<link>https://arxiv.org/abs/2505.24703</link>
<guid>https://arxiv.org/abs/2505.24703</guid>
<content:encoded><![CDATA[
arXiv:2505.24703v1 Announce Type: cross 
Abstract: Deep learning techniques have enabled vast improvements in computer vision technologies. Nevertheless, these models are vulnerable to adversarial patch attacks which catastrophically impair performance. The physically realizable nature of these attacks calls for certifiable defenses, which feature provable guarantees on robustness. While certifiable defenses have been successfully applied to single-label classification, limited work has been done for multi-label classification. In this work, we present PatchDEMUX, a certifiably robust framework for multi-label classifiers against adversarial patches. Our approach is a generalizable method which can extend any existing certifiable defense for single-label classification; this is done by considering the multi-label classification task as a series of isolated binary classification problems to provably guarantee robustness. Furthermore, in the scenario where an attacker is limited to a single patch we propose an additional certification procedure that can provide tighter robustness bounds. Using the current state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a backbone, we find that PatchDEMUX can achieve non-trivial robustness on the MS-COCO and PASCAL VOC datasets while maintaining high clean performance
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes</title>
<link>https://arxiv.org/abs/2505.24704</link>
<guid>https://arxiv.org/abs/2505.24704</guid>
<content:encoded><![CDATA[
arXiv:2505.24704v1 Announce Type: cross 
Abstract: Kernel method-based intensity estimators, formulated within reproducing kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs) have been among the most easy-to-implement and feasible methods for estimating the intensity functions of inhomogeneous Poisson processes. While both approaches share the term "kernel", they are founded on distinct theoretical principles, each with its own strengths and limitations. In this paper, we propose a novel regularized kernel method for Poisson processes based on the least squares loss and show that the resulting intensity estimator involves a specialized variant of the representer theorem: it has the dual coefficient of unity and coincides with classical KIEs. This result provides new theoretical insights into the connection between classical KIEs and kernel method-based intensity estimators, while enabling us to develop an efficient KIE by leveraging advanced techniques from RKHS theory. We refer to the proposed model as the kernel method-based kernel intensity estimator (K$^2$IE). Through experiments on synthetic datasets, we show that K$^2$IE achieves comparable predictive performance while significantly surpassing the state-of-the-art kernel method-based estimator in computational efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery</title>
<link>https://arxiv.org/abs/2505.24727</link>
<guid>https://arxiv.org/abs/2505.24727</guid>
<content:encoded><![CDATA[
arXiv:2505.24727v1 Announce Type: cross 
Abstract: This paper introduces a novel Knockoff-guided compressive sensing framework, referred to as \TheName{}, which enhances signal recovery by leveraging precise false discovery rate (FDR) control during the support identification phase. Unlike LASSO, which jointly performs support selection and signal estimation without explicit error control, our method guarantees FDR control in finite samples, enabling more reliable identification of the true signal support. By separating and controlling the support recovery process through statistical Knockoff filters, our framework achieves more accurate signal reconstruction, especially in challenging scenarios where traditional methods fail. We establish theoretical guarantees demonstrating how FDR control directly ensures recovery performance under weaker conditions than traditional $\ell_1$-based compressive sensing methods, while maintaining accurate signal reconstruction. Extensive numerical experiments demonstrate that our proposed Knockoff-based method consistently outperforms LASSO-based and other state-of-the-art compressive sensing techniques. In simulation studies, our method improves F1-score by up to 3.9x over baseline methods, attributed to principled false discovery rate (FDR) control and enhanced support recovery. The method also consistently yields lower reconstruction and relative errors. We further validate the framework on real-world datasets, where it achieves top downstream predictive performance across both regression and classification tasks, often narrowing or even surpassing the performance gap relative to uncompressed signals. These results establish \TheName{} as a robust and practical alternative to existing approaches, offering both theoretical guarantees and strong empirical performance through statistically grounded support selection.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v1 Announce Type: cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan. It begins by selecting genes with high Signal-to-Noise Ratio (SNR) to capture the most informative features, from which a cost matrix is constructed using cosine distances between cell-type centroids. Importantly, the MESH procedure iteratively refines the cost matrix, leading to a transport plan with significantly enhanced sparsity and interpretability of the resulting correspondence matrices. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, symmetric, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Dynamics of Linear Diffusion Models</title>
<link>https://arxiv.org/abs/2505.24769</link>
<guid>https://arxiv.org/abs/2505.24769</guid>
<content:encoded><![CDATA[
arXiv:2505.24769v1 Announce Type: cross 
Abstract: Diffusion models trained on finite datasets with $N$ samples from a target distribution exhibit a transition from memorisation, where the model reproduces training examples, to generalisation, where it produces novel samples that reflect the underlying data distribution. Understanding this transition is key to characterising the sample efficiency and reliability of generative models, but our theoretical understanding of this transition is incomplete. Here, we analytically study the memorisation-to-generalisation transition in a simple model using linear denoisers, which allow explicit computation of test errors, sampling distributions, and Kullback-Leibler divergences between samples and target distribution. Using these measures, we predict that this transition occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is smaller than the dimension of the inputs $d$, so that only a fraction of relevant directions of variation are present in the training data, we demonstrate how both regularization and early stopping help to prevent overfitting. For $N > d$, we find that the sampling distributions of linear diffusion models approach their optimum (measured by the Kullback-Leibler divergence) linearly with $d/N$, independent of the specifics of the data distribution. Our work clarifies how sample complexity governs generalisation in a simple model of diffusion-based generative models and provides insight into the training dynamics of linear denoisers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models</title>
<link>https://arxiv.org/abs/2505.24784</link>
<guid>https://arxiv.org/abs/2505.24784</guid>
<content:encoded><![CDATA[
arXiv:2505.24784v1 Announce Type: cross 
Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-Manual Joint Camera Calibration and Scene Representation</title>
<link>https://arxiv.org/abs/2505.24819</link>
<guid>https://arxiv.org/abs/2505.24819</guid>
<content:encoded><![CDATA[
arXiv:2505.24819v1 Announce Type: cross 
Abstract: Robot manipulation, especially bimanual manipulation, often requires setting up multiple cameras on multiple robot manipulators. Before robot manipulators can generate motion or even build representations of their environments, the cameras rigidly mounted to the robot need to be calibrated. Camera calibration is a cumbersome process involving collecting a set of images, with each capturing a pre-determined marker. In this work, we introduce the Bi-Manual Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables multiple robot manipulators, each with cameras mounted, to circumvent taking images of calibration markers. By leveraging 3D foundation models for dense, marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the extrinsic transformation from each camera to its end-effector, (ii) the inter-arm relative poses between manipulators, and (iii) a unified, scale-consistent 3D representation of the shared workspace, all from the same captured RGB image sets. The representation, jointly constructed from images captured by cameras on both manipulators, lives in a common coordinate frame and supports collision checking and semantic segmentation to facilitate downstream bimanual coordination tasks. We empirically evaluate the robustness of Bi-JCR on a variety of tabletop environments, and demonstrate its applicability on a variety of downstream tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck</title>
<link>https://arxiv.org/abs/2505.24840</link>
<guid>https://arxiv.org/abs/2505.24840</guid>
<content:encoded><![CDATA[
arXiv:2505.24840v1 Announce Type: cross 
Abstract: This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Recognition in the Wild</title>
<link>https://arxiv.org/abs/2505.24848</link>
<guid>https://arxiv.org/abs/2505.24848</guid>
<content:encoded><![CDATA[
arXiv:2505.24848v1 Announce Type: cross 
Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial to be able to keep a record of the user's interactions with the world, including during reading. In this paper, we introduce a new task of reading recognition to determine when the user is reading. We first introduce the first-of-its-kind large-scale multimodal Reading in the Wild dataset, containing 100 hours of reading and non-reading videos in diverse and realistic scenarios. We then identify three modalities (egocentric RGB, eye gaze, head pose) that can be used to solve the task, and present a flexible transformer model that performs the task using these modalities, either individually or combined. We show that these modalities are relevant and complementary to the task, and investigate how to efficiently and effectively encode each modality. Additionally, we show the usefulness of this dataset towards classifying types of reading, extending current reading understanding studies conducted in constrained settings to larger scale, diversity and realism. Code, model, and data will be public.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical mechanics of extensive-width Bayesian neural networks near interpolation</title>
<link>https://arxiv.org/abs/2505.24849</link>
<guid>https://arxiv.org/abs/2505.24849</guid>
<content:encoded><![CDATA[
arXiv:2505.24849v1 Announce Type: cross 
Abstract: For three decades statistical mechanics has been providing a framework to analyse neural networks. However, the theoretically tractable models, e.g., perceptrons, random features models and kernel machines, or multi-index models and committee machines with few neurons, remained simple compared to those used in applications. In this paper we help reducing the gap between practical networks and their theoretical understanding through a statistical physics analysis of the supervised learning of a two-layer fully connected network with generic weight distribution and activation function, whose hidden layer is large but remains proportional to the inputs dimension. This makes it more realistic than infinitely wide networks where no feature learning occurs, but also more expressive than narrow ones or with fixed inner weights. We focus on the Bayes-optimal learning in the teacher-student scenario, i.e., with a dataset generated by another network with the same architecture. We operate around interpolation, where the number of trainable parameters and of data are comparable and feature learning emerges. Our analysis uncovers a rich phenomenology with various learning transitions as the number of data increases. In particular, the more strongly the features (i.e., hidden neurons of the target) contribute to the observed responses, the less data is needed to learn them. Moreover, when the data is scarce, the model only learns non-linear combinations of the teacher weights, rather than "specialising" by aligning its weights with the teacher's. Specialisation occurs only when enough data becomes available, but it can be hard to find for practical training algorithms, possibly due to statistical-to-computational~gaps.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data</title>
<link>https://arxiv.org/abs/2505.24852</link>
<guid>https://arxiv.org/abs/2505.24852</guid>
<content:encoded><![CDATA[
arXiv:2505.24852v1 Announce Type: cross 
Abstract: On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2505.24853</link>
<guid>https://arxiv.org/abs/2505.24853</guid>
<content:encoded><![CDATA[
arXiv:2505.24853v1 Announce Type: cross 
Abstract: We study the problem of functional retargeting: learning dexterous manipulation policies to track object states from human hand-object demonstrations. We focus on long-horizon, bimanual tasks with articulated objects, which is challenging due to large action space, spatiotemporal discontinuities, and embodiment gap between human and robot hands. We propose DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual object controllers with decaying strength: an object is first driven automatically towards its target states, such that the policy can gradually learn to take over under motion and contact guidance. We release a simulation benchmark with a diverse set of tasks and dexterous hands, and show that DexMachina significantly outperforms baseline methods. Our algorithm and benchmark enable a functional comparison for hardware designs, and we present key findings informed by quantitative and qualitative results. With the recent surge in dexterous hand development, we hope this work will provide a useful platform for identifying desirable hardware capabilities and lower the barrier for contributing to future research. Videos and more at https://project-dexmachina.github.io/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs</title>
<link>https://arxiv.org/abs/2505.24858</link>
<guid>https://arxiv.org/abs/2505.24858</guid>
<content:encoded><![CDATA[
arXiv:2505.24858v1 Announce Type: cross 
Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of $\textit{faithful confidence calibration}$ of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that $\textit{faithfully reflect}$ their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24871</link>
<guid>https://arxiv.org/abs/2505.24871</guid>
<content:encoded><![CDATA[
arXiv:2505.24871v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[
arXiv:2505.24878v1 Announce Type: cross 
Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Connectivity through Network Gradients for Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2209.06932</link>
<guid>https://arxiv.org/abs/2209.06932</guid>
<content:encoded><![CDATA[
arXiv:2209.06932v4 Announce Type: replace 
Abstract: Leveraging sparse networks to connect successive layers in deep neural networks has recently been shown to provide benefits to large-scale state-of-the-art models. However, network connectivity also plays a significant role in the learning performance of shallow networks, such as the classic Restricted Boltzmann Machine (RBM). Efficiently finding sparse connectivity patterns that improve the learning performance of shallow networks is a fundamental problem. While recent principled approaches explicitly include network connections as model parameters that must be optimized, they often rely on explicit penalization or network sparsity as a hyperparameter. This work presents the Network Connectivity Gradients (NCG), an optimization method to find optimal connectivity patterns for RBMs. NCG leverages the idea of network gradients: given a specific connection pattern, it determines the gradient of every possible connection and uses the gradient to drive a continuous connection strength parameter that in turn is used to determine the connection pattern. Thus, learning RBM parameters and learning network connections is truly jointly performed, albeit with different learning rates, and without changes to the model's classic energy-based objective function. The proposed method is applied to the MNIST and other data sets showing that better RBM models are found for the benchmark tasks of sample generation and classification. Results also show that NCG is robust to network initialization and is capable of both adding and removing network connections while learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2303.14537</link>
<guid>https://arxiv.org/abs/2303.14537</guid>
<content:encoded><![CDATA[
arXiv:2303.14537v5 Announce Type: replace 
Abstract: Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design Fundamentals of Diffusion Models: A Survey</title>
<link>https://arxiv.org/abs/2306.04542</link>
<guid>https://arxiv.org/abs/2306.04542</guid>
<content:encoded><![CDATA[
arXiv:2306.04542v4 Announce Type: replace 
Abstract: Diffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark</title>
<link>https://arxiv.org/abs/2306.17100</link>
<guid>https://arxiv.org/abs/2306.17100</guid>
<content:encoded><![CDATA[
arXiv:2306.17100v5 Announce Type: replace 
Abstract: Combinatorial optimization (CO) is fundamental to several real-world applications, from logistics and scheduling to hardware design and resource allocation. Deep reinforcement learning (RL) has recently shown significant benefits in solving CO problems, reducing reliance on domain expertise and improving computational efficiency. However, the absence of a unified benchmarking framework leads to inconsistent evaluations, limits reproducibility, and increases engineering overhead, raising barriers to adoption for new researchers. To address these challenges, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 27 CO problem environments and 23 state-of-the-art baselines. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configurations of diverse environments, policy architectures, RL algorithms, and utilities with extensive documentation. RL4CO helps researchers build on existing successes while exploring and developing their own designs, facilitating the entire research process by decoupling science from heavy engineering. We finally provide extensive benchmark studies to inspire new insights and future work. RL4CO has already attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</title>
<link>https://arxiv.org/abs/2310.00098</link>
<guid>https://arxiv.org/abs/2310.00098</guid>
<content:encoded><![CDATA[
arXiv:2310.00098v2 Announce Type: replace 
Abstract: While federated learning (FL) and differential privacy (DP) have been extensively studied, their application to automatic speech recognition (ASR) remains largely unexplored due to the challenges in training large transformer models. Specifically, large models further exacerbate issues in FL as they are particularly susceptible to gradient heterogeneity across layers, unlike the relatively uniform gradient behavior observed in shallow models. As a result, prior works struggle to converge with standard optimization techniques, even in the absence of DP mechanisms. To the best of our knowledge, no existing work establishes a competitive, practical recipe for FL with DP in the context of ASR. To address this gap, we establish \textbf{the first benchmark for FL with DP in end-to-end ASR}. Our approach centers on per-layer clipping and layer-wise gradient normalization: theoretical analysis reveals that these techniques together mitigate clipping bias and gradient heterogeneity across layers in deeper models. Consistent with these theoretical insights, our empirical results show that FL with DP is viable under strong privacy guarantees, provided a population of at least several million users. Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5, $10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate when extrapolating to high (resp. low) population scales for FL with DP in ASR. Although our experiments focus on ASR, the underlying principles we uncover - particularly those concerning gradient heterogeneity and layer-wise gradient normalization - offer broader guidance for designing scalable, privacy-preserving FL algorithms for large models across domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCNETS: A Modular Causal Learning Framework for Pattern Recognition in Imbalanced Datasets</title>
<link>https://arxiv.org/abs/2401.04139</link>
<guid>https://arxiv.org/abs/2401.04139</guid>
<content:encoded><![CDATA[
arXiv:2401.04139v3 Announce Type: replace 
Abstract: Handling class imbalance remains a central challenge in machine learning, particularly in pattern recognition tasks where rare but critical events-such as fraudulent transactions or medical anomalies-must be identified accurately. Traditional generative models offer a potential remedy through data augmentation but often treat generation and classification as independent processes, leading to distribution mismatch and limited classifier benefit. To address these shortcomings, we propose Causal Cooperative Networks (CCNETS), a modular learning framework that integrates generation, inference, and reconstruction within a unified causal paradigm. CCNETS comprises three cooperative modules: an Explainer for latent feature abstraction, a Reasoner for label prediction, and a Producer for context-aware data generation. These components interact through a causal feedback loop, where classification results guide targeted sample synthesis. A key innovation, the Zoint mechanism, enables adaptive fusion of latent and observable features, enhancing semantic richness and enabling robust decision-making under uncertainty. We evaluate CCNETS on a real-world credit card fraud detection dataset with extreme imbalance (fraud cases < 0.2%). Across three experimental setups-including synthetic training, amplified generation, and direct classifier comparison-CCNETS outperforms baseline methods, achieving higher F1 scores, precision, and recall. Models trained on CCNETS-generated data also demonstrate superior generalization under limited data conditions. These results establish CCNETS as a scalable, interpretable, and hybrid soft computing framework. By causally aligning synthetic data with classifier objectives, CCNETS advances imbalanced pattern recognition and opens new directions for robust, modular learning in real-world applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross Entropy versus Label Smoothing: A Neural Collapse Perspective</title>
<link>https://arxiv.org/abs/2402.03979</link>
<guid>https://arxiv.org/abs/2402.03979</guid>
<content:encoded><![CDATA[
arXiv:2402.03979v3 Announce Type: replace 
Abstract: Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications</title>
<link>https://arxiv.org/abs/2403.00485</link>
<guid>https://arxiv.org/abs/2403.00485</guid>
<content:encoded><![CDATA[
arXiv:2403.00485v3 Announce Type: replace 
Abstract: Geometric graphs are a special kind of graph with geometric features, which are vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To address this issue, researchers proposed a variety of geometric GNNs equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we formalize geometric graph as the data structure, on top of which we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later research for methodology development and experimental evaluation. We also discuss the challenges and future potential directions of geometric GNNs at the end of this survey.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Extrapolation Expedites Alignment</title>
<link>https://arxiv.org/abs/2404.16792</link>
<guid>https://arxiv.org/abs/2404.16792</guid>
<content:encoded><![CDATA[
arXiv:2404.16792v5 Announce Type: replace 
Abstract: Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning</title>
<link>https://arxiv.org/abs/2405.07543</link>
<guid>https://arxiv.org/abs/2405.07543</guid>
<content:encoded><![CDATA[
arXiv:2405.07543v2 Announce Type: replace 
Abstract: Personalization is crucial for the widespread adoption of advanced driver assistance system. To match up with each user's preference, the online evolution capability is a must. However, conventional evolution methods learn from naturalistic driving data, which requires a lot computing power and cannot be applied online. To address this challenge, this paper proposes a lesson learning approach: learning from driver's takeover interventions. By leveraging online takeover data, the driving zone is generated to ensure perceived safety using Gaussian discriminant analysis. Real-time corrections to trajectory planning rewards are enacted through apprenticeship learning. Guided by the objective of optimizing rewards within the constraints of the driving zone, this approach employs model predictive control for trajectory planning. This lesson learning framework is highlighted for its faster evolution capability, adeptness at experience accumulating, assurance of perceived safety, and computational efficiency. Simulation results demonstrate that the proposed system consistently achieves a successful customization without further takeover interventions. Accumulated experience yields a 24% enhancement in evolution efficiency. The average number of learning iterations is only 13.8. The average computation time is 0.08 seconds.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispaRisk: Auditing Fairness Through Usable Information</title>
<link>https://arxiv.org/abs/2405.12372</link>
<guid>https://arxiv.org/abs/2405.12372</guid>
<content:encoded><![CDATA[
arXiv:2405.12372v3 Announce Type: replace 
Abstract: Machine Learning algorithms (ML) impact virtually every aspect of human lives and have found use across diverse sectors including healthcare, finance, and education. Often, ML algorithms have been found to exacerbate societal biases present in datasets leading to adversarial impacts on subsets/groups of individuals and in many cases on minority groups. To effectively mitigate these untoward effects, it is crucial that disparities/biases are identified early in a ML pipeline. This proactive approach facilitates timely interventions to prevent bias amplification and reduce complexity at later stages of model development. In this paper, we leverage recent advancements in usable information theory to introduce DispaRisk, a novel framework designed to proactively assess the potential risks of disparities in datasets during the initial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by benchmarking it against commonly used datasets in fairness research. Our findings demonstrate DispaRisk's capabilities to identify datasets with a high risk of discrimination, detect model families prone to biases within an ML pipeline, and enhance the explainability of these bias risks. This work contributes to the development of fairer ML systems by providing a robust tool for early bias detection and mitigation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Bayesian Filter for Bayes-faithful Data Assimilation</title>
<link>https://arxiv.org/abs/2405.18674</link>
<guid>https://arxiv.org/abs/2405.18674</guid>
<content:encoded><![CDATA[
arXiv:2405.18674v3 Announce Type: replace 
Abstract: State estimation for nonlinear state space models (SSMs) is a challenging task. Existing assimilation methodologies predominantly assume Gaussian posteriors on physical space, where true posteriors become inevitably non-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on nonlinear SSMs. DBF constructs new latent variables $h_t$ in addition to the original physical variables $z_t$ and assimilates observations $o_t$. By (i) constraining the state transition on the new latent space to be linear and (ii) learning a Gaussian inverse observation operator $r(h_t|o_t)$, posteriors remain Gaussian. Notably, the structured design of test distributions enables an analytical formula for the recursive computation, eliminating the accumulation of Monte Carlo sampling errors across time steps. DBF trains the Gaussian inverse observation operators $r(h_t|o_t)$ and other latent SSM parameters (e.g., dynamics matrix) by maximizing the evidence lower bound. Experiments demonstrate that DBF outperforms model-based approaches and latent assimilation methods in tasks where the true posterior distribution on physical space is significantly non-Gaussian.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients</title>
<link>https://arxiv.org/abs/2406.06751</link>
<guid>https://arxiv.org/abs/2406.06751</guid>
<content:encoded><![CDATA[
arXiv:2406.06751v2 Announce Type: replace 
Abstract: We propose a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery. Our work is aligned with the popular DSR framework which focuses on learning a data-specific expression generator, without relying on pretrained models or additional search or planning procedures. Despite the success of existing DSR methods, they are built on recurrent neural networks, solely guided by data fitness, and potentially meet tail barriers that can zero out the policy gradient, causing inefficient model updates. To overcome these limitations, we design a decoder-only architecture that performs attention in the frequency domain and introduce a dual-indexed position encoding to conduct layer-wise generation. Second, we propose a Bayesian information criterion (BIC)-based reward function that can automatically adjust the trade-off between expression complexity and data fitness, without the need for explicit manual tuning. Third, we develop a ranking-based weighted policy update method that eliminates the tail barriers and enhances training effectiveness. Extensive benchmarks and systematic experiments demonstrate the advantages of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening</title>
<link>https://arxiv.org/abs/2406.09291</link>
<guid>https://arxiv.org/abs/2406.09291</guid>
<content:encoded><![CDATA[
arXiv:2406.09291v4 Announce Type: replace 
Abstract: Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing.
  Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.18354</link>
<guid>https://arxiv.org/abs/2406.18354</guid>
<content:encoded><![CDATA[
arXiv:2406.18354v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) excel in learning from network-like data but often lack interpretability, making their application challenging in domains requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold Network (GKAN), a novel GNN model leveraging spline-based activation functions on edges to enhance both accuracy and interpretability. Our experiments on five benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks. In addition to the improved accuracy, GKAN's design inherently provides clear insights into the model's decision-making process, eliminating the need for post-hoc explainability techniques. This paper discusses the methodology, performance, and interpretability of GKAN, highlighting its potential for applications in domains where interpretability is crucial.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Distances from Data with Normalizing Flows and Score Matching</title>
<link>https://arxiv.org/abs/2407.09297</link>
<guid>https://arxiv.org/abs/2407.09297</guid>
<content:encoded><![CDATA[
arXiv:2407.09297v2 Announce Type: replace 
Abstract: Density-based distances (DBDs) provide a principled approach to metric learning by defining distances in terms of the underlying data distribution. By employing a Riemannian metric that increases in regions of low probability density, shortest paths naturally follow the data manifold. Fermat distances, a specific type of DBD, have attractive properties, but existing estimators based on nearest neighbor graphs suffer from poor convergence due to inaccurate density estimates. Moreover, graph-based methods scale poorly to high dimensions, as the proposed geodesics are often insufficiently smooth. We address these challenges in two key ways. First, we learn densities using normalizing flows. Second, we refine geodesics through relaxation, guided by a learned score model. Additionally, we introduce a dimension-adapted Fermat distance that scales intuitively to high dimensions and improves numerical stability. Our work paves the way for the practical use of density-based distances, especially in high-dimensional spaces.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Unlearning with Single Layer Unlearning Gradient</title>
<link>https://arxiv.org/abs/2407.11867</link>
<guid>https://arxiv.org/abs/2407.11867</guid>
<content:encoded><![CDATA[
arXiv:2407.11867v3 Announce Type: replace 
Abstract: Machine unlearning methods aim to remove sensitive or unwanted content from trained models, but typically demand extensive model updates at significant computational cost while potentially degrading model performance on both related and unrelated tasks. We propose Single Layer Unlearning Gradient (SLUG) as an efficient method to unlearn targeted information by updating a single critical layer using a one-time gradient computation. SLUG uses layer importance and gradient alignment metrics to identify the optimal layer for targeted information removal while preserving the model utility. We demonstrate the effectiveness of SLUG for CLIP, Stable Diffusion, and vision-language models (VLMs) in removing concrete (e.g., identities and objects) and abstract concepts (e.g., artistic styles). On the UnlearnCanvas benchmark, SLUG achieves comparable unlearning performance to existing methods while requiring significantly less computational resources. Our proposed approach offers a practical solution for targeted unlearning that is computationally efficient and precise. Our code is available at https://github.com/CSIPlab/SLUG.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning in a recurrent neural network that plays Sokoban</title>
<link>https://arxiv.org/abs/2407.15421</link>
<guid>https://arxiv.org/abs/2407.15421</guid>
<content:encoded><![CDATA[
arXiv:2407.15421v3 Announce Type: replace 
Abstract: Planning is essential for solving complex tasks, yet the internal mechanisms underlying planning in neural networks remain poorly understood. Building on prior work, we analyze a recurrent neural network (RNN) trained on Sokoban, a challenging puzzle requiring sequential, irreversible decisions. We find that the RNN has a causal plan representation which predicts its future actions about 50 steps in advance. The quality and length of the represented plan increases over the first few steps. We uncover a surprising behavior: the RNN "paces" in cycles to give itself extra computation at the start of a level, and show that this behavior is incentivized by training. Leveraging these insights, we extend the trained RNN to significantly larger, out-of-distribution Sokoban puzzles, demonstrating robust representations beyond the training regime. We open-source our model and code, and believe the neural network's interesting behavior makes it an excellent model organism to deepen our understanding of learned planning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks</title>
<link>https://arxiv.org/abs/2407.17963</link>
<guid>https://arxiv.org/abs/2407.17963</guid>
<content:encoded><![CDATA[
arXiv:2407.17963v2 Announce Type: replace 
Abstract: Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Friends in Unexpected Places: Enhancing Local Fairness in Federated Learning through Clustering</title>
<link>https://arxiv.org/abs/2407.19331</link>
<guid>https://arxiv.org/abs/2407.19331</guid>
<content:encoded><![CDATA[
arXiv:2407.19331v3 Announce Type: replace 
Abstract: Federated Learning (FL) has been a pivotal paradigm for collaborative training of machine learning models across distributed datasets. In heterogeneous settings, it has been observed that a single shared FL model can lead to low local accuracy, motivating personalized FL algorithms. In parallel, fair FL algorithms have been proposed to enforce group fairness on the global models. Again, in heterogeneous settings, global and local fairness do not necessarily align, motivating the recent literature on locally fair FL. In this paper, we propose new FL algorithms for heterogeneous settings, spanning the space between personalized and locally fair FL. Building on existing clustering-based personalized FL methods, we incorporate a new fairness metric into cluster assignment, enabling a tunable balance between local accuracy and fairness. Our methods match or exceed the performance of existing locally fair FL approaches, without explicit fairness intervention. We further demonstrate (numerically and analytically) that personalization alone can improve local fairness and that our methods exploit this alignment when present.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECG-FM: An Open Electrocardiogram Foundation Model</title>
<link>https://arxiv.org/abs/2408.05178</link>
<guid>https://arxiv.org/abs/2408.05178</guid>
<content:encoded><![CDATA[
arXiv:2408.05178v2 Announce Type: replace 
Abstract: Conventional task-specific electrocardiogram (ECG) analysis models require large annotated datasets to train. Foundation models mitigate this burden by leveraging self-supervised pretraining; however, the scarcity of open-weight ECG foundation models hinders adoption and cross-study comparability. We present ECG-FM, an open foundation model for ECG analysis, and conduct a study using a dataset of 1.5 million ECGs. ECG-FM is a transformer-based model pretrained using a hybrid contrastive and generative self-supervised learning approach. Our downstream tasks include predicting reduced left ventricular ejection fraction (LVEF) and ECG interpretation labels, where we release a benchmark task on the MIMIC-IV-ECG dataset. We affirm that ECG-FM is robust, label-efficient, and functionally discriminative by showcasing data scaling experiments, performing a latent space analysis, and generating saliency maps. ECG-FM markedly outperforms task-specific models in the small-to-medium-scale data regime and demonstrates cross-dataset generalizability, achieving high AUROC on many clinically salient labels such as atrial fibrillation (0.996) and LVEF<=40% (0.929). We release our code, model weights, and benchmark task at https://github.com/bowang-lab/ECG-FM/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction</title>
<link>https://arxiv.org/abs/2408.08713</link>
<guid>https://arxiv.org/abs/2408.08713</guid>
<content:encoded><![CDATA[
arXiv:2408.08713v5 Announce Type: replace 
Abstract: Modeling high-order feature interactions is crucial for click-through rate (CTR) prediction, and traditional approaches often predefine a maximum interaction order and rely on exhaustive enumeration of feature combinations up to this predefined order. This framework heavily relies on prior domain knowledge to define interaction scope and entails high computational costs from enumeration. Conventional CTR models face a trade-off between improving representation through complex high-order feature interactions and reducing computational inefficiencies associated with these processes. To address this dual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse Efficient Interaction Network (KarSein). Drawing inspiration from the learnable activation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages this mechanism to adaptively transform low-order basic features into high-order feature interactions, offering a novel approach to feature interaction modeling. KarSein extends the capabilities of KAN by introducing a more efficient architecture that significantly reduces computational costs while accommodating two-dimensional embedding vectors as feature inputs. Furthermore, it overcomes the limitation of KAN's its inability to spontaneously capture multiplicative relationships among features.
  Extensive experiments highlight the superiority of KarSein, demonstrating its ability to surpass not only the vanilla implementation of KAN in CTR predictio but also other baseline methods. Remarkably, KarSein achieves exceptional predictive accuracy while maintaining a highly compact parameter size and minimal computational overhead. As the first attempt to apply KAN in the CTR domain, this work introduces KarSein as a novel solution for modeling complex feature interactions, underscoring its transformative potential in advancing CTR prediction task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs</title>
<link>https://arxiv.org/abs/2408.08868</link>
<guid>https://arxiv.org/abs/2408.08868</guid>
<content:encoded><![CDATA[
arXiv:2408.08868v3 Announce Type: replace 
Abstract: The state-of-the-art for training on-device language models for mobile keyboard applications combines federated learning (FL) with differential privacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two variants of DP-FTRL are used in practice, tree aggregation and matrix factorization. However, tree aggregation suffers from significantly suboptimal privacy/utility tradeoffs, while matrix mechanisms require expensive optimization parameterized by hard-to-estimate-in-advance constants, and high runtime memory costs. This paper extends the recently introduced Buffered Linear Toeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRL maintains the ease-of-use advantages of tree aggregation, while essentially matching matrix factorization in terms of utility and privacy. We evaluate BLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulation benchmark, and across four on-device language model tasks in a production FL system. Our empirical results highlight the advantages of the BLT mechanism and elevate the practicality and effectiveness of DP in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.09429</link>
<guid>https://arxiv.org/abs/2408.09429</guid>
<content:encoded><![CDATA[
arXiv:2408.09429v3 Announce Type: replace 
Abstract: Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Causal Discovery without Acyclicity Constraints</title>
<link>https://arxiv.org/abs/2408.13448</link>
<guid>https://arxiv.org/abs/2408.13448</guid>
<content:encoded><![CDATA[
arXiv:2408.13448v4 Announce Type: replace 
Abstract: Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs</title>
<link>https://arxiv.org/abs/2408.13467</link>
<guid>https://arxiv.org/abs/2408.13467</guid>
<content:encoded><![CDATA[
arXiv:2408.13467v3 Announce Type: replace 
Abstract: The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is automatically improved through additional fine-tuning using extra similar data generated by the service LLM. This multi-turn process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness</title>
<link>https://arxiv.org/abs/2409.01249</link>
<guid>https://arxiv.org/abs/2409.01249</guid>
<content:encoded><![CDATA[
arXiv:2409.01249v2 Announce Type: replace 
Abstract: Recent work has proposed neural network pruning techniques to reduce the size of a network while preserving robustness against adversarial examples, i.e., well-crafted inputs inducing a misclassification. These methods, which we refer to as adversarial pruning methods, involve complex and articulated designs, making it difficult to analyze the differences and establish a fair and accurate comparison. In this work, we overcome these issues by surveying current adversarial pruning methods and proposing a novel taxonomy to categorize them based on two main dimensions: the pipeline, defining when to prune; and the specifics, defining how to prune. We then highlight the limitations of current empirical analyses and propose a novel, fair evaluation benchmark to address them. We finally conduct an empirical re-evaluation of current adversarial pruning methods and discuss the results, highlighting the shared traits of top-performing adversarial pruning methods, as well as common issues. We welcome contributions in our publicly-available benchmark at https://github.com/pralab/AdversarialPruningBenchmark
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified theoretical guarantees for stability, consistency, and convergence in neural PDE solvers from non-IID data to physics-informed networks</title>
<link>https://arxiv.org/abs/2409.05030</link>
<guid>https://arxiv.org/abs/2409.05030</guid>
<content:encoded><![CDATA[
arXiv:2409.05030v3 Announce Type: replace 
Abstract: We establish a unified theoretical framework addressing the stability, consistency, and convergence of neural networks under realistic training conditions, specifically, in the presence of non-IID data, geometric constraints, and embedded physical laws. For standard supervised learning with dependent data, we derive uniform stability bounds for gradient-based methods using mixing coefficients and dynamic learning rates. In federated learning with heterogeneous data and non-Euclidean parameter spaces, we quantify model inconsistency via curvature-aware aggregation and information-theoretic divergence. For Physics-Informed Neural Networks (PINNs), we rigorously prove perturbation stability, residual consistency, Sobolev convergence, energy stability for conservation laws, and convergence under adaptive multi-domain refinements. Each result is grounded in variational analysis, compactness arguments, and universal approximation theorems in Sobolev spaces. Our theoretical guarantees are validated across parabolic, elliptic, and hyperbolic PDEs, confirming that residual minimization aligns with physical solution accuracy. This work offers a mathematically principled basis for designing robust, generalizable, and physically coherent neural architectures across diverse learning environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying biological perturbation targets through causal differential networks</title>
<link>https://arxiv.org/abs/2410.03380</link>
<guid>https://arxiv.org/abs/2410.03380</guid>
<content:encoded><![CDATA[
arXiv:2410.03380v4 Announce Type: replace 
Abstract: Identifying variables responsible for changes to a biological system enables applications in drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation</title>
<link>https://arxiv.org/abs/2410.03782</link>
<guid>https://arxiv.org/abs/2410.03782</guid>
<content:encoded><![CDATA[
arXiv:2410.03782v4 Announce Type: replace 
Abstract: Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue that their static nature limits downstream performance while achieving efficiency. In this work, we propose DaWin, a training-free dynamic weight interpolation method that leverages the entropy of individual models over each unlabeled test sample to assess model expertise, and compute per-sample interpolation coefficients dynamically. Unlike previous works that typically rely on additional training to learn such coefficients, our approach requires no training. Then, we propose a mixture modeling approach that greatly reduces inference overhead raised by dynamic interpolation. We validate DaWin on the large-scale visual recognition benchmarks, spanning 14 tasks across robust fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and multi-task learning with eight classification tasks. Results demonstrate that DaWin achieves significant performance gain in considered settings, with minimal computational overhead. We further discuss DaWin's analytic behavior to explain its empirical success.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalization Result for Convergence in Learning-to-Optimize</title>
<link>https://arxiv.org/abs/2410.07704</link>
<guid>https://arxiv.org/abs/2410.07704</guid>
<content:encoded><![CDATA[
arXiv:2410.07704v2 Announce Type: replace 
Abstract: Learning-to-optimize leverages machine learning to accelerate optimization algorithms. While empirical results show tremendous improvements compared to classical optimization algorithms, theoretical guarantees are mostly lacking, such that the outcome cannot be reliably assured. Especially, convergence is hardly studied in learning-to-optimize, because conventional convergence guarantees in optimization are based on geometric arguments, which cannot be applied easily to learned algorithms. Thus, we develop a probabilistic framework that resembles classical optimization and allows for transferring geometric arguments into learning-to-optimize. Based on our new proof-strategy, our main theorem is a generalization result for parametric classes of potentially non-smooth, non-convex loss functions and establishes the convergence of learned optimization algorithms to critical points with high probability. This effectively generalizes the results of a worst-case analysis into a probabilistic framework, and frees the design of the learned algorithm from using safeguards.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</title>
<link>https://arxiv.org/abs/2410.16222</link>
<guid>https://arxiv.org/abs/2410.16222</guid>
<content:encoded><![CDATA[
arXiv:2410.16222v2 Announce Type: replace 
Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, nonparametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After an extensive comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent bigrams, either selecting the ones absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title>
<link>https://arxiv.org/abs/2410.22307</link>
<guid>https://arxiv.org/abs/2410.22307</guid>
<content:encoded><![CDATA[
arXiv:2410.22307v2 Announce Type: replace 
Abstract: The ever-increasing size of open-source Large Language Models (LLMs) renders local deployment impractical for individual users. Decentralized computing has emerged as a cost-effective solution, allowing individuals and small companies to perform LLM inference for users using surplus computational power. However, a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby benefiting from cost savings. We introduce SVIP, a secret-based verifiable LLM inference protocol. Unlike existing solutions based on cryptographic or game-theoretic techniques, our method is computationally effective and does not rest on strong assumptions. Our protocol requires the computing provider to return both the generated text and processed hidden representations from LLMs. We then train a proxy task on these representations, effectively transforming them into a unique model identifier. With our protocol, users can reliably verify whether the computing provider is acting honestly. A carefully integrated secret mechanism further strengthens its security. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per prompt query for verification.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2410.22366</link>
<guid>https://arxiv.org/abs/2410.22366</guid>
<content:encoded><![CDATA[
arXiv:2410.22366v3 Announce Type: replace 
Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients</title>
<link>https://arxiv.org/abs/2410.22815</link>
<guid>https://arxiv.org/abs/2410.22815</guid>
<content:encoded><![CDATA[
arXiv:2410.22815v2 Announce Type: replace 
Abstract: Federated fine-tuning for Large Language Models (LLMs) faces significant challenges due to the heavy communication overhead of transmitting large model updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a significant reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism increases robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization</title>
<link>https://arxiv.org/abs/2411.02355</link>
<guid>https://arxiv.org/abs/2411.02355</guid>
<content:encoded><![CDATA[
arXiv:2411.02355v3 Announce Type: replace 
Abstract: Quantization is a powerful tool for accelerating large language model (LLM) inference, but the accuracy-performance trade-offs across different formats remain unclear. In this paper, we conduct the most comprehensive empirical study to date, evaluating FP8, INT8, and INT4 quantization across academic benchmarks and real-world tasks on the entire Llama-3.1 model family. Through over 500,000 evaluations, our investigation yields several key findings: (1) FP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned INT8 (W8A8-INT) achieves surprisingly low (1-3\%) accuracy degradation, and (3) INT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit quantization. Further, we investigate the optimal quantization format for different deployments by analyzing inference performance through the popular vLLM framework. Our analysis provides clear deployment recommendations: W4A16 is the most cost-efficient for synchronous setups, while W8A8 dominates in asynchronous continuous batching. For mixed workloads, the optimal choice depends on the specific use case. Our findings offer practical, data-driven guidelines for deploying quantized LLMs at scale -- ensuring the best balance between speed, efficiency, and accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Convergence of Softmax Policy Mirror Ascent</title>
<link>https://arxiv.org/abs/2411.12042</link>
<guid>https://arxiv.org/abs/2411.12042</guid>
<content:encoded><![CDATA[
arXiv:2411.12042v2 Announce Type: replace 
Abstract: Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani et al. [2021] introduced a policy gradient method that corresponds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Unlike that for NPG, generalizing SPMA to the linear function approximation (FA) setting does not require compatible function approximation. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems. We prove that SPMA achieves linear convergence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical performance on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA consistently achieves similar or better performance compared to MDPO, PPO and TRPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting</title>
<link>https://arxiv.org/abs/2501.16178</link>
<guid>https://arxiv.org/abs/2501.16178</guid>
<content:encoded><![CDATA[
arXiv:2501.16178v2 Announce Type: replace 
Abstract: In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling. However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models. To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF). Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series. (ii) Achieving cross-band information fusion with a learnable filter. (iii) Using only one shared linear layer or one shallow MLP for sub-series' mapping. We conduct comprehensive experiments, and the results show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task. Moreover, it is noteworthy that the number of parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a single-layer linear model for time-domain prediction. Our code is available at https://github.com/LancelotXWX/SWIFT.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function</title>
<link>https://arxiv.org/abs/2501.19374</link>
<guid>https://arxiv.org/abs/2501.19374</guid>
<content:encoded><![CDATA[
arXiv:2501.19374v2 Announce Type: replace 
Abstract: Recent advancements in data-driven weather forecasting models have delivered deterministic models that outperform the leading operational forecast systems based on traditional, physics-based models. However, these data-driven models are typically trained with a mean squared error loss function, which causes smoothing of fine scales through a "double penalty" effect. We develop a simple, parameter-free modification to this loss function that avoids this problem by separating the loss attributable to decorrelation from the loss attributable to spectral amplitude errors. Fine-tuning the GraphCast model with this new loss function results in sharp deterministic weather forecasts, an increase of the model's effective resolution from 1,250km to 160km, improvements to ensemble spread, and improvements to predictions of tropical cyclone strength and surface wind extremes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sundial: A Family of Highly Capable Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2502.00816</link>
<guid>https://arxiv.org/abs/2502.00816</guid>
<content:encoded><![CDATA[
arXiv:2502.00816v2 Announce Type: replace 
Abstract: We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</title>
<link>https://arxiv.org/abs/2502.03773</link>
<guid>https://arxiv.org/abs/2502.03773</guid>
<content:encoded><![CDATA[
arXiv:2502.03773v4 Announce Type: replace 
Abstract: In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand \cite{bordt2022post}. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests. Our code is publicly available at https://github.com/emlaufer/ExpProof.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Reasoning with Guidelines</title>
<link>https://arxiv.org/abs/2502.04040</link>
<guid>https://arxiv.org/abs/2502.04040</guid>
<content:encoded><![CDATA[
arXiv:2502.04040v2 Announce Type: replace 
Abstract: Training safe LLMs remains a critical challenge. The most widely used method, Refusal Training (RT), struggles to generalize against various Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced methods have been proposed to address this issue, we instead question whether OOD attacks inherently surpass the capability of vanilla RT. Evaluations using Best-of-N (BoN) reveal significant safety improvements as N increases, indicating models possess adequate latent safety knowledge but RT fails to consistently elicit it under OOD scenarios. Further domain adaptation analysis reveals that direct RT causes reliance on superficial shortcuts, resulting in non-generalizable representation mappings. Inspired by our findings, we propose training model to perform safety reasoning for each query. Specifically, we synthesize reasoning supervision aligned with specified guidelines that reflect diverse perspectives on safety knowledge. This encourages model to engage in deeper reasoning, explicitly eliciting and utilizing latent safety knowledge for each query. Extensive experiments show that our method significantly improves model generalization against OOD attacks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaferLLM: Large Language Model Inference at Wafer Scale</title>
<link>https://arxiv.org/abs/2502.04563</link>
<guid>https://arxiv.org/abs/2502.04563</guid>
<content:encoded><![CDATA[
arXiv:2502.04563v3 Announce Type: replace 
Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.
  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.
  Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and 16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces</title>
<link>https://arxiv.org/abs/2502.04959</link>
<guid>https://arxiv.org/abs/2502.04959</guid>
<content:encoded><![CDATA[
arXiv:2502.04959v2 Announce Type: replace 
Abstract: Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance on vision and language tasks across various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGenNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling</title>
<link>https://arxiv.org/abs/2502.06250</link>
<guid>https://arxiv.org/abs/2502.06250</guid>
<content:encoded><![CDATA[
arXiv:2502.06250v2 Announce Type: replace 
Abstract: Solving parametric partial differential equations (PDEs) and associated PDE-based, inverse problems is a central task in engineering and physics, yet existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large amounts of {\em labeled} training data. We propose the Deep Generative Neural Operator (DGenNO), a physics-aware framework that addresses these challenges by leveraging a deep, generative, probabilistic model in combination with a set of lower-dimensional, latent variables that simultaneously encode PDE-inputs and PDE-outputs. This formulation can make use of unlabeled data and significantly improves inverse problem-solving, particularly for discontinuous or discrete-valued input functions. DGenNO enforces physics constraints without labeled data by incorporating as virtual observables, weak-form residuals based on compactly supported radial basis functions (CSRBFs). These relax regularity constraints and eliminate higher-order derivatives from the objective function. We also introduce MultiONet, a novel neural operator architecture, which is a more expressive generalization of the popular DeepONet that significantly enhances the approximating power of the proposed model. These innovations make DGenNO particularly effective for challenging forward and inverse, PDE-based problems, such as those involving multi-phase media. Numerical experiments demonstrate that DGenNO achieves higher accuracy across multiple benchmarks while exhibiting robustness to noise and strong generalization to out-of-distribution cases. Its adaptability, and the ability to handle sparse, noisy data while providing probabilistic estimates, make DGenNO a powerful tool for scientific and engineering applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation</title>
<link>https://arxiv.org/abs/2502.06516</link>
<guid>https://arxiv.org/abs/2502.06516</guid>
<content:encoded><![CDATA[
arXiv:2502.06516v2 Announce Type: replace 
Abstract: Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations. Code is available at https://github.com/soobin-um/BnS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Samples to Question Trained Models</title>
<link>https://arxiv.org/abs/2502.06658</link>
<guid>https://arxiv.org/abs/2502.06658</guid>
<content:encoded><![CDATA[
arXiv:2502.06658v2 Announce Type: replace 
Abstract: There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Advantage-Aligned Online Reinforcement Learning with Offline Data</title>
<link>https://arxiv.org/abs/2502.07937</link>
<guid>https://arxiv.org/abs/2502.07937</guid>
<content:encoded><![CDATA[
arXiv:2502.07937v2 Announce Type: replace 
Abstract: Online reinforcement learning (RL) enhances policies through direct interactions with the environment, but faces challenges related to sample efficiency. In contrast, offline RL leverages extensive pre-collected data to learn policies, but often produces suboptimal results due to limited data coverage. Recent efforts integrate offline and online RL in order to harness the advantages of both approaches. However, effectively combining online and offline RL remains challenging due to issues that include catastrophic forgetting, lack of robustness to data quality and limited sample efficiency in data utilization. In an effort to address these challenges, we introduce A3RL, which incorporates a novel confidence aware Active Advantage Aligned (A3) sampling strategy that dynamically prioritizes data aligned with the policy's evolving needs from both online and offline sources, optimizing policy improvement. Moreover, we provide theoretical insights into the effectiveness of our active sampling strategy and conduct diverse empirical experiments and ablation studies, demonstrating that our method outperforms competing online RL techniques that leverage offline data. Our code will be publicly available at:https://github.com/xuefeng-cs/A3RL.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Force Field: Few-shot Learning of Generalized Physical Reasoning</title>
<link>https://arxiv.org/abs/2502.08987</link>
<guid>https://arxiv.org/abs/2502.08987</guid>
<content:encoded><![CDATA[
arXiv:2502.08987v3 Announce Type: replace 
Abstract: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE) to learn complex object interactions through force field representations, which can be efficiently integrated through an Ordinary Differential Equation (ODE) solver to predict object trajectories. Unlike existing approaches that rely on discrete latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in continuous explicit force fields. Experiments on three challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence</title>
<link>https://arxiv.org/abs/2502.09263</link>
<guid>https://arxiv.org/abs/2502.09263</guid>
<content:encoded><![CDATA[
arXiv:2502.09263v2 Announce Type: replace 
Abstract: Message-passing Graph Neural Networks (GNNs) are often criticized for their limited expressiveness, issues like over-smoothing and over-squashing, and challenges in capturing long-range dependencies. Conversely, Graph Transformers (GTs) are regarded as superior due to their employment of global attention mechanisms, which potentially mitigate these challenges. Literature frequently suggests that GTs outperform GNNs in graph-level tasks, especially for graph classification and regression on small molecular graphs. In this study, we explore the untapped potential of GNNs through an enhanced framework, GNN+, which integrates six widely used techniques: edge feature integration, normalization, dropout, residual connections, feed-forward networks, and positional encoding, to effectively tackle graph-level tasks. We conduct a systematic re-evaluation of three classic GNNs (GCN, GIN, and GatedGCN) enhanced by the GNN+ framework across 14 well-known graph-level datasets. Our results reveal that, contrary to prevailing beliefs, these classic GNNs consistently match or surpass the performance of GTs, securing top-three rankings across all datasets and achieving first place in eight. Furthermore, they demonstrate greater efficiency, running several times faster than GTs on many datasets. This highlights the potential of simple GNN architectures, challenging the notion that complex mechanisms in GTs are essential for superior graph-level performance. Our source code is available at https://github.com/LUOyk1999/GNNPlus.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.11147</link>
<guid>https://arxiv.org/abs/2502.11147</guid>
<content:encoded><![CDATA[
arXiv:2502.11147v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires an LLM to generate long sequences, incurring $O(N)$ time and memory complexities per token, where $N$ is the current sequence length. To reduce complexities, existing sparsity-based algorithms propose to retain Key-Value (KV) vectors, the intermediate representations of only the most critical tokens. However, these algorithms struggle with the "impossible trinity" of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \ll N$). To address the "impossible trinity", in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm RaaS that identifies milestone tokens and retains their KV vectors until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyper-SET: Designing Transformers via Hyperspherical Energy Minimization</title>
<link>https://arxiv.org/abs/2502.11646</link>
<guid>https://arxiv.org/abs/2502.11646</guid>
<content:encoded><![CDATA[
arXiv:2502.11646v3 Announce Type: replace 
Abstract: Transformer-based models have achieved remarkable success, but their core components, Transformer layers, are largely heuristics-driven and engineered from the bottom up, calling for a prototypical model with high interpretability and practical competence. To this end, we conceptualize a principled, top-down approach grounded in energy-based interpretation. Specifically, we formalize token dynamics as a joint maximum likelihood estimation on the hypersphere, featuring two properties: semantic alignment in the high-dimensional space and distributional uniformity in the low-dimensional space. By quantifying them with extended Hopfield energy functions, we instantiate this idea as a constrained energy minimization problem, which enables designs of symmetric attention and feedforward modules with RMS normalization. We further present \textit{Hyper-Spherical Energy Transformer} (Hyper-SET), a recurrent-depth alternative to vanilla Transformers naturally emerging from iterative energy optimization on the hypersphere. With shared parameters across layers, Hyper-SET can scale to arbitrary depth with fewer parameters. Theoretically grounded and compact, it achieves competitive or superior performance across diverse tasks, including Sudoku solving, image classification, and masked image modeling. We also design novel variations under the proposed general principle, such as linear attention and gated feedforward layer. Moreover, we showcase its scalability with depth-wise LoRA. Our results highlight Hyper-SET as a step toward interpretable and principled Transformer design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
arXiv:2502.12115v4 Announce Type: replace 
Abstract: We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
arXiv:2502.12929v2 Announce Type: replace 
Abstract: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic framework developed for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in LLM solutions through compressed and interpretable task representations, resulting in improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art baselines. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Going beyond tabular classification and regression, we show the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our code is open-sourced at: https://github.com/flagshippioneering/Flow-of-Options.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PairBench: Are Vision-Language Models Reliable at Comparing What They See?</title>
<link>https://arxiv.org/abs/2502.15210</link>
<guid>https://arxiv.org/abs/2502.15210</guid>
<content:encoded><![CDATA[
arXiv:2502.15210v3 Announce Type: replace 
Abstract: Understanding how effectively large vision language models (VLMs) compare visual inputs is crucial across numerous applications, yet this fundamental capability remains insufficiently assessed. While VLMs are increasingly deployed for tasks requiring comparative judgment, including automated evaluation, re-ranking, and retrieval-augmented generation, no systematic framework exists to measure their performance in these scenarios. We present PairBench, a simple framework that evaluates VLMs as customizable similarity tools using widely available image datasets. Our approach introduces four key metrics for reliable comparison: alignment with human annotations, consistency across pair ordering, distribution smoothness, and controllability through prompting. Our analysis reveals that no model consistently excels across all metrics, with each demonstrating distinct strengths and weaknesses. Most concerning is the widespread inability of VLMs to maintain symmetric similarity scores. Interestingly, we demonstrate that performance on our benchmark strongly correlates with popular benchmarks used for more complex tasks, while providing additional metrics into controllability, smoothness and ordering. This makes PairBench a unique and comprehensive framework to evaluate the performance of VLMs for automatic evaluation depending on the task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models</title>
<link>https://arxiv.org/abs/2502.15678</link>
<guid>https://arxiv.org/abs/2502.15678</guid>
<content:encoded><![CDATA[
arXiv:2502.15678v2 Announce Type: replace 
Abstract: Pre-trained vision language models still fall short of human visual cognition. In an effort to improve visual cognition and align models with human behavior, we introduce visual stimuli and human judgments on visual cognition tasks, allowing us to systematically evaluate performance across cognitive domains under a consistent environment. We fine-tune models on ground truth data for intuitive physics and causal reasoning and find that this improves model performance in the respective fine-tuning domain. Furthermore, it can improve model alignment with human behavior. However, we find that task-specific fine-tuning does not contribute to robust human-like generalization to data with other visual characteristics or to tasks in other cognitive domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Conformal Guidance: A Framework for Multi-Domain Learning under Uncertainty</title>
<link>https://arxiv.org/abs/2502.16736</link>
<guid>https://arxiv.org/abs/2502.16736</guid>
<content:encoded><![CDATA[
arXiv:2502.16736v3 Announce Type: replace 
Abstract: Learning with guidance has proven effective across a wide range of machine learning domains. Guidance may, for example, come from annotated datasets in supervised learning, pseudo-labels in semi-supervised learning, and expert demonstration policies in reinforcement learning. However, guidance signals can be noisy due to domain shifts and limited data availability and may not generalize well. Blindly trusting such signals when they are noisy, incomplete, or misaligned with the target domain can lead to degraded performance. To address these challenges, we propose $\underline{Ada}$ptive $\underline{Con}$formal $\underline{G}$uidance (AdaConG), a universal, plug-and-play framework that dynamically modulates the influence of guidance signals based on their associated uncertainty, quantified via split conformal prediction (CP). By adaptively adjusting to guidance uncertainty, AdaConG enables models to reduce reliance on potentially misleading signals and enhance learning performance. We validate AdaConG across diverse domains and tasks, including knowledge distillation, semi-supervised image classification, gridworld navigation, and autonomous driving. Experimental results demonstrate that AdaConG improves performance and robustness under imperfect guidance, e.g., in gridworld navigation, it accelerates convergence and achieves over $6\times$ higher rewards than the best-performing baseline. These results highlight AdaConG as a simple yet effective solution for multi-domain learning under uncertainty.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Exploration in Monitored Markov Decision Processes</title>
<link>https://arxiv.org/abs/2502.16772</link>
<guid>https://arxiv.org/abs/2502.16772</guid>
<content:encoded><![CDATA[
arXiv:2502.16772v3 Announce Type: replace 
Abstract: A tenet of reinforcement learning is that the agent always observes rewards. However, this is not true in many realistic settings, e.g., a human observer may not always be available to provide rewards, sensors may be limited or malfunctioning, or rewards may be inaccessible during deployment. Monitored Markov decision processes (Mon-MDPs) have recently been proposed to model such settings. However, existing Mon-MDP algorithms have several limitations: they do not fully exploit the problem structure, cannot leverage a known monitor, lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific initialization, and offer only asymptotic convergence proofs. This paper makes three contributions. First, we introduce a model-based algorithm for Mon-MDPs that addresses these shortcomings. The algorithm employs two instances of model-based interval estimation: one to ensure that observable rewards are reliably captured, and another to learn the minimax-optimal policy. Second, we empirically demonstrate the advantages. We show faster convergence than prior algorithms in over four dozen benchmarks, and even more dramatic improvement when the monitoring process is known. Third, we present the first finite-sample bound on performance. We show convergence to a minimax-optimal policy even when some rewards are never observable.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Traceability in $\ell_p$ Stochastic Convex Optimization</title>
<link>https://arxiv.org/abs/2502.17384</link>
<guid>https://arxiv.org/abs/2502.17384</guid>
<content:encoded><![CDATA[
arXiv:2502.17384v2 Announce Type: replace 
Abstract: In this paper, we investigate the necessity of traceability for accurate learning in stochastic convex optimization (SCO) under $\ell_p$ geometries. Informally, we say a learning algorithm is $m$-traceable if, by analyzing its output, it is possible to identify at least $m$ of its training samples. Our main results uncover a fundamental tradeoff between traceability and excess risk in SCO. For every $p\in [1,\infty)$, we establish the existence of an excess risk threshold below which every sample-efficient learner is traceable with the number of samples which is a constant fraction of its training sample. For $p\in [1,2]$, this threshold coincides with the best excess risk of differentially private (DP) algorithms, i.e., above this threshold, there exist algorithms that are not traceable, which corresponds to a sharp phase transition. For $p \in (2,\infty)$, this threshold instead gives novel lower bounds for DP learning, partially closing an open problem in this setup. En route to establishing these results, we prove a sparse variant of the fingerprinting lemma, which is of independent interest to the community.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Knowledge Identification and Fusion for Language Model Continual Learning</title>
<link>https://arxiv.org/abs/2502.17510</link>
<guid>https://arxiv.org/abs/2502.17510</guid>
<content:encoded><![CDATA[
arXiv:2502.17510v2 Announce Type: replace 
Abstract: Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance</title>
<link>https://arxiv.org/abs/2502.18891</link>
<guid>https://arxiv.org/abs/2502.18891</guid>
<content:encoded><![CDATA[
arXiv:2502.18891v2 Announce Type: replace 
Abstract: In this study, we propose an innovative dynamic classification algorithm aimed at achieving zero missed detections and minimal false positives,acritical in safety-critical domains (e.g., medical diagnostics) where undetected cases risk severe outcomes. The algorithm partitions data in a self-supervised learning-generated way, which allows the model to learn from the training set to understand the data distribution and thereby divides training set and test set into N different subareas. The training and test subsets in the same subarea will have nearly the same boundary. For each subarea, there will be the same type of model, such as linear or random forest model, to predict the results of that subareas. In addition, the algorithm uses subareas boundary to refine predictions results and filter out substandard results without requiring additional models. This approach allows each model to operate within a smaller data range and remove the inaccurate prediction results, thereby improving overall accuracy. Experimental results show that, with minimal data partitioning errors, the algorithm achieves exceptional performance with zero missed detections and minimal false positives, outperforming existing ensembles like XGBoost or LGBM model. Even with larger classification errors, it remains comparable to that of state-of-the-art models.
  Key innovations include self-supervised classification learning, small-range subset predictions, and optimizing the prediction results and eliminate the unqualified ones without the need for additional model support. Although the algorithm still has room for improvement in automatic parameter tuning and efficiency, it demonstrates outstanding performance across multiple datasets. Future work will focus on optimizing the classification components to enhance robustness and adaptability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corporate Fraud Detection in Rich-yet-Noisy Financial Graph</title>
<link>https://arxiv.org/abs/2502.19305</link>
<guid>https://arxiv.org/abs/2502.19305</guid>
<content:encoded><![CDATA[
arXiv:2502.19305v2 Announce Type: replace 
Abstract: Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic</title>
<link>https://arxiv.org/abs/2502.19859</link>
<guid>https://arxiv.org/abs/2502.19859</guid>
<content:encoded><![CDATA[
arXiv:2502.19859v3 Announce Type: replace 
Abstract: This paper introduces the SOAR framework for imitation learning. SOAR is an algorithmic template that learns a policy from expert demonstrations with a primal dual style algorithm that alternates cost and policy updates. Within the policy updates, the SOAR framework uses an actor critic method with multiple critics to estimate the critic uncertainty and build an optimistic critic fundamental to drive exploration. When instantiated in the tabular setting, we get a provable algorithm with guarantees that matches the best known results in $\epsilon$. Practically, the SOAR template is shown to boost consistently the performance of imitation learning algorithms based on Soft Actor Critic such as f-IRL, ML-IRL and CSIL in several MuJoCo environments. Overall, thanks to SOAR, the required number of episodes to achieve the same performance is reduced by half.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOSE: A Systematic Evaluation Method Optimized for Base Models</title>
<link>https://arxiv.org/abs/2503.00812</link>
<guid>https://arxiv.org/abs/2503.00812</guid>
<content:encoded><![CDATA[
arXiv:2503.00812v2 Announce Type: replace 
Abstract: This paper poses two critical issues in evaluating base models (without post-training): (1) Unstable evaluation during training: in the early stages of pre-training, the models lack the capability to answer questions as required, leading to unstable evaluation results. This instability makes it difficult to provide solid conclusions to guide the training, especially for key experiments such as data ablation and scaling law. (2) Inconsistency between base and instruct models: base models generally exhibit poorer evaluation performance compared to corresponding instruct models. This gap poses a challenge for assessing whether a base model with better evaluation can truly lead to a better instruct model. To address these issues, we propose Base model Oriented Systematic Evaluation (BOSE), a method specifically designed to optimize the evaluation of base models. Specifically, BOSE introduces two key innovations: In-Context Light-instruction Prompt (ICLiP) for open-ended tasks and Blank-ppl for multi-choice tasks with candidate options, which transforms the standard perplexity (ppl) metric into a fill-in-the-blank format to mitigate early-stage evaluation fluctuations. Furthermore, we are the first to propose Kendall's rank correlation to quantitatively measure the evaluation stability and consistency. Experimental results demonstrate that BOSE significantly enhances both the stability of evaluations during pre-training and the consistency between base and instruct models, thereby providing more reliable guidance for the LLMs' training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Materials Foundation Model via Hybrid Invariant-Equivariant Architectures</title>
<link>https://arxiv.org/abs/2503.05771</link>
<guid>https://arxiv.org/abs/2503.05771</guid>
<content:encoded><![CDATA[
arXiv:2503.05771v2 Announce Type: replace 
Abstract: Machine learning interatomic potentials (MLIPs) can predict energy, force, and stress of materials and enable a wide range of downstream discovery tasks. A key design choice in MLIPs involves the trade-off between invariant and equivariant architectures. Invariant models offer computational efficiency but may not perform as well, especially when predicting high-order outputs. In contrast, equivariant models can capture high-order symmetries, but are computationally expensive. In this work, we propose HIENet, a hybrid invariant-equivariant materials interatomic potential model that integrates both invariant and equivariant message passing layers, while provably satisfying key physical constraints. HIENet achieves state-of-the-art performance with considerable computational speedups over prior models. Experimental results on both common benchmarks and downstream materials discovery tasks demonstrate the efficiency and effectiveness of HIENet.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mirror Online Conformal Prediction with Intermittent Feedback</title>
<link>https://arxiv.org/abs/2503.10345</link>
<guid>https://arxiv.org/abs/2503.10345</guid>
<content:encoded><![CDATA[
arXiv:2503.10345v3 Announce Type: replace 
Abstract: Online conformal prediction enables the runtime calibration of a pre-trained artificial intelligence model using feedback on its performance. Calibration is achieved through set predictions that are updated via online rules so as to ensure long-term coverage guarantees. While recent research has demonstrated the benefits of incorporating prior knowledge into the calibration process, this has come at the cost of replacing coverage guarantees with less tangible regret guarantees based on the quantile loss. This work introduces intermittent mirror online conformal prediction (IM-OCP), a novel runtime calibration framework that integrates prior knowledge, while maintaining long-term coverage and achieving sub-linear regret. IM-OCP features closed-form updates with minimal memory complexity, and is designed to operate under potentially intermittent feedback.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs</title>
<link>https://arxiv.org/abs/2503.17229</link>
<guid>https://arxiv.org/abs/2503.17229</guid>
<content:encoded><![CDATA[
arXiv:2503.17229v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) frequently generate hallucinated content, posing significant challenges for applications where factuality is crucial. While existing hallucination detection methods typically operate at the sentence level or passage level, we propose FactSelfCheck, a novel black-box sampling-based method that enables fine-grained fact-level detection. Our approach represents text as knowledge graphs consisting of facts in the form of triples. Through analyzing factual consistency across multiple LLM responses, we compute fine-grained hallucination scores without requiring external resources or training data. Our evaluation demonstrates that FactSelfCheck performs competitively with leading sentence-level sampling-based methods while providing more detailed insights. Most notably, our fact-level approach significantly improves hallucination correction, achieving a 35.5% increase in factual content compared to the baseline, while sentence-level SelfCheckGPT yields only a 10.6% improvement. The granular nature of our detection enables more precise identification and correction of hallucinated content. Additionally, we contribute a new dataset for evaluating sampling-based methods - FavaMultiSamples.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NdLinear: Don't Flatten! Building Superior Neural Architectures by Preserving N-D Structure</title>
<link>https://arxiv.org/abs/2503.17353</link>
<guid>https://arxiv.org/abs/2503.17353</guid>
<content:encoded><![CDATA[
arXiv:2503.17353v2 Announce Type: replace 
Abstract: Many high-impact machine learning tasks involve multi-dimensional data such as images, volumetric medical scans, and multivariate time-series. Yet, most neural architectures flatten these inputs, discarding critical cross-dimension information. We introduce $\textbf{NdLinear}$, a novel linear transformation that circumvents this destructive flattening by operating directly on tensors. NdLinear applies transformations separately along each data dimension, thereby preserving the native data structure. Extensive experiments demonstrate NdLinear's capacity to significantly enhance representational power, achieve dramatic parameter reductions (often by orders of magnitude), and maintain a favorable computational profile. For instance, when applied to Large Language Model finetuning, our $\textbf{NdLinear-LoRA}$ delivers comparable or improved accuracy on reasoning tasks using up to $9\times$ fewer trainable parameters than standard LoRA. These broad advantages of NdLinear are consistently validated across diverse neural architectures (CNNs, RNNs, Transformers, MLPs) and data domains, including vision, language, time-series, and tabular tasks. As a versatile, drop-in replacement for standard linear layers, NdLinear processes data in its original N-dimensional form, offering a foundational component for developing more efficient and powerful next-generation neural architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEMMA: Learning from Errors for MatheMatical Advancement in LLMs</title>
<link>https://arxiv.org/abs/2503.17439</link>
<guid>https://arxiv.org/abs/2503.17439</guid>
<content:encoded><![CDATA[
arXiv:2503.17439v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning</title>
<link>https://arxiv.org/abs/2504.02922</link>
<guid>https://arxiv.org/abs/2504.02922</guid>
<content:encoded><![CDATA[
arXiv:2504.02922v2 Announce Type: replace 
Abstract: Model diffing is the study of how fine-tuning changes a model's representations and internal algorithms. Many behaviors of interest are introduced during fine-tuning, and model diffing offers a promising lens to interpret such behaviors. Crosscoders are a recent model diffing method that learns a shared dictionary of interpretable concepts represented as latent directions in both the base and fine-tuned models, allowing us to track how concepts shift or emerge during fine-tuning. Notably, prior work has observed concepts with no direction in the base model, and it was hypothesized that these model-specific latents were concepts introduced during fine-tuning. However, we identify two issues which stem from the crosscoders L1 training loss that can misattribute concepts as unique to the fine-tuned model, when they really exist in both models. We develop Latent Scaling to flag these issues by more accurately measuring each latent's presence across models. In experiments comparing Gemma 2 2B base and chat models, we observe that the standard crosscoder suffers heavily from these issues. Building on these insights, we train a crosscoder with BatchTopK loss and show that it substantially mitigates these issues, finding more genuinely chat-specific and highly interpretable concepts. We recommend practitioners adopt similar techniques. Using the BatchTopK crosscoder, we successfully identify a set of chat-specific latents that are both interpretable and causally effective, representing concepts such as $\textit{false information}$ and $\textit{personal question}$, along with multiple refusal-related latents that show nuanced preferences for different refusal triggers. Overall, our work advances best practices for the crosscoder-based methodology for model diffing and demonstrates that it can provide concrete insights into how chat-tuning modifies model behavior.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling bounded rational decision-making through Wasserstein constraints</title>
<link>https://arxiv.org/abs/2504.03743</link>
<guid>https://arxiv.org/abs/2504.03743</guid>
<content:encoded><![CDATA[
arXiv:2504.03743v2 Announce Type: replace 
Abstract: Modelling bounded rational decision-making through information constrained processing provides a principled approach for representing departures from rationality within a reinforcement learning framework, while still treating decision-making as an optimization process. However, existing approaches are generally based on Entropy, Kullback-Leibler divergence, or Mutual Information. In this work, we highlight issues with these approaches when dealing with ordinal action spaces. Specifically, entropy assumes uniform prior beliefs, missing the impact of a priori biases on decision-makings. KL-Divergence addresses this, however, has no notion of "nearness" of actions, and additionally, has several well known potentially undesirable properties such as the lack of symmetry, and furthermore, requires the distributions to have the same support (e.g. positive probability for all actions). Mutual information is often difficult to estimate. Here, we propose an alternative approach for modeling bounded rational RL agents utilising Wasserstein distances. This approach overcomes the aforementioned issues. Crucially, this approach accounts for the nearness of ordinal actions, modeling "stickiness" in agent decisions and unlikeliness of rapidly switching to far away actions, while also supporting low probability actions, zero-support prior distributions, and is simple to calculate directly.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2504.05258</link>
<guid>https://arxiv.org/abs/2504.05258</guid>
<content:encoded><![CDATA[
arXiv:2504.05258v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation</title>
<link>https://arxiv.org/abs/2504.11992</link>
<guid>https://arxiv.org/abs/2504.11992</guid>
<content:encoded><![CDATA[
arXiv:2504.11992v2 Announce Type: replace 
Abstract: A domain (distribution) shift between training and test data often hinders the real-world performance of deep neural networks, necessitating unsupervised domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged as a solution for practical scenarios where access to source data is restricted and target data is received as a continuous stream. However, the open-world nature of many real-world applications additionally introduces category shifts meaning that the source and target label spaces may differ. Online source-free universal domain adaptation (SF-UniDA) addresses this challenge. Existing methods mainly rely on self-training with pseudo-labels, yet the relationship between pseudo-labeling and adaptation outcomes has not been studied yet. To bridge this gap, we conduct a systematic analysis through controlled experiments with simulated pseudo-labeling, offering valuable insights into pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap between the current state-of-the-art and the upper bound of adaptation achieved with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables effective adaptation even with moderate pseudo-label accuracy, while a cross-entropy (CE) loss, though less robust to pseudo-label errors, achieves superior results when pseudo-labeling approaches perfection. Lastly, our findings indicate that pseudo-label accuracy is in general more crucial than quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels is beneficial. Overall, our study highlights the critical role of pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive future advancements in the field. Our code is available at https://github.com/pascalschlachter/PLAnalysis.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Framework for OOD Robustness in Transformers using Gevrey Classes</title>
<link>https://arxiv.org/abs/2504.12991</link>
<guid>https://arxiv.org/abs/2504.12991</guid>
<content:encoded><![CDATA[
arXiv:2504.12991v2 Announce Type: replace 
Abstract: We study the robustness of Transformer language models under semantic out-of-distribution (OOD) shifts, where training and test data lie in disjoint latent spaces. Using Wasserstein-1 distance and Gevrey-class smoothness, we derive sub-exponential upper bounds on prediction error. Our theoretical framework explains how smoothness governs generalization under distributional drift. We validate these findings through controlled experiments on arithmetic and Chain-of-Thought tasks with latent permutations and scalings. Results show empirical degradation aligns with our bounds, highlighting the geometric and functional principles underlying OOD generalization in Transformers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softpick: No Attention Sink, No Massive Activations with Rectified Softmax</title>
<link>https://arxiv.org/abs/2504.20966</link>
<guid>https://arxiv.org/abs/2504.20966</guid>
<content:encoded><![CDATA[
arXiv:2504.20966v2 Announce Type: replace 
Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M and 1.8B parameter models demonstrate that softpick achieves 0\% sink rate consistently. The softpick transformers produce hidden states with significantly lower kurtosis and creates sparse attention maps. Quantized models using softpick outperform softmax on standard benchmarks, with a particularly pronounced advantage at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
<link>https://arxiv.org/abs/2505.02712</link>
<guid>https://arxiv.org/abs/2505.02712</guid>
<content:encoded><![CDATA[
arXiv:2505.02712v2 Announce Type: replace 
Abstract: Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Optimal Sample Complexity for Learning with Label Proportions</title>
<link>https://arxiv.org/abs/2505.05355</link>
<guid>https://arxiv.org/abs/2505.05355</guid>
<content:encoded><![CDATA[
arXiv:2505.05355v2 Announce Type: replace 
Abstract: We investigate Learning from Label Proportions (LLP), a partial information setting where examples in a training set are grouped into bags, and only aggregate label values in each bag are available. Despite the partial observability, the goal is still to achieve small regret at the level of individual examples. We give results on the sample complexity of LLP under square loss, showing that our sample complexity is essentially optimal. From an algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc variance reduction techniques. On one hand, our theoretical results improve in important ways on the existing literature on LLP, specifically in the way the sample complexity depends on the bag size. On the other hand, we validate our algorithmic solutions on several datasets, demonstrating improved empirical performance (better accuracy for less samples) against recent baselines.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning</title>
<link>https://arxiv.org/abs/2505.11578</link>
<guid>https://arxiv.org/abs/2505.11578</guid>
<content:encoded><![CDATA[
arXiv:2505.11578v3 Announce Type: replace 
Abstract: This research confronts the challenge of substantial physical equation discrepancies encountered in the generation of spatiotemporal physical fields through data-driven trained models. A spatiotemporal physical field generation model, named HMT-PF, is developed based on the hybrid Mamba-Transformer architecture, incorporating unstructured grid information as input. A fine-tuning block, enhanced with physical information, is introduced to effectively reduce the physical equation discrepancies. The physical equation residuals are computed through a point query mechanism for efficient gradient evaluation, then encoded into latent space for refinement. The fine-tuning process employs a self-supervised learning approach to achieve physical consistency while maintaining essential field characteristics. Results show that the hybrid Mamba-Transformer model achieves good performance in generating spatiotemporal fields, while the physics-informed fine-tuning mechanism further reduces significant physical errors effectively. A MSE-R evaluation method is developed to assess the accuracy and realism of physical field generation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Use Their Depth Efficiently?</title>
<link>https://arxiv.org/abs/2505.13898</link>
<guid>https://arxiv.org/abs/2505.13898</guid>
<content:encoded><![CDATA[
arXiv:2505.13898v2 Announce Type: replace 
Abstract: Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis</title>
<link>https://arxiv.org/abs/2505.14742</link>
<guid>https://arxiv.org/abs/2505.14742</guid>
<content:encoded><![CDATA[
arXiv:2505.14742v2 Announce Type: replace 
Abstract: Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoTime: A Dataset Suite for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.15072</link>
<guid>https://arxiv.org/abs/2505.15072</guid>
<content:encoded><![CDATA[
arXiv:2505.15072v2 Announce Type: replace 
Abstract: While multimodal data sources are increasingly available from real-world forecasting, most existing research remains on unimodal time series. In this work, we present MoTime, a suite of multimodal time series forecasting datasets that pair temporal signals with external modalities such as text, metadata, and images. Covering diverse domains, MoTime supports structured evaluation of modality utility under two scenarios: 1) the common forecasting task, where varying-length history is available, and 2) cold-start forecasting, where no historical data is available. Experiments show that external modalities can improve forecasting performance in both scenarios, with particularly strong benefits for short series in some datasets, though the impact varies depending on data characteristics. By making datasets and findings publicly available, we aim to support more comprehensive and realistic benchmarks in future multimodal time series forecasting research.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16400</link>
<guid>https://arxiv.org/abs/2505.16400</guid>
<content:encoded><![CDATA[
arXiv:2505.16400v2 Announce Type: replace 
Abstract: Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Are Concepts Erased From Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.17013</link>
<guid>https://arxiv.org/abs/2505.17013</guid>
<content:encoded><![CDATA[
arXiv:2505.17013v4 Announce Type: replace 
Abstract: Concept erasure, the ability to selectively prevent a model from generating specific concepts, has attracted growing interest, with various approaches emerging to address the challenge. However, it remains unclear how thoroughly these methods erase the target concept. We begin by proposing two conceptual models for the erasure mechanism in diffusion models: (i) reducing the likelihood of generating the target concept, and (ii) interfering with the model's internal guidance mechanisms. To thoroughly assess whether a concept has been truly erased from the model, we introduce a suite of independent evaluations. Our evaluation framework includes adversarial attacks, novel probing techniques, and analysis of the model's alternative generations in place of the erased concept. Our results shed light on the tension between minimizing side effects and maintaining robustness to adversarial prompts. Broadly, our work underlines the importance of comprehensive evaluation for erasure in diffusion models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17552</link>
<guid>https://arxiv.org/abs/2505.17552</guid>
<content:encoded><![CDATA[
arXiv:2505.17552v2 Announce Type: replace 
Abstract: De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should We Simultaneously Calibrate Multiple Computer Models?</title>
<link>https://arxiv.org/abs/2505.18176</link>
<guid>https://arxiv.org/abs/2505.18176</guid>
<content:encoded><![CDATA[
arXiv:2505.18176v2 Announce Type: replace 
Abstract: In an increasing number of applications designers have access to multiple computer models which typically have different levels of fidelity and cost. Traditionally, designers calibrate these models one at a time against some high-fidelity data (e.g., experiments). In this paper, we question this tradition and assess the potential of calibrating multiple computer models at the same time. To this end, we develop a probabilistic framework that is founded on customized neural networks (NNs) that are designed to calibrate an arbitrary number of computer models. In our approach, we (1) consider the fact that most computer models are multi-response and that the number and nature of calibration parameters may change across the models, and (2) learn a unique probability distribution for each calibration parameter of each computer model, (3) develop a loss function that enables our NN to emulate all data sources while calibrating the computer models, and (4) aim to learn a visualizable latent space where model-form errors can be identified. We test the performance of our approach on analytic and engineering problems to understand the potential advantages and pitfalls in simultaneous calibration of multiple computer models. Our method can improve predictive accuracy, however, it is prone to non-identifiability issues in higher-dimensional input spaces that are normally constrained by underlying physics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-PiSSA: High-Rank Distributed Orthogonal Adaptation</title>
<link>https://arxiv.org/abs/2505.18777</link>
<guid>https://arxiv.org/abs/2505.18777</guid>
<content:encoded><![CDATA[
arXiv:2505.18777v2 Announce Type: replace 
Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce High-rank Distributed PiSSA (HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, we evaluate HD-PiSSA across various challenging downstream tasks, including mathematics, code generation, and multi-task learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks, demonstrating its benefits from the extra optimization flexibility.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians</title>
<link>https://arxiv.org/abs/2505.19458</link>
<guid>https://arxiv.org/abs/2505.19458</guid>
<content:encoded><![CDATA[
arXiv:2505.19458v2 Announce Type: replace 
Abstract: The theoretical understanding of self-attention (SA) has been steadily progressing. A prominent line of work studies a class of SA layers that admit an energy function decreased by state updates. While it provides valuable insights into inherent biases in signal propagation, it often relies on idealized assumptions or additional constraints not necessarily present in standard SA. Thus, to broaden our understanding, this work aims to relax these energy constraints and provide an energy-agnostic characterization of inference dynamics by dynamical systems analysis. In more detail, we first consider relaxing the symmetry and single-head constraints traditionally required in energy-based formulations. Next, to investigate more general SA architectures capable of oscillatory dynamics without necessarily admitting an energy function, we analyze the Jacobian matrix of the state. We reveal that normalization layers effectively normalize the Jacobian's complex eigenvalues, forcing the dynamics close to a critical state. This significantly enhances inference performance. Furthermore, we utilize the Jacobian perspective to develop regularization methods for training and a pseudo-energy for monitoring inference dynamics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Nonparametric Instrumental Variable Regression</title>
<link>https://arxiv.org/abs/2112.14249</link>
<guid>https://arxiv.org/abs/2112.14249</guid>
<content:encoded><![CDATA[
arXiv:2112.14249v4 Announce Type: replace-cross 
Abstract: Several causal parameters in short panel data models are functionals of a nested nonparametric instrumental variable regression (nested NPIV). Recent examples include mediated, time varying, and long term treatment effects identified using proxy variables. In econometrics, examples arise in triangular simultaneous equations and hedonic price systems. However, it appears that explicit mean square convergence rates for nested NPIV are unknown, preventing inference on some of these parameters with generic machine learning. A major challenge is compounding ill posedness due to the nested inverse problems. To limit how ill posedness compounds, we introduce two techniques: relative well posedness, and multiple robustness to ill posedness. With these techniques, we provide explicit mean square rates for nested NPIV and efficient inference for recently identified causal parameters. Our nonasymptotic analysis accommodates neural networks, random forests, and reproducing kernel Hilbert spaces. It extends to causal functions, e.g. heterogeneous long term treatment effects.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gold Standard Dataset for the Reviewer Assignment Problem</title>
<link>https://arxiv.org/abs/2303.16750</link>
<guid>https://arxiv.org/abs/2303.16750</guid>
<content:encoded><![CDATA[
arXiv:2303.16750v2 Announce Type: replace-cross 
Abstract: Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the "similarity score" -- a numerical estimate of the expertise of a reviewer in reviewing a paper -- and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of publicly available gold-standard data. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.
  Using our dataset, we compare several widely used similarity algorithms and offer key insights. First, all algorithms exhibit significant error, with misranking rates between 12%-30% in easier cases and 36%-43% in harder ones. Second, most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the SPECTER2 algorithm performs best. Interestingly, classical TF-IDF matches SPECTER2 in accuracy when given access to full submission texts. In contrast, off-the-shelf LLMs lag behind specialized approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles</title>
<link>https://arxiv.org/abs/2306.14853</link>
<guid>https://arxiv.org/abs/2306.14853</guid>
<content:encoded><![CDATA[
arXiv:2306.14853v3 Announce Type: replace-cross 
Abstract: In this work, we consider bilevel optimization when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product (HVP) oracle, one can provably find an $\epsilon$-stationary point within ${\mathcal{O}}(\epsilon^{-2})$ oracle calls. However, the HVP oracle may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\tilde{\mathcal{O}}(\epsilon^{-3})$. In this paper, we incorporate a two-time-scale update to improve their method to achieve the near-optimal $\tilde {\mathcal{O}}(\epsilon^{-2})$ first-order oracle complexity. Our analysis is highly extensible. In the stochastic setting, our algorithm can achieve the stochastic first-order oracle complexity of $\tilde {\mathcal{O}}(\epsilon^{-4})$ and $\tilde {\mathcal{O}}(\epsilon^{-6})$ when the stochastic noises are only in the upper-level objective and in both level objectives, respectively. When the objectives have higher-order smoothness conditions, our deterministic method can escape saddle points by injecting noise, and can be accelerated to achieve a faster rate of $\tilde {\mathcal{O}}(\epsilon^{-1.75})$ using Nesterov's momentum.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery</title>
<link>https://arxiv.org/abs/2308.13135</link>
<guid>https://arxiv.org/abs/2308.13135</guid>
<content:encoded><![CDATA[
arXiv:2308.13135v2 Announce Type: replace-cross 
Abstract: We propose a nonparametric additive model for estimating interpretable value functions in reinforcement learning, with an application in optimizing postoperative recovery through personalized, adaptive recommendations. While reinforcement learning has achieved significant success in various domains, recent methods often rely on black-box approaches such as neural networks, which hinder the examination of individual feature contributions to a decision-making policy. Our novel method offers a flexible technique for estimating action-value functions without explicit parametric assumptions, overcoming the limitations of the linearity assumption of classical algorithms. By incorporating local kernel regression and basis expansion, we obtain a sparse, additive representation of the action-value function, enabling local approximation and retrieval of nonlinear, independent contributions of select state features and the interactions between joint feature pairs. We validate our approach through a simulation study and apply it to spine disease recovery, uncovering recommendations aligned with clinical knowledge. This method bridges the gap between flexible machine learning techniques and the interpretability required in healthcare applications, paving the way for more personalized interventions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From the Pursuit of Universal AGI Architecture to Systematic Approach to Heterogenous AGI: Addressing Alignment, Energy, &amp; AGI Grand Challenges</title>
<link>https://arxiv.org/abs/2310.15274</link>
<guid>https://arxiv.org/abs/2310.15274</guid>
<content:encoded><![CDATA[
arXiv:2310.15274v3 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) faces a trifecta of grand challenges: the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. We present SAGI, a Systematic Approach to AGI that utilizes system design principles to overcome the energy wall and alignment challenges. This paper asserts that AGI can be realized through multiplicity of design specific pathways and customized through system design rather than a singular overarching architecture. AGI systems may exhibit diver architectural configurations and capabilities, contingent upon their intended use cases. Alignment, a challenge broadly recognized as AIs most formidable, is the one that depends most critically on system design and serves as its primary driving force as a foundational criterion for AGI. Capturing the complexities of human morality for alignment requires architectural support to represent the intricacies of moral decision-making and the pervasive ethical processing at every level, with performance reliability exceeding that of human moral judgment. Hence, requiring a more robust architecture towards safety and alignment goals, without replicating or resembling the human brain.
  We argue that system design (such as feedback loops, energy and performance optimization) on learning substrates (capable of learning its system architecture) is more fundamental to achieving AGI goals and guarantees, superseding classical symbolic, emergentist and hybrid approaches. Through learning of the system architecture itself, the resulting AGI is not a product of spontaneous emergence but of systematic design and deliberate engineering, with core features, including an integrated moral architecture, deeply embedded within its architecture. The approach aims to guarantee design goals such as alignment, efficiency by self-learning system architecture.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNBleed: Inference Attacks to Unveil Private Edges in Graphs with Realistic Access to GNN Models</title>
<link>https://arxiv.org/abs/2311.16139</link>
<guid>https://arxiv.org/abs/2311.16139</guid>
<content:encoded><![CDATA[
arXiv:2311.16139v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have become indispensable tools for learning from graph structured data, catering to various applications such as social network analysis and fraud detection for financial services. At the heart of these networks are the edges, which are crucial in guiding GNN models' predictions. In many scenarios, these edges represent sensitive information, such as personal associations or financial dealings, which require privacy assurance. However, their contributions to GNN model predictions may, in turn, be exploited by the adversary to compromise their privacy. Motivated by these conflicting requirements, this paper investigates edge privacy in contexts where adversaries possess only black-box access to the target GNN model, restricted further by access controls, preventing direct insights into arbitrary node outputs. Moreover, we are the first to extensively examine situations where the target graph continuously evolves, a common trait of many real-world graphs. In this setting, we present a range of attacks that leverage the message-passing mechanism of GNNs. We evaluated the effectiveness of our attacks using nine real-world datasets, encompassing both static and dynamic graphs, across four different GNN architectures. The results demonstrate that our attack outperforms existing methods across various GNN architectures, consistently achieving an F1 score of at least 0.8 in static scenarios. Furthermore, our attack retains robustness in dynamic graph scenarios, maintaining F1 scores up to 0.8, unlike previous methods that only achieve F1 scores around 0.2.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space</title>
<link>https://arxiv.org/abs/2312.02849</link>
<guid>https://arxiv.org/abs/2312.02849</guid>
<content:encoded><![CDATA[
arXiv:2312.02849v4 Announce Type: replace-cross 
Abstract: We develop a theory of finite-dimensional polyhedral subsets over the Wasserstein space and optimization of functionals over them via first-order methods. Our main application is to the problem of mean-field variational inference, which seeks to approximate a distribution $\pi$ over $\mathbb{R}^d$ by a product measure $\pi^\star$. When $\pi$ is strongly log-concave and log-smooth, we provide (1) approximation rates certifying that $\pi^\star$ is close to the minimizer $\pi^\star_\diamond$ of the KL divergence over a \emph{polyhedral} set $\mathcal{P}_\diamond$, and (2) an algorithm for minimizing $\text{KL}(\cdot\|\pi)$ over $\mathcal{P}_\diamond$ based on accelerated gradient descent over $\R^d$. As a byproduct of our analysis, we obtain the first end-to-end analysis for gradient-based algorithms for MFVI.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum</title>
<link>https://arxiv.org/abs/2401.06738</link>
<guid>https://arxiv.org/abs/2401.06738</guid>
<content:encoded><![CDATA[
arXiv:2401.06738v3 Announce Type: replace-cross 
Abstract: Stochastic heavy ball momentum (SHB) is commonly used to train machine learning models, and often provides empirical improvements over stochastic gradient descent. By primarily focusing on strongly-convex quadratics, we aim to better understand the theoretical advantage of SHB and subsequently improve the method. For strongly-convex quadratics, Kidambi et al. (2018) show that SHB (with a mini-batch of size $1$) cannot attain accelerated convergence, and hence has no theoretical benefit over SGD. They conjecture that the practical gain of SHB is a by-product of using larger mini-batches. We first substantiate this claim by showing that SHB can attain an accelerated rate when the mini-batch size is larger than a threshold $b^*$ that depends on the condition number $\kappa$. Specifically, we prove that with the same step-size and momentum parameters as in the deterministic setting, SHB with a sufficiently large mini-batch size results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence when measuring the distance to the optimal solution in the $\ell_2$ norm, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. We prove a lower-bound which demonstrates that a $\kappa$ dependence in $b^*$ is necessary. To ensure convergence to the minimizer, we design a noise-adaptive multi-stage algorithm that results in an $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}}\right) + \frac{\sigma}{\sqrt{T}}\right)$ rate when measuring the distance to the optimal solution in the $\ell_2$ norm. We also consider the general smooth, strongly-convex setting and propose the first noise-adaptive SHB variant that converges to the minimizer at an $O(\exp(-\frac{T}{\kappa}) + \frac{\sigma^2}{T})$ rate when measuring the distance to the optimal solution in the squared $\ell_2$ norm. We empirically demonstrate the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models</title>
<link>https://arxiv.org/abs/2401.08491</link>
<guid>https://arxiv.org/abs/2401.08491</guid>
<content:encoded><![CDATA[
arXiv:2401.08491v3 Announce Type: replace-cross 
Abstract: The generation of toxic content by large language models (LLMs) remains a critical challenge for the safe deployment of language technology. We propose a novel framework for implicit knowledge editing and controlled text generation by fine-tuning LLMs with a prototype-based contrastive perplexity objective. Central to our method is the construction of hard negatives - toxic outputs that are generated through adversarial paraphrasing to be semantically similar and model probability to their non-toxic counterparts. By training on these challenging and realistic pairs, our approach ensures robust and stable contrastive optimization. Experimental results in the domain of detoxification demonstrate that our method significantly reduces toxic generation while maintaining strong performance on downstream tasks such as commonsense reasoning and reading comprehension. Our findings highlight the effectiveness of exploiting hard negatives for attribute-aware fine-tuning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Leakage Detection through Approximate Bayes-optimal Prediction</title>
<link>https://arxiv.org/abs/2401.14283</link>
<guid>https://arxiv.org/abs/2401.14283</guid>
<content:encoded><![CDATA[
arXiv:2401.14283v3 Announce Type: replace-cross 
Abstract: In today's data-driven world, the proliferation of publicly available information raises security concerns due to the information leakage (IL) problem. IL involves unintentionally exposing sensitive information to unauthorized parties via observable system information. Conventional statistical approaches rely on estimating mutual information (MI) between observable and secret information for detecting ILs, face challenges of the curse of dimensionality, convergence, computational complexity, and MI misestimation. Though effective, emerging supervised machine learning based approaches to detect ILs are limited to binary system sensitive information and lack a comprehensive framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to quantify and detect IL accurately. Using automated machine learning, we demonstrate that MI can be accurately estimated by approximating the typically unknown Bayes predictor's log-loss and accuracy. Based on this, we show how MI can effectively be estimated to detect ILs. Our method performs superior to state-of-the-art baselines in an empirical study considering synthetic and real-world OpenSSL TLS server datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Hypothesis Testing for Softmax Models and Leverage Score Models</title>
<link>https://arxiv.org/abs/2405.06003</link>
<guid>https://arxiv.org/abs/2405.06003</guid>
<content:encoded><![CDATA[
arXiv:2405.06003v2 Announce Type: replace-cross 
Abstract: Softmax distributions are widely used in machine learning, including Large Language Models (LLMs), where the attention unit uses softmax distributions. We abstract the attention unit as the softmax model, where given a vector input, the model produces an output drawn from the softmax distribution (which depends on the vector input). We consider the fundamental problem of binary hypothesis testing in the setting of softmax models. That is, given an unknown softmax model, which is known to be one of the two given softmax models, how many queries are needed to determine which one is the truth? We show that the sample complexity is asymptotically $O(\epsilon^{-2})$ where $\epsilon$ is a certain distance between the parameters of the models. Furthermore, we draw an analogy between the softmax model and the leverage score model, an important tool for algorithm design in linear algebra and graph theory. The leverage score model, on a high level, is a model which, given a vector input, produces an output drawn from a distribution dependent on the input. We obtain similar results for the binary hypothesis testing problem for leverage score models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Misspecification in Simulation-based Inference through Data-driven Calibration</title>
<link>https://arxiv.org/abs/2405.08719</link>
<guid>https://arxiv.org/abs/2405.08719</guid>
<content:encoded><![CDATA[
arXiv:2405.08719v2 Announce Type: replace-cross 
Abstract: Driven by steady progress in deep generative modeling, simulation-based inference (SBI) has emerged as the workhorse for inferring the parameters of stochastic simulators. However, recent work has demonstrated that model misspecification can compromise the reliability of SBI, preventing its adoption in important applications where only misspecified simulators are available. This work introduces robust posterior estimation~(RoPE), a framework that overcomes model misspecification with a small real-world calibration set of ground-truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport~(OT) problem between learned representations of real-world and simulated observations, allowing RoPE to learn a model of the misspecification without placing additional assumptions on its nature. RoPE demonstrates how OT and a calibration set provide a controllable balance between calibrated uncertainty and informative inference, even under severely misspecified simulators. Results on four synthetic tasks and two real-world problems with ground-truth labels demonstrate that RoPE outperforms baselines and consistently returns informative and calibrated credible intervals.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead</title>
<link>https://arxiv.org/abs/2407.00066</link>
<guid>https://arxiv.org/abs/2407.00066</guid>
<content:encoded><![CDATA[
arXiv:2407.00066v4 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disturbance-based Discretization, Differentiable IDS Channel, and an IDS-Correcting Code for DNA Storage</title>
<link>https://arxiv.org/abs/2407.18929</link>
<guid>https://arxiv.org/abs/2407.18929</guid>
<content:encoded><![CDATA[
arXiv:2407.18929v4 Announce Type: replace-cross 
Abstract: Insertion, deletion, and substitution (IDS) error-correcting codes have garnered increased attention with recent advancements in DNA storage technology. However, a universal method for designing tailored IDS-correcting codes across varying channel settings remains underexplored. We present an autoencoder-based approach, THEA-code, aimed at efficiently generating IDS-correcting codes for complex IDS channels. In the work, a disturbance-based discretization is proposed to discretize the features of the autoencoder, and a simulated differentiable IDS channel is developed as a differentiable alternative for IDS operations. These innovations facilitate the successful convergence of the autoencoder, producing channel-customized IDS-correcting codes that demonstrate commendable performance across complex IDS channels, particularly in the realistic DNA storage channel.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains</title>
<link>https://arxiv.org/abs/2409.17275</link>
<guid>https://arxiv.org/abs/2409.17275</guid>
<content:encoded><![CDATA[
arXiv:2409.17275v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs' generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q\&amp;A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query's embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q\&amp;A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method</title>
<link>https://arxiv.org/abs/2410.00059</link>
<guid>https://arxiv.org/abs/2410.00059</guid>
<content:encoded><![CDATA[
arXiv:2410.00059v2 Announce Type: replace-cross 
Abstract: Illegitimate reproduction, distribution and derivation of Deep Neural Network (DNN) models can inflict economic loss, reputation damage and even privacy infringement. Passive DNN intellectual property (IP) protection methods such as watermarking and fingerprinting attempt to prove the ownership upon IP violation, but they are often too late to stop catastrophic damage of IP abuse and too feeble against strong adversaries. In this paper, we propose IDEA, an Inverse Domain Expert Adaptation based proactive DNN IP protection method featuring active authorization and source traceability. IDEA generalizes active authorization as an inverse problem of domain adaptation. The multi-adaptive optimization is solved by a mixture-of-experts model with one real and two fake experts. The real expert re-optimizes the source model to correctly classify test images with a unique model user key steganographically embedded. The fake experts are trained to output random prediction on test images without or with incorrect user key embedded by minimizing their mutual information (MI) with the real expert. The MoE model is knowledge distilled into a unified protected model to avoid leaking the expert model features by maximizing their MI with additional multi-layer attention and contrastive representation loss optimization. IDEA not only prevents unauthorized users without the valid key to access the functional model, but also enable the model owner to validate the deployed model and trace the source of IP infringement. We extensively evaluate IDEA on five datasets and four DNN models to demonstrate its effectiveness in authorization control, culprit tracing success rate, and robustness against various attacks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Representation Learning of Neuronal Dynamics and Behavior</title>
<link>https://arxiv.org/abs/2410.00665</link>
<guid>https://arxiv.org/abs/2410.00665</guid>
<content:encoded><![CDATA[
arXiv:2410.00665v2 Announce Type: replace-cross 
Abstract: Understanding how neuronal networks reorganize in response to external stimuli and give rise to behavior is a central challenge in neuroscience and artificial intelligence. However, existing methods often fail to capture the evolving structure of neural connectivity in ways that capture its relationship to behavior, especially in dynamic, uncertain, or high-dimensional settings with sufficient resolution or interpretability. We introduce the Temporal Attention-enhanced Variational Graph Recurrent Neural Network (TAVRNN), a novel framework that models time-varying neuronal connectivity by integrating probabilistic graph learning with temporal attention mechanisms. TAVRNN learns latent dynamics at the single-unit level while maintaining interpretable population-level representations, to identify key connectivity patterns linked to behavior. TAVRNN generalizes across diverse neural systems and modalities, demonstrating state-of-the-art classification and clustering performance. We validate TAVRNN on three diverse datasets: (1) electrophysiological data from a freely behaving rat, (2) primate somatosensory cortex recordings during a reaching task, and (3) biological neurons in the DishBrain platform interacting with a virtual game environment. Our method outperforms state-of-the-art dynamic embedding techniques, revealing previously unreported relationships between adaptive behavior and the evolving topological organization of neural networks. These findings demonstrate that TAVRNN offers a powerful and generalizable approach for modeling neural dynamics across experimental and synthetic biological systems. Its architecture is modality-agnostic and scalable, making it applicable across a wide range of neural recording platforms and behavioral paradigms.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Large Motion Models with Million-Level Human Motions</title>
<link>https://arxiv.org/abs/2410.03311</link>
<guid>https://arxiv.org/abs/2410.03311</guid>
<content:encoded><![CDATA[
arXiv:2410.03311v3 Announce Type: replace-cross 
Abstract: Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named \projname, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://beingbeyond.github.io/Being-M0/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DemoShapley: Valuation of Demonstrations for In-Context Learning</title>
<link>https://arxiv.org/abs/2410.07523</link>
<guid>https://arxiv.org/abs/2410.07523</guid>
<content:encoded><![CDATA[
arXiv:2410.07523v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) using in-context learning (ICL) excel in many tasks without task-specific fine-tuning. However, demonstration selection and ordering greatly impact ICL effectiveness. To address this, we propose DemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to assess the influence of individual demonstrations. DemoShapley captures how each example influences performance in different contexts, unlike other influence-based methods that rely on a fixed number of demonstrations. Beta-DemoShapley further enhances this framework by incorporating the Beta distribution, allowing users to assign higher weights to smaller cardinalities, which aligns with ICL's prompt length and computational constraints. Our findings show that the proposed algorithms improve model performance by selecting quality demonstrations, and enhancing generalization to out-of-distribution tasks. It also identifies noise-compromised data and promotes fairness in LLMs, protecting model performance and ensuring robustness across various scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence</title>
<link>https://arxiv.org/abs/2410.13460</link>
<guid>https://arxiv.org/abs/2410.13460</guid>
<content:encoded><![CDATA[
arXiv:2410.13460v2 Announce Type: replace-cross 
Abstract: Many court systems are overwhelmed all over the world, leading to huge backlogs of pending cases. Effective triage systems, like those in emergency rooms, could ensure proper prioritization of open cases, optimizing time and resource allocation in the court system. In this work, we introduce the Criticality Prediction dataset, a novel resource for evaluating case prioritization. Our dataset features a two-tier labeling system: (1) the binary LD-Label, identifying cases published as Leading Decisions (LD), and (2) the more granular Citation-Label, ranking cases by their citation frequency and recency, allowing for a more nuanced evaluation. Unlike existing approaches that rely on resource-intensive manual annotations, we algorithmically derive labels leading to a much larger dataset than otherwise possible. We evaluate several multilingual models, including both smaller fine-tuned models and large language models in a zero-shot setting. Our results show that the fine-tuned models consistently outperform their larger counterparts, thanks to our large training set. Our results highlight that for highly domain-specific tasks like ours, large training sets are still valuable.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimension reduction via score ratio matching</title>
<link>https://arxiv.org/abs/2410.19990</link>
<guid>https://arxiv.org/abs/2410.19990</guid>
<content:encoded><![CDATA[
arXiv:2410.19990v2 Announce Type: replace-cross 
Abstract: Gradient-based dimension reduction decreases the cost of Bayesian inference and probabilistic modeling by identifying maximally informative (and informed) low-dimensional projections of the data and parameters, allowing high-dimensional problems to be reformulated as cheaper low-dimensional problems. A broad family of such techniques identify these projections and provide error bounds on the resulting posterior approximations, via eigendecompositions of certain diagnostic matrices. Yet these matrices require gradients or even Hessians of the log-likelihood, excluding the purely data-driven setting and many problems of simulation-based inference. We propose a framework, derived from score-matching, to extend gradient-based dimension reduction to problems where gradients are unavailable. Specifically, we formulate an objective function to directly learn the score ratio function needed to compute the diagnostic matrices, propose a tailored parameterization for the score ratio network, and introduce regularization methods that capitalize on the hypothesized low-dimensional structure. We also introduce a novel algorithm to iteratively identify the low-dimensional reduced basis vectors more accurately with limited data based on eigenvalue deflation methods. We show that our approach outperforms standard score-matching for problems with low-dimensional structure, and demonstrate its effectiveness for PDE-constrained Bayesian inverse problems and conditional generative modeling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Rapid Autonomy Validation</title>
<link>https://arxiv.org/abs/2411.03328</link>
<guid>https://arxiv.org/abs/2411.03328</guid>
<content:encoded><![CDATA[
arXiv:2411.03328v2 Announce Type: replace-cross 
Abstract: We are motivated by the problem of autonomous vehicle performance validation. A key challenge is that an autonomous vehicle requires testing in every kind of driving scenario it could encounter, including rare events, to provide a strong case for safety and show there is no edge-case pathological behavior. Autonomous vehicle companies rely on potentially millions of miles driven in realistic simulation to expose the driving stack to enough miles to estimate rates and severity of collisions. To address scalability and coverage, we propose the use of a behavior foundation model, specifically a masked autoencoder (MAE), trained to reconstruct driving scenarios. We leverage the foundation model in two complementary ways: we (i) use the learned embedding space to group qualitatively similar scenarios together and (ii) fine-tune the model to label scenario difficulty based on the likelihood of a collision upon simulation. We use the difficulty scoring as importance weighting for the groups of scenarios. The result is an approach which can more rapidly estimate the rates and severity of collisions by prioritizing hard scenarios while ensuring exposure to every kind of driving scenario.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework</title>
<link>https://arxiv.org/abs/2411.06160</link>
<guid>https://arxiv.org/abs/2411.06160</guid>
<content:encoded><![CDATA[
arXiv:2411.06160v3 Announce Type: replace-cross 
Abstract: Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoBAM: LoRA-Based Backdoor Attack on Model Merging</title>
<link>https://arxiv.org/abs/2411.16746</link>
<guid>https://arxiv.org/abs/2411.16746</guid>
<content:encoded><![CDATA[
arXiv:2411.16746v4 Announce Type: replace-cross 
Abstract: Model merging is an emerging technique that integrates multiple models fine-tuned on different tasks to create a versatile model that excels in multiple domains. This scheme, in the meantime, may open up backdoor attack opportunities where one single malicious model can jeopardize the integrity of the merged model. Existing works try to demonstrate the risk of such attacks by assuming substantial computational resources, focusing on cases where the attacker can fully fine-tune the pre-trained model. Such an assumption, however, may not be feasible given the increasing size of machine learning models. In practice where resources are limited and the attacker can only employ techniques like Low-Rank Adaptation (LoRA) to produce the malicious model, it remains unclear whether the attack can still work and pose threats. In this work, we first identify that the attack efficacy is significantly diminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method that yields high attack success rate with minimal training resources. The key idea of LoBAM is to amplify the malicious weights in an intelligent way that effectively enhances the attack efficacy. We demonstrate that our design can lead to improved attack success rate through extensive empirical experiments across various model merging scenarios. Moreover, we show that our method is highly stealthy and is difficult to detect and defend against.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Attention: Efficient LLM Inference over Long Sequences</title>
<link>https://arxiv.org/abs/2411.17116</link>
<guid>https://arxiv.org/abs/2411.17116</guid>
<content:encoded><![CDATA[
arXiv:2411.17116v3 Announce Type: replace-cross 
Abstract: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</title>
<link>https://arxiv.org/abs/2412.04905</link>
<guid>https://arxiv.org/abs/2412.04905</guid>
<content:encoded><![CDATA[
arXiv:2412.04905v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) enabled dialogue systems have become one of the central modes in human-machine interaction, which bring about vast amounts of conversation logs and increasing demand for dialogue generation. The dialogue's life-cycle spans from $\textit{Prelude}$ through $\textit{Interlocution}$ to $\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes of dialogue-related studies, there is a lack of systematic investigation into the dialogue stages to frame benchmark construction that covers comprehensive dialogue elements. This hinders the precise modeling, generation and assessment of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce a new research task--$\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. On this basis, we further build the DEMO agent with the adept ability to model dialogue elements via imitation learning. Extensive experiments on DEMO indicate that current representative LLMs still have considerable potential for enhancement, and our DEMO agent performs well in both dialogue element modeling and out-of-domain tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Probabilistic Vision-Language Models</title>
<link>https://arxiv.org/abs/2412.06014</link>
<guid>https://arxiv.org/abs/2412.06014</guid>
<content:encoded><![CDATA[
arXiv:2412.06014v3 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable success in classification, retrieval, and generative tasks. For this, VLMs deterministically map images and text descriptions to a joint latent space in which their similarity is assessed using the cosine similarity. However, a deterministic mapping of inputs fails to capture uncertainties over concepts arising from domain shifts when used in downstream tasks. In this work, we propose post-hoc uncertainty estimation in VLMs that does not require additional training. Our method leverages a Bayesian posterior approximation over the last layers in VLMs and analytically quantifies uncertainties over cosine similarities. We demonstrate its effectiveness for uncertainty quantification and support set selection in active learning. Compared to baselines, we obtain improved and well-calibrated predictive uncertainties, interpretable uncertainty estimates, and sample-efficient active learning. Our results show promise for safety-critical applications of large-scale models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm</title>
<link>https://arxiv.org/abs/2412.07223</link>
<guid>https://arxiv.org/abs/2412.07223</guid>
<content:encoded><![CDATA[
arXiv:2412.07223v5 Announce Type: replace-cross 
Abstract: This paper provides a unique approach with AI algorithms to predict emerging stock markets volatility. Traditionally, stock volatility is derived from historical volatility,Monte Carlo simulation and implied volatility as well. In this paper, the writer designs a consolidated model with back-propagation neural network and genetic algorithm to predict future volatility of emerging stock markets and found that the results are quite accurate with low errors.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space</title>
<link>https://arxiv.org/abs/2412.15032</link>
<guid>https://arxiv.org/abs/2412.15032</guid>
<content:encoded><![CDATA[
arXiv:2412.15032v2 Announce Type: replace-cross 
Abstract: This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to 512$\times$512 resolution without using the latent diffusion paradigm and beats latent diffusion (using SD-VAE) with only 1/4 training cost. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why 'image diffusion can be seen as spectral autoregression', bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is https://github.com/forever208/DCTdiff.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust random graph matching in Gaussian models via vector approximate message passing</title>
<link>https://arxiv.org/abs/2412.16457</link>
<guid>https://arxiv.org/abs/2412.16457</guid>
<content:encoded><![CDATA[
arXiv:2412.16457v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on the matching recovery problem between a pair of correlated Gaussian Wigner matrices with a latent vertex correspondence. We are particularly interested in a robust version of this problem such that our observation is a perturbed input $(A+E,B+F)$ where $(A,B)$ is a pair of correlated Gaussian Wigner matrices and $E,F$ are adversarially chosen matrices supported on an unknown $\epsilon n * \epsilon n$ principle minor of $A,B$, respectively. We propose a vector approximate message passing (vector AMP) algorithm that succeeds in polynomial time as long as the correlation $\rho$ between $(A,B)$ is a non-vanishing constant and $\epsilon = o\big( \tfrac{1}{(\log n)^{20}} \big)$.
  The main methodological inputs for our result are the iterative random graph matching algorithm proposed in \cite{DL22+, DL23+} and the spectral cleaning procedure proposed in \cite{IS24+}. To the best of our knowledge, our algorithm is the first efficient random graph matching type algorithm that is robust under any adversarial perturbations of $n^{1-o(1)}$ size.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?</title>
<link>https://arxiv.org/abs/2412.16772</link>
<guid>https://arxiv.org/abs/2412.16772</guid>
<content:encoded><![CDATA[
arXiv:2412.16772v2 Announce Type: replace-cross 
Abstract: The ongoing revolution in language modeling has led to various novel applications, some of which rely on the emerging social abilities of large language models (LLMs). Already, many turn to the new cyber friends for advice during the pivotal moments of their lives and trust them with the deepest secrets, implying that accurate shaping of the LLM's personality is paramount. To this end, state-of-the-art approaches exploit a vast variety of training data, and prompt the model to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e., make decisions when presented with a social situation) in line with the ascribed personality (ii) if their behavior can be finely controlled. We use classic psychological experiments, the Milgram experiment and the Ultimatum Game, as social interaction testbeds and apply personality prompting to open- and closed-source LLMs from 4 different vendors. Our experiments reveal failure modes of the prompt-based modulation of the models' behavior that are shared across all models tested and persist under prompt perturbations. These findings challenge the optimistic sentiment toward personality prompting generally held in the community.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Framework for Ranking LLM-Based Chatbots</title>
<link>https://arxiv.org/abs/2412.18407</link>
<guid>https://arxiv.org/abs/2412.18407</guid>
<content:encoded><![CDATA[
arXiv:2412.18407v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties -- an integral aspect of human-judged comparisons -- significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Causal Network Of Face-selective Regions In Human Brain During Movie Watching</title>
<link>https://arxiv.org/abs/2501.02333</link>
<guid>https://arxiv.org/abs/2501.02333</guid>
<content:encoded><![CDATA[
arXiv:2501.02333v2 Announce Type: replace-cross 
Abstract: Understanding the causal interactions in some brain tasks, such as face processing, remains a challenging and ambiguous process for researchers. In this study, we address this issue by employing a novel causal discovery method -Directed Acyclic Graphs via M-matrices for Acyclicity (DAGMA)- to investigate the causal structure of the brain's face-selective network and gain deeper insights into its mechanism. Using fMRI data of natural movie stimuli, we extract causal network of face-selective regions and analyze how frames containing faces influence this network. Specifically, our findings reveal that the presence of faces in the stimuli, causally affects the number of identified connections within the network. Additionally, our results highlight the crucial role of subcortical regions in satisfying causal sufficiency, emphasizing it's importance in causal studies of brain. This study provides a new perspective on understanding the causal architecture of the face-selective network of the brain, motivating further research on neural causality.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomy-of-Experts Models</title>
<link>https://arxiv.org/abs/2501.13074</link>
<guid>https://arxiv.org/abs/2501.13074</guid>
<content:encoded><![CDATA[
arXiv:2501.13074v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Pareto Optimality for the Multinomial Logistic Bandit</title>
<link>https://arxiv.org/abs/2501.19277</link>
<guid>https://arxiv.org/abs/2501.19277</guid>
<content:encoded><![CDATA[
arXiv:2501.19277v2 Announce Type: replace-cross 
Abstract: We provide a new online learning algorithm for tackling the Multinomial Logit Bandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial nature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based method that achieves Pareto optimality by balancing regret minimization and estimation error of the assortment revenues and the MNL parameters. We develop theoretical guarantees characterizing the tradeoff between regret and estimation error for the MNL-Bandit problem through information-theoretic bounds, and propose a modified UCB algorithm that incorporates forced exploration to improve parameter estimation accuracy while maintaining low regret. Our analysis sheds critical insights into how to optimally balance the collected revenues and the treatment estimation in dynamic assortment optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Quadratic Feature Analysis: Information Geometry Approach for Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2502.00168</link>
<guid>https://arxiv.org/abs/2502.00168</guid>
<content:encoded><![CDATA[
arXiv:2502.00168v3 Announce Type: replace-cross 
Abstract: Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Directly computing discriminability is often impractical, so an alternative approach is to learn features that maximize a distance or dissimilarity measure between classes. The Fisher-Rao distance is an important information geometry distance in statistical manifolds. It is induced by the Fisher information metric, a tool widely used for understanding neural representations. Despite its theoretical and pratical appeal, Fisher-Rao distances between classes have not been used as a maximization objective in supervised feature learning. Here, we present Supervised Quadratic Feature Analysis (SQFA), a linear dimensionality reduction method that maximizes Fisher-Rao distances between class distributions, by exploiting the information geometry of the symmetric positive definite manifold. SQFA maximizes distances using first- and second-order statistics, and its features allow for quadratic discriminability (i.e. QDA performance) matching or surpassing state-of-the-art methods on real-world datasets. We theoretically motivate Fisher-Rao distances as a proxy for quadratic discriminability, and compare its performance to other popular distances (e.g. Wasserstein distances). SQFA provides a flexible state-of-the-art method for dimensionality reduction. Its successful use of Fisher-Rao distances between classes motivates future research directions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment</title>
<link>https://arxiv.org/abs/2502.02017</link>
<guid>https://arxiv.org/abs/2502.02017</guid>
<content:encoded><![CDATA[
arXiv:2502.02017v2 Announce Type: replace-cross 
Abstract: Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</title>
<link>https://arxiv.org/abs/2502.03444</link>
<guid>https://arxiv.org/abs/2502.03444</guid>
<content:encoded><![CDATA[
arXiv:2502.03444v2 Announce Type: replace-cross 
Abstract: Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing limits for in-context learning</title>
<link>https://arxiv.org/abs/2502.03503</link>
<guid>https://arxiv.org/abs/2502.03503</guid>
<content:encoded><![CDATA[
arXiv:2502.03503v2 Announce Type: replace-cross 
Abstract: We examine limits of in-context learning (ICL) in transformer models trained from scratch, focusing on function approximation tasks as a controlled setting to uncover fundamental behaviors. While we show empirically that transformer models can generalize, approximating unseen classes of polynomial (non linear) functions, they cannot generalize beyond certain values. We provide both empirical and mathematical arguments explaining that these limitations stem from architectural components, namely layer normalization and the attention scoring function, softmax. Together, our findings reveal structural constraints on ICL that are often masked in more complex NLP tasks but that need to be understood to improve robustness and interpretability in transformer-based models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Imbalanced Annotations for Effective In-Context Learning</title>
<link>https://arxiv.org/abs/2502.04037</link>
<guid>https://arxiv.org/abs/2502.04037</guid>
<content:encoded><![CDATA[
arXiv:2502.04037v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. However, these datasets often exhibit long-tailed class distributions in real-world scenarios, leading to biased demonstration selection. In this work, we show that such class imbalances significantly degrade the ICL performance across various tasks, regardless of selection methods. Moreover, classical rebalancing methods, which focus solely on class weights, yield poor performance due to neglecting condition bias--skewed feature distributions within classes. To address this, we propose Reweighting with Conditional Bias (dubbed RCB), a simple and complementary approach to enhance ICL performance under class imbalance. In particular, RCB estimates conditional bias using a balanced subset and re-weights demonstration scores based on both class weight and conditional bias. In effect, RCB prevents over-selection from dominant classes while preserving the efficacy of current selection methods. Extensive experiments on common benchmarks demonstrate the effectiveness of our method, improving the average accuracy of current selection methods by up to 5.42%.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Oversight for Superhuman AI via Recursive Self-Critiquing</title>
<link>https://arxiv.org/abs/2502.04675</link>
<guid>https://arxiv.org/abs/2502.04675</guid>
<content:encoded><![CDATA[
arXiv:2502.04675v3 Announce Type: replace-cross 
Abstract: As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) \textit{Critique of critique can be easier than critique itself}, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) \textit{This difficulty relationship is recursively held}, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. We further conduct Human-AI and AI-AI experiments to investigate the potential of utilizing recursive self-critiquing for AI supervision. Our results highlight recursive critique as a promising approach for scalable AI oversight.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Otter: Generating Tests from Issues to Validate SWE Patches</title>
<link>https://arxiv.org/abs/2502.05368</link>
<guid>https://arxiv.org/abs/2502.05368</guid>
<content:encoded><![CDATA[
arXiv:2502.05368v2 Announce Type: replace-cross 
Abstract: While there has been plenty of work on generating tests from existing code, there has been limited work on generating tests from issues. A correct test must validate the code patch that resolves the issue. This paper focuses on the scenario where that code patch does not yet exist. Doing so supports two major use-cases. First, it supports TDD (test-driven development), the discipline of "test first, write code later" that has well-documented benefits for human software engineers. Second, it also validates SWE (software engineering) agents, which generate code patches for resolving issues. This paper introduces TDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an LLM-based solution for this task. Otter augments LLMs with rule-based analysis to check and repair their outputs, and introduces a novel self-reflective action planner. Experiments show Otter outperforming state-of-the-art systems for generating tests from issues, in addition to enhancing systems that generate patches from issues. We hope that Otter helps make developers more productive at resolving issues and leads to more robust, well-tested code.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features</title>
<link>https://arxiv.org/abs/2502.10682</link>
<guid>https://arxiv.org/abs/2502.10682</guid>
<content:encoded><![CDATA[
arXiv:2502.10682v2 Announce Type: replace-cross 
Abstract: Effective deepfake detection tools are becoming increasingly essential to the growing usage of deepfakes in unethical practices. There exists a wide range of deepfake generation techniques, which makes it challenging to develop an accurate universal detection mechanism. The 2025 IEEE Signal Processing Cup (\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images containing significant class imbalance. The images in the dataset are generated from multiple deepfake image generators, for training machine learning model(s) to emphasize the generalization of deepfake detection. To this end, we proposed a disjoint set-based multistage training method to address the class imbalance and devised an ensemble-based architecture \emph{CAE-Net}. Our architecture consists of a convolution- and attention-based ensemble network, and employs three different neural network architectures: EfficientNet, Data-Efficient Image Transformer (DeiT), and ConvNeXt with wavelet transform to capture both local and global features of deepfakes. We visualize the specific regions that these models focus on for classification using Grad-CAM, and empirically demonstrate the effectiveness of these models in grouping real and fake images into cohesive clusters using t-SNE plots. Individually, the EfficientNet B0 architecture has achieved 90.79\% accuracy, whereas the ConvNeXt and the DeiT architecture have achieved 89.49\% and 89.32\% accuracy, respectively. With these networks, our weighted ensemble model achieves an excellent accuracy of 94.63\% on the validation dataset of the SP Cup 2025 competition. The equal error rate of 4.72\% and the Area Under the ROC curve of 97.37\% further confirm the stability of our proposed method. Finally, the robustness of our proposed model against adversarial perturbation attacks is tested as well, showing the inherent defensive properties of the ensemble approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Making Agent Tools</title>
<link>https://arxiv.org/abs/2502.11705</link>
<guid>https://arxiv.org/abs/2502.11705</guid>
<content:encoded><![CDATA[
arXiv:2502.11705v2 Announce Type: replace-cross 
Abstract: Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BaxBench: Can LLMs Generate Correct and Secure Backends?</title>
<link>https://arxiv.org/abs/2502.11844</link>
<guid>https://arxiv.org/abs/2502.11844</guid>
<content:encoded><![CDATA[
arXiv:2502.11844v3 Announce Type: replace-cross 
Abstract: Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repo2Run: Automated Building Executable Environment for Code Repository at Scale</title>
<link>https://arxiv.org/abs/2502.13681</link>
<guid>https://arxiv.org/abs/2502.13681</guid>
<content:encoded><![CDATA[
arXiv:2502.13681v3 Announce Type: replace-cross 
Abstract: Scaling up executable code data is significant for improving language models' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Metric Structures of Deep Latent Variable Models</title>
<link>https://arxiv.org/abs/2502.13757</link>
<guid>https://arxiv.org/abs/2502.13757</guid>
<content:encoded><![CDATA[
arXiv:2502.13757v3 Announce Type: replace-cross 
Abstract: Deep latent variable models learn condensed representations of data that, hopefully, reflect the inner workings of the studied phenomena. Unfortunately, these latent representations are not statistically identifiable, meaning they cannot be uniquely determined. Domain experts, therefore, need to tread carefully when interpreting these. Current solutions limit the lack of identifiability through additional constraints on the latent variable model, e.g. by requiring labeled training data, or by restricting the expressivity of the model. We change the goal: instead of identifying the latent variables, we identify relationships between them such as meaningful distances, angles, and volumes. We prove this is feasible under very mild model conditions and without additional labeled data. We empirically demonstrate that our theory results in more reliable latent distances, offering a principled path forward in extracting trustworthy conclusions from deep latent variable models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.17821</link>
<guid>https://arxiv.org/abs/2502.17821</guid>
<content:encoded><![CDATA[
arXiv:2502.17821v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. However, in certain scenarios, particularly in resource-constrained environments, some modalities available during training may be absent during inference. While existing frameworks effectively utilize multiple data sources during training and enable inference with reduced modalities, they are primarily designed for single-agent settings. This poses a critical limitation in dynamic environments such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. Conversely, some works explore multi-agent collaboration but without addressing missing modality at test time. To overcome these limitations, we propose Collaborative Auxiliary Modality Learning (CAML), a novel multi-modal multi-agent framework that enables agents to collaborate and share multi-modal data during training, while allowing inference with reduced modalities during testing. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that CAML achieves up to a ${\bf 58.1}\%$ improvement in accident detection. Additionally, we validate CAML on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\bf 10.6}\%$ improvement in mIoU.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</title>
<link>https://arxiv.org/abs/2502.19668</link>
<guid>https://arxiv.org/abs/2502.19668</guid>
<content:encoded><![CDATA[
arXiv:2502.19668v2 Announce Type: replace-cross 
Abstract: Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a $\textbf{Su}$pervised $\textbf{Pre}$-training framework for $\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME is pre-trained using structured diagnostic labels derived from ECG report entities through a one-time offline extraction with Large Language Models (LLMs), which help denoise, standardize cardiac concepts, and improve clinical representation learning. By fusing ECG signals with textual cardiac queries instead of fixed labels, SuPreME enables zero-shot classification of unseen conditions without further fine-tuning. We evaluate SuPreME on six downstream datasets covering 106 cardiac conditions, achieving superior zero-shot AUC performance of $77.20\%$, surpassing state-of-the-art eSSLs by $4.98\%$. Results demonstrate SuPreME's effectiveness in leveraging structured, clinically relevant knowledge for high-quality ECG representations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders</title>
<link>https://arxiv.org/abs/2503.00131</link>
<guid>https://arxiv.org/abs/2503.00131</guid>
<content:encoded><![CDATA[
arXiv:2503.00131v3 Announce Type: replace-cross 
Abstract: We demonstrate transfer learning capabilities in a machine-learned algorithm trained for particle-flow reconstruction in high energy particle colliders. This paper presents a cross-detector fine-tuning study, where we initially pretrain the model on a large full simulation dataset from one detector design, and subsequently fine-tune the model on a sample with a different collider and detector design. Specifically, we use the Compact Linear Collider detector (CLICdet) model for the initial training set and demonstrate successful knowledge transfer to the CLIC-like detector (CLD) proposed for the Future Circular Collider in electron-positron mode. We show that with an order of magnitude less samples from the second dataset, we can achieve the same performance as a costly training from scratch, across particle-level and event-level performance metrics, including jet and missing transverse momentum resolution. Furthermore, we find that the fine-tuned model achieves comparable performance to the traditional rule-based particle-flow approach on event-level metrics after training on 100,000 CLD events, whereas a model trained from scratch requires at least 1 million CLD events to achieve similar reconstruction performance. To our knowledge, this represents the first full-simulation cross-detector transfer learning study for particle-flow reconstruction. These findings offer valuable insights towards building large foundation models that can be fine-tuned across different detector designs and geometries, helping to accelerate the development cycle for new detectors and opening the door to rapid detector design and optimization using machine learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approaching the Harm of Gradient Attacks While Only Flipping Labels</title>
<link>https://arxiv.org/abs/2503.00140</link>
<guid>https://arxiv.org/abs/2503.00140</guid>
<content:encoded><![CDATA[
arXiv:2503.00140v2 Announce Type: replace-cross 
Abstract: Machine learning systems deployed in distributed or federated environments are highly susceptible to adversarial manipulations, particularly availability attacks -adding imperceptible perturbations to training data, thereby rendering the trained model unavailable. Prior research in distributed machine learning has demonstrated such adversarial effects through the injection of gradients or data poisoning. In this study, we aim to enhance comprehension of the potential of weaker (and more probable) adversaries by posing the following inquiry: Can availability attacks be inflicted solely through the flipping of a subset of training labels, without altering features, and under a strict flipping budget? We analyze the extent of damage caused by constrained label flipping attacks. Focusing on a distributed classification problem, (1) we propose a novel formalization of label flipping attacks on logistic regression models and derive a greedy algorithm that is provably optimal at each training step. (2) To demonstrate that availability attacks can be approached by label flipping alone, we show that a budget of only $0.1\%$ of labels at each training step can reduce the accuracy of the model by $6\%$, and that some models can perform worse than random guessing when up to $25\%$ of labels are flipped. (3) We shed light on an interesting interplay between what the attacker gains from more write-access versus what they gain from more flipping budget. (4) we define and compare the power of targeted label flipping attack to that of an untargeted label flipping attack.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiLTra-Bench: The Swiss Legal Translation Benchmark</title>
<link>https://arxiv.org/abs/2503.01372</link>
<guid>https://arxiv.org/abs/2503.01372</guid>
<content:encoded><![CDATA[
arXiv:2503.01372v2 Announce Type: replace-cross 
Abstract: In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
<link>https://arxiv.org/abs/2503.01606</link>
<guid>https://arxiv.org/abs/2503.01606</guid>
<content:encoded><![CDATA[
arXiv:2503.01606v2 Announce Type: replace-cross 
Abstract: Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks</title>
<link>https://arxiv.org/abs/2503.04378</link>
<guid>https://arxiv.org/abs/2503.04378</guid>
<content:encoded><![CDATA[
arXiv:2503.04378v2 Announce Type: replace-cross 
Abstract: Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs</title>
<link>https://arxiv.org/abs/2503.07067</link>
<guid>https://arxiv.org/abs/2503.07067</guid>
<content:encoded><![CDATA[
arXiv:2503.07067v2 Announce Type: replace-cross 
Abstract: Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis</title>
<link>https://arxiv.org/abs/2503.11101</link>
<guid>https://arxiv.org/abs/2503.11101</guid>
<content:encoded><![CDATA[
arXiv:2503.11101v3 Announce Type: replace-cross 
Abstract: Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of "positive" and "negative" samples, where positive pairs (e.g., variation of the same image/object) are brought together in the embedding space, and negative pairs (e.g., views from different images/objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Relativity of Causal Knowledge</title>
<link>https://arxiv.org/abs/2503.11718</link>
<guid>https://arxiv.org/abs/2503.11718</guid>
<content:encoded><![CDATA[
arXiv:2503.11718v2 Announce Type: replace-cross 
Abstract: Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment</title>
<link>https://arxiv.org/abs/2503.18991</link>
<guid>https://arxiv.org/abs/2503.18991</guid>
<content:encoded><![CDATA[
arXiv:2503.18991v3 Announce Type: replace-cross 
Abstract: Robust alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based -- training a reward model on preference pairs and optimizing with reinforcement learning (RL) -- or reward-free -- directly fine-tuning on ranked outputs. Recent research shows that well-tuned reward-based pipelines remain the most robust, and single-response demonstrations can outperform pairwise preference data. However, two key challenges remain: (i) imbalanced safety datasets that over-represent common hazards while neglecting long-tail threats; and (ii) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. To address these limitations, we propose \textbf{DR-IRL}, which dynamically adjusts rewards through inverse reinforcement learning. We first construct a balanced safety dataset of seven harmful categories using Chain-of-Draft (CoD) template prompts, which reduce token usage and generation time compared to Chain-of-Thought (CoT). We then train category-specific reward models on this dataset via IRL. Finally, to align the LLM, we introduce \textbf{GRPO-S} (Group Relative Policy Optimization--Scaling), a variant of GRPO that scales the reward during optimization to task difficulty -- data-level hardness measured by CLIP similarity and model-level responsiveness measured by reward gaps. Extensive experiments on multiple benchmarks and LLMs demonstrate that DR-IRL outperforms all baselines in safety alignment while maintaining usefulness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Benchmarking with LLaMA2: Evaluating Code Development Performance Across Multiple Programming Languages</title>
<link>https://arxiv.org/abs/2503.19217</link>
<guid>https://arxiv.org/abs/2503.19217</guid>
<content:encoded><![CDATA[
arXiv:2503.19217v2 Announce Type: replace-cross 
Abstract: The rapid evolution of large language models (LLMs) has opened new possibilities for automating various tasks in software development. This paper evaluates the capabilities of the Llama 2-70B model in automating these tasks for scientific applications written in commonly used programming languages. Using representative test problems, we assess the model's capacity to generate code, documentation, and unit tests, as well as its ability to translate existing code between commonly used programming languages. Our comprehensive analysis evaluates the compilation, runtime behavior, and correctness of the generated and translated code. Additionally, we assess the quality of automatically generated code, documentation and unit tests. Our results indicate that while Llama 2-70B frequently generates syntactically correct and functional code for simpler numerical tasks, it encounters substantial difficulties with more complex, parallelized, or distributed computations, requiring considerable manual corrections. We identify key limitations and suggest areas for future improvements to better leverage AI-driven automation in scientific computing workflows.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative Recommender</title>
<link>https://arxiv.org/abs/2504.10545</link>
<guid>https://arxiv.org/abs/2504.10545</guid>
<content:encoded><![CDATA[
arXiv:2504.10545v2 Announce Type: replace-cross 
Abstract: Recent advances in recommender systems have underscored the complementary strengths of generative modeling and pretrained language models. We propose HSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.
  We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the Amazon Reviews 2023 dataset and the Steam dataset. We compare its performance against both the original HSTU-based recommender and a variant augmented with embeddings from OpenAI's state-of-the-art \texttt{text-embedding-3-large} model. Despite the latter being trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- achieves better performance in nearly all cases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant on all but one metric, where it is marginally lower, and matches it on another. These findings highlight the effectiveness of contrastive text embeddings in compute-efficient recommendation settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing</title>
<link>https://arxiv.org/abs/2504.13883</link>
<guid>https://arxiv.org/abs/2504.13883</guid>
<content:encoded><![CDATA[
arXiv:2504.13883v2 Announce Type: replace-cross 
Abstract: This study estimates cognitive effort (CE) based on functional near-infrared spectroscopy (fNIRS) data and performance scores using a hybrid deep learning model. The estimation of CE enables educators to modify material to enhance learning effectiveness and student engagement. Relative neural efficiency (RNE) and relative neural involvement (RNI) are two metrics that have been used to represent CE. To estimate RNE and RNI we need hemodynamic response in the brain and the performance score of a task.We collected oxygenated hemoglobin ($\Delta \mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based educational game, each with a 30-second response time. We used deep learning models to predict the performance score and estimate RNE and RNI to understand CE. The study compares traditional machine learning techniques with deep learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine which approach provides better accuracy in predicting performance scores. The result shows that the hybrid CNN-GRU gives better performance with 78.36\% training accuracy and 73.08\% test accuracy than other models. We performed XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%). This suggests that the features learned from this hybrid model generalize better even in traditional machine learning algorithms. We used the $\Delta \mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive effort in our four test cases. Our result shows that even with moderate accuracy, the predicted RNE and RNI closely follows the actual trends. we also observed that when participants were in a state of high CE, introducing rest led decrease of CE. These findings can be helpful to design and improve learning environments and provide valuable insights in learning materials.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns</title>
<link>https://arxiv.org/abs/2504.15815</link>
<guid>https://arxiv.org/abs/2504.15815</guid>
<content:encoded><![CDATA[
arXiv:2504.15815v2 Announce Type: replace-cross 
Abstract: Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
<link>https://arxiv.org/abs/2505.02746</link>
<guid>https://arxiv.org/abs/2505.02746</guid>
<content:encoded><![CDATA[
arXiv:2505.02746v2 Announce Type: replace-cross 
Abstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
arXiv:2505.05528v3 Announce Type: replace-cross 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition</title>
<link>https://arxiv.org/abs/2505.08052</link>
<guid>https://arxiv.org/abs/2505.08052</guid>
<content:encoded><![CDATA[
arXiv:2505.08052v2 Announce Type: replace-cross 
Abstract: This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics</title>
<link>https://arxiv.org/abs/2505.12583</link>
<guid>https://arxiv.org/abs/2505.12583</guid>
<content:encoded><![CDATA[
arXiv:2505.12583v2 Announce Type: replace-cross 
Abstract: Recent Foundation Model-enabled robotics (FMRs) display greatly improved general-purpose skills, enabling more adaptable automation than conventional robotics. Their ability to handle diverse tasks thus creates new opportunities to replace human labor. However, unlike general foundation models, FMRs interact with the physical world, where their actions directly affect the safety of humans and surrounding objects, requiring careful deployment and control. Based on this proposition, our survey comprehensively summarizes robot control approaches to mitigate physical risks by covering all the lifespan of FMRs ranging from pre-deployment to post-accident stage. Specifically, we broadly divide the timeline into the following three phases: (1) pre-deployment phase, (2) pre-incident phase, and (3) post-incident phase. Throughout this survey, we find that there is much room to study (i) pre-incident risk mitigation strategies, (ii) research that assumes physical interaction with humans, and (iii) essential issues of foundation models themselves. We hope that this survey will be a milestone in providing a high-resolution analysis of the physical risks of FMRs and their control, contributing to the realization of a good human-robot relationship.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators</title>
<link>https://arxiv.org/abs/2505.14314</link>
<guid>https://arxiv.org/abs/2505.14314</guid>
<content:encoded><![CDATA[
arXiv:2505.14314v2 Announce Type: replace-cross 
Abstract: Attention mechanisms, particularly within Transformer architectures and large language models (LLMs), have revolutionized sequence modeling in machine learning and artificial intelligence applications. To compute attention for increasingly long sequences, specialized accelerators have been proposed to execute key attention steps directly in hardware. Among the various recently proposed architectures, those based on variants of the FlashAttention algorithm, originally designed for GPUs, stand out due to their optimized computation, tiling capabilities, and reduced memory traffic. In this work, we focus on optimizing the kernel of floating-point-based FlashAttention using new hardware operators that fuse the computation of exponentials and vector multiplications, e.g., e^x, V. The proposed ExpMul hardware operators significantly reduce the area and power costs of FlashAttention-based hardware accelerators. When implemented in a 28nm ASIC technology, they achieve improvements of 28.8% in area and 17.6% in power, on average, compared to state-of-the-art hardware architectures with separate exponentials and vector multiplications hardware operators.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Curriculum for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14970</link>
<guid>https://arxiv.org/abs/2505.14970</guid>
<content:encoded><![CDATA[
arXiv:2505.14970v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Alignment Can Be Not Superficial With Explicit Safety Signals</title>
<link>https://arxiv.org/abs/2505.17072</link>
<guid>https://arxiv.org/abs/2505.17072</guid>
<content:encoded><![CDATA[
arXiv:2505.17072v2 Announce Type: replace-cross 
Abstract: Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</title>
<link>https://arxiv.org/abs/2505.17779</link>
<guid>https://arxiv.org/abs/2505.17779</guid>
<content:encoded><![CDATA[
arXiv:2505.17779v2 Announce Type: replace-cross 
Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
<link>https://arxiv.org/abs/2505.17836</link>
<guid>https://arxiv.org/abs/2505.17836</guid>
<content:encoded><![CDATA[
arXiv:2505.17836v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model</title>
<link>https://arxiv.org/abs/2505.17917</link>
<guid>https://arxiv.org/abs/2505.17917</guid>
<content:encoded><![CDATA[
arXiv:2505.17917v2 Announce Type: replace-cross 
Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous indirect and total treatment effects and identifying relevant subgroups within a mediation framework. The procedure comprises four key steps. First, we compute individual-level conditional average indirect/total treatment effect Second, we construct a distance matrix based on pairwise differences. Third, we apply tSNE to project this matrix into a low-dimensional Euclidean space, followed by K-means clustering to identify subgroup structures. Finally, we calibrate and refine the clusters using a threshold-based procedure to determine the optimal configuration. To the best of our knowledge, this is the first approach specifically designed to capture treatment effect heterogeneity in the presence of mediation. Experimental results validate the robustness and effectiveness of the proposed framework. Application to the real-world Jobs II dataset highlights the broad adaptability and potential applicability of our method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models</title>
<link>https://arxiv.org/abs/2505.19240</link>
<guid>https://arxiv.org/abs/2505.19240</guid>
<content:encoded><![CDATA[
arXiv:2505.19240v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. While prior reviews have addressed these issues, they often focus on individual limitations or consider them within the broader context of evaluating overall model performance. This survey addresses the gap by presenting a data-driven, semi-automated review of research on limitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation papers using keyword filtering and LLM-based classification, validated against expert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM), we identify between 7 and 15 prominent types of limitations discussed in recent LLM research across the ACL and arXiv datasets. We find that LLM-related research increases nearly sixfold in ACL and nearly fifteenfold in arXiv between 2022 and 2025, while LLLMs research grows even faster, by a factor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2025. We offer a quantitative view of trends in LLM limitations research and release a dataset of annotated abstracts and a validated methodology, available at: https://github.com/a-kostikova/LLLMs-Survey.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures</title>
<link>https://arxiv.org/abs/2505.19521</link>
<guid>https://arxiv.org/abs/2505.19521</guid>
<content:encoded><![CDATA[
arXiv:2505.19521v2 Announce Type: replace-cross 
Abstract: Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations</title>
<link>https://arxiv.org/abs/2505.21356</link>
<guid>https://arxiv.org/abs/2505.21356</guid>
<content:encoded><![CDATA[
arXiv:2505.21356v3 Announce Type: replace-cross 
Abstract: Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders by providing standardized evaluations of vocal function. Traditionally, expert raters use standard scales such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. This study proposes Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to extract high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we also introduce VOQANet+, which integrates low-level speech descriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings into a hybrid representation. Unlike prior studies focused only on vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level speech (PVQD-S subset) to improve generalizability. Results show that sentence-based input outperforms vowel-based input, especially at the patient level, underscoring the value of longer utterances for capturing perceptual voice attributes. VOQANet consistently surpasses baseline methods in root mean squared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even better performance. Additional experiments under noisy conditions show that VOQANet+ maintains high prediction accuracy and robustness, supporting its potential for real-world and telehealth deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models</title>
<link>https://arxiv.org/abs/2503.13503</link>
<guid>https://arxiv.org/abs/2503.13503</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, AI4Science, Data Quality, Model Capability

Summary: 
SciHorizon is a proposed comprehensive assessment framework for evaluating AI4Science readiness from both scientific data and LLM perspectives. It includes a generalizable framework for assessing AI-ready scientific data, comprising dimensions such as Quality, FAIRness, Explainability, and Compliance. The study recommends AI-ready datasets for Earth, Life, and Materials Sciences based on data resource papers. Additionally, the framework evaluates the capabilities of LLMs across various scientific disciplines using 16 assessment dimensions and five core indicators. The assessment covers Mathematics, Physics, Chemistry, Life Sciences, Earth, and Space Sciences, with over 50 LLMs evaluated using benchmark datasets. All results are publicly available online at www.scihorizon.cn/en. 

<br /><br />Summary: SciHorizon proposes a comprehensive framework for assessing AI4Science readiness, evaluating AI-ready scientific data and LLM capabilities. Recommendations for datasets in Earth, Life, and Materials Sciences are provided, along with an evaluation of over 50 LLMs across multiple scientific disciplines. <div>
arXiv:2503.13503v3 Announce Type: replace 
Abstract: In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18447</link>
<guid>https://arxiv.org/abs/2505.18447</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, transfer learning, policy, performance guarantees, negative transfer

Summary:
The paper introduces a novel framework for transfer reinforcement learning that addresses challenges such as the lack of performance guarantees and the risk of negative transfer. The framework is based on the pessimism principle and constructs a conservative estimation of the target domain's performance. This approach ensures safe and reliable decision-making while also avoiding negative transfer when multiple source domains are involved. Two types of conservative estimations are developed, and their effectiveness is rigorously characterized. Efficient distributed algorithms with convergence guarantees are proposed. The framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning. <div>
arXiv:2505.18447v2 Announce Type: replace 
Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a target environment with limited data by leveraging abundant data from related source domains. However, it faces two key challenges: the lack of performance guarantees for the transferred policy, which can lead to undesired actions, and the risk of negative transfer when multiple source domains are involved. We propose a novel framework based on the pessimism principle, which constructs and optimizes a conservative estimation of the target domain's performance. Our framework effectively addresses the two challenges by providing an optimized lower bound on target performance, ensuring safe and reliable decisions, and by exhibiting monotonic improvement with respect to the quality of the source domains, thereby avoiding negative transfer. We construct two types of conservative estimations, rigorously characterize their effectiveness, and develop efficient distributed algorithms with convergence guarantees. Our framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
<div> dataset, model suite, GPU code transpilation, assembly-level translation, domain-specific language models
<br />
Summary:<br />
The paper introduces CASS, a dataset and model suite for GPU code transpilation across architectures. The dataset contains 70k verified code pairs for source-level and assembly-level translation. The CASS family of domain-specific language models achieves high accuracy in source and assembly translation, surpassing commercial baselines. The generated code maintains native performance in the majority of test cases. A benchmark, CASS-Bench, with 16 GPU domains and ground-truth execution is introduced for evaluation. All data, models, and tools are open-sourced to encourage advancements in GPU compiler tooling, binary compatibility, and hardware translation guided by large language models.
<br /> <div>
arXiv:2505.16968v3 Announce Type: replace-cross 
Abstract: We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA <--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20569</link>
<guid>https://arxiv.org/abs/2505.20569</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Hallucination, Vision-Language Models, Contrastive Decoding, Retrieval Visual Contrastive Decoding, Logit Level<br />
<br />
Summary: 
The article introduces RVCD (Retrieval Visual Contrastive Decoding) as a method to address the challenge of Object Hallucination in Large Vision-Language Models. RVCD builds upon previous studies on contrastive decoding and leverages both negative and positive images at the logit level to suppress OH. By referencing AI-generated images representing a single concept, RVCD demonstrates significant improvements over existing decoding-based methods. This advanced approach offers a promising solution to the persistence of OH in vision-language models, enhancing the model's ability to accurately interpret and generate visual content. <div>
arXiv:2505.20569v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data</title>
<link>https://arxiv.org/abs/2505.20688</link>
<guid>https://arxiv.org/abs/2505.20688</guid>
<content:encoded><![CDATA[
<div> FDR control, neuroimaging, spatial dependencies, statistical methods, Alzheimer's Disease Neuroimaging Initiative<br />
<br />
Summary: 
The article proposes a novel spatial FDR control method, fcHMRF-LIS, for voxel-wise multiple testing in neuroimaging data analysis. By integrating the LIS-based testing procedure with a fully connected hidden Markov random field (fcHMRF), the method effectively captures complex spatial dependencies. It addresses three key challenges in neuroimaging applications: maintaining low variability in false discovery proportion and false non-discovery proportion, achieving computational scalability for high-resolution data, and accurately controlling FDR while reducing false non-discovery rates. The proposed method outperforms existing approaches in terms of accuracy and computational efficiency, as demonstrated through extensive simulations and application to an Alzheimer's disease dataset from the ADNI. The efficient expectation-maximization algorithm and innovative techniques such as CRF-RNN and permutohedral lattice filtering contribute to the method's success in accurately identifying neurobiologically relevant brain regions. <br /><br /> <div>
arXiv:2505.20688v2 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short of jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the time complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation</title>
<link>https://arxiv.org/abs/2505.20745</link>
<guid>https://arxiv.org/abs/2505.20745</guid>
<content:encoded><![CDATA[
<div> acoustics, representation models, auscultation, heart rate estimation, pre-training

Summary:
- The study investigates the encoding of auscultation in self-supervised acoustic representation foundation models (FMs) for vital sign analysis.
- Six FMs, including HuBERT, wav2vec2, wavLM, Whisper, CLAP, and an in-house CLAP model, are evaluated using a phonocardiogram dataset and heart rate estimation model.
- Results show that FMs offer comparable performance to a baseline method relying on acoustic features for heart rate estimation.
- The in-house CLAP model's audio encoder outperforms the baseline, achieving a lower mean absolute error despite domain mismatch.
- This research demonstrates the potential of pre-trained FMs in enhancing heart sound analysis and vital sign monitoring. 

<br /><br />Summary: <div>
arXiv:2505.20745v2 Announce Type: replace-cross 
Abstract: Auscultation, particularly heart sound, is a non-invasive technique that provides essential vital sign information. Recently, self-supervised acoustic representation foundation models (FMs) have been proposed to offer insights into acoustics-based vital signs. However, there has been little exploration of the extent to which auscultation is encoded in these pre-trained FM representations. In this work, using a publicly available phonocardiogram (PCG) dataset and a heart rate (HR) estimation model, we conduct a layer-wise investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM, Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP model. Additionally, we implement the baseline method from Nie et al., 2024 (which relies on acoustic features) and show that overall, representation vectors from pre-trained foundation models (FMs) offer comparable performance to the baseline. Notably, HR estimation using the representations from the audio encoder of the in-house CLAP model outperforms the results obtained from the baseline, achieving a lower mean absolute error (MAE) across various train/validation/test splits despite the domain mismatch.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection</title>
<link>https://arxiv.org/abs/2505.20956</link>
<guid>https://arxiv.org/abs/2505.20956</guid>
<content:encoded><![CDATA[
<div> Active learning, Bioacoustic sound event detection, Mismatch-first farthest-traversal, Committee voting disagreement, Diversity analysis<br />
Summary:<br />
Bioacoustic sound event detection (BioSED) is essential for biodiversity conservation but faces challenges like limited annotated data, sparse events, and species diversity. This study introduces the mismatch-first farthest-traversal (MFFT) method for efficient model development with limited labeling budget. The method combines committee voting disagreement and diversity analysis and achieves a mean average precision (mAP) of 68% in cold-start scenarios and 71% in warm-start scenarios with only 2.3% of annotations. This performance is close to fully-supervised results. MFFT excels in detecting rare species, crucial for monitoring endangered species. The study also refines a BioSED dataset for active learning algorithm evaluation, highlighting MFFT's practical value in BioSED tasks. <div>
arXiv:2505.20956v2 Announce Type: replace-cross 
Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeSocial: Blockchain-based Decentralized Social Networks</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
<div> Decentralized, social network, learning framework, blockchain, user-driven <br />
<br />
Summary: The research introduces DeSocial, a decentralized social network learning framework deployed on an Ethereum local development chain. It gives users the ability to choose algorithms for personalized social predictions in a blockchain environment. Users can evaluate multiple backbone models on their local subgraph, select the most suitable backbone for personalized prediction, and aggregate prediction results through majority voting from validation nodes. The framework shows significant improvement compared to centralized models, emphasizing user empowerment in blockchain-based decentralized social networks. Multi-node validation and personalized algorithm selection based on blockchain play a key role in enhancing social predictions in a decentralized setting. The implementation of the framework is available on GitHub for further exploration and development. <br /> <div>
arXiv:2505.21388v2 Announce Type: replace-cross 
Abstract: Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs</title>
<link>https://arxiv.org/abs/2505.22686</link>
<guid>https://arxiv.org/abs/2505.22686</guid>
<content:encoded><![CDATA[
<div> LSTM, GRU, BiLSTM, BiGRU, KAN, TKAN, SiLU, GeLU, MiSH, tropical Africa, weather forecasting, deep recurrent neural networks, temperature, precipitation, pressure<br />
<br />
Summary:<br />
- Study benchmarks deep neural networks for weather forecasting in tropical Africa.
- Models evaluated on temperature, precipitation, and pressure data from Abidjan and Kigali.
- KAN shows high accuracy in temperature prediction.
- TKAN variants improve precipitation forecasting in low-rainfall regions.
- RNNs outperform KAN models for atmospheric pressure prediction.
- Spline-based neural architectures offer efficient and data-efficient forecasting capabilities. <div>
arXiv:2505.22686v1 Announce Type: new 
Abstract: Weather forecasting is crucial for managing risks and economic planning, particularly in tropical Africa, where extreme events severely impact livelihoods. Yet, existing forecasting methods often struggle with the region's complex, non-linear weather patterns. This study benchmarks deep recurrent neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily forecasting of temperature, precipitation, and pressure in two tropical cities: Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce two customized variants of $ \texttt{TKAN}$ that replace its original $\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH}, respectively. Using station-level meteorological data spanning from 2010 to 2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$ achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali, $\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize absolute errors for precipitation forecasting in low-rainfall regimes. The customized $\texttt{TKAN}$ models demonstrate improvements over the standard $\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming $\texttt{KAN}$-based models in this task. These results highlight the potential of spline-based neural architectures for efficient and data-efficient forecasting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlimLLM: Accurate Structured Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.22689</link>
<guid>https://arxiv.org/abs/2505.22689</guid>
<content:encoded><![CDATA[
<div> structured pruning, large language models, SlimLLM, channel pruning, attention head pruning 

Summary: 
- Large language models (LLMs) are powerful but computationally expensive, limiting their deployment.
- Structured pruning is an effective way to compress LLM parameters, but determining importance and minimizing performance loss are key challenges.
- SlimLLM is a new structured pruning method for LLMs that evaluates importance based on entire channels or attention heads.
- The approach considers interdependence among elements within sub-modules, improving pruning effectiveness.
- A linear regression strategy for the output matrix enables quick performance recovery and a layer-based importance ratio determines the pruning ratio for each layer.
- SlimLLM outperforms other methods and achieves state-of-the-art performance on the LLaMA benchmark. 

<br /><br />Summary: <div>
arXiv:2505.22689v1 Announce Type: new 
Abstract: Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.22694</link>
<guid>https://arxiv.org/abs/2505.22694</guid>
<content:encoded><![CDATA[
<div> Parameter-Efficient Fine-Tuning, Large Language Models, Low-Rank Adaptation, Mixture of Low-Rank Experts, Multi-task scenarios
Summary:
In this study, the authors propose a novel approach, Mixture of Low-Rank Experts (MoRE), for multi-task Parameter-Efficient Fine-Tuning of Large Language Models (LLMs). MoRE aligns different ranks of Low-Rank Adaptation (LoRA) modules with different tasks, creating low-rank experts, and utilizes an adaptive rank selector to choose the appropriate expert for each task. By jointly training these experts, MoRE enhances the adaptability and efficiency of LoRA in multi-task scenarios. Extensive experiments on various multi-task benchmarks with different LLMs show that MoRE outperforms traditional LoRA and its variants in multi-task settings without increasing inference cost. The released model and code aim to benefit the research community by facilitating further exploration of efficient fine-tuning methods for LLMs.<br /><br />Summary: <div>
arXiv:2505.22694v1 Announce Type: new 
Abstract: With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aims to achieve efficient fine-tuning of LLMs with fewer parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA) introduces low-rank matrices to approximate the incremental tuning parameters and achieves impressive performance over multiple scenarios. After that, plenty of improvements have been proposed for further improvement. However, these methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting the efficiency and effectiveness of LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for multi-task PEFT. Specifically, instead of using an individual LoRA for each task, we align different ranks of LoRA module with different tasks, which we named low-rank experts. Moreover, we design a novel adaptive rank selector to select the appropriate expert for each task. By jointly training low-rank experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task scenarios. Finally, we conduct extensive experiments over multiple multi-task benchmarks along with different LLMs to verify model performance. Experimental results demonstrate that compared to traditional LoRA and its variants, MoRE significantly improves the performance of LLMs in multi-task scenarios and incurs no additional inference cost. We also release the model and code to facilitate the community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning</title>
<link>https://arxiv.org/abs/2505.22695</link>
<guid>https://arxiv.org/abs/2505.22695</guid>
<content:encoded><![CDATA[
<div> Keywords: ride-hailing, optimization, fairness, language models, intelligent transportation systems

Summary: 
The article introduces the LLM-ODDR framework, leveraging Large Language Models for optimizing order dispatching and driver repositioning in ride-hailing platforms. It addresses challenges in driver income fairness, interpretability, and adaptability to real-world dynamics. The framework includes components for Multi-objective-guided Order Value Refinement, Fairness-aware Order Dispatching, and Spatiotemporal Demand-Aware Driver Repositioning. The JointDR-GPT model is developed for ODDR tasks with domain knowledge. Experiments with real-world data from Manhattan taxi operations show significant performance improvements compared to traditional methods. This is the first exploration of using LLMs as decision-making agents in ride-hailing ODDR tasks, providing foundational insights for integrating advanced language models into intelligent transportation systems.<br /><br />Summary: <div>
arXiv:2505.22695v1 Announce Type: new 
Abstract: Ride-hailing platforms face significant challenges in optimizing order dispatching and driver repositioning operations in dynamic urban environments. Traditional approaches based on combinatorial optimization, rule-based heuristics, and reinforcement learning often overlook driver income fairness, interpretability, and adaptability to real-world dynamics. To address these gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models (LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in ride-hailing services. LLM-ODDR framework comprises three key components: (1) Multi-objective-guided Order Value Refinement, which evaluates orders by considering multiple objectives to determine their overall value; (2) Fairness-aware Order Dispatching, which balances platform revenue with driver income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning, which optimizes idle vehicle placement based on historical patterns and projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for ODDR tasks with domain knowledge. Extensive experiments on real-world datasets from Manhattan taxi operations demonstrate that our framework significantly outperforms traditional methods in terms of effectiveness, adaptability to anomalous conditions, and decision interpretability. To our knowledge, this is the first exploration of LLMs as decision-making agents in ride-hailing ODDR tasks, establishing foundational insights for integrating advanced language models within intelligent transportation systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?</title>
<link>https://arxiv.org/abs/2505.22696</link>
<guid>https://arxiv.org/abs/2505.22696</guid>
<content:encoded><![CDATA[
<div> transfer learning, reinforcement learning, neuroevolution, curriculum, robotics

Summary:
Neuroevolution (NE) is investigated for its transfer learning capabilities, with two benchmarks introduced: stepping gates and ecorobot. Stepping gates task neural networks with emulating logic circuits, while ecorobot extends the Brax physics engine with obstacles and different robotic morphologies. A curriculum is present in both benchmarks to evaluate skill transfer across tasks of increasing complexity. The empirical analysis shows that NE methods vary in their transfer abilities and often outperform RL baselines. These findings support the potential of NE as a foundation for building adaptable agents and highlight future challenges for scaling NE to complex, real-world problems.
<br /><br />Summary: <div>
arXiv:2505.22696v1 Announce Type: new 
Abstract: The ability to continuously and efficiently transfer skills across tasks is a hallmark of biological intelligence and a long-standing goal in artificial systems. Reinforcement learning (RL), a dominant paradigm for learning in high-dimensional control tasks, is known to suffer from brittleness to task variations and catastrophic forgetting. Neuroevolution (NE) has recently gained attention for its robustness, scalability, and capacity to escape local optima. In this paper, we investigate an understudied dimension of NE: its transfer learning capabilities. To this end, we introduce two benchmarks: a) in stepping gates, neural networks are tasked with emulating logic circuits, with designs that emphasize modular repetition and variation b) ecorobot extends the Brax physics engine with objects such as walls and obstacles and the ability to easily switch between different robotic morphologies. Crucial in both benchmarks is the presence of a curriculum that enables evaluating skill transfer across tasks of increasing complexity. Our empirical analysis shows that NE methods vary in their transfer abilities and frequently outperform RL baselines. Our findings support the potential of NE as a foundation for building more adaptable agents and highlight future challenges for scaling NE to complex, real-world problems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Update Your Transformer to the Latest Release: Re-Basin of Task Vectors</title>
<link>https://arxiv.org/abs/2505.22697</link>
<guid>https://arxiv.org/abs/2505.22697</guid>
<content:encoded><![CDATA[
<div> transfer learning, fine-tuning, foundation models, data-free transfer, Transformer models 
Summary: 
- The study explores the possibility of transferring fine-tuning to a new version of a pretrained model without retraining.
- The authors propose a methodology based on weight permutations and model re-basin principles to re-base modifications made to the original base model.
- The approach is tailored for Transformer models, addressing challenges such as residual connections and multi-head attention layers.
- A two-level method rooted in spectral theory is introduced, involving attention heads permutation and parameter adjustment within selected pairs of heads.
- Extensive experiments on visual and textual tasks demonstrate successful transfer of fine-tuned knowledge to new pre-trained backbones without the need for additional training data. 
<br /><br />Summary: <div>
arXiv:2505.22697v1 Announce Type: new 
Abstract: Foundation models serve as the backbone for numerous specialized models developed through fine-tuning. However, when the underlying pretrained model is updated or retrained (e.g., on larger and more curated datasets), the fine-tuned model becomes obsolete, losing its utility and requiring retraining. This raises the question: is it possible to transfer fine-tuning to a new release of the model? In this work, we investigate how to transfer fine-tuning to a new checkpoint without having to re-train, in a data-free manner. To do so, we draw principles from model re-basin and provide a recipe based on weight permutations to re-base the modifications made to the original base model, often called task vector. In particular, our approach tailors model re-basin for Transformer models, taking into account the challenges of residual connections and multi-head attention layers. Specifically, we propose a two-level method rooted in spectral theory, initially permuting the attention heads and subsequently adjusting parameters within select pairs of heads. Through extensive experiments on visual and textual tasks, we achieve the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on a single training step or datapoint. Code is available at https://github.com/aimagelab/TransFusion.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Rate-Constrained Optimization with Applications to Fair Learning</title>
<link>https://arxiv.org/abs/2505.22703</link>
<guid>https://arxiv.org/abs/2505.22703</guid>
<content:encoded><![CDATA[
<div> Optimization, Trustworthy ML, Fairness Constraints, Differential Privacy, Algorithm <br />
Summary: Many trustworthy ML problems involve minimizing model error while meeting prediction rate constraints on selected marginals, such as group fairness constraints. This study explores constrained minimization under differential privacy (DP). The RaCO-DP algorithm, a DP adaptation of Stochastic Gradient Descent-Ascent (SGDA), tackles rate constraint problems by solving their Lagrangian formulation. The algorithm's privacy cost is equivalent to privately estimating a histogram for the mini-batch at each optimization step. Convergence is proven through an innovative analysis leveraging the dual parameter's linear structure. Empirical tests on learning under group fairness constraints reveal that RaCO-DP outperforms existing private learning methods in fairness-utility trade-offs. <br /><br /> <div>
arXiv:2505.22703v1 Announce Type: new 
Abstract: Many problems in trustworthy ML can be formulated as minimization of the model error under constraints on the prediction rates of the model for suitably-chosen marginals, including most group fairness constraints (demographic parity, equality of odds, etc.). In this work, we study such constrained minimization problems under differential privacy (DP). Standard DP optimization techniques like DP-SGD rely on the loss function's decomposability into per-sample contributions. However, rate constraints introduce inter-sample dependencies, violating the decomposability requirement. To address this, we develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm which solves the Lagrangian formulation of rate constraint problems. We demonstrate that the additional privacy cost of incorporating these constraints reduces to privately estimating a histogram over the mini-batch at each optimization step. We prove the convergence of our algorithm through a novel analysis of SGDA that leverages the linear structure of the dual parameter. Finally, empirical results on learning under group fairness constraints demonstrate that our method Pareto-dominates existing private learning approaches in fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference</title>
<link>https://arxiv.org/abs/2505.22758</link>
<guid>https://arxiv.org/abs/2505.22758</guid>
<content:encoded><![CDATA[
<div> kernel, large language models, single-batch inference, FlashFormer, transformer-based 

Summary:
FlashFormer is a new kernel designed to accelerate single-batch inference for transformer-based large language models. The kernel aims to optimize performance for low-batch inference scenarios, where memory bandwidth and kernel launch overheads are key factors. Compared to existing state-of-the-art inference kernels, FlashFormer demonstrates significant speedups across various model sizes and quantization settings. This specialized kernel is particularly useful for applications such as edge deployment and latency-sensitive tasks, where low-batch inference is crucial. The size and compute characteristics of modern language models have driven the need for tailored kernels, and FlashFormer addresses the specific challenges of single-batch inference, providing efficient solutions for improving performance and efficiency in this context. <div>
arXiv:2505.22758v1 Announce Type: new 
Abstract: The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for training and inference. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads contribute are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, a proof-of-concept kernel for accelerating single-batch inference for transformer-based large language models. Across various model sizes and quantizations settings, we observe nontrivial speedups compared to existing state-of-the-art inference kernels.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time augmentation improves efficiency in conformal prediction</title>
<link>https://arxiv.org/abs/2505.22764</link>
<guid>https://arxiv.org/abs/2505.22764</guid>
<content:encoded><![CDATA[
<div> test-time augmentation, conformal classifier, predicted classes, probabilistic guarantee, inductive biases

Summary: 
Test-time augmentation (TTA) is shown to reduce the size of predicted classes produced by conformal classifiers while providing probabilistic guarantees. The approach is flexible, computationally efficient, and requires no model retraining. It can be combined with any conformal score and reduces prediction set sizes by 10%-14% on average. An evaluation across three datasets, three models, two conformal scoring methods, different guarantee strengths, and distribution shifts demonstrates the effectiveness of TTA in enhancing the conformal pipeline. <div>
arXiv:2505.22764v1 Announce Type: new 
Abstract: A conformal classifier produces a set of predicted classes and provides a probabilistic guarantee that the set includes the true class. Unfortunately, it is often the case that conformal classifiers produce uninformatively large sets. In this work, we show that test-time augmentation (TTA)--a technique that introduces inductive biases during inference--reduces the size of the sets produced by conformal classifiers. Our approach is flexible, computationally efficient, and effective. It can be combined with any conformal score, requires no model retraining, and reduces prediction set sizes by 10%-14% on average. We conduct an evaluation of the approach spanning three datasets, three models, two established conformal scoring methods, different guarantee strengths, and several distribution shifts to show when and why test-time augmentation is a useful addition to the conformal pipeline.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.22768</link>
<guid>https://arxiv.org/abs/2505.22768</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, DRAGON encoder, Multivariate de Bruijn Graphs, symbolic representation, neural modeling

Summary: 
Time series forecasting is a challenging task due to temporal heterogeneity and high dimensionality. In this work, the DRAGON encoder is introduced, utilizing Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between symbolic representations and neural modeling. DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure, allowing dynamic context recovery through graph-based attention. Integrated into a dual-branch architecture, DRAGON enhances conventional CNN-based encoders with symbolic, structure-aware representations. The code developed for this study is available on GitHub <br /><br />Summary: <div>
arXiv:2505.22768v1 Announce Type: new 
Abstract: Time series forecasting remains a challenging task for foundation models due to temporal heterogeneity, high dimensionality, and the lack of inherent symbolic structure. In this work, we propose DRAGON (Discrete Representation and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between symbolic representations and neural modeling. DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure, enabling dynamic context recovery via graph-based attention. Integrated as an auxiliary module within a dual-branch architecture, DRAGON augments conventional CNN-based encoders with symbolic, structure-aware representations. All code developed for this study is available at: https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Value-Aware Model Learning with Stochastic Environment Models</title>
<link>https://arxiv.org/abs/2505.22772</link>
<guid>https://arxiv.org/abs/2505.22772</guid>
<content:encoded><![CDATA[
<div> value-aware, model learning, MuZero loss, surrogate losses, calibrated

Summary:
- The paper discusses value-aware model learning and the popular MuZero loss used in model-based reinforcement learning.
- It highlights that the MuZero loss, along with similar losses, are uncalibrated surrogate losses, leading to incorrect model and value function recovery.
- The authors propose corrections to address this issue and improve model and value function predictions.
- The study explores the relationship between loss calibration, latent model architectures, and auxiliary losses in training MuZero-style agents.
- The findings suggest that while deterministic models can predict accurate values, learning calibrated stochastic models remains beneficial. 

<br /><br />Summary: <div>
arXiv:2505.22772v1 Announce Type: new 
Abstract: The idea of value-aware model learning, that models should produce accurate value estimates, has gained prominence in model-based reinforcement learning. The MuZero loss, which penalizes a model's value function prediction compared to the ground-truth value function, has been utilized in several prominent empirical works in the literature. However, theoretical investigation into its strengths and weaknesses is limited. In this paper, we analyze the family of value-aware model learning losses, which includes the popular MuZero loss. We show that these losses, as normally used, are uncalibrated surrogate losses, which means that they do not always recover the correct model and value function. Building on this insight, we propose corrections to solve this issue. Furthermore, we investigate the interplay between the loss calibration, latent model architectures, and auxiliary losses that are commonly employed when training MuZero-style agents. We show that while deterministic models can be sufficient to predict accurate values, learning calibrated stochastic models is still advantageous.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Models Have a Supply Chain Problem</title>
<link>https://arxiv.org/abs/2505.22778</link>
<guid>https://arxiv.org/abs/2505.22778</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, open ecosystem, supply-chain risks, Sigstore, transparency

Summary: 
Machine Learning models are widely available online, offering exciting opportunities for non-experts. However, the open ecosystem also poses risks, such as the potential for malicious attacks or vulnerabilities in the models. Sigstore, a solution for open-source software supply chains, could be used to enhance transparency in open ML models. By enabling model publishers to sign their models and verify properties of the datasets used, Sigstore could mitigate the risks associated with the current ecosystem. This approach would help prevent attackers from substituting models with harmful content, ensure models are trained on secure frameworks, and validate the integrity of the data used. Through transparency and verification mechanisms, Sigstore could establish trust and enhance the security of open ML models. 

<br /><br />Summary: <div>
arXiv:2505.22778v1 Announce Type: new 
Abstract: Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models and prove properties about the datasets they use.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Latent Space Dynamics of Neural Models</title>
<link>https://arxiv.org/abs/2505.22785</link>
<guid>https://arxiv.org/abs/2505.22785</guid>
<content:encoded><![CDATA[
<div> latent space, autoencoder models, neural networks, dynamical systems, vector field

Summary:
Neural networks can be viewed as dynamical systems that operate on a latent manifold, creating a vector field through the encoding-decoding map of autoencoder models. Training neural models introduces biases leading to attractor points in this vector field. This representation allows for analyzing generalization and memorization during training, extracting prior knowledge from network parameters encoded in attractors, and identifying out-of-distribution samples based on their trajectories. The approach is validated on vision foundation models, demonstrating its practicality. <div>
arXiv:2505.22785v1 Announce Type: new 
Abstract: Neural networks transform high-dimensional data into compact, structured representations, often modeled as elements of a lower dimensional latent space. In this paper, we present an alternative interpretation of neural models as dynamical systems acting on the latent manifold. Specifically, we show that autoencoder models implicitly define a latent vector field on the manifold, derived by iteratively applying the encoding-decoding map, without any additional training. We observe that standard training procedures introduce inductive biases that lead to the emergence of attractor points within this vector field. Drawing on this insight, we propose to leverage the vector field as a representation for the network, providing a novel tool to analyze the properties of the model and the data. This representation enables to: (i) analyze the generalization and memorization regimes of neural models, even throughout training; (ii) extract prior knowledge encoded in the network's parameters from the attractors, without requiring any input data; (iii) identify out-of-distribution samples from their trajectories in the vector field. We further validate our approach on vision foundation models, showcasing the applicability and effectiveness of our method in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Preimage Approximation for Neural Network Certification</title>
<link>https://arxiv.org/abs/2505.22798</link>
<guid>https://arxiv.org/abs/2505.22798</guid>
<content:encoded><![CDATA[
<div> certification, neural networks, preimage approximation, scalability, algorithmic improvements <br />
Summary: <br />
The article focuses on the need for effective neural network certification in safety- and security-critical applications, specifically against "patch attacks" that obscure parts of images. It introduces improvements to the PREMAP algorithm for preimage approximation, including tighter bounds, adaptive Monte Carlo sampling, and enhanced branching heuristics. These enhancements result in significant efficiency gains, making the method applicable to convolutional neural networks that were previously impractical. Through experiments on reinforcement learning control benchmarks, the study showcases the scalability and potential of preimage approximation for reliability and robustness certification in neural networks. <div>
arXiv:2505.22798v1 Announce Type: new 
Abstract: The growing reliance on artificial intelligence in safety- and security-critical applications demands effective neural network certification. A challenging real-world use case is certification against ``patch attacks'', where adversarial patches or lighting conditions obscure parts of images, for example traffic signs. One approach to certification, which also gives quantitative coverage estimates, utilizes preimages of neural networks, i.e., the set of inputs that lead to a specified output. However, these preimage approximation methods, including the state-of-the-art PREMAP algorithm, struggle with scalability. This paper presents novel algorithmic improvements to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics. We demonstrate efficiency improvements of at least an order of magnitude on reinforcement learning control benchmarks, and show that our method scales to convolutional neural networks that were previously infeasible. Our results demonstrate the potential of preimage approximation methodology for reliability and robustness certification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment</title>
<link>https://arxiv.org/abs/2505.22803</link>
<guid>https://arxiv.org/abs/2505.22803</guid>
<content:encoded><![CDATA[
<div> Alignment, Uncertainty, Neural Networks, Calibration, CLUE

Summary:
CLUE (Calibration via Learning Uncertainty-Error Alignment) introduces a novel approach for reliable uncertainty estimation in neural networks. It focuses on aligning predicted uncertainty with observed error during training, leading to well-calibrated models with accurate uncertainty estimates. The method optimizes predictive performance and calibration simultaneously using summary statistics of uncertainty and loss. CLUE is fully differentiable, domain-agnostic, and seamlessly integrates into standard training pipelines. Extensive experiments across various tasks show that CLUE achieves superior calibration quality and competitive predictive performance compared to existing techniques, even in out-of-distribution and domain-shift scenarios. The approach does not incur significant computational overhead, making it practical for real-world applications. <div>
arXiv:2505.22803v1 Announce Type: new 
Abstract: Reliable uncertainty estimation is critical for deploying neural networks (NNs) in real-world applications. While existing calibration techniques often rely on post-hoc adjustments or coarse-grained binning methods, they remain limited in scalability, differentiability, and generalization across domains. In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error Alignment), a novel approach that explicitly aligns predicted uncertainty with observed error during training, grounded in the principle that well-calibrated models should produce uncertainty estimates that match their empirical loss. CLUE adopts a novel loss function that jointly optimizes predictive performance and calibration, using summary statistics of uncertainty and loss as proxies. The proposed method is fully differentiable, domain-agnostic, and compatible with standard training pipelines. Through extensive experiments on vision, regression, and language modeling tasks, including out-of-distribution and domain-shift scenarios, we demonstrate that CLUE achieves superior calibration quality and competitive predictive performance with respect to state-of-the-art approaches without imposing significant computational overhead.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Factor: Quality Is a Dataset-Intrinsic Property</title>
<link>https://arxiv.org/abs/2505.22813</link>
<guid>https://arxiv.org/abs/2505.22813</guid>
<content:encoded><![CDATA[
<div> dataset quality, machine learning classifiers, model architecture, dataset size, class balance

Summary:
Quality of datasets is an intrinsic factor that significantly influences the performance of machine learning classifiers, independent of model architecture, dataset size, and class balance. Through experimentation with thousands of controlled datasets and various classifier architectures, a strong correlation was found between classifier performance and dataset quality. This correlation suggests that dataset quality is a fundamental property linked to the quality of the constituent classes within the datasets. The study highlights dataset quality as a separate target for optimizing machine-learning-based classification, alongside size, class balance, and model architecture. This research provides valuable insights for enhancing the performance of machine learning classifiers by considering and improving dataset quality as a key factor. <div>
arXiv:2505.22813v1 Announce Type: new 
Abstract: In the universal quest to optimize machine-learning classifiers, three factors -- model architecture, dataset size, and class balance -- have been shown to influence test-time performance but do not fully account for it. Previously, evidence was presented for an additional factor that can be referred to as dataset quality, but it was unclear whether this was actually a joint property of the dataset and the model architecture, or an intrinsic property of the dataset itself. If quality is truly dataset-intrinsic and independent of model architecture, dataset size, and class balance, then the same datasets should perform better (or worse) regardless of these other factors. To test this hypothesis, here we create thousands of datasets, each controlled for size and class balance, and use them to train classifiers with a wide range of architectures, from random forests and support-vector machines to deep networks. We find that classifier performance correlates strongly by subset across architectures ($R^2=0.79$), supporting quality as an intrinsic property of datasets independent of dataset size and class balance and of model architecture. Digging deeper, we find that dataset quality appears to be an emergent property of something more fundamental: the quality of datasets' constituent classes. Thus, quality joins size, class balance, and model architecture as an independent correlate of performance and a separate target for optimizing machine-learning-based classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Learning with Response Time</title>
<link>https://arxiv.org/abs/2505.22820</link>
<guid>https://arxiv.org/abs/2505.22820</guid>
<content:encoded><![CDATA[
<div> response time data, human preference learning, reward model elicitation, Evidence Accumulation Drift Diffusion model, Neyman-orthogonal loss functions

Summary:
This paper introduces a method to incorporate response time data into human preference learning frameworks, enhancing reward model elicitation. By integrating the Evidence Accumulation Drift Diffusion model, the study utilizes response time information to capture preference strength. The proposed Neyman-orthogonal loss functions ensure optimal convergence rates for reward model learning compared to traditional preference learning approaches. The theoretical analysis reveals exponential error scaling in conventional methods for linear reward functions, which is significantly reduced to polynomial scaling through the response time-augmented approach. The study extends its findings to non-parametric reward function spaces, demonstrating convergence properties for more complex reward models. Extensive experiments in image preference learning validate the theoretical conclusions. <div>
arXiv:2505.22820v1 Announce Type: new 
Abstract: This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow</title>
<link>https://arxiv.org/abs/2505.22825</link>
<guid>https://arxiv.org/abs/2505.22825</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Optimal Power Flow, Datasets, Evaluation Metrics, PGLearn <br />
Summary: <br />
Machine Learning techniques are increasingly being used to address Optimal Power Flow (OPF) problems in energy grids. The lack of standardized datasets and evaluation metrics has hindered progress in this field. To tackle this challenge, PGLearn introduces a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. These datasets capture real-life operating conditions and include time series data for large-scale systems. PGLearn supports multiple OPF formulations and provides a toolkit for training and benchmarking ML models. By making datasets publicly available and promoting standardized evaluation metrics, PGLearn aims to democratize research in machine learning applications for OPF, reducing the burden of data generation and enabling fair comparisons between methodologies. Demanding open, standardized datasets and evaluation metrics, PGLearn accelerates research and innovation in ML for OPF. <div>
arXiv:2505.22825v1 Announce Type: new 
Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have recently garnered significant attention, reflecting a broader trend of leveraging ML to approximate and/or accelerate the resolution of complex optimization problems. These developments are necessitated by the increased volatility and scale in energy production for modern and future grids. However, progress in ML for OPF is hindered by the lack of standardized datasets and evaluation metrics, from generating and solving OPF instances, to training and benchmarking machine learning models. To address this challenge, this paper introduces PGLearn, a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. PGLearn provides datasets that are representative of real-life operating conditions, by explicitly capturing both global and local variability in the data generation, and by, for the first time, including time series data for several large-scale systems. In addition, it supports multiple OPF formulations, including AC, DC, and second-order cone formulations. Standardized datasets are made publicly available to democratize access to this field, reduce the burden of data generation, and enable the fair comparison of various methodologies. PGLearn also includes a robust toolkit for training, evaluating, and benchmarking machine learning models for OPF, with the goal of standardizing performance evaluation across the field. By promoting open, standardized datasets and evaluation metrics, PGLearn aims at democratizing and accelerating research and innovation in machine learning applications for optimal power flow problems. Datasets are available for download at https://www.huggingface.co/PGLearn.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies</title>
<link>https://arxiv.org/abs/2505.22829</link>
<guid>https://arxiv.org/abs/2505.22829</guid>
<content:encoded><![CDATA[
<div> Keywords: distribution shift, AI safety, conceptual synergies, methodological synergies, integration <br />
Summary: This paper explores the intersection of distribution shift and AI safety by analyzing their conceptual and methodological connections. The authors identify two types of relationships between specific causes of distribution shift and fine-grained AI safety issues: methods targeting a particular shift type can also address corresponding safety goals, and some shifts and safety issues can be formally converted into each other, allowing for shared methodology. By highlighting these synergies, the paper advocates for a more integrated approach to distribution shift and AI safety research. This unified perspective could lead to a deeper understanding of how addressing distribution shift challenges can contribute to achieving AI safety objectives, and vice versa. The findings emphasize the importance of fundamental integration between these two areas of research for advancing the development of safe and robust AI systems. <br /><br />Summary: <div>
arXiv:2505.22829v1 Announce Type: new 
Abstract: This paper bridges distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies. While prior discussions often focus on narrow cases or informal analogies, we establish two types connections between specific causes of distribution shift and fine-grained AI safety issues: (1) methods addressing a specific shift type can help achieve corresponding safety goals, or (2) certain shifts and safety issues can be formally reduced to each other, enabling mutual adaptation of their methods. Our findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Diffusion Models Improve Adversarial Robustness?</title>
<link>https://arxiv.org/abs/2505.22839</link>
<guid>https://arxiv.org/abs/2505.22839</guid>
<content:encoded><![CDATA[
<div> randomness, compression, diffusion models, adversarial robustness, purification

Summary:
Diffusion models have been found to enhance adversarial robustness empirically, but the precise mechanisms behind this improvement have been unclear. Contrary to intuition, the models actually increase the distance to clean samples rather than decreasing it. The purified images are heavily influenced by the internal randomness of the diffusion models, leading to a compression effect within each randomness configuration. When evaluating robustness under fixed randomness, the improvement drops to 24% on CIFAR-10, much lower than previous reports. The remaining robustness gain is strongly correlated with the model's ability to compress the input space, making the compression rate a reliable indicator of robustness without requiring gradient-based analysis. These findings shed light on how diffusion-based purification works and provide insights for developing more effective adversarial purification systems.<br /><br />Summary: <div>
arXiv:2505.22839v1 Announce Type: new 
Abstract: Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness. While some intuitive explanations have been proposed, the precise mechanisms underlying these improvements remain unclear. In this work, we systematically investigate how and how well diffusion models improve adversarial robustness. First, we observe that diffusion models intriguingly increase, rather than decrease, the $\ell_p$ distance to clean samples--challenging the intuition that purification denoises inputs closer to the original data. Second, we find that the purified images are heavily influenced by the internal randomness of diffusion models, where a compression effect arises within each randomness configuration. Motivated by this observation, we evaluate robustness under fixed randomness and find that the improvement drops to approximately 24% on CIFAR-10--substantially lower than prior reports approaching 70%. Importantly, we show that this remaining robustness gain strongly correlates with the model's ability to compress the input space, revealing the compression rate as a reliable robustness indicator without requiring gradient-based analysis. Our findings provide novel insights into the mechanisms underlying diffusion-based purification, and offer guidance for developing more effective and principled adversarial purification systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction</title>
<link>https://arxiv.org/abs/2505.22840</link>
<guid>https://arxiv.org/abs/2505.22840</guid>
<content:encoded><![CDATA[
<div> Keywords: sepsis prediction, machine learning, deep neural network, accuracy, precision 

Summary: 
The study introduces the SXI++ LNM, a machine learning scoring system designed to enhance sepsis prediction by incorporating multiple algorithms and deep neural networks. The model was trained and evaluated using different dataset distributions to ensure robustness in clinical applications. Results show that the SXI++ LNM outperformed existing methods in three use cases, achieving an impressive AUC of 0.99 with high precision and accuracy. The model demonstrated a precision of 99.9% and an accuracy of 99.99%, showcasing its reliability in predicting sepsis. The utilization of a deep neural network has proved to be effective in enhancing predictive performance, with the model showcasing exceptional accuracy and precision. The findings suggest that the SXI++ LNM could be a valuable tool in improving sepsis prediction and diagnosis, showcasing its potential in clinical settings. 

<br /><br />Summary: <div>
arXiv:2505.22840v1 Announce Type: new 
Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people globally and causing 11 million deaths annually. Despite medical advancements, predicting sepsis remains a challenge due to non-specific symptoms and complex pathophysiology. The SXI++ LNM is a machine learning scoring system that refines sepsis prediction by leveraging multiple algorithms and deep neural networks. This study aims to improve robustness in clinical applications and evaluates the predictive performance of the SXI++ LNM for sepsis prediction. The model, utilizing a deep neural network, was trained and tested using multiple scenarios with different dataset distributions. The model's performance was assessed against unseen test data, and accuracy, precision, and area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the state of the art in three use cases, achieving an AUC of 0.99 (95% CI: 0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0) and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study</title>
<link>https://arxiv.org/abs/2505.22841</link>
<guid>https://arxiv.org/abs/2505.22841</guid>
<content:encoded><![CDATA[
<div> diffusion models, generative sampling, regularization, memorization, variance control  
Summary:  
Diffusion models are now the standard in high-fidelity generative sampling but can be prone to memorization. The empirical score can overfit the dataset, leading to reverse-time SDE samples being mostly training points. Introducing a kernel-smoothed empirical score can help reduce noise variance and improve generalization by balancing bias and variance. Regularization on the score or increasing the size of the training dataset can prevent memorization. The spectral decomposition of the forward diffusion suggests better variance control under certain conditions. Reverse diffusion with a kernel-smoothed empirical score can be seen as a gradient descent towards a Log-Exponential Double-Kernel Density Estimator, highlighting the regularization mechanisms in denoising diffusions. Even simple regularization without learning can mitigate memorization and enhance generalization. Experimental results on synthetic and MNIST datasets support these findings. <div>
arXiv:2505.22841v1 Announce Type: new 
Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling, yet they can, in principle, be prone to memorization. In this case, their learned score overfits the finite dataset so that the reverse-time SDE samples are mostly training points. In this paper, we interpret the empirical score as a noisy version of the true score and show that its covariance matrix is asymptotically a re-weighted data PCA. In large dimension, the small time limit makes the noise variance blow up while simultaneously reducing spatial correlation. To reduce this variance, we introduce a kernel-smoothed empirical score and analyze its bias-variance trade-off. We derive asymptotic bounds on the Kullback-Leibler divergence between the true distribution and the one generated by the modified reverse SDE. Regularization on the score has the same effect as increasing the size of the training dataset, and thus helps prevent memorization. A spectral decomposition of the forward diffusion suggests better variance control under some regularity conditions of the true data distribution. Reverse diffusion with kernel-smoothed empirical score can be reformulated as a gradient descent drifted toward a Log-Exponential Double-Kernel Density Estimator (LED-KDE). This perspective highlights two regularization mechanisms taking place in denoising diffusions: an initial Gaussian kernel first diffuses mass isotropically in the ambient space, while a second kernel applied in score space concentrates and spreads that mass along the data manifold. Hence, even a straightforward regularization-without any learning-already mitigates memorization and enhances generalization. Numerically, we illustrate our results with several experiments on synthetic and MNIST datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation</title>
<link>https://arxiv.org/abs/2505.22846</link>
<guid>https://arxiv.org/abs/2505.22846</guid>
<content:encoded><![CDATA[
<div> interactive theorem proving, generative artificial intelligence, Rocq generation, premise selection, self-attentive embedder model 

Summary: 
The paper evaluates different approaches to Rocq proof generation and suggests improvements, emphasizing the importance of premise selection. A novel approach utilizing a self-attentive embedder model shows promising results with a 28% performance increase. A multi-stage agentic system designed for formal verification effectively addresses the challenge of writing Rocq proofs. An ablation study is conducted, highlighting the use of multi-agent debate in proof synthesis planning. The research showcases the potential of combining interactive theorem proving with generative artificial intelligence for Rocq proof generation, offering insights for enhancing performance and efficiency.<br /><br />Summary: <div>
arXiv:2505.22846v1 Announce Type: new 
Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We highlight the importance of thorough premise selection for generating Rocq proofs and propose a novel approach, leveraging retrieval via a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and show the use of multi-agent debate on the planning stage of proof synthesis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel</title>
<link>https://arxiv.org/abs/2505.22861</link>
<guid>https://arxiv.org/abs/2505.22861</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, causal reasoning, physics-informed kernel, exploration, virtual tools
Summary:
Causal-PIK is proposed for tasks involving complex interactions with unknown dynamics, leveraging Bayesian optimization and a Physics-Informed Kernel for efficient search and improved actions. Experimental results on Virtual Tools and PHYRE benchmarks show Causal-PIK outperforms state-of-the-art methods, requiring fewer actions. Comparison to human studies, including a new user study on PHYRE, reveals Causal-PIK's competitiveness on challenging tasks even for human problem solvers. This method enhances causal reasoning in planning processes, offering a promising approach for tasks with intricate causal interactions. <div>
arXiv:2505.22861v1 Announce Type: new 
Abstract: Tasks that involve complex interactions between objects with unknown dynamics make planning before execution difficult. These tasks require agents to iteratively improve their actions after actively exploring causes and effects in the environment. For these type of tasks, we propose Causal-PIK, a method that leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel to help guide efficient search for the best next action. Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. We also compare Causal-PIK to human studies, including results from a new user study we conducted on the PHYRE benchmark. We find that Causal-PIK remains competitive on tasks that are very challenging, even for human problem-solvers.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Offline RL via Efficient and Expressive Shortcut Models</title>
<link>https://arxiv.org/abs/2505.22866</link>
<guid>https://arxiv.org/abs/2505.22866</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, flow models, offline reinforcement learning, shortcut models, scalable

Summary: 
Offline reinforcement learning faces challenges in applying diffusion and flow models due to their iterative noise sampling processes. The paper introduces Scalable Offline Reinforcement Learning (SORL), which utilizes shortcut models to scale training and inference efficiently in a one-stage training procedure. SORL's policy can handle complex data distributions and achieves strong performance in various offline RL tasks. At test time, SORL utilizes the learned Q-function for sequential and parallel inference scaling. The algorithm shows positive scaling behavior with increased test-time compute. By leveraging shortcut models, SORL simplifies policy optimization and demonstrates promising results in offline reinforcement learning applications.<br /><br />Summary: <div>
arXiv:2505.22866v1 Announce Type: new 
Abstract: Diffusion and flow models have emerged as powerful generative approaches capable of modeling diverse and multimodal behavior. However, applying these models to offline reinforcement learning (RL) remains challenging due to the iterative nature of their noise sampling processes, making policy optimization difficult. In this paper, we introduce Scalable Offline Reinforcement Learning (SORL), a new offline RL algorithm that leverages shortcut models - a novel class of generative models - to scale both training and inference. SORL's policy can capture complex data distributions and can be trained simply and efficiently in a one-stage training procedure. At test time, SORL introduces both sequential and parallel inference scaling by using the learned Q-function as a verifier. We demonstrate that SORL achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute. We release the code at nico-espinosadice.github.io/projects/sorl.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints</title>
<link>https://arxiv.org/abs/2505.22881</link>
<guid>https://arxiv.org/abs/2505.22881</guid>
<content:encoded><![CDATA[
<div> uncertainty, constraints, machine learning, optimization, stochastic <br />
<br />
Summary: <br />
The study introduces an extension of contextual stochastic linear optimization (CSLO) with inequality constraints influenced by uncertain parameters predicted by a machine learning model. The contextual uncertainty sets are constructed using methods like conformal prediction to handle the uncertainty in constraints. A new loss function called "Smart Predict-then-Optimize with Robust Constraints" (SPO-RC) is introduced, which is feasibility-sensitive and measures the decision error of predicted objective parameters. A convex surrogate called SPO-RC+ is also introduced, and its Fisher consistency with SPO-RC is proven. Training on truncated datasets and correcting sample selection bias using importance reweighting techniques are proposed to enhance performance. Experimental results on fractional knapsack and alloy production problems demonstrate the effectiveness of SPO-RC+ in handling uncertain constraints and the improvement in performance by combining truncation with importance reweighting. <div>
arXiv:2505.22881v1 Announce Type: new 
Abstract: We study an extension of contextual stochastic linear optimization (CSLO) that, in contrast to most of the existing literature, involves inequality constraints that depend on uncertain parameters predicted by a machine learning model. To handle the constraint uncertainty, we use contextual uncertainty sets constructed via methods like conformal prediction. Given a contextual uncertainty set method, we introduce the "Smart Predict-then-Optimize with Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the SPO loss that measures decision error of predicted objective parameters. We also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with SPO-RC. To enhance performance, we train on truncated datasets where true constraint parameters lie within the uncertainty sets, and we correct the induced sample selection bias using importance reweighting techniques. Through experiments on fractional knapsack and alloy production problem instances, we demonstrate that SPO-RC+ effectively handles uncertainty in constraints and that combining truncation with importance reweighting can further improve performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning</title>
<link>https://arxiv.org/abs/2505.22899</link>
<guid>https://arxiv.org/abs/2505.22899</guid>
<content:encoded><![CDATA[
<div> keywords: Follow the Regularized Leader, Online Convex Optimization, dynamic regret guarantees, agile iterates, pruning <br />
Summary: <br />
This article revisits the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets, focusing on achieving dynamic regret guarantees. It highlights that FTRL can produce "agile" iterates and recover known dynamic regret bounds through optimistic composition of future costs and linearization of past costs. By comparing FTRL against dynamic comparators, it provides a way to interpolate between greedy and agile updates, offering refined control over regret terms and optimism without cyclic dependence. The analysis shows that it is not the lazy projection style of FTRL hindering dynamic regret, but the decoupling of the algorithm's state from its iterates. Pruning is introduced to synchronize the state and iterates when necessary, allowing for minimal recursive regularization akin to AdaFTRL. <div>
arXiv:2505.22899v1 Announce Type: new 
Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets, focusing on achieving dynamic regret guarantees. Prior work has highlighted the framework's limitations in dynamic environments due to its tendency to produce "lazy" iterates. However, building on insights showing FTRL's ability to produce "agile" iterates, we show that it can indeed recover known dynamic regret bounds through optimistic composition of future costs and careful linearization of past costs, which can lead to pruning some of them. This new analysis of FTRL against dynamic comparators yields a principled way to interpolate between greedy and agile updates and offers several benefits, including refined control over regret terms, optimism without cyclic dependence, and the application of minimal recursive regularization akin to AdaFTRL. More broadly, we show that it is not the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but the decoupling of the algorithm's state (linearized history) from its iterates, allowing the state to grow arbitrarily. Instead, pruning synchronizes these two when necessary.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining Foundation Models for Computational Science: A Call for Clarity and Rigor</title>
<link>https://arxiv.org/abs/2505.22904</link>
<guid>https://arxiv.org/abs/2505.22904</guid>
<content:encoded><![CDATA[
<div> foundation models, computational science, generality, reusability, scalability

Summary:
In this paper, the authors address the lack of a universally accepted definition of foundation models in computational science. They propose a formal definition grounded in generality, reusability, and scalability, essential for these models to exhibit. Drawing parallels with traditional methods like finite element and finite volume, they introduce the Data-Driven Finite Element Method (DD-FEM). This framework combines the modular structure of classical FEM with the representation power of data-driven learning. DD-FEM effectively addresses key challenges such as scalability, adaptability, and physics consistency in foundation models for computational science. By integrating traditional numerical methods with modern AI paradigms, this work establishes a strong foundation for evaluating and developing innovative approaches towards future foundation models in computational science.<br /><br />Summary: <div>
arXiv:2505.22904v1 Announce Type: new 
Abstract: The widespread success of foundation models in natural language processing and computer vision has inspired researchers to extend the concept to scientific machine learning and computational science. However, this position paper argues that as the term "foundation model" is an evolving concept, its application in computational science is increasingly used without a universally accepted definition, potentially creating confusion and diluting its precise scientific meaning. In this paper, we address this gap by proposing a formal definition of foundation models in computational science, grounded in the core values of generality, reusability, and scalability. We articulate a set of essential and desirable characteristics that such models must exhibit, drawing parallels with traditional foundational methods, like the finite element and finite volume methods. Furthermore, we introduce the Data-Driven Finite Element Method (DD-FEM), a framework that fuses the modular structure of classical FEM with the representational power of data-driven learning. We demonstrate how DD-FEM addresses many of the key challenges in realizing foundation models for computational science, including scalability, adaptability, and physics consistency. By bridging traditional numerical methods with modern AI paradigms, this work provides a rigorous foundation for evaluating and developing novel approaches toward future foundation models in computational science.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference</title>
<link>https://arxiv.org/abs/2505.22913</link>
<guid>https://arxiv.org/abs/2505.22913</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured sparsity, KV cache compression, pruning strategies, custom attention kernel, decode performance

Summary:
The study demonstrates that unstructured sparsity significantly enhances KV cache compression for Large Language Models (LLMs), achieving up to 70% sparsity without sacrificing accuracy or requiring fine-tuning. Per-token magnitude-based pruning proves highly effective for both Key and Value caches, surpassing previous structured pruning methods. The Key cache benefits from outlier elements, while the Value cache benefits from simple magnitude-based pruning. KV cache size is a bottleneck in decode performance due to high memory overhead, addressed by a bitmap-based sparse format and a custom attention kernel. This allows for compressing and computing over compressed caches pruned to arbitrary sparsity patterns, accelerating memory-bound operations in decode computations. The custom attention kernel and bitmap-based format enable significant compression of KV cache, leading to longer context lengths and increased tokens/sec throughput compared to dense inference. The pruning mechanism and sparse attention kernel are accessible at https://github.com/dhjoo98/mustafar<br /><br />Summary: The study highlights the efficacy of unstructured sparsity in improving KV cache compression for LLMs, utilizing per-token magnitude-based pruning for Key and Value caches. A custom attention kernel and bitmap-based sparse format address the issue of KV cache size as a bottleneck in decode performance, enabling significant acceleration of memory-bound operations and increased throughput compared to dense inference. <div>
arXiv:2505.22913v1 Announce Type: new 
Abstract: We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking</title>
<link>https://arxiv.org/abs/2505.22922</link>
<guid>https://arxiv.org/abs/2505.22922</guid>
<content:encoded><![CDATA[
<div> Efficient Pre-training, Large Language Models, Memory Efficiency, Benchmark Evaluation, Performance Improvement
<br />
Summary: 
This article explores methods for making large language models more memory and parameter efficient during pre-training. It discusses the challenges posed by the increasing size of LLMs and the computational resources required for training and fine-tuning. The study surveys state-of-the-art approaches for efficient pre-training and evaluates several memory efficient methods to determine their performance across various model sizes. The researchers find that full-rank training performs best with the right optimizer and hyperparameters, and that incorporating high-rank updates in low-rank approaches can enhance their performance. Additionally, they propose two practical techniques, weight refactorization and momentum reset, to improve efficient pre-training methods. Applying these techniques to a low-rank model results in lower perplexity than popular memory efficient algorithms while using less memory. <div>
arXiv:2505.22922v1 Announce Type: new 
Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple domains, large language models (LLMs) have grown at an unprecedented rate, with some recent models containing trillions of parameters. This growth is accompanied by substantial computational challenges, particularly regarding the memory and compute resources required for training and fine-tuning. Numerous approaches have been explored to address these issues, such as LoRA. While these methods are effective for fine-tuning, their application to pre-training is significantly more challenging due to the need to learn vast datasets. Motivated by this issue, we aim to address the following questions: Can parameter- or memory-efficient methods enhance pre-training efficiency while achieving performance comparable to full-model training? How can the performance gap be narrowed? To this end, the contributions of this work are the following. (1) We begin by conducting a comprehensive survey that summarizes state-of-the-art methods for efficient pre-training. (2) We perform a benchmark evaluation of several representative memory efficient pre-training approaches to comprehensively evaluate their performance across model sizes. We observe that with a proper choice of optimizer and hyperparameters, full-rank training delivers the best performance, as expected. We also notice that incorporating high-rank updates in low-rank approaches is the key to improving their performance. (3) Finally, we propose two practical techniques, namely weight refactorization and momentum reset, to enhance the performance of efficient pre-training methods. We observe that applying these techniques to the low-rank method (on a 1B model) can achieve a lower perplexity than popular memory efficient algorithms such as GaLore and Fira, while simultaneously using about 25% less memory.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2505.22935</link>
<guid>https://arxiv.org/abs/2505.22935</guid>
<content:encoded><![CDATA[
<div> Bernoulli edge-flip corruptions, denoisers, Graph Diffusion Models, noise conditioning, high-dimensional graph data <br />
Summary: <br />
This study questions the necessity of explicit noise-level conditioning in Graph Diffusion Models (GDMs) by investigating whether denoisers can infer noise levels implicitly from corrupted graph structures. The research focuses on Bernoulli edge-flip corruptions and extends to more complex scenarios involving structure-attribute noise. Empirical evaluations on synthetic and real-world graph datasets, using models like GDSS and DiGress, show that unconditional GDMs can achieve comparable or better performance than conditioned models. Additionally, unconditional models require fewer parameters (4-6% reduction) and less computation time (8-10% decrease). The results suggest that the inherent high-dimensional nature of graph data may contain enough information for the denoising process, paving the way for simpler and more efficient GDM architectures. <div>
arXiv:2505.22935v1 Announce Type: new 
Abstract: Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Graph Grammars for Sequence-based Learning</title>
<link>https://arxiv.org/abs/2505.22949</link>
<guid>https://arxiv.org/abs/2505.22949</guid>
<content:encoded><![CDATA[
<div> Keywords: Directed acyclic graphs, DAGs, encoder, decoder, grammar-based approach

Summary:
Directed acyclic graphs (DAGs) are commonly used in various applications such as electronic circuits and neural architectures. While there are effective encoders for DAGs, decoding them poses a challenge due to the multiple topological orders of nodes. This work introduces a grammar-based approach to create a sequential representation of a DAG in a principled and compact manner. The graph is seen as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules, enabling lossless compression of the data. This representation has diverse applications, including constructing generative models for graph generation, learning latent spaces for property prediction, and using sequence representational continuity for Bayesian Optimization over structured data. The code for this approach is available on GitHub for further exploration. 

<br /><br />Summary: Directed acyclic graphs are widely used but decoding them is challenging. A grammar-based approach is proposed to create a sequential representation of DAGs for various applications such as generative models and property prediction, enabling efficient data compression and Bayesian optimization. <div>
arXiv:2505.22949v1 Announce Type: new 
Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in practice, with examples that include electronic circuits, Bayesian networks, and neural architectures. While many effective encoders exist for DAGs, it remains challenging to decode them in a principled manner, because the nodes of a DAG can have many different topological orders. In this work, we propose a grammar-based approach to constructing a principled, compact and equivalent sequential representation of a DAG. Specifically, we view a graph as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules. Equivalently, the procedure to construct such a description can be viewed as a lossless compression of the data. Such a representation has many uses, including building a generative model for graph generation, learning a latent space for property prediction, and leveraging the sequence representational continuity for Bayesian Optimization over structured data. Code is available at https://github.com/shiningsunnyday/induction.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming</title>
<link>https://arxiv.org/abs/2505.22967</link>
<guid>https://arxiv.org/abs/2505.22967</guid>
<content:encoded><![CDATA[
<div> framework, safety-constrained, graph evolution, workflow generation, agentic reasoning <br />
Summary: <br />
MermaidFlow is a framework that aims to improve the robustness and executability of workflow generation in autonomous agents by redefining the search space through safety-constrained graph evolution. Workflows are represented using Mermaid, a structured and human-interpretable graph language, to ensure verifiability. Domain-aware evolutionary operators such as crossover, mutation, insertion, and deletion are formulated to maintain semantic correctness while enhancing structural diversity. The framework consistently achieves better success rates and faster convergence to executable plans on agent reasoning benchmarks without changing task settings or evaluation protocols. The experimental results highlight the scalability and interpretability of safety-constrained graph evolution in developing reliable agentic reasoning systems. <div>
arXiv:2505.22967v1 Announce Type: new 
Abstract: Despite the promise of autonomous agentic reasoning, existing workflow generation methods frequently produce fragile, unexecutable plans due to unconstrained LLM-driven construction. We introduce MermaidFlow, a framework that redefines the agentic search space through safety-constrained graph evolution. At its core, MermaidFlow represent workflows as a verifiable intermediate representation using Mermaid, a structured and human-interpretable graph language. We formulate domain-aware evolutionary operators, i.e., crossover, mutation, insertion, and deletion, to preserve semantic correctness while promoting structural diversity, enabling efficient exploration of a high-quality, statically verifiable workflow space. Without modifying task settings or evaluation protocols, MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark. The experimental results demonstrate that safety-constrained graph evolution offers a scalable, modular foundation for robust and interpretable agentic reasoning systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiReg: Equivariance Regularized Diffusion for Inverse Problems</title>
<link>https://arxiv.org/abs/2505.22973</link>
<guid>https://arxiv.org/abs/2505.22973</guid>
<content:encoded><![CDATA[
<div> diffusion models, image restoration, inverse problems, Equivariance Regularized, Bayesian framework

Summary: 
The article introduces Equivariance Regularized (EquiReg) diffusion as a novel approach to enhance reconstructions in diffusion-based inverse problem solvers. These solvers utilize a likelihood term to generate data consistent with the posterior distribution. EquiReg improves reconstructions by penalizing diffusion trajectories that deviate from the data manifold, leading to more stable and consistent results compared to current methods relying on isotropic Gaussian approximations. The framework defines a distribution-dependent equivariance error, identifies functions with low error for on-manifold samples, and utilizes these functions to regularize the diffusion sampling process. When applied to various solvers, EquiReg demonstrates superior performance in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations. <div>
arXiv:2505.22973v1 Announce Type: new 
Abstract: Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. In the Bayesian framework, diffusion-based inverse solvers incorporate a likelihood term to guide the prior sampling process, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood term, many current methods rely on isotropic Gaussian approximations, which lead to deviations from the data manifold and result in inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. EquiReg enhances reconstructions by reweighting diffusion trajectories and penalizing those that deviate from the data manifold. We define a new distribution-dependent equivariance error, empirically identify functions that exhibit low error for on-manifold samples and higher error for off-manifold samples, and leverage these functions to regularize the diffusion sampling process. When applied to a variety of solvers, EquiReg outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Approach to Improving Fairness in K-means Clustering</title>
<link>https://arxiv.org/abs/2505.22984</link>
<guid>https://arxiv.org/abs/2505.22984</guid>
<content:encoded><![CDATA[
<div> Algorithm, Fairness, K-means, Clustering, Optimization  
Summary:  
- The popular K-means clustering algorithm may have biases in terms of sensitive variables, leading to unfair outcomes.
- A two-stage optimization approach is proposed to improve the fairness of K-means clustering.
- Two efficient algorithms are developed to identify data points that impact fairness, focusing on nearest outside data points and highly mixed data points.
- Experiments on benchmark datasets demonstrate significant improvement in fairness without compromising clustering quality.
- The proposed algorithms can be easily applied to various clustering algorithms and fairness metrics.  
<br /><br />Summary: <div>
arXiv:2505.22984v1 Announce Type: new 
Abstract: The popular K-means clustering algorithm potentially suffers from a major weakness for further analysis or interpretation. Some cluster may have disproportionately more (or fewer) points from one of the subpopulations in terms of some sensitive variable, e.g., gender or race. Such a fairness issue may cause bias and unexpected social consequences. This work attempts to improve the fairness of K-means clustering with a two-stage optimization formulation--clustering first and then adjust cluster membership of a small subset of selected data points. Two computationally efficient algorithms are proposed in identifying those data points that are expensive for fairness, with one focusing on nearest data points outside of a cluster and the other on highly 'mixed' data points. Experiments on benchmark datasets show substantial improvement on fairness with a minimal impact to clustering quality. The proposed algorithms can be easily extended to a broad class of clustering algorithms or fairness metrics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.22985</link>
<guid>https://arxiv.org/abs/2505.22985</guid>
<content:encoded><![CDATA[
<div> Echo State Network, PatchEchoClassifier, human activity recognition, knowledge distillation, energy-efficient

Summary: 
PatchEchoClassifier is introduced as an energy-efficient classifier for time-series data, specifically for human activity recognition using sensor signals. The model utilizes an Echo State Network mechanism and a tokenizer to extract patch-level representations. It incorporates a knowledge distillation framework to transfer knowledge from a high-capacity MLP-Mixer teacher to the lightweight reservoir-based student model. Experimental results show that PatchEchoClassifier achieves over 80% accuracy while reducing computational cost significantly, requiring only a fraction of the FLOPS compared to DeepConvLSTM. These findings highlight PatchEchoClassifier as a promising solution for real-time and energy-efficient human activity recognition in edge computing environments. <div>
arXiv:2505.22985v1 Announce Type: new 
Abstract: This paper aims to develop an energy-efficient classifier for time-series data by introducing PatchEchoClassifier, a novel model that leverages a reservoir-based mechanism known as the Echo State Network (ESN). The model is designed for human activity recognition (HAR) using one-dimensional sensor signals and incorporates a tokenizer to extract patch-level representations. To train the model efficiently, we propose a knowledge distillation framework that transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight reservoir-based student model. Experimental evaluations on multiple HAR datasets demonstrate that our model achieves over 80 percent accuracy while significantly reducing computational cost. Notably, PatchEchoClassifier requires only about one-sixth of the floating point operations (FLOPS) compared to DeepConvLSTM, a widely used convolutional baseline. These results suggest that PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Preserving Adaptive Rounding</title>
<link>https://arxiv.org/abs/2505.22988</link>
<guid>https://arxiv.org/abs/2505.22988</guid>
<content:encoded><![CDATA[
<div> quantization, post-training, linear layers, Hessian, algorithm  
Summary:  
The article introduces a new post-training quantization algorithm, named YAQA, that aims to produce a compressed model with an output distribution close to the original model. Unlike existing methods that focus on minimizing activation error in linear layers independently, YAQA considers the impact of subsequent layers by using Kronecker-factored approximations of each layer's Hessian with respect to the full model KL divergence. YAQA consists of two components: computation of tractable sketches of the full layerwise Hessian and a quantizer-independent rounding algorithm with theoretical guarantees. Empirical results show that YAQA reduces the KL divergence to the original model by approximately 30% across various models and quantizers, while also achieving state-of-the-art performance on downstream tasks. <div>
arXiv:2505.22988v1 Announce Type: new 
Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed model whose output distribution is as close to the original model's as possible. To do this tractably, almost all LLM PTQ algorithms quantize linear layers by independently minimizing the immediate activation error. However, this localized objective ignores the effect of subsequent layers, so reducing it does not necessarily give a closer model. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses Kronecker-factored approximations of each linear layer's Hessian with respect to the \textit{full model} KL divergence. YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be tractably computed for hundred-billion parameter LLMs, and a quantizer-independent rounding algorithm that uses these sketches and comes with theoretical guarantees. Across a wide range of models and quantizers, YAQA empirically reduces the KL divergence to the original model by $\approx 30\%$ while achieving state of the art performance on downstream tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Number of Clusters in a Dataset: A Regularized K-means Approach</title>
<link>https://arxiv.org/abs/2505.22991</link>
<guid>https://arxiv.org/abs/2505.22991</guid>
<content:encoded><![CDATA[
<div> Regularized k-means algorithm; ideal clusters; additive regularization; multiplicative regularization; hyperparameter selection <br />
Summary:<br />
The study focuses on determining the correct number of clusters in unlabeled datasets using regularized k-means algorithms. The research derives rigorous bounds for the hyperparameter lambda, assuming clusters to be ideal spheres to mimic k-means clusters. It explores both additive and multiplicative regularization functions to reduce ambiguity in finding multiple solutions. Consensus among solutions helps alleviate the issue in certain scenarios. Experiments showcase the algorithm's performance as clusters deviate from the ideal assumption, highlighting the effectiveness of the approach in practical situations where ideal conditions may not hold. The analysis provides valuable insights into hyperparameter selection and the impact of regularizations on clustering outcomes.  <br /> <div>
arXiv:2505.22991v1 Announce Type: new 
Abstract: Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $\lambda k$, where $k$ is the number of clusters and $\lambda$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $\lambda$. In this paper, we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation</title>
<link>https://arxiv.org/abs/2505.22994</link>
<guid>https://arxiv.org/abs/2505.22994</guid>
<content:encoded><![CDATA[
<div> neuromodulation, artificial neural networks, conditioning, manifold optimization, generalization<br />
Summary:<br />
The article explores a novel approach inspired by neuromodulation to efficiently learn a range of similar tasks in artificial neural networks. Instead of conditioning a network upon task context by injecting context as input, the proposed method involves learning weights that are smoothly parameterized functions of task context variables. This approach optimizes a smooth manifold in weight space with a predefined topology, allowing for improved generalization across tasks. By training manifolds with various topologies and using conditioning to select points on the manifold during inference, the method outperforms traditional conditioning techniques and better generalizes to out-of-distribution samples. This strategy offers a principled and effective alternative to traditional conditioning in artificial neural networks. <div>
arXiv:2505.22994v1 Announce Type: new 
Abstract: One frequently wishes to learn a range of similar tasks as efficiently as possible, re-using knowledge across tasks. In artificial neural networks, this is typically accomplished by conditioning a network upon task context by injecting context as input. Brains have a different strategy: the parameters themselves are modulated as a function of various neuromodulators such as serotonin. Here, we take inspiration from neuromodulation and propose to learn weights which are smoothly parameterized functions of task context variables. Rather than optimize a weight vector, i.e. a single point in weight space, we optimize a smooth manifold in weight space with a predefined topology. To accomplish this, we derive a formal treatment of optimization of manifolds as the minimization of a loss functional subject to a constraint on volumetric movement, analogous to gradient descent. During inference, conditioning selects a single point on this manifold which serves as the effective weight matrix for a particular sub-task. This strategy for conditioning has two main advantages. First, the topology of the manifold (whether a line, circle, or torus) is a convenient lever for inductive biases about the relationship between tasks. Second, learning in one state smoothly affects the entire manifold, encouraging generalization across states. To verify this, we train manifolds with several topologies, including straight lines in weight space (for conditioning on e.g. noise level in input data) and ellipses (for rotated images). Despite their simplicity, these parameterizations outperform conditioning identical networks by input concatenation and better generalize to out-of-distribution samples. These results suggest that modulating weights over low-dimensional manifolds offers a principled and effective alternative to traditional conditioning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Bargaining with Utility-based Feedback</title>
<link>https://arxiv.org/abs/2505.22998</link>
<guid>https://arxiv.org/abs/2505.22998</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, negotiation strategies, opponent-aware reasoning, utility theory, structured feedback

Summary:
The article introduces a new framework for enhancing large language models (LLMs) in realistic bargaining scenarios. It presents a benchmark dataset called BargainArena with six complex scenarios to improve strategy modeling. The evaluation metrics are human-aligned and economically grounded, incorporating agent utility and negotiation power to promote opponent-aware reasoning. A structured feedback mechanism allows LLMs to refine their bargaining strategies iteratively and collaborate with in-context learning prompts. Experimental results show that LLMs often exhibit misaligned negotiation strategies with human preferences, but the structured feedback mechanism improves their performance significantly, fostering deeper strategic reasoning and opponent-awareness. This framework aims to address the challenges LLMs face in understanding and adapting to complex human factors in bargaining interactions. 

<br /><br />Summary: <div>
arXiv:2505.22998v1 Announce Type: new 
Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges for large language models (LLMs) due to limitations in strategic depth and adaptation to complex human factors. Existing benchmarks often fail to capture this real-world complexity. To address this and enhance LLM capabilities in realistic bargaining, we introduce a comprehensive framework centered on utility-based feedback. Our contributions are threefold: (1) BargainArena, a novel benchmark dataset with six intricate scenarios (e.g., deceptive practices, monopolies) to facilitate diverse strategy modeling; (2) human-aligned, economically-grounded evaluation metrics inspired by utility theory, incorporating agent utility and negotiation power, which implicitly reflect and promote opponent-aware reasoning (OAR); and (3) a structured feedback mechanism enabling LLMs to iteratively refine their bargaining strategies. This mechanism can positively collaborate with in-context learning (ICL) prompts, including those explicitly designed to foster OAR. Experimental results show that LLMs often exhibit negotiation strategies misaligned with human preferences, and that our structured feedback mechanism significantly improves their performance, yielding deeper strategic and opponent-aware reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Cross-domain Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23003</link>
<guid>https://arxiv.org/abs/2505.23003</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, robust RL, offline algorithms, simulator data, sample efficiency  
Summary:<br />
The paper introduces HYDRO, a novel Hybrid Cross-Domain Robust RL framework that addresses challenges in robust reinforcement learning. It aims to learn robust policies under uncertainties by maximizing value within an uncertainty set. HYDRO utilizes an online simulator to complement limited offline datasets, tackling the dynamics mismatch issue. Using uncertainty filtering and prioritized sampling, HYDRO selects relevant and reliable simulator samples, minimizing performance gaps between the simulator and worst-case models. Experimental results show HYDRO outperforms existing methods in various tasks, highlighting its potential to enhance sample efficiency in offline robust RL. <br /> <div>
arXiv:2505.23003v1 Announce Type: new 
Abstract: Robust reinforcement learning (RL) aims to learn policies that remain effective despite uncertainties in its environment, which frequently arise in real-world applications due to variations in environment dynamics. The robust RL methods learn a robust policy by maximizing value under the worst-case models within a predefined uncertainty set. Offline robust RL algorithms are particularly promising in scenarios where only a fixed dataset is available and new data cannot be collected. However, these approaches often require extensive offline data, and gathering such datasets for specific tasks in specific environments can be both costly and time-consuming. Using an imperfect simulator offers a faster, cheaper, and safer way to collect data for training, but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO, the first Hybrid Cross-Domain Robust RL framework designed to address these challenges. HYDRO utilizes an online simulator to complement the limited amount of offline datasets in the non-trivial context of robust RL. By measuring and minimizing performance gaps between the simulator and the worst-case models in the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized sampling to select the most relevant and reliable simulator samples. Our extensive experiments demonstrate HYDRO's superior performance over existing methods across various tasks, underscoring its potential to improve sample efficiency in offline robust RL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining</title>
<link>https://arxiv.org/abs/2505.23004</link>
<guid>https://arxiv.org/abs/2505.23004</guid>
<content:encoded><![CDATA[
<div> vision language model, CLIP, MLLM, QLIP, visual understanding

Summary:
QLIP is introduced as a drop-in replacement for the CLIP vision encoder in Multimodal Large Language Models (MLLMs), addressing limitations such as fixed input resolutions and inability to produce separated embeddings for dissimilar images. Two key factors, mesoscopic bias and interpolation bias, are identified as underlying these limitations. QLIP integrates seamlessly with existing MLLMs, enhancing both coarse-grained and fine-grained visual understanding without requiring retraining. It introduces an image quadtree with content-aware patchification to improve visual question answering accuracy in LLaVA v1.5 models, achieving up to a 13.6 percent increase in detailed understanding performance on challenging benchmarks like $V^{\ast}.Summary: <div>
arXiv:2505.23004v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens, aligning visual and textual signals within a shared latent space to facilitate crossmodal representation learning. The CLIP model is a widely adopted foundational vision language model whose vision encoder has played a critical role in the development of MLLMs such as LLaVA. However, the CLIP vision encoder suffers from notable limitations including being constrained to only handling fixed input resolutions and a failure to produce separated embeddings for dissimilar images. Replacing the vision encoder of an existing model typically incurs substantial computational costs because such a change often necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the CLIP vision encoder: mesoscopic bias and interpolation bias. To address these issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly integrated with existing MLLMs with only a few lines of code and can enhance both coarse-grained and fine-grained visual understanding, without re-training. QLIP is designed around an image quadtree which replaces the standard uniform grid patches with a novel content aware patchification. Our experimental results demonstrate that QLIP improves the general visual question answering accuracy of the LLaVA v1.5 model series across various model sizes--without requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts detailed understanding performance on the challenging $V^{\ast}$ benchmark by up to 13.6 percent.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.23009</link>
<guid>https://arxiv.org/abs/2505.23009</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Speech, EmergentTTS, benchmark, evaluation, dataset

Summary: 
EmergentTTS-Eval is a new benchmark for Text-to-Speech (TTS) models, focusing on nuanced and semantically complex text. It covers six challenging scenarios including emotions, foreign words, and syntactic complexity. The benchmark automates test-case generation and evaluation, making it easily extensible. Using a model-as-a-judge approach with a Large Audio Language Model (LALM), it assesses speech across various dimensions such as emotion, prosody, and pronunciation accuracy. State-of-the-art TTS systems like 11Labs, Deepgram, and OpenAI's 4o-mini-TTS were evaluated on EmergentTTS-Eval, revealing performance differences. The model-as-a-judge approach shows robust TTS assessment and correlates well with human preferences. The evaluation code and dataset are open-sourced for further exploration. <br /><br />Summary: <div>
arXiv:2505.23009v1 Announce Type: new 
Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation $\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the $\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Complexity Control Facilitates Reasoning Ability of LLMs</title>
<link>https://arxiv.org/abs/2505.23013</link>
<guid>https://arxiv.org/abs/2505.23013</guid>
<content:encoded><![CDATA[
<div> control, complexity, language models, generalizability, scaling law

Summary:<br />
This study focuses on enhancing the generalizability of large language models (LLMs) by controlling model complexity through adjustments in initialization rate and weight decay coefficient. The researchers demonstrate that by effectively managing model complexity, LLMs show improved scaling laws across various model sizes and data sizes. Comparing benchmark performances of LLMs pretrained on 1T tokens with different complexity hyperparameters, the study highlights the benefits of constant initialization rates for faster scaling law descent. This approach suggests that complexity control is a promising avenue for advancing LLM capabilities in a more reliable manner. <div>
arXiv:2505.23013v1 Announce Type: new 
Abstract: The reasoning ability of large language models (LLMs) has been rapidly advancing in recent years, attracting interest in more fundamental approaches that can reliably enhance their generalizability. This work demonstrates that model complexity control, conveniently implementable by adjusting the initialization rate and weight decay coefficient, improves the scaling law of LLMs consistently over varying model sizes and data sizes. This gain is further illustrated by comparing the benchmark performance of 2.4B models pretrained on 1T tokens with different complexity hyperparameters. Instead of fixing the initialization std, we found that a constant initialization rate (the exponent of std) enables the scaling law to descend faster in both model and data sizes. These results indicate that complexity control is a promising direction for the continual advancement of LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations</title>
<link>https://arxiv.org/abs/2505.23014</link>
<guid>https://arxiv.org/abs/2505.23014</guid>
<content:encoded><![CDATA[
<div> message passing, graph neural networks, hyperbolic partial differential equations, spectral graph neural networks, topological structure<br />
Summary:<br />
This paper presents a novel approach to graph neural networks by formulating message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs). By mapping node representations into a solution space spanned by eigenvectors describing the topological structure of graphs, the system allows for the decomposition of node features into a superposition of these eigenvectors, enhancing interpretability and extracting fundamental characteristics of the topology. The use of hyperbolic PDEs also establishes a connection with spectral graph neural networks, improving their performance across various graph tasks. Additionally, the introduction of polynomials to approximate filter functions enhances flexibility and further boosts the effectiveness of spectral GNNs. Experimental results demonstrate the strong performance and flexibility of this hyperbolic PDE paradigm in improving spectral GNNs. <br /> <div>
arXiv:2505.23014v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the topological features of graph data. Traditional GNNs learns node features in a spatial domain unrelated to the topology, which can hardly ensure topological features. In this paper, we formulates message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs), constituting a dynamical system that explicitly maps node representations into a particular solution space. This solution space is spanned by a set of eigenvectors describing the topological structure of graphs. Within this system, for any moment in time, a node features can be decomposed into a superposition of the basis of eigenvectors. This not only enhances the interpretability of message passing but also enables the explicit extraction of fundamental characteristics about the topological structure. Furthermore, by solving this system of hyperbolic partial differential equations, we establish a connection with spectral graph neural networks (spectral GNNs), serving as a message passing enhancement paradigm for spectral GNNs.We further introduce polynomials to approximate arbitrary filter functions. Extensive experiments demonstrate that the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also significantly enhances the performance of various spectral GNNs across diverse graph tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.23017</link>
<guid>https://arxiv.org/abs/2505.23017</guid>
<content:encoded><![CDATA[
<div> generative model, probabilistic time series forecasting, KoopmanNet, KalmanNet, long-term forecasting
Summary:
- The article introduces $K^2$VAE, a generative model for Probabilistic Time Series Forecasting (PTSF) that addresses the challenges of Long-term Probabilistic Time Series Forecasting (LPTSF).
- $K^2$VAE leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system.
- It then uses a KalmanNet to refine predictions and model uncertainty in the linear system, reducing error accumulation in long-term forecasting.
- Extensive experiments show that $K^2$VAE outperforms existing methods in both short- and long-term PTSF, providing a more efficient and accurate solution. 
- The model shows promise in improving forecasting accuracy and efficiency across various fields such as economics, energy, and transportation.<br /><br />Summary: <div>
arXiv:2505.23017v1 Announce Type: new 
Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference</title>
<link>https://arxiv.org/abs/2505.23022</link>
<guid>https://arxiv.org/abs/2505.23022</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, SLOs, SCORPIO, throughput, heterogeneous workloads 

Summary: 
SCORPIO is a new Large Language Model (LLM) serving system that focuses on optimizing system goodput and meeting Service Level Objectives (SLOs) such as Time to First Token (TTFT) and Time Per Output Token (TPOT). The system is designed to handle workloads with varying SLO requirements by employing adaptive scheduling strategies across admission control, queue management, and batch selection. SCORPIO includes a TTFT Guard that prioritizes requests based on deadlines and rejects unattainable ones, as well as a TPOT Guard that uses a credit-based batching mechanism and VBS-based admission control. These modules are supported by a predictive module. Evaluation results show that SCORPIO significantly improves system goodput and SLO adherence compared to existing methods, with improvements of up to 14.4X in goodput and 46.5% in SLO adherence. <br /><br />Summary: <div>
arXiv:2505.23022v1 Announce Type: new 
Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum throughput. They often neglect Service Level Objectives (SLOs) such as Time to First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving system designed to maximize system goodput and SLO attainment for workloads with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for adaptive scheduling across admission control, queue management, and batch selection. SCORPIO features a TTFT Guard, which employs least-deadline-first reordering and rejects unattainable requests, and a TPOT Guard, which utilizes a VBS-based admission control and a novel credit-based batching mechanism. Both guards are supported by a predictive module. Evaluations demonstrate that SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Federated Prompt Learning for Vision Language Model</title>
<link>https://arxiv.org/abs/2505.23024</link>
<guid>https://arxiv.org/abs/2505.23024</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Model, prompt learning, federated learning, label skew, domain shift

Summary: 
The study explores the application of prompt learning with Vision Language Models (VLM) in federated learning scenarios. It compares the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under challenges like label skew and domain shift. Various configurations such as client scale, aggregation strategies, and prompt length are evaluated to understand the robustness of Federated Prompt Learning (FPL). Strategies for enhancing prompt learning in complex scenarios, where label skew and domain shift coexist, are also discussed, including leveraging both prompt types when computational resources allow. The findings provide practical insights for optimizing prompt learning in federated settings and contribute to the broader deployment of VLMs in privacy-preserving environments. 

<br /><br />Summary: <div>
arXiv:2505.23024v1 Announce Type: new 
Abstract: The Vision Language Model (VLM) excels in aligning vision and language representations, and prompt learning has emerged as a key technique for adapting such models to downstream tasks. However, the application of prompt learning with VLM in federated learning (\fl{}) scenarios remains underexplored. This paper systematically investigates the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under data heterogeneity challenges, including label skew and domain shift. We conduct extensive experiments to evaluate the impact of various \fl{} and prompt configurations, such as client scale, aggregation strategies, and prompt length, to assess the robustness of Federated Prompt Learning (FPL). Furthermore, we explore strategies for enhancing prompt learning in complex scenarios where label skew and domain shift coexist, including leveraging both prompt types when computational resources allow. Our findings offer practical insights into optimizing prompt learning in federated settings, contributing to the broader deployment of VLMs in privacy-preserving environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift</title>
<link>https://arxiv.org/abs/2505.23027</link>
<guid>https://arxiv.org/abs/2505.23027</guid>
<content:encoded><![CDATA[
<div> subpopulation shift, machine learning models, ensemble of diverse classifiers, feature extractor network, prototypical classifiers <br />
Summary: <br />
The article discusses the challenge of subpopulation shift in machine learning models and proposes a novel solution using Diverse Prototypical Ensembles (DPEs). Traditional methods rely on re-weighting strategies based on assumptions about subpopulations, which may not be feasible for many real-world datasets. DPEs involve using an ensemble of diverse classifiers that focus on different features and samples, thus adapting to the risk associated with subpopulations. By replacing the linear classification layer with a mixture of prototypical classifiers in a feature extractor network, DPEs outperform prior approaches in worst-group accuracy on nine real-world datasets. This method shows promising results in addressing subpopulation shift without the need for explicit annotations of group membership. <div>
arXiv:2505.23027v1 Announce Type: new 
Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation distributibetween theween the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks</title>
<link>https://arxiv.org/abs/2505.23032</link>
<guid>https://arxiv.org/abs/2505.23032</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, Prior-data Fitted Networks, neural scaling law, uncertainty quantification, meta-learning <br />
Summary:
This research introduces a Bayesian approach using Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. The method incorporates a prior distribution to generate synthetic functions that mimic real-world scaling laws, enabling the model to learn the extrapolation. The study validates the effectiveness of the proposed framework by comparing it against existing point estimation and Bayesian methods on real neural scaling laws. Results demonstrate superior performance, particularly in scenarios with limited data such as Bayesian active learning, highlighting the method's potential for reliable and uncertainty-aware extrapolation in practical applications. <div>
arXiv:2505.23032v1 Announce Type: new 
Abstract: Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data</title>
<link>https://arxiv.org/abs/2505.23042</link>
<guid>https://arxiv.org/abs/2505.23042</guid>
<content:encoded><![CDATA[
arXiv:2505.23042v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models have inspired the development of foundation models across various domains. In this study, we evaluate the efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected in a graduate classroom. Unlike previous studies that primarily evaluate LEMs using data from controlled clinical settings, our work assesses their applicability to real-world environments. We train a binary classifier that distinguishes between normal and elevated stress states using resting-state EEG data recorded from 18 graduate students during a class session. The best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a 5-second window, significantly outperforming traditional stress classifiers in both accuracy and inference efficiency. We further evaluate the robustness of the fine-tuned LEM under random data shuffling and reduced channel counts. These results demonstrate the capability of LEMs to effectively process real-world EEG data and highlight their potential to revolutionize brain-computer interface applications by shifting the focus from model-centric to data-centric design.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation</title>
<link>https://arxiv.org/abs/2505.23048</link>
<guid>https://arxiv.org/abs/2505.23048</guid>
<content:encoded><![CDATA[
arXiv:2505.23048v1 Announce Type: new 
Abstract: Trajectory data is crucial for various applications but often suffers from incompleteness due to device limitations and diverse collection scenarios. Existing imputation methods rely on sparse trajectory or travel information, such as velocity, to infer missing points. However, these approaches assume that sparse trajectories retain essential behavioral patterns, which place significant demands on data acquisition and overlook the potential of large-scale human trajectory embeddings. To address this, we propose ProDiff, a trajectory imputation framework that uses only two endpoints as minimal information. It integrates prototype learning to embed human movement patterns and a denoising diffusion probabilistic model for robust spatiotemporal reconstruction. Joint training with a tailored loss function ensures effective imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by 6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927 correlation between generated and real trajectories, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</title>
<link>https://arxiv.org/abs/2505.23049</link>
<guid>https://arxiv.org/abs/2505.23049</guid>
<content:encoded><![CDATA[
arXiv:2505.23049v1 Announce Type: new 
Abstract: Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation - especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are available at https://github.com/Axel-gu/DenoiseRotator.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2505.23055</link>
<guid>https://arxiv.org/abs/2505.23055</guid>
<content:encoded><![CDATA[
arXiv:2505.23055v1 Announce Type: new 
Abstract: Clinical decision-making is inherently complex and fast-paced, particularly in emergency departments (EDs) where critical, rapid and high-stakes decisions are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools that combine signs, symptoms, and clinical variables into decision trees to make consistent and accurate diagnoses. CDR usage is often hindered by the clinician's cognitive load, limiting their ability to quickly recall and apply the appropriate rules. We introduce CDR-Agent, a novel LLM-based system designed to enhance ED decision-making by autonomously identifying and applying the most appropriate CDRs based on unstructured clinical notes. To validate CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\% (synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. Moreover, CDR-Agent significantly reduces computational overhead. Using these datasets, we demonstrated that CDR-Agent not only selects relevant CDRs efficiently, but makes cautious yet effective imaging decisions by minimizing unnecessary interventions while successfully identifying most positively diagnosed cases, outperforming traditional LLM prompting approaches. Code for our work can be found at: https://github.com/zhenxianglance/medagent-cdr-agent
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINGO: Constrained Inference for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2505.23061</link>
<guid>https://arxiv.org/abs/2505.23061</guid>
<content:encoded><![CDATA[
arXiv:2505.23061v1 Announce Type: new 
Abstract: Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</title>
<link>https://arxiv.org/abs/2505.23062</link>
<guid>https://arxiv.org/abs/2505.23062</guid>
<content:encoded><![CDATA[
arXiv:2505.23062v1 Announce Type: new 
Abstract: Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification</title>
<link>https://arxiv.org/abs/2505.23063</link>
<guid>https://arxiv.org/abs/2505.23063</guid>
<content:encoded><![CDATA[
arXiv:2505.23063v1 Announce Type: new 
Abstract: Crop disease detection and classification is a critical challenge in agriculture, with major implications for productivity, food security, and environmental sustainability. While deep learning models such as CNN and ViT have shown excellent performance in classifying plant diseases from images, their large-scale deployment is often limited by data privacy concerns. Federated Learning (FL) addresses this issue, but centralized FL remains vulnerable to single-point failures and scalability limits. In this paper, we introduce a novel Decentralized Federated Learning (DFL) framework that uses validation loss (Loss_val) both to guide model sharing between peers and to correct local training via an adaptive loss function controlled by weighting parameter. We conduct extensive experiments using PlantVillage datasets with three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the impact of weighting parameter, the number of shared models, the number of clients, and the use of Loss_val versus Loss_train of other clients. Results demonstrate that our DFL approach not only improves accuracy and convergence speed, but also ensures better generalization and robustness across heterogeneous data environments making it particularly well-suited for privacy-preserving agricultural applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Learning with Bayesian-Oriented Gradient Calibration</title>
<link>https://arxiv.org/abs/2505.23071</link>
<guid>https://arxiv.org/abs/2505.23071</guid>
<content:encoded><![CDATA[
arXiv:2505.23071v1 Announce Type: new 
Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to improve predictive accuracy. However, existing methods mainly aggregate gradients with fixed weights and treat all dimensions equally, overlooking the intrinsic gradient uncertainty of each modality. This may lead to (i) excessive updates in sensitive dimensions, degrading performance, and (ii) insufficient updates in less sensitive dimensions, hindering learning. To address this issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for MML to explicitly model the gradient uncertainty and guide the model optimization towards the optimal direction. Specifically, we first model each modality's gradient as a random variable and derive its probability distribution, capturing the full uncertainty in the gradient space. Then, we propose an effective method that converts the precision (inverse variance) of each gradient distribution into a scalar evidence. This evidence quantifies the confidence of each modality in every gradient dimension. Using these evidences, we explicitly quantify per-dimension uncertainties and fuse them via a reduced Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a calibrated update direction that balances sensitivity and conservatism across dimensions. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and advantages of the proposed method.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Boosting Decision Tree with LSTM for Investment Prediction</title>
<link>https://arxiv.org/abs/2505.23084</link>
<guid>https://arxiv.org/abs/2505.23084</guid>
<content:encoded><![CDATA[
arXiv:2505.23084v1 Announce Type: new 
Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term Memory) networks with LightGBM and CatBoost for stock price prediction. The framework processes time-series financial data and evaluates performance using seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the strengths of sequential and tree-based approaches. Experimental results show that the proposed framework improves accuracy by 10 to 15 percent compared to individual models and reduces error during market changes. This study highlights the potential of ensemble methods for financial forecasting and provides a flexible design for integrating new machine learning techniques.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Spherical Transformer for Efficient Molecular Modeling</title>
<link>https://arxiv.org/abs/2505.23086</link>
<guid>https://arxiv.org/abs/2505.23086</guid>
<content:encoded><![CDATA[
arXiv:2505.23086v1 Announce Type: new 
Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced molecular system modeling by employing group representations. However, their message passing processes, which rely on tensor product-based convolutions, are limited by insufficient non-linearity and incomplete group representations, thereby restricting expressiveness. To overcome these limitations, we introduce the Equivariant Spherical Transformer (EST), a novel framework that leverages a Transformer structure within the spatial domain of group representations after Fourier transform. We theoretically and empirically demonstrate that EST can encompass the function space of tensor products while achieving superior expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed through a uniform sampling strategy for the Fourier transform. Our experiments demonstrate state-of-the-art performance by EST on various molecular benchmarks, including OC20 and QM9.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAP: Revisiting Weight Decomposition for Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.23094</link>
<guid>https://arxiv.org/abs/2505.23094</guid>
<content:encoded><![CDATA[
arXiv:2505.23094v1 Announce Type: new 
Abstract: The rapid development of large language models has revolutionized natural language processing, but their fine-tuning remains computationally expensive, hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have emerged as solutions. Recent work like DoRA attempts to further decompose weight adaptation into direction and magnitude components. However, existing formulations often define direction heuristically at the column level, lacking a principled geometric foundation. In this paper, we propose MAP, a novel framework that reformulates weight matrices as high-dimensional vectors and decouples their adaptation into direction and magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors. This design enables more interpretable and flexible adaptation, and can be seamlessly integrated into existing PEFT methods. Extensive experiments show that MAP significantly improves performance when coupling with existing methods, offering a simple yet powerful enhancement to existing PEFT methods. Given the universality and simplicity of MAP, we hope it can serve as a default setting for designing future PEFT methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Search for Vehicle Routing with Multiple Time Windows</title>
<link>https://arxiv.org/abs/2505.23098</link>
<guid>https://arxiv.org/abs/2505.23098</guid>
<content:encoded><![CDATA[
arXiv:2505.23098v1 Announce Type: new 
Abstract: In this study, we propose a reinforcement learning-based adaptive variable neighborhood search (RL-AVNS) method designed for effectively solving the Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional adaptive approaches that rely solely on historical operator performance, our method integrates a reinforcement learning framework to dynamically select neighborhood operators based on real-time solution states and learned experience. We introduce a fitness metric that quantifies customers' temporal flexibility to improve the shaking phase, and employ a transformer-based neural policy network to intelligently guide operator selection during the local search. Extensive computational experiments are conducted on realistic scenarios derived from the replenishment of unmanned vending machines, characterized by multiple clustered replenishment windows. Results demonstrate that RL-AVNS significantly outperforms traditional variable neighborhood search (VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics, achieving substantial improvements in solution quality and computational efficiency across various instance scales and time window complexities. Particularly notable is the algorithm's capability to generalize effectively to problem instances not encountered during training, underscoring its practical utility for complex logistics scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight Spectra Induced Efficient Model Adaptation</title>
<link>https://arxiv.org/abs/2505.23099</link>
<guid>https://arxiv.org/abs/2505.23099</guid>
<content:encoded><![CDATA[
arXiv:2505.23099v1 Announce Type: new 
Abstract: Large-scale foundation models have demonstrated remarkable versatility across a wide range of downstream tasks. However, fully fine-tuning these models incurs prohibitive computational costs, motivating the development of Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces low-rank updates to pre-trained weights. Despite their empirical success, the underlying mechanisms by which PEFT modifies model parameters remain underexplored. In this work, we present a systematic investigation into the structural changes of weight matrices during fully fine-tuning. Through singular value decomposition (SVD), we reveal that fine-tuning predominantly amplifies the top singular values while leaving the remainder largely intact, suggesting that task-specific knowledge is injected into a low-dimensional subspace. Furthermore, we find that the dominant singular vectors are reoriented in task-specific directions, whereas the non-dominant subspace remains stable. Building on these insights, we propose a novel method that leverages learnable rescaling of top singular directions, enabling precise modulation of the most influential components without disrupting the global structure. Our approach achieves consistent improvements over strong baselines across multiple tasks, highlighting the efficacy of structurally informed fine-tuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics</title>
<link>https://arxiv.org/abs/2505.23105</link>
<guid>https://arxiv.org/abs/2505.23105</guid>
<content:encoded><![CDATA[
arXiv:2505.23105v1 Announce Type: new 
Abstract: When accelerators fail in modern ML datacenters, operators migrate the affected ML training or inference jobs to entirely new racks. This approach, while preserving network performance, is highly inefficient, requiring datacenters to reserve full racks of idle accelerators for fault tolerance. In this paper, we address this resource inefficiency by introducing LUMION, a novel reconfigurable optical fabric for connecting accelerators within a datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically integrates spare accelerators into ongoing workloads as failures occur, thereby maintaining consistent performance without costly migrations. We show the benefits of LUMION by building an end-to-end hardware prototype. Our experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a healthy one and restarts the ML job within ~ 1 second of the failure. LUMION achieves higher inter-GPU bandwidth compared to traditional electrical racks after replacing failed accelerators with spare ones, leading to nearly 2X improvement in fine-tuning throughput.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery</title>
<link>https://arxiv.org/abs/2505.23106</link>
<guid>https://arxiv.org/abs/2505.23106</guid>
<content:encoded><![CDATA[
arXiv:2505.23106v1 Announce Type: new 
Abstract: Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs -- a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2505.23116</link>
<guid>https://arxiv.org/abs/2505.23116</guid>
<content:encoded><![CDATA[
arXiv:2505.23116v1 Announce Type: new 
Abstract: Time series forecasting with exogenous variables is a critical emerging paradigm that presents unique challenges in modeling dependencies between variables. Traditional models often struggle to differentiate between endogenous and exogenous variables, leading to inefficiencies and overfitting. In this paper, we introduce CrossLinear, a novel Linear-based forecasting model that addresses these challenges by incorporating a plug-and-play cross-correlation embedding module. This lightweight module captures the dependencies between variables with minimal computational cost and seamlessly integrates into existing neural networks. Specifically, it captures time-invariant and direct variable dependencies while disregarding time-varying or indirect dependencies, thereby mitigating the risk of overfitting in dependency modeling and contributing to consistent performance improvements. Furthermore, CrossLinear employs patch-wise processing and a global linear head to effectively capture both short-term and long-term temporal dependencies, further improving its forecasting precision. Extensive experiments on 12 real-world datasets demonstrate that CrossLinear achieves superior performance in both short-term and long-term forecasting tasks. The ablation study underscores the effectiveness of the cross-correlation embedding module. Additionally, the generalizability of this module makes it a valuable plug-in for various forecasting tasks across different domains. Codes are available at https://github.com/mumiao2000/CrossLinear.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking</title>
<link>https://arxiv.org/abs/2505.23117</link>
<guid>https://arxiv.org/abs/2505.23117</guid>
<content:encoded><![CDATA[
arXiv:2505.23117v1 Announce Type: new 
Abstract: In the era of large-scale training, model merging has evolved into a tool for creating multitasking models efficiently. It enables the knowledge of models to be fused, without the need for heavy computation as required in traditional multitask learning. Existing merging methods often assume that entries at identical positions in weight matrices serve the same function, enabling straightforward entry-wise comparison and merging. However, this assumption overlooks the complexity of finetuned neural networks, where neurons may develop distinct feature compositions, making direct entry-wise merging problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, where entry-wise merging becomes possible. We showcase the effectiveness of DRM across various settings ranging from smaller encoder-based such as ViT and DeBERTa, encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B. Our experimental results show that DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings. Moreover, our analysis reveals renormalization as the crucial component for creating a robust and even joint space for merging, significantly contributing to the method's performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs</title>
<link>https://arxiv.org/abs/2505.23131</link>
<guid>https://arxiv.org/abs/2505.23131</guid>
<content:encoded><![CDATA[
arXiv:2505.23131v1 Announce Type: new 
Abstract: We study the problem of assigning operations in a dataflow graph to devices to minimize execution time in a work-conserving system, with emphasis on complex machine learning workloads. Prior learning-based methods often struggle due to three key limitations: (1) reliance on bulk-synchronous systems like TensorFlow, which under-utilize devices due to barrier synchronization; (2) lack of awareness of the scheduling mechanism of underlying systems when designing learning-based methods; and (3) exclusive dependence on reinforcement learning, ignoring the structure of effective heuristics designed by experts. In this paper, we propose \textsc{Doppler}, a three-stage framework for training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen operations on devices. Our experiments show that \textsc{Doppler} outperforms all baseline methods across tasks by reducing system execution time and additionally demonstrates sampling efficiency by reducing per-episode training time.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERINA: Benchmarking Verifiable Code Generation</title>
<link>https://arxiv.org/abs/2505.23135</link>
<guid>https://arxiv.org/abs/2505.23135</guid>
<content:encoded><![CDATA[
arXiv:2505.23135v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners</title>
<link>https://arxiv.org/abs/2505.23150</link>
<guid>https://arxiv.org/abs/2505.23150</guid>
<content:encoded><![CDATA[
arXiv:2505.23150v1 Announce Type: new 
Abstract: Recent advances in language modeling and vision stem from training large models on diverse, multi-task data. This paradigm has had limited impact in value-based reinforcement learning (RL), where improvements are often driven by small models trained in a single-task context. This is because in multi-task RL sparse rewards and gradient conflicts make optimization of temporal difference brittle. Practical workflows for generalist policies therefore avoid online training, instead cloning expert trajectories or distilling collections of single-task policies into one agent. In this work, we show that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi-task training. We test our approach on 7 multi-task benchmarks with over 280 unique tasks, spanning high degree-of-freedom humanoid control and discrete vision-based RL. We find that, despite its simplicity, the proposed approach leads to state-of-the-art single and multi-task performance, as well as sample-efficient transfer to new tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Arm Identification with Possibly Biased Offline Data</title>
<link>https://arxiv.org/abs/2505.23165</link>
<guid>https://arxiv.org/abs/2505.23165</guid>
<content:encoded><![CDATA[
arXiv:2505.23165v1 Announce Type: new 
Abstract: We study the best arm identification (BAI) problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials. We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions. To address this, we propose the LUCB-H algorithm, which introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows that LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. We also derive an instance-dependent lower bound that matches the upper bound of LUCB-H in certain scenarios. Numerical experiments further demonstrate the robustness and adaptability of LUCB-H in effectively incorporating offline data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization</title>
<link>https://arxiv.org/abs/2505.23173</link>
<guid>https://arxiv.org/abs/2505.23173</guid>
<content:encoded><![CDATA[
arXiv:2505.23173v1 Announce Type: new 
Abstract: Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. While Multi-source Domain Generalization (MDG) has shown promise in addressing this challenge by leveraging multiple source domains during training, its practical application is limited by the significant costs and difficulties associated with creating multi-domain datasets. To address this limitation, we propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms. Through extensive experiments with PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze the effectiveness of PMDG across multiple datasets and architectures. Our analysis reveals several key findings, including a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data. These comprehensive empirical results provide valuable insights for future research in domain generalization. Our code is available at https://github.com/s-enmt/PseudoDomainBed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2505.23176</link>
<guid>https://arxiv.org/abs/2505.23176</guid>
<content:encoded><![CDATA[
arXiv:2505.23176v1 Announce Type: new 
Abstract: To improve the training efficiency of federated learning (FL), previous research has employed low-rank decomposition techniques to reduce communication overhead. In this paper, we seek to enhance the performance of these low-rank decomposition methods. Specifically, we focus on three key issues related to decomposition in FL: what to decompose, how to decompose, and how to aggregate. Subsequently, we introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), each targeting a specific issue. These techniques are complementary and can be applied simultaneously to achieve optimal performance. Additionally, we provide a rigorous theoretical analysis to ensure the convergence of the proposed MUD. Extensive experimental results show that our approach achieves faster convergence and superior accuracy compared to relevant baseline methods. The code is available at https://github.com/Leopold1423/fedmud-icml25.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification</title>
<link>https://arxiv.org/abs/2505.23181</link>
<guid>https://arxiv.org/abs/2505.23181</guid>
<content:encoded><![CDATA[
arXiv:2505.23181v1 Announce Type: new 
Abstract: Contrastive learning has emerged as a competent approach for unsupervised representation learning. However, the design of an optimal augmentation strategy, although crucial for contrastive learning, is less explored for time series classification tasks. Existing predefined time-domain augmentation methods are primarily adopted from vision and are not specific to time series data. Consequently, this cross-modality incompatibility may distort the semantically relevant information of time series by introducing mismatched patterns into the data. To address this limitation, we present a novel perspective from the frequency domain and identify three advantages for downstream classification: global, independent, and compact. To fully utilize the three properties, we propose the lightweight yet effective Frequency Refined Augmentation (FreRA) tailored for time series contrastive learning on classification tasks, which can be seamlessly integrated with contrastive learning frameworks in a plug-and-play manner. Specifically, FreRA automatically separates critical and unimportant frequency components. Accordingly, we propose semantic-aware Identity Modification and semantic-agnostic Self-adaptive Modification to protect semantically relevant information in the critical frequency components and infuse variance into the unimportant ones respectively. Theoretically, we prove that FreRA generates semantic-preserving views. Empirically, we conduct extensive experiments on two benchmark datasets, including UCR and UEA archives, as well as five large-scale datasets on diverse applications. FreRA consistently outperforms ten leading baselines on time series classification, anomaly detection, and transfer learning tasks, demonstrating superior capabilities in contrastive representation learning and generalization in transfer learning scenarios across diverse datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation</title>
<link>https://arxiv.org/abs/2505.23182</link>
<guid>https://arxiv.org/abs/2505.23182</guid>
<content:encoded><![CDATA[
arXiv:2505.23182v1 Announce Type: new 
Abstract: Collaborative training methods like Federated Learning (FL) and Split Learning (SL) enable distributed machine learning without sharing raw data. However, FL assumes clients can train entire models, which is infeasible for large-scale models. In contrast, while SL alleviates the client memory constraint in FL by offloading most training to the server, it increases network latency due to its sequential nature. Other methods address the conundrum by using local loss functions for parallel client-side training to improve efficiency, but they lack server feedback and potentially suffer poor accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation Gradient Estimation), a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models. These auxiliary models periodically adapt to emulate server behavior on local datasets. We show that FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of communication rounds. This result matches FedAvg, while significantly reducing communication costs and client memory requirements. Our empirical results also verify that it outperforms existing state-of-the-art FSL methods, offering both communication efficiency and accuracy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Is Better Than One: Rotations Scale LoRAs</title>
<link>https://arxiv.org/abs/2505.23184</link>
<guid>https://arxiv.org/abs/2505.23184</guid>
<content:encoded><![CDATA[
arXiv:2505.23184v1 Announce Type: new 
Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates large language models (LLMs) to efficiently adapt to diverse tasks. However, traditional gating mechanisms that route inputs to the best experts may fundamentally hinder LLMs' scalability, leading to poor generalization and underfitting issues. We identify that the root cause lies in the restricted expressiveness of existing weighted-sum mechanisms, both within and outside the convex cone of LoRA representations. This motivates us to propose RadarGate, a novel geometrically inspired gating method that introduces rotational operations of LoRAs representations to boost the expressiveness and facilitate richer feature interactions among multiple LoRAs for scalable LLMs. Specifically, we first fuse each LoRA representation to other LoRAs using a learnable component and then feed the output to a rotation matrix. This matrix involves learnable parameters that define the relative angular relationship between LoRA representations. Such a simple yet effective mechanism provides an extra degree of freedom, facilitating the learning of cross-LoRA synergies and properly tracking the challenging poor generalization and underfitting issues as the number of LoRA grows. Extensive experiments on 6 public benchmarks across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We also provide valuable insights, revealing that the rotations to each pair of representations are contrastive, encouraging closer alignment of semantically similar representations during geometrical transformation while pushing distance ones further apart. We will release our code to the community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Effective Receptive Field of Message-Passing Neural Networks</title>
<link>https://arxiv.org/abs/2505.23185</link>
<guid>https://arxiv.org/abs/2505.23185</guid>
<content:encoded><![CDATA[
arXiv:2505.23185v1 Announce Type: new 
Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for processing and analyzing graph-structured data. However, their effectiveness is often hindered by phenomena such as over-squashing, where long-range dependencies or interactions are inadequately captured and expressed in the MPNN output. This limitation mirrors the challenges of the Effective Receptive Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical receptive field is underutilized in practice. In this work, we show and theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired by recent advances in ERF augmentation for CNNs, we propose an Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address these problems in MPNNs. Our method incorporates a hierarchical coarsening of the graph, enabling message-passing across multiscale representations and facilitating long-range interactions without excessive depth or parameterization. Through extensive evaluations on benchmarks such as the Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over baseline MPNNs in capturing long-range dependencies while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer</title>
<link>https://arxiv.org/abs/2505.23190</link>
<guid>https://arxiv.org/abs/2505.23190</guid>
<content:encoded><![CDATA[
arXiv:2505.23190v1 Announce Type: new 
Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to address the steady-state Radiative Transfer Equation (RTE). The RTE is a differential-integral equation that governs the propagation of radiation through a participating medium, with applications spanning diverse domains such as neutron transport, atmospheric radiative transfer, heat transfer, and optical imaging. Our proposed DeepRTE framework leverages pre-trained attention-based neural networks to solve the RTE with high accuracy and computational efficiency. The efficacy of the proposed approach is substantiated through comprehensive numerical experiments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics</title>
<link>https://arxiv.org/abs/2505.23194</link>
<guid>https://arxiv.org/abs/2505.23194</guid>
<content:encoded><![CDATA[
arXiv:2505.23194v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning method. In standard LoRA layers, one of the matrices, $A$ or $B$, is initialized to zero, ensuring that fine-tuning starts from the pretrained model. However, there is no theoretical support for this practice. In this paper, we investigate the impact of non-zero initialization on LoRA's fine-tuning dynamics from an infinite-width perspective. Our analysis reveals that, compared to zero initialization, simultaneously initializing $A$ and $B$ to non-zero values improves LoRA's robustness to suboptimal learning rates, particularly smaller ones. Further analysis indicates that although the non-zero initialization of $AB$ introduces random noise into the pretrained weight, it generally does not affect fine-tuning performance. In other words, fine-tuning does not need to strictly start from the pretrained model. The validity of our findings is confirmed through extensive experiments across various models and datasets. The code is available at https://github.com/Leopold1423/non_zero_lora-icml25.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v1 Announce Type: new 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This "prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Daunce: Data Attribution through Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2505.23223</link>
<guid>https://arxiv.org/abs/2505.23223</guid>
<content:encoded><![CDATA[
arXiv:2505.23223v1 Announce Type: new 
Abstract: Training data attribution (TDA) methods aim to identify which training examples influence a model's predictions on specific test data most. By quantifying these influences, TDA supports critical applications such as data debugging, curation, and valuation. Gradient-based TDA methods rely on gradients and second-order information, limiting their applicability at scale. While recent random projection-based methods improve scalability, they often suffer from degraded attribution accuracy. Motivated by connections between uncertainty and influence functions, we introduce Daunce - a simple yet effective data attribution approach through uncertainty estimation. Our method operates by fine-tuning a collection of perturbed models and computing the covariance of per-example losses across these models as the attribution score. Daunce is scalable to large language models (LLMs) and achieves more accurate attribution compared to existing TDA methods. We validate Daunce on tasks ranging from vision tasks to LLM fine-tuning, and further demonstrate its compatibility with black-box model access. Applied to OpenAI's GPT models, our method achieves, to our knowledge, the first instance of data attribution on proprietary LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizability vs. Counterfactual Explainability Trade-Off</title>
<link>https://arxiv.org/abs/2505.23225</link>
<guid>https://arxiv.org/abs/2505.23225</guid>
<content:encoded><![CDATA[
arXiv:2505.23225v1 Announce Type: new 
Abstract: In this work, we investigate the relationship between model generalization and counterfactual explainability in supervised learning. We introduce the notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) -- the probability of finding perturbations of a data point within its $\varepsilon$-neighborhood that result in a label change. We provide a theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the model's decision boundary, showing that $\varepsilon$-VCP tends to increase with model overfitting. Our findings establish a rigorous connection between poor generalization and the ease of counterfactual generation, revealing an inherent trade-off between generalization and counterfactual explainability. Empirical results validate our theory, suggesting $\varepsilon$-VCP as a practical proxy for quantitatively characterizing overfitting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method</title>
<link>https://arxiv.org/abs/2505.23228</link>
<guid>https://arxiv.org/abs/2505.23228</guid>
<content:encoded><![CDATA[
arXiv:2505.23228v1 Announce Type: new 
Abstract: The rapid growth in feature dimension may introduce implicit associations between features and labels in multi-label datasets, making the relationships between features and labels increasingly complex. Moreover, existing methods often adopt low-dimensional linear decomposition to explore the associations between features and labels. However, linear decomposition struggles to capture complex nonlinear associations and may lead to misalignment between the feature space and the label space. To address these two critical challenges, we propose innovative solutions. First, we design a random walk graph that integrates feature-feature, label-label, and feature-label relationships to accurately capture nonlinear and implicit indirect associations, while optimizing the latent representations of associations between features and labels after low-rank decomposition. Second, we align the variable spaces by leveraging low-dimensional representation coefficients, while preserving the manifold structure between the original high-dimensional multi-label data and the low-dimensional representation space. Extensive experiments and ablation studies conducted on seven benchmark datasets and three representative datasets using various evaluation metrics demonstrate the superiority of the proposed method\footnote{Code: https://github.com/Heilong623/-GRW-}.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivalence of stochastic and deterministic policy gradients</title>
<link>https://arxiv.org/abs/2505.23244</link>
<guid>https://arxiv.org/abs/2505.23244</guid>
<content:encoded><![CDATA[
arXiv:2505.23244v1 Announce Type: new 
Abstract: Policy gradients in continuous control have been derived for both stochastic and deterministic policies. Here we study the relationship between the two. In a widely-used family of MDPs involving Gaussian control noise and quadratic control costs, we show that the stochastic and deterministic policy gradients, natural gradients, and state value functions are identical; while the state-control value functions are different. We then develop a general procedure for constructing an MDP with deterministic policy that is equivalent to a given MDP with stochastic policy. The controls of this new MDP are the sufficient statistics of the stochastic policy in the original MDP. Our results suggest that policy gradient methods can be unified by approximating state value functions rather than state-control value functions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Participant Contributions in Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.23246</link>
<guid>https://arxiv.org/abs/2505.23246</guid>
<content:encoded><![CDATA[
arXiv:2505.23246v1 Announce Type: new 
Abstract: Federated learning (FL) enables multiple clients to collaboratively train models without sharing their data. Measuring participant contributions in FL is crucial for incentivizing clients and ensuring transparency. While various methods have been proposed for contribution measurement, they are designed exclusively for centralized federated learning (CFL), where a central server collects and aggregates client models, along with evaluating their contributions. Meanwhile, decentralized federated learning (DFL), in which clients exchange models directly without a central server, has gained significant attention for mitigating communication bottlenecks and eliminating a single point of failure. However, applying existing contribution measurement methods to DFL is challenging due to the presence of multiple global models and the absence of a central server. In this study, we present novel methodologies for measuring participant contributions in DFL. We first propose DFL-Shapley, an extension of the Shapley value tailored for DFL, adapting this widely used CFL metric to decentralized settings. Given the impracticality of computing the ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable approximation that estimates overall contributions by accumulating round-wise Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios and compare them with existing CFL metrics. The experimental results confirm DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity to DFL-Shapley across various settings, highlighting their effectiveness as contribution metrics in DFL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating RLHF Training with Reward Variance Increase</title>
<link>https://arxiv.org/abs/2505.23247</link>
<guid>https://arxiv.org/abs/2505.23247</guid>
<content:encoded><![CDATA[
arXiv:2505.23247v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Access Diffusion Fisher: Within the Outer Product Span Space</title>
<link>https://arxiv.org/abs/2505.23264</link>
<guid>https://arxiv.org/abs/2505.23264</guid>
<content:encoded><![CDATA[
arXiv:2505.23264v1 Announce Type: new 
Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the second-order diffusion Fisher information (DF), defined as the negative Hessian of log density, into various downstream tasks and theoretical analysis. However, current practices typically approximate the diffusion Fisher by applying auto-differentiation to the learned score network. This black-box method, though straightforward, lacks any accuracy guarantee and is time-consuming. In this paper, we show that the diffusion Fisher actually resides within a space spanned by the outer products of score and initial data. Based on the outer-product structure, we develop two efficient approximation algorithms to access the trace and matrix-vector multiplication of DF, respectively. These algorithms bypass the auto-differentiation operations with time-efficient vector-product calculations. Furthermore, we establish the approximation error bounds for the proposed algorithms. Experiments in likelihood evaluation and adjoint optimization demonstrate the superior accuracy and reduced computational cost of our proposed algorithms. Additionally, based on the novel outer-product formulation of DF, we design the first numerical verification experiment for the optimal transport property of the general PF-ODE deduced map.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2505.23270</link>
<guid>https://arxiv.org/abs/2505.23270</guid>
<content:encoded><![CDATA[
arXiv:2505.23270v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data</title>
<link>https://arxiv.org/abs/2505.23285</link>
<guid>https://arxiv.org/abs/2505.23285</guid>
<content:encoded><![CDATA[
arXiv:2505.23285v1 Announce Type: new 
Abstract: Land cover and land use (LULC) changes are key applications of satellite imagery, and they have critical roles in resource management, urbanization, protection of soils and the environment, and enhancing sustainable development. The literature has heavily utilized multispectral spatiotemporal satellite data alongside advanced machine learning algorithms to monitor and predict LULC changes. This study analyzes and compares LULC changes across various governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using annual time steps. For the chosen region, multispectral spatiotemporal data were acquired from the open-source Sentinel-2 satellite dataset. Supervised machine learning algorithms were used to train and classify different land covers, such as water bodies, crops, urban, etc. The constructed model was subsequently applied within the study region, allowing for an effective comparative evaluation of LULC changes within the given timeframe.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Generative Modeling for Conditional Independence Testing</title>
<link>https://arxiv.org/abs/2505.23309</link>
<guid>https://arxiv.org/abs/2505.23309</guid>
<content:encoded><![CDATA[
arXiv:2505.23309v1 Announce Type: new 
Abstract: Determining conditional independence (CI) relationships between random variables is a fundamental yet challenging task in machine learning and statistics, especially in high-dimensional settings. Existing generative model-based CI testing methods, such as those utilizing generative adversarial networks (GANs), often struggle with undesirable modeling of conditional distributions and training instability, resulting in subpar performance. To address these issues, we propose a novel CI testing method via score-based generative modeling, which achieves precise Type I error control and strong testing power. Concretely, we first employ a sliced conditional score matching scheme to accurately estimate conditional score and use Langevin dynamics conditional sampling to generate null hypothesis samples, ensuring precise Type I error control. Then, we incorporate a goodness-of-fit stage into the method to verify generated samples and enhance interpretability in practice. We theoretically establish the error bound of conditional distributions modeled by score-based generative models and prove the validity of our CI tests. Extensive experiments on both synthetic and real-world datasets show that our method significantly outperforms existing state-of-the-art methods, providing a promising way to revitalize generative model-based CI testing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing</title>
<link>https://arxiv.org/abs/2505.23320</link>
<guid>https://arxiv.org/abs/2505.23320</guid>
<content:encoded><![CDATA[
arXiv:2505.23320v1 Announce Type: new 
Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable for a modern classifier: They are easily interpretable, highly scalable, and offer adaptable complexity. However, traditional methods for learning BNCs have historically underperformed when compared to leading classification methods such as random forests. Recent parameter smoothing techniques using hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve performance competitive with random forests on categorical data, but these techniques are relatively inflexible, and require a complicated, specialized sampling process. In this paper, we introduce a novel method for parameter estimation that uses a log-linear regression to approximate the behaviour of HDPs. As a linear model, our method is remarkably flexible and simple to interpret, and can leverage the vast literature on learning linear models. Our experiments show that our method can outperform HDP smoothing while being orders of magnitude faster, remaining competitive with random forests on categorical data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X2Graph for Cancer Subtyping Prediction on Biological Tabular Data</title>
<link>https://arxiv.org/abs/2505.23334</link>
<guid>https://arxiv.org/abs/2505.23334</guid>
<content:encoded><![CDATA[
arXiv:2505.23334v1 Announce Type: new 
Abstract: Despite the transformative impact of deep learning on text, audio, and image datasets, its dominance in tabular data, especially in the medical domain where data are often scarce, remains less clear. In this paper, we propose X2Graph, a novel deep learning method that achieves strong performance on small biological tabular datasets. X2Graph leverages external knowledge about the relationships between table columns, such as gene interactions, to convert each sample into a graph structure. This transformation enables the application of standard message passing algorithms for graph modeling. Our X2Graph method demonstrates superior performance compared to existing tree-based and deep learning methods across three cancer subtyping datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Model Learning for Improved Elastic Student Models</title>
<link>https://arxiv.org/abs/2505.23337</link>
<guid>https://arxiv.org/abs/2505.23337</guid>
<content:encoded><![CDATA[
arXiv:2505.23337v1 Announce Type: new 
Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Positional Autoencoders as Self-supervised Learners</title>
<link>https://arxiv.org/abs/2505.23345</link>
<guid>https://arxiv.org/abs/2505.23345</guid>
<content:encoded><![CDATA[
arXiv:2505.23345v1 Announce Type: new 
Abstract: Graph self-supervised learning seeks to learn effective graph representations without relying on labeled data. Among various approaches, graph autoencoders (GAEs) have gained significant attention for their efficiency and scalability. Typically, GAEs take incomplete graphs as input and predict missing elements, such as masked nodes or edges. While effective, our experimental investigation reveals that traditional node or edge masking paradigms primarily capture low-frequency signals in the graph and fail to learn the expressive structural information. To address these issues, we propose Graph Positional Autoencoders (GraphPAE), which employs a dual-path architecture to reconstruct both node features and positions. Specifically, the feature path uses positional encoding to enhance the message-passing processing, improving GAE's ability to predict the corrupted information. The position path, on the other hand, leverages node representations to refine positions and approximate eigenvectors, thereby enabling the encoder to learn diverse frequency information. We conduct extensive experiments to verify the effectiveness of GraphPAE, including heterophilic node classification, graph property prediction, and transfer learning. The results demonstrate that GraphPAE achieves state-of-the-art performance and consistently outperforms baselines by a large margin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms</title>
<link>https://arxiv.org/abs/2505.23347</link>
<guid>https://arxiv.org/abs/2505.23347</guid>
<content:encoded><![CDATA[
arXiv:2505.23347v1 Announce Type: new 
Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge service Platforms (CCPs) are playing an increasingly important role in meeting the increasing demand. Although stream scheduling plays a critical role in optimizing CCPs' revenue, most optimization strategies struggle to achieve practical results due to various anomalies in unstable CCPs. Additionally, the substantial scale of CCPs magnifies the difficulties of anomaly detection in time-sensitive scheduling. To tackle these challenges, this paper proposes Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a strategy pool; in the post-scheduling stage, upon request arrival, it triggers an appropriate scheduling based on a pre-generated strategy to implement the scheduling process. Extensive experiments on realistic datasets show that Sentinel significantly reduces anomaly frequency by 70%, improves revenue by 74%, and doubles the scheduling speed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reward Fairness in RLHF: From a Resource Allocation Perspective</title>
<link>https://arxiv.org/abs/2505.23349</link>
<guid>https://arxiv.org/abs/2505.23349</guid>
<content:encoded><![CDATA[
arXiv:2505.23349v1 Announce Type: new 
Abstract: Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control</title>
<link>https://arxiv.org/abs/2505.23355</link>
<guid>https://arxiv.org/abs/2505.23355</guid>
<content:encoded><![CDATA[
arXiv:2505.23355v1 Announce Type: new 
Abstract: Climate control is crucial for greenhouse production as it directly affects crop growth and resource use. Reinforcement learning (RL) has received increasing attention in this field, but still faces challenges, including limited training efficiency and high reliance on initial learning conditions. Interactive RL, which combines human (grower) input with the RL agent's learning, offers a potential solution to overcome these challenges. However, interactive RL has not yet been applied to greenhouse climate control and may face challenges related to imperfect inputs. Therefore, this paper aims to explore the possibility and performance of applying interactive RL with imperfect inputs into greenhouse climate control, by: (1) developing three representative interactive RL algorithms tailored for greenhouse climate control (reward shaping, policy shaping and control sharing); (2) analyzing how input characteristics are often contradicting, and how the trade-offs between them make grower's inputs difficult to perfect; (3) proposing a neural network-based approach to enhance the robustness of interactive RL agents under limited input availability; (4) conducting a comprehensive evaluation of the three interactive RL algorithms with imperfect inputs in a simulated greenhouse environment. The demonstration shows that interactive RL incorporating imperfect grower inputs has the potential to improve the performance of the RL agent. RL algorithms that influence action selection, such as policy shaping and control sharing, perform better when dealing with imperfect inputs, achieving 8.4% and 6.8% improvement in profit, respectively. In contrast, reward shaping, an algorithm that manipulates the reward function, is sensitive to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the importance of selecting an appropriate mechanism when incorporating imperfect inputs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Spectral Backpropagation for Efficient Neural Network Training</title>
<link>https://arxiv.org/abs/2505.23369</link>
<guid>https://arxiv.org/abs/2505.23369</guid>
<content:encoded><![CDATA[
arXiv:2505.23369v1 Announce Type: new 
Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training under resource constraints by projecting gradients onto principal eigenvectors, reducing complexity and promoting flat minima. Five extensions are proposed, dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics, to address challenges in robustness, fewshot learning, and hardware efficiency. Supported by a third order stochastic differential equation (SDE) and a PAC Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive experiments and visualizations. Future work focuses on scalability, bias mitigation, and ethical considerations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models</title>
<link>https://arxiv.org/abs/2505.23378</link>
<guid>https://arxiv.org/abs/2505.23378</guid>
<content:encoded><![CDATA[
arXiv:2505.23378v1 Announce Type: new 
Abstract: Speaker-dependent modelling can substantially improve performance in speech-based health monitoring applications. While mixed-effect models are commonly used for such speaker adaptation, they require computationally expensive retraining for each new observation, making them impractical in a production environment. We reformulate this task as a meta-learning problem and explore three approaches of increasing complexity: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Using pre-trained speech embeddings, we evaluate these methods on a large longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting time since sleep from speech as a function of fatigue, a symptom commonly associated with ill-health. Our results demonstrate that all meta-learning approaches tested outperformed both cross-sectional and conventional mixed-effects models, with a transformer-based method achieving the strongest performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Modeling Method for Pathloss Model Discovery</title>
<link>https://arxiv.org/abs/2505.23383</link>
<guid>https://arxiv.org/abs/2505.23383</guid>
<content:encoded><![CDATA[
arXiv:2505.23383v1 Announce Type: new 
Abstract: Modeling propagation is the cornerstone for designing and optimizing next-generation wireless systems, with a particular emphasis on 5G and beyond era. Traditional modeling methods have long relied on statistic-based techniques to characterize propagation behavior across different environments. With the expansion of wireless communication systems, there is a growing demand for methods that guarantee the accuracy and interoperability of modeling. Artificial intelligence (AI)-based techniques, in particular, are increasingly being adopted to overcome this challenge, although the interpretability is not assured with most of these methods. Inspired by recent advancements in AI, this paper proposes a novel approach that accelerates the discovery of path loss models while maintaining interpretability. The proposed method automates the model formulation, evaluation, and refinement, facilitating model discovery. We evaluate two techniques: one based on Deep Symbolic Regression, offering full interpretability, and the second based on Kolmogorov-Arnold Networks, providing two levels of interpretability. Both approaches are evaluated on two synthetic and two real-world datasets. Our results show that Kolmogorov-Arnold Networks achieve R^2 values close to 1 with minimal prediction error, while Deep Symbolic Regression generates compact models with moderate accuracy. Moreover, on the selected examples, we demonstrate that automated methods outperform traditional methods, achieving up to 75% reduction in prediction errors, offering accurate and explainable solutions with potential to increase the efficiency of discovering next-generation path loss models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Buffer-free Class-Incremental Learning with Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23412</link>
<guid>https://arxiv.org/abs/2505.23412</guid>
<content:encoded><![CDATA[
arXiv:2505.23412v1 Announce Type: new 
Abstract: Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must not only learn new classes over time without forgetting previous ones but also handle inputs from unknown classes that a closed-set model would misclassify. Recent works address both issues by (i)~training multi-head models using the task-incremental learning framework, and (ii) predicting the task identity employing out-of-distribution (OOD) detectors. While effective, the latter mainly relies on joint training with a memory buffer of past data, raising concerns around privacy, scalability, and increased training time. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. We uncover that these methods, when applied appropriately at inference time, can serve as a strong substitute for buffer-based OOD detection. We show that this buffer-free approach achieves comparable or superior performance to buffer-based methods both in terms of class-incremental learning and the rejection of unknown samples. Experimental results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings, offering new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[
arXiv:2505.23415v1 Announce Type: new 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses</title>
<link>https://arxiv.org/abs/2505.23421</link>
<guid>https://arxiv.org/abs/2505.23421</guid>
<content:encoded><![CDATA[
arXiv:2505.23421v1 Announce Type: new 
Abstract: In China's competitive fresh e-commerce market, optimizing operational strategies, especially inventory management in front-end warehouses, is key to enhance customer satisfaction and to gain a competitive edge. Front-end warehouses are placed in residential areas to ensure the timely delivery of fresh goods and are usually in small size. This brings the challenge of deciding which goods to stock and in what quantities, taking into account capacity constraints. To address this issue, traditional predict-then-optimize (PTO) methods that predict sales and then decide on inventory often don't align prediction with inventory goals, as well as fail to prioritize consumer satisfaction. This paper proposes a multi-task Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes product selection and inventory management, aiming to increase consumer satisfaction by maximizing the full order fulfillment rate. Our method employs a 0-1 mixed integer programming model OM1 to determine historically optimal inventory levels, and then uses a product selection model PM1 and the stocking model PM2 for prediction. The combined results are further refined through a post-processing algorithm OM2. Experimental results from JD.com's 7Fresh platform demonstrate the robustness and significant advantages of our OTPTO method. Compared to the PTO approach, our OTPTO method substantially enhances the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and narrows the gap to the optimal full order fulfillment rate by 5.27%. These findings substantiate the efficacy of the OTPTO method in managing inventory at front-end warehouses of fresh e-commerce platforms and provide valuable insights for future research in this domain.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced DACER Algorithm with High Diffusion Efficiency</title>
<link>https://arxiv.org/abs/2505.23426</link>
<guid>https://arxiv.org/abs/2505.23426</guid>
<content:encoded><![CDATA[
arXiv:2505.23426v1 Announce Type: new 
Abstract: Due to their expressive capacity, diffusion models have shown great promise in offline RL and imitation learning. Diffusion Actor-Critic with Entropy Regulator (DACER) extended this capability to online RL by using the reverse diffusion process as a policy approximator, trained end-to-end with policy gradient methods, achieving strong performance. However, this comes at the cost of requiring many diffusion steps, which significantly hampers training efficiency, while directly reducing the steps leads to noticeable performance degradation. Critically, the lack of inference efficiency becomes a significant bottleneck for applying diffusion policies in real-time online RL settings. To improve training and inference efficiency while maintaining or even enhancing performance, we propose a Q-gradient field objective as an auxiliary optimization target to guide the denoising process at each diffusion step. Nonetheless, we observe that the independence of the Q-gradient field from the diffusion time step negatively impacts the performance of the diffusion policy. To address this, we introduce a temporal weighting mechanism that enables the model to efficiently eliminate large-scale noise in the early stages and refine actions in the later stages. Experimental results on MuJoCo benchmarks and several multimodal tasks demonstrate that the DACER2 algorithm achieves state-of-the-art performance in most MuJoCo control tasks with only five diffusion steps, while also exhibiting stronger multimodality compared to DACER.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers</title>
<link>https://arxiv.org/abs/2505.23427</link>
<guid>https://arxiv.org/abs/2505.23427</guid>
<content:encoded><![CDATA[
arXiv:2505.23427v1 Announce Type: new 
Abstract: Depression is a debilitating mood disorder negatively impacting millions worldwide. While researchers have explored multiple verbal and non-verbal behavioural cues for automated depression assessment, head motion has received little attention thus far. Further, the common practice of validating machine learning models via a single dataset can limit model generalisability. This work examines the effectiveness and generalisability of models utilising elementary head motion units, termed kinemes, for depression severity estimation. Specifically, we consider three depression datasets from different western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt datasets) with varied contextual and recording settings to investigate the generalisability of the derived kineme patterns via two methods: (i) k-fold cross-validation over individual/multiple datasets, and (ii) model reuse on other datasets. Evaluating classification and regression performance with classical machine learning methods, our results show that: (1) head motion patterns are efficient biomarkers for estimating depression severity, achieving highly competitive performance for both classification and regression tasks on a variety of datasets, including achieving the second best Mean Absolute Error (MAE) on the AVEC2013 dataset, and (2) kineme-based features are more generalisable than (a) raw head motion descriptors for binary severity classification, and (b) other visual behavioural cues for severity estimation (regression).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Aware Policy Optimization for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.23433</link>
<guid>https://arxiv.org/abs/2505.23433</guid>
<content:encoded><![CDATA[
arXiv:2505.23433v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and Potential at k (a novel metric quantifying an LLM's reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5 percent average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Abstention Pairwise Learning to Rank</title>
<link>https://arxiv.org/abs/2505.23437</link>
<guid>https://arxiv.org/abs/2505.23437</guid>
<content:encoded><![CDATA[
arXiv:2505.23437v1 Announce Type: new 
Abstract: Ranking systems influence decision-making in high-stakes domains like health, education, and employment, where they can have substantial economic and social impacts. This makes the integration of safety mechanisms essential. One such mechanism is $\textit{abstention}$, which enables algorithmic decision-making system to defer uncertain or low-confidence decisions to human experts. While abstention have been predominantly explored in the context of classification tasks, its application to other machine learning paradigms remains underexplored. In this paper, we introduce a novel method for abstention in pairwise learning-to-rank tasks. Our approach is based on thresholding the ranker's conditional risk: the system abstains from making a decision when the estimated risk exceeds a predefined threshold. Our contributions are threefold: a theoretical characterization of the optimal abstention strategy, a model-agnostic, plug-in algorithm for constructing abstaining ranking models, and a comprehensive empirical evaluations across multiple datasets, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Regularization Methods for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.23442</link>
<guid>https://arxiv.org/abs/2505.23442</guid>
<content:encoded><![CDATA[
arXiv:2505.23442v1 Announce Type: new 
Abstract: Knowledge graph completion (KGC) has attracted considerable attention in recent years because it is critical to improving the quality of knowledge graphs. Researchers have continuously explored various models. However, most previous efforts have neglected to take advantage of regularization from a deeper perspective and therefore have not been used to their full potential. This paper rethinks the application of regularization methods in KGC. Through extensive empirical studies on various KGC models, we find that carefully designed regularization not only alleviates overfitting and reduces variance but also enables these models to break through the upper bounds of their original performance. Furthermore, we introduce a novel sparse-regularization method that embeds the concept of rank-based selective sparsity into the KGC regularizer. The core idea is to selectively penalize those components with significant features in the embedding vector, thus effectively ignoring many components that contribute little and may only represent noise. Various comparative experiments on multiple datasets and multiple models show that the SPR regularization method is better than other regularization methods and can enable the KGC model to further break through the performance margin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Classification with Non-Linear Classifiers</title>
<link>https://arxiv.org/abs/2505.23443</link>
<guid>https://arxiv.org/abs/2505.23443</guid>
<content:encoded><![CDATA[
arXiv:2505.23443v1 Announce Type: new 
Abstract: In strategic classification, the standard supervised learning setting is extended to support the notion of strategic user behavior in the form of costly feature manipulations made in response to a classifier. While standard learning supports a broad range of model classes, the study of strategic classification has, so far, been dedicated mostly to linear classifiers. This work aims to expand the horizon by exploring how strategic behavior manifests under non-linear classifiers and what this implies for learning. We take a bottom-up approach showing how non-linearity affects decision boundary points, classifier expressivity, and model classes complexity. A key finding is that universal approximators (e.g., neural nets) are no longer universal once the environment is strategic. We demonstrate empirically how this can create performance gaps even on an unrestricted model class.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23448</link>
<guid>https://arxiv.org/abs/2505.23448</guid>
<content:encoded><![CDATA[
arXiv:2505.23448v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Guidance Is a Controllable Policy Improvement Operator</title>
<link>https://arxiv.org/abs/2505.23458</link>
<guid>https://arxiv.org/abs/2505.23458</guid>
<content:encoded><![CDATA[
arXiv:2505.23458v1 Announce Type: new 
Abstract: At the core of reinforcement learning is the idea of learning beyond the performance in the data. However, scaling such systems has proven notoriously tricky. In contrast, techniques from generative modeling have proven remarkably scalable and are simple to train. In this work, we combine these strengths, by deriving a direct relation between policy improvement and guidance of diffusion models. The resulting framework, CFGRL, is trained with the simplicity of supervised learning, yet can further improve on the policies in the data. On offline RL tasks, we observe a reliable trend -- increased guidance weighting leads to increased performance. Of particular importance, CFGRL can operate without explicitly learning a value function, allowing us to generalize simple supervised methods (e.g., goal-conditioned behavioral cloning) to further prioritize optimality, gaining performance for "free" across the board.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment</title>
<link>https://arxiv.org/abs/2505.23459</link>
<guid>https://arxiv.org/abs/2505.23459</guid>
<content:encoded><![CDATA[
arXiv:2505.23459v1 Announce Type: new 
Abstract: Ensuring convergence of policy gradient methods in federated reinforcement learning (FRL) under environment heterogeneity remains a major challenge. In this work, we first establish that heterogeneity, perhaps counter-intuitively, can necessitate optimal policies to be non-deterministic or even time-varying, even in tabular environments. Subsequently, we prove global convergence results for federated policy gradient (FedPG) algorithms employing local updates, under a {\L}ojasiewicz condition that holds only for each individual agent, in both entropy-regularized and non-regularized scenarios. Crucially, our theoretical analysis shows that FedPG attains linear speed-up with respect to the number of agents, a property central to efficient federated learning. Leveraging insights from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient method that employs a carefully constructed softmax-inspired parameterization coupled with an appropriate regularization scheme. We further demonstrate explicit convergence rates for b-RS-FedPG toward near-optimal stationary policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG consistently outperform federated Q-learning on heterogeneous settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Labeling Functions with Limited Labeled Data</title>
<link>https://arxiv.org/abs/2505.23470</link>
<guid>https://arxiv.org/abs/2505.23470</guid>
<content:encoded><![CDATA[
arXiv:2505.23470v1 Announce Type: new 
Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for labeling data by combining the outputs of user-provided labeling functions (LFs) on unlabeled datapoints. However, the quality of the generated labels depends directly on the accuracy of the LFs. In this work, we study the problem of fixing LFs based on a small set of labeled examples. Towards this goal, we develop novel techniques for repairing a set of LFs by minimally changing their results on the labeled examples such that the fixed LFs ensure that (i) there is sufficient evidence for the correct label of each labeled datapoint and (ii) the accuracy of each repaired LF is sufficiently high. We model LFs as conditional rules which enables us to refine them, i.e., to selectively change their output for some inputs. We demonstrate experimentally that our system improves the quality of LFs based on surprisingly small sets of labeled datapoints.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training</title>
<link>https://arxiv.org/abs/2505.23489</link>
<guid>https://arxiv.org/abs/2505.23489</guid>
<content:encoded><![CDATA[
arXiv:2505.23489v1 Announce Type: new 
Abstract: We present a thermodynamic interpretation of the stationary behavior of stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural network training. We show that SGD implicitly minimizes a free energy function $F=U-TS$, balancing training loss $U$ and the entropy of the weights distribution $S$, with temperature $T$ determined by the LR. This perspective offers a new lens on why high LRs prevent training from converging to the loss minima and how different LRs lead to stabilization at different loss levels. We empirically validate the free energy framework on both underparameterized (UP) and overparameterized (OP) models. UP models consistently follow free energy minimization, with temperature increasing monotonically with LR, while for OP models, the temperature effectively drops to zero at low LRs, causing SGD to minimize the loss directly and converge to an optimum. We attribute this mismatch to differences in the signal-to-noise ratio of stochastic gradients near optima, supported by both a toy example and neural network experiments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Errors of Imperfect Multitask Learners When Distributions Shift</title>
<link>https://arxiv.org/abs/2505.23496</link>
<guid>https://arxiv.org/abs/2505.23496</guid>
<content:encoded><![CDATA[
arXiv:2505.23496v1 Announce Type: new 
Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic uncertainty about the data it will encounter at test-time, i.e., to identify the distribution of test (target) data. Many real-world learning settings introduce sources of epistemic uncertainty that can not be resolved on the basis of training (source) data alone: The source data may arise from multiple tasks (multitask learning), the target data may differ systematically from the source data tasks (distribution shift), and/or the learner may not arrive at an accurate characterization of the source data (imperfect learning). We introduce a principled definition of epistemic error, and provide a generic, decompositional epistemic error bound. Our error bound is the first to (i) consider epistemic error specifically, (ii) accommodate all the sources of epistemic uncertainty above, and (iii) separately attribute the error to each of multiple aspects of the learning procedure and environment. As corollaries of the generic result, we provide (i) epistemic error bounds specialized to the settings of Bayesian transfer learning and distribution shift within $\epsilon$-neighborhoods, and (ii) a set of corresponding generalization bounds. Finally, we provide a novel definition of negative transfer, and validate its insights in a synthetic experimental setting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2505.23506</link>
<guid>https://arxiv.org/abs/2505.23506</guid>
<content:encoded><![CDATA[
arXiv:2505.23506v1 Announce Type: new 
Abstract: In recent years various supervised learning methods that disentangle aleatoric and epistemic uncertainty based on second-order distributions have been proposed. We argue that these methods fail to capture critical components of epistemic uncertainty, particularly due to the often-neglected component of model bias. To show this, we make use of a more fine-grained taxonomy of epistemic uncertainty sources in machine learning models, and analyse how the classical bias-variance decomposition of the expected prediction error can be decomposed into different parts reflecting these uncertainties. By using a simulation-based evaluation protocol which encompasses epistemic uncertainty due to both procedural- and data-driven uncertainty components, we illustrate that current methods rarely capture the full spectrum of epistemic uncertainty. Through theoretical insights and synthetic experiments, we show that high model bias can lead to misleadingly low estimates of epistemic uncertainty, and common second-order uncertainty quantification methods systematically blur bias-induced errors into aleatoric estimates, thereby underrepresenting epistemic uncertainty. Our findings underscore that meaningful aleatoric estimates are feasible only if all relevant sources of epistemic uncertainty are properly represented.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity</title>
<link>https://arxiv.org/abs/2505.23520</link>
<guid>https://arxiv.org/abs/2505.23520</guid>
<content:encoded><![CDATA[
arXiv:2505.23520v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context lengths face significant computational challenges during the pre-filling phase, primarily due to the quadratic complexity of self-attention. Existing methods typically employ dynamic pattern matching and block-sparse low-level implementations. However, their reliance on local information for pattern identification fails to capture global contexts, and the coarse granularity of blocks leads to persistent internal sparsity, resulting in suboptimal accuracy and efficiency. To address these limitations, we propose \textbf{AnchorAttention}, a difference-aware, dynamic sparse attention mechanism that efficiently identifies critical attention regions at a finer stripe granularity while adapting to global contextual information, achieving superior speed and accuracy. AnchorAttention comprises three key components: (1) \textbf{Pattern-based Anchor Computation}, leveraging the commonalities present across all inputs to rapidly compute a set of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe Sparsity Identification}, performing difference-aware comparisons with the anchor to quickly obtain discrete coordinates of significant regions in a stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation}, replacing the traditional contiguous KV block loading approach with simultaneous discrete KV position loading to maximize sparsity rates while preserving full hardware computational potential. With its finer-grained sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at the same recall level, significantly reducing computation time. Compared to previous state-of-the-art methods, at a text length of 128k, it achieves a speedup of 1.44$\times$ while maintaining higher recall rates.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating AllReduce with a Persistent Straggler</title>
<link>https://arxiv.org/abs/2505.23523</link>
<guid>https://arxiv.org/abs/2505.23523</guid>
<content:encoded><![CDATA[
arXiv:2505.23523v1 Announce Type: new 
Abstract: Distributed machine learning workloads use data and tensor parallelism for training and inference, both of which rely on the AllReduce collective to synchronize gradients or activations. However, bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler that is slower to reach the synchronization barrier required to begin the collective. To address this challenge, we propose StragglAR: an AllReduce algorithm that accelerates distributed training and inference in the presence of persistent stragglers. StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier. StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22% speedup over state-of-the-art AllReduce algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalizing Flows are Capable Models for RL</title>
<link>https://arxiv.org/abs/2505.23527</link>
<guid>https://arxiv.org/abs/2505.23527</guid>
<content:encoded><![CDATA[
arXiv:2505.23527v1 Announce Type: new 
Abstract: Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images</title>
<link>https://arxiv.org/abs/2505.23528</link>
<guid>https://arxiv.org/abs/2505.23528</guid>
<content:encoded><![CDATA[
arXiv:2505.23528v1 Announce Type: new 
Abstract: The present study performs a comprehensive fairness analysis of machine learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases associated with age, race, and gender in a multi-cohort dataset, as well as the influence of proxy features encoding these sensitive attributes, are investigated. The reliability of various fairness definitions and metrics in the identification of such biases is also assessed. Based on the most appropriate fairness measures, a comparative analysis of widely used pre-processing, in-processing, and post-processing bias mitigation strategies is performed. Moreover, a novel composite measure is introduced to quantify the trade-off between fairness and performance by considering the F1-score and the equalized odds ratio, making it appropriate for medical diagnostic applications. The obtained results reveal the existence of biases related to age and race, while no significant gender bias is observed. The deployed mitigation strategies yield varying improvements in terms of fairness across the different sensitive attributes and studied subproblems. For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning</title>
<link>https://arxiv.org/abs/2505.23529</link>
<guid>https://arxiv.org/abs/2505.23529</guid>
<content:encoded><![CDATA[
arXiv:2505.23529v1 Announce Type: new 
Abstract: Graph Representation Learning (GRL) is a fundamental task in machine learning, aiming to encode high-dimensional graph-structured data into low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used in GRL because they can avoid expensive human annotation. In this work, we propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of input subgraph characteristics while generating subgraphs with a controlled distribution. We then employ optimal transport distances, more precisely the Wasserstein and Gromov-Wasserstein distances, to effectively measure the similarity between subgraphs, enhancing the robustness of the contrastive learning process. Extensive experiments across multiple benchmarks demonstrate that \method~outperforms or presents competitive performance against state-of-the-art approaches. Our findings provide insights into the design of SSL methods for GRL, emphasizing the importance of the distribution of the generated contrastive pairs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Aware Tensor Network Structure Search</title>
<link>https://arxiv.org/abs/2505.23537</link>
<guid>https://arxiv.org/abs/2505.23537</guid>
<content:encoded><![CDATA[
arXiv:2505.23537v1 Announce Type: new 
Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (SOTA) algorithms are computationally expensive as they require extensive function evaluations, which is prohibitive for real-world applications. In addition, existing methods ignore valuable domain information inherent in real-world tensor data and lack transparency in their identified TN structures. To this end, we propose a novel TN-SS framework, termed the tnLLM, which incorporates domain information about the data and harnesses the reasoning capabilities of large language models (LLMs) to directly predict suitable TN structures. The proposed framework involves a domain-aware prompting pipeline which instructs the LLM to infer suitable TN structures based on the real-world relationships between tensor modes. In this way, our approach is capable of not only iteratively optimizing the objective function, but also generating domain-aware explanations for the identified structures. Experimental results demonstrate that tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Furthermore, we demonstrate that the LLM-enabled domain information can be used to find good initializations in the search space for sampling-based SOTA methods to accelerate their convergence while preserving theoretical performance guarantees.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis</title>
<link>https://arxiv.org/abs/2505.23552</link>
<guid>https://arxiv.org/abs/2505.23552</guid>
<content:encoded><![CDATA[
arXiv:2505.23552v1 Announce Type: new 
Abstract: This paper investigates the comparative performance of two fundamental approaches to solving linear regression problems: the closed-form Moore-Penrose pseudoinverse and the iterative gradient descent method. Linear regression is a cornerstone of predictive modeling, and the choice of solver can significantly impact efficiency and accuracy. I review and discuss the theoretical underpinnings of both methods, analyze their computational complexity, and evaluate their empirical behavior on synthetic datasets with controlled characteristics, as well as on established real-world datasets. My results delineate the conditions under which each method excels in terms of computational time, numerical stability, and predictive accuracy. This work aims to provide practical guidance for researchers and practitioners in machine learning when selecting between direct, exact solutions and iterative, approximate solutions for linear regression tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling</title>
<link>https://arxiv.org/abs/2505.23555</link>
<guid>https://arxiv.org/abs/2505.23555</guid>
<content:encoded><![CDATA[
arXiv:2505.23555v1 Announce Type: new 
Abstract: Federated LoRA has emerged as a promising technique for efficiently fine-tuning large language models (LLMs) on distributed devices by reducing the number of trainable parameters. However, existing approaches often inadequately overlook the theoretical and practical implications of system and data heterogeneity, thereby failing to optimize the overall training efficiency, particularly in terms of wall-clock time. In this paper, we propose an adaptive federated LoRA strategy with independent client sampling to minimize the convergence wall-clock time of federated fine-tuning under both computation and communication heterogeneity. We first derive a new convergence bound for federated LoRA with arbitrary and independent client sampling, notably without requiring the stringent bounded gradient assumption. Then, we introduce an adaptive bandwidth allocation scheme that accounts for heterogeneous client resources and system bandwidth constraints. Based on the derived theory, we formulate and solve a non-convex optimization problem to jointly determine the LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock convergence time. An efficient and low-complexity algorithm is developed to approximate the solution. Finally, extensive experiments demonstrate that our approach significantly reduces wall-clock training time compared to state-of-the-art methods across various models and datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models</title>
<link>https://arxiv.org/abs/2505.23564</link>
<guid>https://arxiv.org/abs/2505.23564</guid>
<content:encoded><![CDATA[
arXiv:2505.23564v1 Announce Type: new 
Abstract: Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRO: A Python Library for Distributionally Robust Optimization in Machine Learning</title>
<link>https://arxiv.org/abs/2505.23565</link>
<guid>https://arxiv.org/abs/2505.23565</guid>
<content:encoded><![CDATA[
arXiv:2505.23565v1 Announce Type: new 
Abstract: We introduce dro, an open-source Python library for distributionally robust optimization (DRO) for regression and classification problems. The library implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO methods. Furthermore, dro is compatible with both scikit-learn and PyTorch. Through vectorization and optimization approximation techniques, dro reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets. Comprehensive documentation is available at https://python-dro.org.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Likelihood Learning of Latent Dynamics Without Reconstruction</title>
<link>https://arxiv.org/abs/2505.23569</link>
<guid>https://arxiv.org/abs/2505.23569</guid>
<content:encoded><![CDATA[
arXiv:2505.23569v1 Announce Type: new 
Abstract: We introduce a novel unsupervised learning method for time series data with latent dynamical structure: the recognition-parametrized Gaussian state space model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian Gaussian latents explaining statistical dependence between observations at different time steps, combining the intuition of contrastive methods with the flexible tools of probabilistic generative models. Unlike contrastive approaches, the RP-GSSM is a valid probabilistic model learned via maximum likelihood. Unlike generative approaches, the RP-GSSM has no need for an explicit network mapping from latents to observations, allowing it to focus model capacity on inference of latents. The model is both tractable and expressive: it admits exact inference thanks to its jointly Gaussian latent prior, while maintaining expressivity with an arbitrarily nonlinear neural network link between observations and latents. These qualities allow the RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary losses, or optimizer scheduling. We show how this approach outperforms alternatives on problems that include learning nonlinear stochastic dynamics from video, with or without background distractors. Our results position the RP-GSSM as a useful foundation model for a variety of downstream applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model</title>
<link>https://arxiv.org/abs/2505.23579</link>
<guid>https://arxiv.org/abs/2505.23579</guid>
<content:encoded><![CDATA[
arXiv:2505.23579v1 Announce Type: new 
Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models, despite strong sequence representation, struggle with multi-step reasoning and lack inherent transparent, biologically intuitive explanations. We introduce BioReason, a pioneering architecture that, for the first time, deeply integrates a DNA foundation model with a Large Language Model (LLM). This novel connection enables the LLM to directly process and reason with genomic information as a fundamental input, fostering a new form of multimodal biological understanding. BioReason's sophisticated multi-step reasoning is developed through supervised fine-tuning and targeted reinforcement learning, guiding the system to generate logical, biologically coherent deductions. On biological reasoning benchmarks including KEGG-based disease pathway prediction - where accuracy improves from 88% to 97% - and variant effect prediction, BioReason demonstrates an average 15% performance gain over strong single-modality baselines. BioReason reasons over unseen biological entities and articulates decision-making through interpretable, step-by-step biological traces, offering a transformative approach for AI in biology that enables deeper mechanistic insights and accelerates testable hypothesis generation from genomic data. Data, code, and checkpoints are publicly available at https://github.com/bowang-lab/BioReason
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Time Series Forecasting via Instance-aware Post-hoc Revision</title>
<link>https://arxiv.org/abs/2505.23583</link>
<guid>https://arxiv.org/abs/2505.23583</guid>
<content:encoded><![CDATA[
arXiv:2505.23583v1 Announce Type: new 
Abstract: Time series forecasting plays a vital role in various real-world applications and has attracted significant attention in recent decades. While recent methods have achieved remarkable accuracy by incorporating advanced inductive biases and training strategies, we observe that instance-level variations remain a significant challenge. These variations--stemming from distribution shifts, missing data, and long-tail patterns--often lead to suboptimal forecasts for specific instances, even when overall performance appears strong. To address this issue, we propose a model-agnostic framework, PIR, designed to enhance forecasting performance through Post-forecasting Identification and Revision. Specifically, PIR first identifies biased forecasting instances by estimating their accuracy. Based on this, the framework revises the forecasts using contextual information, including covariates and historical time series, from both local and global perspectives in a post-processing fashion. Extensive experiments on real-world datasets with mainstream forecasting models demonstrate that PIR effectively mitigates instance-level errors and significantly improves forecasting reliability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy RL with Optimal Reward Baseline</title>
<link>https://arxiv.org/abs/2505.23585</link>
<guid>https://arxiv.org/abs/2505.23585</guid>
<content:encoded><![CDATA[
arXiv:2505.23585v1 Announce Type: new 
Abstract: Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Training of Federated Learning via Second-Order Methods</title>
<link>https://arxiv.org/abs/2505.23588</link>
<guid>https://arxiv.org/abs/2505.23588</guid>
<content:encoded><![CDATA[
arXiv:2505.23588v1 Announce Type: new 
Abstract: This paper explores second-order optimization methods in Federated Learning (FL), addressing the critical challenges of slow convergence and the excessive communication rounds required to achieve optimal performance from the global model. While existing surveys in FL primarily focus on challenges related to statistical and device label heterogeneity, as well as privacy and security concerns in first-order FL methods, less attention has been given to the issue of slow model training. This slow training often leads to the need for excessive communication rounds or increased communication costs, particularly when data across clients are highly heterogeneous. In this paper, we examine various FL methods that leverage second-order optimization to accelerate the training process. We provide a comprehensive categorization of state-of-the-art second-order FL methods and compare their performance based on convergence speed, computational cost, memory usage, transmission overhead, and generalization of the global model. Our findings show the potential of incorporating Hessian curvature through second-order optimization into FL and highlight key challenges, such as the efficient utilization of Hessian and its inverse in FL. This work lays the groundwork for future research aimed at developing scalable and efficient federated optimization methods for improving the training of the global model in FL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models</title>
<link>https://arxiv.org/abs/2505.23593</link>
<guid>https://arxiv.org/abs/2505.23593</guid>
<content:encoded><![CDATA[
arXiv:2505.23593v1 Announce Type: new 
Abstract: Post-training of foundation language models has emerged as a promising research domain in federated learning (FL) with the goal to enable privacy-preserving model improvements and adaptations to user's downstream tasks. Recent advances in this area adopt centralized post-training approaches that build upon black-box foundation language models where there is no access to model weights and architecture details. Although the use of black-box models has been successful in centralized post-training, their blind replication in FL raises several concerns. Our position is that using black-box models in FL contradicts the core principles of federation such as data privacy and autonomy. In this position paper, we critically analyze the usage of black-box models in federated post-training, and provide a detailed account of various aspects of openness and their implications for FL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Performance for Code Generation on Noisy Tasks</title>
<link>https://arxiv.org/abs/2505.23598</link>
<guid>https://arxiv.org/abs/2505.23598</guid>
<content:encoded><![CDATA[
arXiv:2505.23598v1 Announce Type: new 
Abstract: This paper investigates the ability of large language models (LLMs) to recognise and solve tasks which have been obfuscated beyond recognition. Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we compare performance across multiple models and obfuscation methods, such as noise and redaction. We demonstrate that all evaluated LLMs can solve tasks obfuscated to a level where the text would be unintelligible to human readers, and does not contain key pieces of instruction or context. We introduce the concept of eager pattern matching to describe this behaviour, which is not observed in tasks published after the models' knowledge cutoff date, indicating strong memorisation or overfitting to training data, rather than legitimate reasoning about the presented problem. We report empirical evidence of distinct performance decay patterns between contaminated and unseen datasets. We discuss the implications for benchmarking and evaluations of model behaviour, arguing for caution when designing experiments using standard datasets. We also propose measuring the decay of performance under obfuscation as a possible strategy for detecting dataset contamination and highlighting potential safety risks and interpretability issues for automated software systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Transferring Transferability: Towards a Theory for Size Generalization</title>
<link>https://arxiv.org/abs/2505.23599</link>
<guid>https://arxiv.org/abs/2505.23599</guid>
<content:encoded><![CDATA[
arXiv:2505.23599v1 Announce Type: new 
Abstract: Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model</title>
<link>https://arxiv.org/abs/2505.23606</link>
<guid>https://arxiv.org/abs/2505.23606</guid>
<content:encoded><![CDATA[
arXiv:2505.23606v1 Announce Type: new 
Abstract: Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Model Design for Explainable Machine Learning-based Electricity Applications</title>
<link>https://arxiv.org/abs/2505.23607</link>
<guid>https://arxiv.org/abs/2505.23607</guid>
<content:encoded><![CDATA[
arXiv:2505.23607v1 Announce Type: new 
Abstract: The transition from traditional power grids to smart grids, significant increase in the use of renewable energy sources, and soaring electricity prices has triggered a digital transformation of the energy infrastructure that enables new, data driven, applications often supported by machine learning models. However, the majority of the developed machine learning models rely on univariate data. To date, a structured study considering the role meta-data and additional measurements resulting in multivariate data is missing. In this paper we propose a taxonomy that identifies and structures various types of data related to energy applications. The taxonomy can be used to guide application specific data model development for training machine learning models. Focusing on a household electricity forecasting application, we validate the effectiveness of the proposed taxonomy in guiding the selection of the features for various types of models. As such, we study of the effect of domain, contextual and behavioral features on the forecasting accuracy of four interpretable machine learning techniques and three openly available datasets. Finally, using a feature importance techniques, we explain individual feature contributions to the forecasting accuracy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Generalized Skew Spectrum of Graphs</title>
<link>https://arxiv.org/abs/2505.23609</link>
<guid>https://arxiv.org/abs/2505.23609</guid>
<content:encoded><![CDATA[
arXiv:2505.23609v1 Announce Type: new 
Abstract: This paper proposes a family of permutation-invariant graph embeddings, generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded in group theory and harmonic analysis, our method introduces a new class of graph invariants that are isomorphism-invariant and capable of embedding richer graph structures - including attributed graphs, multilayer graphs, and hypergraphs - which the Skew Spectrum could not handle. Our generalization further defines a family of functions that enables a trade-off between computational complexity and expressivity. By applying generalization-preserving heuristics to this family, we improve the Skew Spectrum's expressivity at the same computational cost. We formally prove the invariance of our generalization, demonstrate its improved expressiveness through experiments, and discuss its efficient computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-time Scaling of Diffusion Models through Classical Search</title>
<link>https://arxiv.org/abs/2505.23614</link>
<guid>https://arxiv.org/abs/2505.23614</guid>
<content:encoded><![CDATA[
arXiv:2505.23614v1 Announce Type: new 
Abstract: Classical search algorithms have long underpinned modern artificial intelligence. In this work, we tackle the challenge of inference-time control in diffusion models -- adapting generated outputs to meet diverse test-time objectives -- using principles from classical search. We propose a general framework that orchestrates local and global search to efficiently navigate the generative space. It employs a theoretically grounded local search via annealed Langevin MCMC and performs compute-efficient global exploration using breadth-first and depth-first tree search. We evaluate our approach on a range of challenging domains, including planning, offline reinforcement learning, and image generation. Across all tasks, we observe significant gains in both performance and efficiency. These results show that classical search provides a principled and practical foundation for inference-time scaling in diffusion models. Project page at diffusion-inference-scaling.github.io.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Interpretable Differentiable Logic Networks for Tabular Regression</title>
<link>https://arxiv.org/abs/2505.23615</link>
<guid>https://arxiv.org/abs/2505.23615</guid>
<content:encoded><![CDATA[
arXiv:2505.23615v1 Announce Type: new 
Abstract: Neural networks (NNs) achieve outstanding performance in many domains; however, their decision processes are often opaque and their inference can be computationally expensive in resource-constrained environments. We recently proposed Differentiable Logic Networks (DLNs) to address these issues for tabular classification based on relaxing discrete logic into a differentiable form, thereby enabling gradient-based learning of networks built from binary logic operations. DLNs offer interpretable reasoning and substantially lower inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically, we redesign the final output layer to support continuous targets and unify the original two-phase training procedure into a single differentiable stage. We evaluate the resulting model on 15 public regression benchmarks, comparing it with modern neural networks and classical regression baselines. Regression DLNs match or exceed baseline accuracy while preserving interpretability and fast inference. Our results show that DLNs are a viable, cost-effective alternative for regression tasks, especially where model transparency and computational efficiency are important.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection</title>
<link>https://arxiv.org/abs/2505.23627</link>
<guid>https://arxiv.org/abs/2505.23627</guid>
<content:encoded><![CDATA[
arXiv:2505.23627v1 Announce Type: new 
Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly approached post-hoc by comparing automatic speech recognition (ASR) transcriptions to the target reading text. However, post-hoc methods perform poorly when ASR inaccurately transcribes verbatim speech. To improve on current methods for reading error annotation, we propose a novel end-to-end architecture that incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection. Our contributions include: first, demonstrating that incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning, and second, showing that it is feasible to augment speech recognition tasks for end-to-end miscue detection. We conducted two case studies -- children's read-aloud and adult atypical speech -- and found that our proposed strategies improve verbatim transcription and miscue detection compared to current state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment</title>
<link>https://arxiv.org/abs/2505.23634</link>
<guid>https://arxiv.org/abs/2505.23634</guid>
<content:encoded><![CDATA[
arXiv:2505.23634v1 Announce Type: new 
Abstract: The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent work has shown the MCP is susceptible to retrieval-based "falsely benign" attacks (FBAs), allowing malicious system access and credential theft, but requiring that users download compromised files directly to their systems. Herein, we show that the threat model of MCP-based attacks is significantly broader than previously thought, i.e., attackers need only post malicious content online to deceive MCP agents into carrying out their attacks on unsuspecting victims' systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP dataset of FBAs and (truly) benign samples to explore the effectiveness of direct preference optimization (DPO) for the refusal training of large language models (LLMs). While DPO improves model guardrails against such attacks, we show that the efficacy of refusal learning varies drastically depending on the model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to refuse extremely poorly. Thus, to further improve FBA refusals, we introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG. We show that RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment, thus drastically improving guardrails against MCP-based attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global optimization of graph acquisition functions for neural architecture search</title>
<link>https://arxiv.org/abs/2505.23640</link>
<guid>https://arxiv.org/abs/2505.23640</guid>
<content:encoded><![CDATA[
arXiv:2505.23640v1 Announce Type: new 
Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and data-efficient tool for neural architecture search (NAS). Most existing graph BO works focus on developing graph surrogates models, i.e., metrics of networks and/or different kernels to quantify the similarity between networks. However, the acquisition optimization, as a discrete optimization task over graph structures, is not well studied due to the complexity of formulating the graph search space and acquisition functions. This paper presents explicit optimization formulations for graph input space including properties such as reachability and shortest paths, which are used later to formulate graph kernels and the acquisition function. We theoretically prove that the proposed encoding is an equivalent representation of the graph space and provide restrictions for the NAS domain with either node or edge labels. Numerical results over several NAS benchmarks show that our method efficiently finds the optimal architecture for most cases, highlighting its efficacy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Chain of Thought Enables Parallel Exploration and Reasoning</title>
<link>https://arxiv.org/abs/2505.23648</link>
<guid>https://arxiv.org/abs/2505.23648</guid>
<content:encoded><![CDATA[
arXiv:2505.23648v1 Announce Type: new 
Abstract: Current language models generate chain-of-thought traces by autoregressively sampling tokens from a finite vocabulary. While this discrete sampling has achieved remarkable success, conducting chain-of-thought with continuously-valued tokens (CoT2) offers a richer and more expressive alternative. Our work examines the benefits of CoT2 through logical reasoning tasks that inherently require search capabilities and provide optimization and exploration methods for CoT2. Theoretically, we show that CoT2 allows the model to track multiple traces in parallel and quantify its benefits for inference efficiency. Notably, one layer transformer equipped with CoT2 can provably solve the combinatorial "subset sum problem" given sufficient embedding dimension. These insights lead to a novel and effective supervision strategy where we match the softmax outputs to the empirical token distributions of a set of target traces. Complementing this, we introduce sampling strategies that unlock policy optimization and self-improvement for CoT2. Our first strategy samples and composes $K$ discrete tokens at each decoding step to control the level of parallelism, and reduces to standard CoT when $K=1$. Our second strategy relies on continuous exploration over the probability simplex. Experiments confirm that policy optimization with CoT2 indeed improves the performance of the model beyond its initial discrete or continuous supervision.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.23651</link>
<guid>https://arxiv.org/abs/2505.23651</guid>
<content:encoded><![CDATA[
arXiv:2505.23651v1 Announce Type: new 
Abstract: Model merging has emerged as a powerful technique for combining task-specific weights, achieving superior performance in multi-target domain adaptation. However, when applied to practical scenarios, such as quantized models, new challenges arise. In practical scenarios, quantization is often applied to target-specific data, but this process restricts the domain of interest and introduces discretization effects, making model merging highly non-trivial. In this study, we analyze the impact of quantization on model merging through the lens of error barriers. Leveraging these insights, we propose a novel post-training quantization, HDRQ - Hessian and distant regularizing quantization - that is designed to consider model merging for multi-target domain adaptation. Our approach ensures that the quantization process incurs minimal deviation from the source pre-trained model while flattening the loss surface to facilitate smooth model merging. To our knowledge, this is the first study on this challenge, and extensive experiments confirm its effectiveness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does Transformer Learn Implicit Reasoning?</title>
<link>https://arxiv.org/abs/2505.23653</link>
<guid>https://arxiv.org/abs/2505.23653</guid>
<content:encoded><![CDATA[
arXiv:2505.23653v1 Announce Type: new 
Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly -- producing correct answers without explicitly verbalizing intermediate steps -- but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction</title>
<link>https://arxiv.org/abs/2505.23663</link>
<guid>https://arxiv.org/abs/2505.23663</guid>
<content:encoded><![CDATA[
arXiv:2505.23663v1 Announce Type: new 
Abstract: The cost and accuracy of simulating complex physical systems using the Finite Element Method (FEM) scales with the resolution of the underlying mesh. Adaptive meshes improve computational efficiency by refining resolution in critical regions, but typically require task-specific heuristics or cumbersome manual design by a human expert. We propose Adaptive Meshing By Expert Reconstruction (AMBER), a supervised learning approach to mesh adaptation. Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e., a function mapping from the geometry to the local element size of the target mesh, and uses this prediction to produce a new intermediate mesh using an out-of-the-box mesh generator. This process is enabled through a hierarchical graph neural network, and relies on data augmentation by automatically projecting expert labels onto AMBER-generated data during training. We evaluate AMBER on 2D and 3D datasets, including classical physics problems, mechanical components, and real-world industrial designs with human expert meshes. AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including ones using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds</title>
<link>https://arxiv.org/abs/2505.23673</link>
<guid>https://arxiv.org/abs/2505.23673</guid>
<content:encoded><![CDATA[
arXiv:2505.23673v1 Announce Type: new 
Abstract: Bayesian optimization (BO) with preference-based feedback has recently garnered significant attention due to its emerging applications. We refer to this problem as Bayesian Optimization from Human Feedback (BOHF), which differs from conventional BO by learning the best actions from a reduced feedback model, where only the preference between two actions is revealed to the learner at each time step. The objective is to identify the best action using a limited number of preference queries, typically obtained through costly human feedback. Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model, provides regret bounds for the performance of several algorithms. In this work, within the same framework we develop tighter performance guarantees. Specifically, we derive regret bounds of $\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the maximum information gain$\unicode{x2014}$a kernel-specific complexity term$\unicode{x2014}$and $T$ is the number of queries. Our results significantly improve upon existing bounds. Notably, for common kernels, we show that the order-optimal sample complexities of conventional BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are recovered. In other words, the same number of preferential samples as scalar-valued samples is sufficient to find a nearly optimal solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Mode Connectivity via Parameter Space Symmetry</title>
<link>https://arxiv.org/abs/2505.23681</link>
<guid>https://arxiv.org/abs/2505.23681</guid>
<content:encoded><![CDATA[
arXiv:2505.23681v1 Announce Type: new 
Abstract: Neural network minima are often connected by curves along which train and test loss remain nearly constant, a phenomenon known as mode connectivity. While this property has enabled applications such as model merging and fine-tuning, its theoretical explanation remains unclear. We propose a new approach to exploring the connectedness of minima using parameter space symmetry. By linking the topology of symmetry groups to that of the minima, we derive the number of connected components of the minima of linear networks and show that skip connections reduce this number. We then examine when mode connectivity and linear mode connectivity hold or fail, using parameter symmetries which account for a significant part of the minimum. Finally, we provide explicit expressions for connecting curves in the minima induced by symmetry. Using the curvature of these curves, we derive conditions under which linear mode connectivity approximately holds. Our findings highlight the role of continuous symmetries in understanding the neural network loss landscape.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Compositional Functions with Transformers from Easy-to-Hard Data</title>
<link>https://arxiv.org/abs/2505.23683</link>
<guid>https://arxiv.org/abs/2505.23683</guid>
<content:encoded><![CDATA[
arXiv:2505.23683v1 Announce Type: new 
Abstract: Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, in this work we study the learnability of the $k$-fold composition task, which requires computing an interleaved composition of $k$ input permutations and $k$ hidden permutations, and can be expressed by a transformer with $O(\log k)$ layers. On the negative front, we prove a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the $k$-fold composition task distribution must have sample size exponential in $k$, thus establishing a statistical-computational gap. On the other hand, we show that this function class can be efficiently learned, with runtime and sample complexity polynomial in $k$, by gradient descent on an $O(\log k)$-depth transformer via two different curriculum learning strategies: one in which data consists of $k'$-fold composition functions with $k' \le k$ presented in increasing difficulty, and another in which all such data is presented simultaneously. Our work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms</title>
<link>https://arxiv.org/abs/2505.23696</link>
<guid>https://arxiv.org/abs/2505.23696</guid>
<content:encoded><![CDATA[
arXiv:2505.23696v1 Announce Type: new 
Abstract: Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditional methods like Gr\"obner and Border bases are fundamental but suffer from high computational costs, which have motivated recent Deep Learning approaches to improve efficiency, albeit at the expense of output correctness. In this work, we introduce the Oracle Border Basis Algorithm, the first Deep Learning approach that accelerates Border basis computation while maintaining output guarantees. To this end, we design and train a Transformer-based oracle that identifies and eliminates computationally expensive reduction steps, which we find to dominate the algorithm's runtime. By selectively invoking this oracle during critical phases of computation, we achieve substantial speedup factors of up to 3.5x compared to the base algorithm, without compromising the correctness of results. To generate the training data, we develop a sampling method and provide the first sampling theorem for border bases. We construct a tokenization and embedding scheme tailored to monomial-centered algebraic computations, resulting in a compact and expressive input representation, which reduces the number of tokens to encode an $n$-variate polynomial by a factor of $O(n)$. Our learning approach is data efficient, stable, and a practical enhancement to traditional computer algebra algorithms and symbolic computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCoFlex: Model-agnostic diverse counterfactuals with flexible control</title>
<link>https://arxiv.org/abs/2505.23700</link>
<guid>https://arxiv.org/abs/2505.23700</guid>
<content:encoded><![CDATA[
arXiv:2505.23700v1 Announce Type: new 
Abstract: Counterfactual explanations play a pivotal role in explainable artificial intelligence (XAI) by offering intuitive, human-understandable alternatives that elucidate machine learning model decisions. Despite their significance, existing methods for generating counterfactuals often require constant access to the predictive model, involve computationally intensive optimization for each instance and lack the flexibility to adapt to new user-defined constraints without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic, conditional generative framework that produces multiple diverse counterfactuals in a single forward pass. Leveraging conditional normalizing flows trained solely on labeled data, DiCoFlex addresses key limitations by enabling real-time user-driven customization of constraints such as sparsity and actionability at inference time. Extensive experiments on standard benchmark datasets show that DiCoFlex outperforms existing methods in terms of validity, diversity, proximity, and constraint adherence, making it a practical and scalable solution for counterfactual generation in sensitive decision-making domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs</title>
<link>https://arxiv.org/abs/2505.23702</link>
<guid>https://arxiv.org/abs/2505.23702</guid>
<content:encoded><![CDATA[
arXiv:2505.23702v1 Announce Type: new 
Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes classical finite volume (FV) methods for solving hyperbolic conservation laws. Hyperbolic partial differential equations (PDEs) are challenging to solve, particularly conservation laws whose physically relevant solutions contain shocks and discontinuities. FV methods are widely used for their mathematical properties: convergence to entropy solutions, flow conservation, or total variation diminishing, but often lack accuracy and flexibility in complex settings. Neural Finite Volume addresses these limitations by learning update rules over extended spatial and temporal stencils while preserving conservation structure. It supports both supervised training on solution data (NFV) and unsupervised training via weak-form residual loss (UNFV). Applied to first-order conservation laws, (U)NFV achieves up to 10x lower error than Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin solvers with far less complexity. On traffic modeling problems, both from PDEs and from experimental highway data, (U)NFV captures nonlinear wave dynamics with significantly higher fidelity and scalability than traditional FV approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better</title>
<link>https://arxiv.org/abs/2505.23705</link>
<guid>https://arxiv.org/abs/2505.23705</guid>
<content:encoded><![CDATA[
arXiv:2505.23705v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning</title>
<link>https://arxiv.org/abs/2505.23719</link>
<guid>https://arxiv.org/abs/2505.23719</guid>
<content:encoded><![CDATA[
arXiv:2505.23719v1 Announce Type: new 
Abstract: In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as context for forecasting future values, making powerful forecasting tools accessible to non-experts and increasing the performance when training data are scarce. Most existing zero-shot forecasting approaches rely on transformer architectures, which, despite their success in language, often fall short of expectations in time series forecasting, where recurrent models like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited for time series modeling due to their state-tracking capabilities, they lack strong in-context learning abilities. We introduce TiRex that closes this gap by leveraging xLSTM, an enhanced LSTM with competitive in-context learning skills. Unlike transformers, state-space models, or parallelizable RNNs such as RWKV, TiRex retains state-tracking, a critical property for long-horizon forecasting. To further facilitate its state-tracking ability, we propose a training-time masking strategy called CPM. TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models including TabPFN-TS (Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce) across both short- and long-term forecasts.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents</title>
<link>https://arxiv.org/abs/2505.23720</link>
<guid>https://arxiv.org/abs/2505.23720</guid>
<content:encoded><![CDATA[
arXiv:2505.23720v1 Announce Type: new 
Abstract: This paper considers a contextual bandit problem involving multiple agents, where a learner sequentially observes the contexts and the agent's reported arms, and then selects the arm that maximizes the system's overall reward. Existing work in contextual bandits assumes that agents truthfully report their arms, which is unrealistic in many real-life applications. For instance, consider an online platform with multiple sellers; some sellers may misrepresent product quality to gain an advantage, such as having the platform preferentially recommend their products to online users. To address this challenge, we propose an algorithm, COBRA, for contextual bandit problems involving strategic agents that disincentivize their strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee. Our experimental results also validate the different performance aspects of our proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffER: Categorical Diffusion for Chemical Retrosynthesis</title>
<link>https://arxiv.org/abs/2505.23721</link>
<guid>https://arxiv.org/abs/2505.23721</guid>
<content:encoded><![CDATA[
arXiv:2505.23721v1 Announce Type: new 
Abstract: Methods for automatic chemical retrosynthesis have found recent success through the application of models traditionally built for natural language processing, primarily through transformer neural networks. These models have demonstrated significant ability to translate between the SMILES encodings of chemical products and reactants, but are constrained as a result of their autoregressive nature. We propose DiffER, an alternative template-free method for retrosynthesis prediction in the form of categorical diffusion, which allows the entire output SMILES sequence to be predicted in unison. We construct an ensemble of diffusion models which achieves state-of-the-art performance for top-1 accuracy and competitive performance for top-3, top-5, and top-10 accuracy among template-free methods. We prove that DiffER is a strong baseline for a new class of template-free model, capable of learning a variety of synthetic techniques used in laboratory settings and outperforming a variety of other template-free methods on top-k accuracy metrics. By constructing an ensemble of categorical diffusion models with a novel length prediction component with variance, our method is able to approximately sample from the posterior distribution of reactants, producing results with strong metrics of confidence and likelihood. Furthermore, our analyses demonstrate that accurate prediction of the SMILES sequence length is key to further boosting the performance of categorical diffusion models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA</title>
<link>https://arxiv.org/abs/2505.23724</link>
<guid>https://arxiv.org/abs/2505.23724</guid>
<content:encoded><![CDATA[
arXiv:2505.23724v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuLoCo: Muon is a practical inner optimizer for DiLoCo</title>
<link>https://arxiv.org/abs/2505.23725</link>
<guid>https://arxiv.org/abs/2505.23725</guid>
<content:encoded><![CDATA[
arXiv:2505.23725v1 Announce Type: new 
Abstract: DiLoCo is a powerful framework for training large language models (LLMs) under networking constraints with advantages for increasing parallelism and accelerator utilization in data center settings. Despite significantly reducing communication frequency, however, DiLoCo's communication steps still involve all-reducing a complete copy of the model's parameters. While existing works have explored ways to reduce communication in DiLoCo, the role of error feedback accumulators and the effect of the inner-optimizer on compressibility remain under-explored. In this work, we investigate the effectiveness of standard compression methods including Top-k sparsification and quantization for reducing the communication overhead of DiLoCo when paired with two local optimizers (AdamW and Muon). Our experiments pre-training decoder-only transformer language models (LMs) reveal that leveraging Muon as the inner optimizer for DiLoCo along with an error-feedback accumulator allows to aggressively compress the communicated delta to 2-bits with next to no performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo) significantly outperforms DiLoCo while communicating 8X less and having identical memory complexity.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast</title>
<link>https://arxiv.org/abs/2505.23732</link>
<guid>https://arxiv.org/abs/2505.23732</guid>
<content:encoded><![CDATA[
arXiv:2505.23732v1 Announce Type: new 
Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods typically learn by na\"ively aligning audio samples with corresponding text prompts. Consequently, this approach fails to capture the ordinal nature of emotions, hindering inter-emotion understanding and often resulting in a wide modality gap between the audio and text embeddings due to insufficient alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a supervised contrastive learning approach that uses dimensional attributes of emotional speech and natural language prompts to jointly capture fine-grained emotion variations and improve cross-modal alignment. Our approach utilizes a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space. EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities, measured via a cross-modal retrieval task.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?</title>
<link>https://arxiv.org/abs/2505.23749</link>
<guid>https://arxiv.org/abs/2505.23749</guid>
<content:encoded><![CDATA[
arXiv:2505.23749v1 Announce Type: new 
Abstract: After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average -- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature $\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REOrdering Patches Improves Vision Models</title>
<link>https://arxiv.org/abs/2505.23751</link>
<guid>https://arxiv.org/abs/2505.23751</guid>
<content:encoded><![CDATA[
arXiv:2505.23751v1 Announce Type: new 
Abstract: Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Immunization from a Condition Number Perspective</title>
<link>https://arxiv.org/abs/2505.23760</link>
<guid>https://arxiv.org/abs/2505.23760</guid>
<content:encoded><![CDATA[
arXiv:2505.23760v1 Announce Type: new 
Abstract: Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Information: An Information-Theoretic Perspective on Preference Optimization</title>
<link>https://arxiv.org/abs/2505.23761</link>
<guid>https://arxiv.org/abs/2505.23761</guid>
<content:encoded><![CDATA[
arXiv:2505.23761v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model</title>
<link>https://arxiv.org/abs/2505.11047</link>
<guid>https://arxiv.org/abs/2505.11047</guid>
<content:encoded><![CDATA[
arXiv:2505.11047v1 Announce Type: cross 
Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology based on multi-objective optimization to balance battery degradation and V2G revenue according to EV user preference. Given the lack of accurate and generalizable battery degradation models, we leverage input convex neural networks (ICNNs) to develop a data-driven degradation model trained on extensive experimental datasets. This approach enables our model to capture nonconvex dependencies on battery temperature and time while maintaining convexity with respect to the charging rate. Such a partial convexity property ensures that the second stage of our methodology remains computationally efficient. In the second stage, we integrate our data-driven degradation model into a multi-objective optimization framework to generate an optimal smart charging profile for each EV. This profile effectively balances the trade-off between financial benefits from V2G participation and battery degradation, controlled by a hyperparameter reflecting the user prioritization of battery health. Numerical simulations show the high accuracy of the ICNN model in predicting battery degradation for unseen data. Finally, we present a trade-off curve illustrating financial benefits from V2G versus losses from battery health degradation based on user preferences and showcase smart charging strategies under realistic scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data</title>
<link>https://arxiv.org/abs/2505.22670</link>
<guid>https://arxiv.org/abs/2505.22670</guid>
<content:encoded><![CDATA[
arXiv:2505.22670v1 Announce Type: cross 
Abstract: Building Information Modeling (BIM) has revolutionized the construction industry by providing a comprehensive digital representation of building structures throughout their lifecycle. However, existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making. This study proposes a unified network-based representation method that integrates the "semantic-spatial-topological" multi-dimensional design features of BIM models. By extending the IFC (Industry Foundation Classes) standard, we introduce local spatial relationships and topological connections between components to enrich the network structure. This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM, and holds significant potential for the representation and learning of design patterns.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models</title>
<link>https://arxiv.org/abs/2505.22674</link>
<guid>https://arxiv.org/abs/2505.22674</guid>
<content:encoded><![CDATA[
arXiv:2505.22674v1 Announce Type: cross 
Abstract: Predicting protein complex structures is essential for protein function analysis, protein design, and drug discovery. While AI methods like AlphaFold can predict accurate structural models for many protein complexes, reliably estimating the quality of these predicted models (estimation of model accuracy, or EMA) for model ranking and selection remains a major challenge. A key barrier to developing effective machine learning-based EMA methods is the lack of large, diverse, and well-annotated datasets for training and evaluation. To address this gap, we introduce PSBench, a benchmark suite comprising four large-scale, labeled datasets generated during the 15th and 16th community-wide Critical Assessment of Protein Structure Prediction (CASP15 and CASP16). PSBench includes over one million structural models covering a wide range of protein sequence lengths, complex stoichiometries, functional classes, and modeling difficulties. Each model is annotated with multiple complementary quality scores at the global, local, and interface levels. PSBench also provides multiple evaluation metrics and baseline EMA methods to facilitate rigorous comparisons. To demonstrate PSBench's utility, we trained and evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE was blindly tested in CASP16 (2024), where it ranked among the top-performing EMA methods. These results highlight PSBench as a valuable resource for advancing EMA research in protein complex modeling. PSBench is publicly available at: https://github.com/BioinfoMachineLearning/PSBench.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book</title>
<link>https://arxiv.org/abs/2505.22678</link>
<guid>https://arxiv.org/abs/2505.22678</guid>
<content:encoded><![CDATA[
arXiv:2505.22678v1 Announce Type: cross 
Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model stock price movements is crucial for achieving profitable outcomes. However, this task is challenging due to the high-dimensional and volatile nature of the original data. Even recent deep learning models often struggle to capture price movement patterns effectively, particularly without well-designed features. We observed that raw LOB data exhibits inherent symmetry between the ask and bid sides, and the bid-ask differences demonstrate greater stability and lower complexity compared to the original data. Building on this insight, we propose a novel approach in which leverages the Siamese architecture to enhance the performance of existing deep learning models. The core idea involves processing the ask and bid sides separately using the same module with shared parameters. We applied our Siamese-based methods to several widely used strong baselines and validated their effectiveness using data from 14 military industry stocks in the Chinese A-share market. Furthermore, we integrated multi-head attention (MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate its role in modeling stock price movements. Our experiments used raw data and widely used Order Flow Imbalance (OFI) features as input with some strong baseline models. The results show that our method improves the performance of strong baselines in over 75$% of cases, excluding the Multi-Layer Perception (MLP) baseline, which performed poorly and is not considered practical. Furthermore, we found that Multi-Head Attention can enhance model performance, particularly over shorter forecasting horizons.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning</title>
<link>https://arxiv.org/abs/2505.22684</link>
<guid>https://arxiv.org/abs/2505.22684</guid>
<content:encoded><![CDATA[
arXiv:2505.22684v1 Announce Type: cross 
Abstract: Community partitioning is crucial in network analysis, with modularity optimization being the prevailing technique. However, traditional modularity-based methods often overlook fairness, a critical aspect in real-world applications. To address this, we introduce protected group networks and propose a novel fairness-modularity metric. This metric extends traditional modularity by explicitly incorporating fairness, and we prove that minimizing it yields naturally fair partitions for protected groups while maintaining theoretical soundness. We develop a general optimization framework for fairness partitioning and design the efficient Fair Fast Newman (FairFN) algorithm, enhancing the Fast Newman (FN) method to optimize both modularity and fairness. Experiments show FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, especially on unbalanced datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges</title>
<link>https://arxiv.org/abs/2505.22688</link>
<guid>https://arxiv.org/abs/2505.22688</guid>
<content:encoded><![CDATA[
arXiv:2505.22688v1 Announce Type: cross 
Abstract: The COVID-19 pandemic response relied heavily on statistical and machine learning models to predict key outcomes such as case prevalence and fatality rates. These predictions were instrumental in enabling timely public health interventions that helped break transmission cycles. While most existing models are grounded in traditional epidemiological data, the potential of alternative datasets, such as those derived from genomic information and human behavior, remains underexplored. In the current study, we investigated the usefulness of diverse modalities of feature sets in predicting case surges. Our results highlight the relative effectiveness of biological (e.g., mutations), public health (e.g., case counts, policy interventions) and human behavioral features (e.g., mobility and social media conversations) in predicting country-level case surges. Importantly, we uncover considerable heterogeneity in predictive performance across countries and feature modalities, suggesting that surge prediction models may need to be tailored to specific national contexts and pandemic phases. Overall, our work highlights the value of integrating alternative data sources into existing disease surveillance frameworks to enhance the prediction of pandemic dynamics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood</title>
<link>https://arxiv.org/abs/2505.22743</link>
<guid>https://arxiv.org/abs/2505.22743</guid>
<content:encoded><![CDATA[
arXiv:2505.22743v1 Announce Type: cross 
Abstract: In a variety of physically relevant settings for learning from quantum data, designing protocols that can computationally efficiently extract information remains largely an art, and there are important cases where we believe this to be impossible, that is, where there is an information-computation gap. While there is a large array of tools in the classical literature for giving evidence for average-case hardness of statistical inference problems, the corresponding tools in the quantum literature are far more limited. One such framework in the classical literature, the low-degree method, makes predictions about hardness of inference problems based on the failure of estimators given by low-degree polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree hardness. We use this to obtain the first information-computation gaps for learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it to prove hardness for learning random shallow quantum circuit states in a challenging model where states can be measured in adaptively chosen bases. To our knowledge, the ability to model adaptivity within the low-degree framework was open even in classical settings. In addition, we also obtain a low-degree hardness result for quantum error mitigation against strategies with single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and identify the threshold at which this problem becomes computationally hard for protocols that perform local measurements. Interestingly, the complexity landscape for this problem shifts when going from local measurements to more entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning Stabilizers with Noise and for agnostically learning product states.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis</title>
<link>https://arxiv.org/abs/2505.22746</link>
<guid>https://arxiv.org/abs/2505.22746</guid>
<content:encoded><![CDATA[
arXiv:2505.22746v1 Announce Type: cross 
Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool - Genetic Programming (StarBASE-GP), an automated framework for discovering meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective optimization strategy to evolve machine learning pipelines that simultaneously maximize explanatory power (r2) and minimize pipeline complexity. Biological domain knowledge is integrated at multiple stages, including the use of nine inheritance encoding strategies to model deviations from additivity, a custom linkage disequilibrium pruning node that minimizes redundancy among features, and a dynamic variant recommendation system that prioritizes informative candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of Rattus norvegicus (brown rat) to identify variants associated with body mass index, benchmarking its performance against a random baseline and a biologically naive version of the tool. StarBASE-GP consistently evolves Pareto fronts with superior performance, yielding higher accuracy in identifying both ground truth and novel quantitative trait loci, highlighting relevant targets for future validation. By incorporating evolutionary search and relevant biological theory into a flexible automated machine learning framework, StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-orthogonalizing attractor neural networks emerging from the free energy principle</title>
<link>https://arxiv.org/abs/2505.22749</link>
<guid>https://arxiv.org/abs/2505.22749</guid>
<content:encoded><![CDATA[
arXiv:2505.22749v1 Announce Type: cross 
Abstract: Attractor dynamics are a hallmark of many complex systems, including the brain. Understanding how such self-organizing dynamics emerge from first principles is crucial for advancing our understanding of neuronal computations and the design of artificial intelligence systems. Here we formalize how attractor networks emerge from the free energy principle applied to a universal partitioning of random dynamical systems. Our approach obviates the need for explicitly imposed learning and inference rules and identifies emergent, but efficient and biologically plausible inference and learning dynamics for such self-organizing systems. These result in a collective, multi-level Bayesian active inference process. Attractors on the free energy landscape encode prior beliefs; inference integrates sensory data into posterior beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and via simulations, we establish that the proposed networks favor approximately orthogonalized attractor representations, a consequence of simultaneously optimizing predictive accuracy and model complexity. These attractors efficiently span the input subspace, enhancing generalization and the mutual information between hidden causes and observable effects. Furthermore, while random data presentation leads to symmetric and sparse couplings, sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics, offering a natural extension to conventional Boltzmann Machines. Our findings offer a unifying theory of self-organizing attractor networks, providing novel insights for AI and neuroscience.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Elements of Problem Solving: What "Math" Does RL Teach?</title>
<link>https://arxiv.org/abs/2505.22756</link>
<guid>https://arxiv.org/abs/2505.22756</guid>
<content:encoded><![CDATA[
arXiv:2505.22756v1 Announce Type: cross 
Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing the reasoning capabilities of LLMs, especially with reinforcement learning (RL) methods such as GRPO showing significant performance gains. However, accuracy metrics alone do not support fine-grained assessment of capabilities and fail to reveal which problem-solving skills have been internalized. To better understand these capabilities, we propose to decompose problem solving into fundamental capabilities: Plan (mapping questions to sequences of steps), Execute (correctly performing solution steps), and Verify (identifying the correctness of a solution). Empirically, we find that GRPO mainly enhances the execution skill-improving execution robustness on problems the model already knows how to solve-a phenomenon we call temperature distillation. More importantly, we show that RL-trained models struggle with fundamentally new problems, hitting a 'coverage wall' due to insufficient planning skills. To explore RL's impact more deeply, we construct a minimal, synthetic solution-tree navigation task as an analogy for mathematical problem-solving. This controlled setup replicates our empirical findings, confirming RL primarily boosts execution robustness. Importantly, in this setting, we identify conditions under which RL can potentially overcome the coverage wall through improved exploration and generalization to new solution paths. Our findings provide insights into the role of RL in enhancing LLM reasoning, expose key limitations, and suggest a path toward overcoming these barriers. Code is available at https://github.com/cfpark00/RL-Wall.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-convex entropic mean-field optimization via Best Response flow</title>
<link>https://arxiv.org/abs/2505.22760</link>
<guid>https://arxiv.org/abs/2505.22760</guid>
<content:encoded><![CDATA[
arXiv:2505.22760v1 Announce Type: cross 
Abstract: We study the problem of minimizing non-convex functionals on the space of probability measures, regularized by the relative entropy (KL divergence) with respect to a fixed reference measure, as well as the corresponding problem of solving entropy-regularized non-convex-non-concave min-max problems. We utilize the Best Response flow (also known in the literature as the fictitious play flow) and study how its convergence is influenced by the relation between the degree of non-convexity of the functional under consideration, the regularization parameter and the tail behaviour of the reference measure. In particular, we demonstrate how to choose the regularizer, given the non-convex functional, so that the Best Response operator becomes a contraction with respect to the $L^1$-Wasserstein distance, which then ensures the existence of its unique fixed point, which is then shown to be the unique global minimizer for our optimization problem. This extends recent results where the Best Response flow was applied to solve convex optimization problems regularized by the relative entropy with respect to arbitrary reference measures, and with arbitrary values of the regularization parameter. Our results explain precisely how the assumption of convexity can be relaxed, at the expense of making a specific choice of the regularizer. Additionally, we demonstrate how these results can be applied in reinforcement learning in the context of policy optimization for Markov Decision Processes and Markov games with softmax parametrized policies in the mean-field regime.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIAS-SAM: Medical Image Anomaly Segmentation without thresholding</title>
<link>https://arxiv.org/abs/2505.22762</link>
<guid>https://arxiv.org/abs/2505.22762</guid>
<content:encoded><![CDATA[
arXiv:2505.22762v1 Announce Type: cross 
Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score. The code is available at: https://github.com/warpcut/MIAS-SAM
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games</title>
<link>https://arxiv.org/abs/2505.22781</link>
<guid>https://arxiv.org/abs/2505.22781</guid>
<content:encoded><![CDATA[
arXiv:2505.22781v1 Announce Type: cross 
Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel algorithm designed to compute approximate Nash equilibria for ergodic Mean-Field Games (MFG) in finite state-action spaces. Building on the well-established performance of TRPO in the reinforcement learning (RL) setting, we extend its methodology to the MFG framework, leveraging its stability and robustness in policy optimization. Under standard assumptions in the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing theoretical guarantees on its convergence. Our results cover both the exact formulation of the algorithm and its sample-based counterpart, where we derive high-probability guarantees and finite sample complexity. This work advances MFG optimization by bridging RL techniques with mean-field decision-making, offering a theoretically grounded approach to solving complex multi-agent problems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters</title>
<link>https://arxiv.org/abs/2505.22783</link>
<guid>https://arxiv.org/abs/2505.22783</guid>
<content:encoded><![CDATA[
arXiv:2505.22783v1 Announce Type: cross 
Abstract: We investigate the end-to-end altitude estimation performance of a convolutional autoencoder-based interference mitigation approach for frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we show that a Temporal Convolutional Network (TCN) autoencoder effectively exploits temporal correlations in the received signal, providing superior interference suppression compared to a Least Mean Squares (LMS) adaptive filter. Unlike existing approaches, the present method operates directly on the received FMCW signal. Additionally, we identify key challenges in applying deep learning to wideband FMCW interference mitigation and outline directions for future research to enhance real-time feasibility and generalization to arbitrary interference conditions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation</title>
<link>https://arxiv.org/abs/2505.22805</link>
<guid>https://arxiv.org/abs/2505.22805</guid>
<content:encoded><![CDATA[
arXiv:2505.22805v1 Announce Type: cross 
Abstract: In order to navigate safely and reliably in off-road and unstructured environments, robots must detect anomalies that are out-of-distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels in a learned feature space and detect semantically meaningful edits, enabling accurate anomaly detection for off-road navigation. Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution free M-estimation</title>
<link>https://arxiv.org/abs/2505.22807</link>
<guid>https://arxiv.org/abs/2505.22807</guid>
<content:encoded><![CDATA[
arXiv:2505.22807v1 Announce Type: cross 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a (univariate) convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient and Effective LLMs with Multi-Boolean Architectures</title>
<link>https://arxiv.org/abs/2505.22811</link>
<guid>https://arxiv.org/abs/2505.22811</guid>
<content:encoded><![CDATA[
arXiv:2505.22811v1 Announce Type: cross 
Abstract: Weight binarization has emerged as a promising strategy to drastically reduce the complexity of large language models (LLMs). It is mainly classified into two approaches: post-training binarization and finetuning with training-aware binarization methods. The first approach, while having low complexity, leads to significant loss of information from the original LLMs, resulting in poor performance. The second approach, on the other hand, relies heavily on full-precision latent weights for gradient approximation of binary weights, which not only remains suboptimal but also introduces substantial complexity. In this paper, we introduce a novel framework that effectively transforms LLMs into multi-kernel Boolean parameters, for the first time, finetunes them directly in the Boolean domain, eliminating the need for expensive latent weights. This significantly reduces complexity during both finetuning and inference. Through extensive and insightful experiments across a wide range of LLMs, we demonstrate that our method outperforms recent ultra low-bit quantization and binarization methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction</title>
<link>https://arxiv.org/abs/2505.22815</link>
<guid>https://arxiv.org/abs/2505.22815</guid>
<content:encoded><![CDATA[
arXiv:2505.22815v1 Announce Type: cross 
Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to the unaligned nature of multi-channel signals and the prevalence of extensive missing data. Existing methods struggle to capture reliable temporal patterns from such data due to significant missing values. While pre-trained foundation models show potential for addressing these challenges, they are typically designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel information and its success in RTS forecasting, we propose VIMTS, a framework adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing values, VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. Then it leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions from focused contexts. In addition, we integrate self-supervised learning for improved IMTS modeling by adapting the visual MAE to IMTS data. Extensive experiments demonstrate VIMTS's superior performance and few-shot capability, advancing the application of visual foundation models in more general time series tasks. Our code is available at https://github.com/WHU-HZY/VIMTS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation</title>
<link>https://arxiv.org/abs/2505.22842</link>
<guid>https://arxiv.org/abs/2505.22842</guid>
<content:encoded><![CDATA[
arXiv:2505.22842v1 Announce Type: cross 
Abstract: Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding</title>
<link>https://arxiv.org/abs/2505.22857</link>
<guid>https://arxiv.org/abs/2505.22857</guid>
<content:encoded><![CDATA[
arXiv:2505.22857v1 Announce Type: cross 
Abstract: Statistical n-gram language models are widely used for context-biasing tasks in Automatic Speech Recognition (ASR). However, existing implementations lack computational efficiency due to poor parallelization, making context-biasing less appealing for industrial use. This work rethinks data structures for statistical n-gram language models to enable fast and parallel operations for GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable greedy decoding for all major ASR model types - including transducers, attention encoder-decoder models, and CTC - with less than 7% computational overhead. The proposed approach can eliminate more than 50% of the accuracy gap between greedy and beam search for out-of-domain scenarios while avoiding significant slowdown caused by beam search. The implementation of the proposed NGPU-LM is open-sourced.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permissioned LLMs: Enforcing Access Control in Large Language Models</title>
<link>https://arxiv.org/abs/2505.22860</link>
<guid>https://arxiv.org/abs/2505.22860</guid>
<content:encoded><![CDATA[
arXiv:2505.22860v1 Announce Type: cross 
Abstract: In enterprise settings, organizational data is segregated, siloed and carefully protected by elaborate access control frameworks. These access control structures can completely break down if an LLM fine-tuned on the siloed data serves requests, for downstream tasks, from individuals with disparate access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs that superimpose the organizational data access control structures on query responses they generate. We formalize abstractions underpinning the means to determine whether access control enforcement happens correctly over LLM query responses. Our formalism introduces the notion of a relevant response that can be used to prove whether a PermLLM mechanism has been implemented correctly. We also introduce a novel metric, called access advantage, to empirically evaluate the efficacy of a PermLLM mechanism. We introduce three novel PermLLM mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired access control. We furthermore present two instantiations of access advantage--(i) Domain Distinguishability Index (DDI) based on Membership Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility evaluation. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP), in addition to evaluating the validity of DDI and UGI metrics themselves for quantifying access control in LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems</title>
<link>https://arxiv.org/abs/2505.22868</link>
<guid>https://arxiv.org/abs/2505.22868</guid>
<content:encoded><![CDATA[
arXiv:2505.22868v1 Announce Type: cross 
Abstract: In this paper, we propose the CrossNAS framework, an automated approach for exploring a vast, multidimensional search space that spans various design abstraction layers-circuits, architecture, and systems-to optimize the deployment of machine learning workloads on analog processing-in-memory (PIM) systems. CrossNAS leverages the single-path one-shot weight-sharing strategy combined with the evolutionary search for the first time in the context of PIM system mapping and optimization. CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperforming previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models</title>
<link>https://arxiv.org/abs/2505.22869</link>
<guid>https://arxiv.org/abs/2505.22869</guid>
<content:encoded><![CDATA[
arXiv:2505.22869v1 Announce Type: cross 
Abstract: Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
arXiv:2505.22914v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging</title>
<link>https://arxiv.org/abs/2505.22934</link>
<guid>https://arxiv.org/abs/2505.22934</guid>
<content:encoded><![CDATA[
arXiv:2505.22934v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong performance but is expensive for deployment and storage. Recent works explore model merging to combine multiple task-specific models into a single multi-task model without additional training. However, existing merging methods often fail for models fine-tuned with low-rank adaptation (LoRA), due to significant performance degradation. In this paper, we show that this issue arises from a previously overlooked interplay between model parameters and data distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM) to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates relevant to one task do not adversely shift outputs for others. Our approach can seamlessly integrate with most existing merging algorithms, reducing the unintended interference among tasks. Extensive experiments on eight datasets, tested with three widely used LMs and two large LMs, demonstrate that our method not only boosts merging performance but also preserves single-task accuracy. Furthermore, our approach exhibits greater robustness to the hyperparameters of merging. These results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Social Choice: The Next Generation</title>
<link>https://arxiv.org/abs/2505.22939</link>
<guid>https://arxiv.org/abs/2505.22939</guid>
<content:encoded><![CDATA[
arXiv:2505.22939v1 Announce Type: cross 
Abstract: A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates</title>
<link>https://arxiv.org/abs/2505.22943</link>
<guid>https://arxiv.org/abs/2505.22943</guid>
<content:encoded><![CDATA[
arXiv:2505.22943v1 Announce Type: cross 
Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NegVQA: Can Vision Language Models Understand Negation?</title>
<link>https://arxiv.org/abs/2505.22946</link>
<guid>https://arxiv.org/abs/2505.22946</guid>
<content:encoded><![CDATA[
arXiv:2505.22946v1 Announce Type: cross 
Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness</title>
<link>https://arxiv.org/abs/2505.22960</link>
<guid>https://arxiv.org/abs/2505.22960</guid>
<content:encoded><![CDATA[
arXiv:2505.22960v1 Announce Type: cross 
Abstract: The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
arXiv:2505.22961v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scaling Laws for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2505.22964</link>
<guid>https://arxiv.org/abs/2505.22964</guid>
<content:encoded><![CDATA[
arXiv:2505.22964v1 Announce Type: cross 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning coordinated badminton skills for legged manipulators</title>
<link>https://arxiv.org/abs/2505.22974</link>
<guid>https://arxiv.org/abs/2505.22974</guid>
<content:encoded><![CDATA[
arXiv:2505.22974v1 Announce Type: cross 
Abstract: Coordinating the motion between lower and upper limbs and aligning limb control with perception are substantial challenges in robotics, particularly in dynamic environments. To this end, we introduce an approach for enabling legged mobile manipulators to play badminton, a task that requires precise coordination of perception, locomotion, and arm swinging. We propose a unified reinforcement learning-based control policy for whole-body visuomotor skills involving all degrees of freedom to achieve effective shuttlecock tracking and striking. This policy is informed by a perception noise model that utilizes real-world camera data, allowing for consistent perception error levels between simulation and deployment and encouraging learned active perception behaviors. Our method includes a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques to enhance deployment readiness. Extensive experimental results in a variety of environments validate the robot's capability to predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players, demonstrating the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design</title>
<link>https://arxiv.org/abs/2505.22990</link>
<guid>https://arxiv.org/abs/2505.22990</guid>
<content:encoded><![CDATA[
arXiv:2505.22990v1 Announce Type: cross 
Abstract: RF/Analog design is essential for bridging digital technologies with real-world signals, ensuring the functionality and reliability of a wide range of electronic systems. However, analog design procedures are often intricate, time-consuming and reliant on expert intuition, and hinder the time and cost efficiency of circuit development. To overcome the limitations of the manual circuit design, we introduce MenTeR - a multiagent workflow integrated into an end-to-end analog design framework. By employing multiple specialized AI agents that collaboratively address different aspects of the design process, such as specification understanding, circuit optimization, and test bench validation, MenTeR reduces the dependency on frequent trial-and-error-style intervention. MenTeR not only accelerates the design cycle time but also facilitates a broader exploration of the design space, demonstrating robust capabilities in handling real-world analog systems. We believe that MenTeR lays the groundwork for future "RF/Analog Copilots" that can collaborate seamlessly with human designers.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features</title>
<link>https://arxiv.org/abs/2505.22997</link>
<guid>https://arxiv.org/abs/2505.22997</guid>
<content:encoded><![CDATA[
arXiv:2505.22997v1 Announce Type: cross 
Abstract: Traditional classifiers often assume feature independence or rely on overly simplistic relationships, leading to poor performance in settings where real-world dependencies matter. We introduce the Deep Copula Classifier (DCC), a generative model that separates the learning of each feature's marginal distribution from the modeling of their joint dependence structure via neural network-parameterized copulas. For each class, lightweight neural networks are used to flexibly and adaptively capture feature interactions, making DCC particularly effective when classification is driven by complex dependencies. We establish that DCC converges to the Bayes-optimal classifier under standard conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for r-smooth copula densities. Beyond theoretical guarantees, we outline several practical extensions, including high-dimensional scalability through vine and factor copula architectures, semi-supervised learning via entropy regularization, and online adaptation using streaming gradient methods. By unifying statistical rigor with the representational power of neural networks, DCC offers a mathematically grounded and interpretable framework for dependency-aware classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction</title>
<link>https://arxiv.org/abs/2505.23034</link>
<guid>https://arxiv.org/abs/2505.23034</guid>
<content:encoded><![CDATA[
arXiv:2505.23034v1 Announce Type: cross 
Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety. While large language models (LLMs) show promise in pharmaceutical tasks, their effectiveness in DDI prediction remains challenging. Inspired by the well-established clinical practice where physicians routinely reference similar historical cases to guide their decisions through case-based reasoning (CBR), we propose CBR-DDI, a novel framework that distills pharmacological principles from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI constructs a knowledge repository by leveraging LLMs to extract pharmacological insights and graph neural networks (GNNs) to model drug associations. A hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to effectively retrieve and reuse relevant cases. We further introduce a representative sampling strategy for dynamic case refinement. Extensive experiments demonstrate that CBR-DDI achieves state-of-the-art performance, with a significant 28.7% accuracy improvement over both popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization</title>
<link>https://arxiv.org/abs/2505.23056</link>
<guid>https://arxiv.org/abs/2505.23056</guid>
<content:encoded><![CDATA[
arXiv:2505.23056v1 Announce Type: cross 
Abstract: We study the convergence of the shuffling gradient method, a popular algorithm employed to minimize the finite-sum function with regularization, in which functions are passed to apply (Proximal) Gradient Descent (GD) one by one whose order is determined by a permutation on the indices of functions. In contrast to its easy implementation and effective performance in practice, the theoretical understanding remains limited. A recent advance by (Liu & Zhou, 2024b) establishes the first last-iterate convergence results under various settings, especially proving the optimal rates for smooth (strongly) convex optimization. However, their bounds for nonsmooth (strongly) convex functions are only as fast as Proximal GD. In this work, we provide the first improved last-iterate analysis for the nonsmooth case demonstrating that the widely used Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies are both provably faster than Proximal GD, reflecting the benefit of randomness. As an important implication, we give the first (nearly) optimal convergence result for the suffix average under the $\textsf{RR}$ sampling scheme in the general convex case, matching the lower bound shown by (Koren et al., 2022).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing</title>
<link>https://arxiv.org/abs/2505.23066</link>
<guid>https://arxiv.org/abs/2505.23066</guid>
<content:encoded><![CDATA[
arXiv:2505.23066v1 Announce Type: cross 
Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have made some improvements, they still have a speed bottleneck when facing large amounts of data. To address this issue, we propose an innovative algorithm called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves higher efficiency by first employing granular-balls, which reduces the data size needed to processed. The search process is then accelerated by adopting a Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the time-consuming steps, such as distance calculation, of the HNSW via quantization, further reducing the time complexity of the construct and search process. By combining the use of granular-balls and quantization of the HNSW method, our approach manages to take advantage of these treatments and significantly reduces the time complexity of the $k$NN-like algorithms, as revealed by a comprehensive complexity analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble</title>
<link>https://arxiv.org/abs/2505.23075</link>
<guid>https://arxiv.org/abs/2505.23075</guid>
<content:encoded><![CDATA[
arXiv:2505.23075v1 Announce Type: cross 
Abstract: Despite the growing clinical adoption of large language models (LLMs), current approaches heavily rely on single model architectures. To overcome risks of obsolescence and rigid dependence on single model systems, we present a novel framework, termed the Consensus Mechanism. Mimicking clinical triage and multidisciplinary clinical decision-making, the Consensus Mechanism implements an ensemble of specialized medical expert agents enabling improved clinical decision making while maintaining robust adaptability. This architecture enables the Consensus Mechanism to be optimized for cost, latency, or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an increase in accuracy on MedQA ($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA ($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These accuracy gains extended to differential diagnosis generation, where our system demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs. F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX (Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ = 45.2%).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Methods with Online Scaling Part I. Theoretical Foundations</title>
<link>https://arxiv.org/abs/2505.23081</link>
<guid>https://arxiv.org/abs/2505.23081</guid>
<content:encoded><![CDATA[
arXiv:2505.23081v1 Announce Type: cross 
Abstract: This paper establishes the theoretical foundations of the online scaled gradient methods (OSGM), a framework that utilizes online learning to adapt stepsizes and provably accelerate first-order methods. OSGM quantifies the effectiveness of a stepsize by a feedback function motivated from a convergence measure and uses the feedback to adjust the stepsize through an online learning algorithm. Consequently, instantiations of OSGM achieve convergence rates that are asymptotically no worse than the optimal stepsize. OSGM yields desirable convergence guarantees on smooth convex problems, including 1) trajectory-dependent global convergence on smooth convex objectives; 2) an improved complexity result on smooth strongly convex problems, and 3) local superlinear convergence. Notably, OSGM constitutes a new family of first-order methods with non-asymptotic superlinear convergence, joining the celebrated quasi-Newton methods. Finally, OSGM explains the empirical success of the popular hypergradient-descent heuristic in optimization for machine learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals</title>
<link>https://arxiv.org/abs/2505.23124</link>
<guid>https://arxiv.org/abs/2505.23124</guid>
<content:encoded><![CDATA[
arXiv:2505.23124v1 Announce Type: cross 
Abstract: We initiate the study of a repeated principal-agent problem over a finite horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of agents arriving in an adversarial order. At each round, the principal strategically chooses one of the $N$ arms to incentivize for an arriving agent of unknown type. The agent then chooses an arm based on its own utility and the provided incentive, and the principal receives a corresponding reward. The objective is to minimize regret against the best incentive in hindsight. Without prior knowledge of agent behavior, we show that the problem becomes intractable, leading to linear regret. We analyze two key settings where sublinear regret is achievable. In the first setting, the principal knows the arm each agent type would select greedily for any given incentive. Under this setting, we propose an algorithm that achieves a regret bound of $O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to a $\log K$ factor. In the second setting, an agent's response varies smoothly with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under this setting, we show that there is an algorithm with a regret bound of $\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to logarithmic factors. Finally, we extend our algorithmic results for both settings by allowing the principal to incentivize multiple arms simultaneously in each round.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing</title>
<link>https://arxiv.org/abs/2505.23145</link>
<guid>https://arxiv.org/abs/2505.23145</guid>
<content:encoded><![CDATA[
arXiv:2505.23145v1 Announce Type: cross 
Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes</title>
<link>https://arxiv.org/abs/2505.23160</link>
<guid>https://arxiv.org/abs/2505.23160</guid>
<content:encoded><![CDATA[
arXiv:2505.23160v1 Announce Type: cross 
Abstract: This paper introduces a novel adaptive framework for processing dynamic flow signals over simplicial complexes, extending classical least-mean-squares (LMS) methods to high-order topological domains. Building on discrete Hodge theory, we present a topological LMS algorithm that efficiently processes streaming signals observed over time-varying edge subsets. We provide a detailed stochastic analysis of the algorithm, deriving its stability conditions, steady-state mean-square-error, and convergence speed, while exploring the impact of edge sampling on performance. We also propose strategies to design optimal edge sampling probabilities, minimizing rate while ensuring desired estimation accuracy. Assuming partial knowledge of the complex structure (e.g., the underlying graph), we introduce an adaptive topology inference method that integrates with the proposed LMS framework. Additionally, we propose a distributed version of the algorithm and analyze its stability and mean-square-error properties. Empirical results on synthetic and real-world traffic data demonstrate that our approach, in both centralized and distributed settings, outperforms graph-based LMS methods by leveraging higher-order topological features.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Inversion turns CLIP into a Decoder</title>
<link>https://arxiv.org/abs/2505.23161</link>
<guid>https://arxiv.org/abs/2505.23161</guid>
<content:encoded><![CDATA[
arXiv:2505.23161v1 Announce Type: cross 
Abstract: CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows</title>
<link>https://arxiv.org/abs/2505.23196</link>
<guid>https://arxiv.org/abs/2505.23196</guid>
<content:encoded><![CDATA[
arXiv:2505.23196v1 Announce Type: cross 
Abstract: Conformal prediction provides a model-agnostic framework for uncertainty quantification with finite-sample validity guarantees, making it an attractive tool for constructing reliable prediction sets. However, existing approaches commonly rely on residual-based conformity scores, which impose geometric constraints and struggle when the underlying distribution is multimodal. In particular, they tend to produce overly conservative prediction areas centred around the mean, often failing to capture the true shape of complex predictive distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction Areas with Normalising-Flows), a conformal prediction framework that uses density-based conformity scores. By leveraging flow-based models, JAPAN estimates the (predictive) density and constructs prediction areas by thresholding on the estimated density scores, enabling compact, potentially disjoint, and context-adaptive regions that retain finite-sample coverage guarantees. We theoretically motivate the efficiency of JAPAN and empirically validate it across multivariate regression and forecasting tasks, demonstrating good calibration and tighter prediction areas compared to existing baselines. We also provide several \emph{extensions} adding flexibility to our proposed framework.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM</title>
<link>https://arxiv.org/abs/2505.23207</link>
<guid>https://arxiv.org/abs/2505.23207</guid>
<content:encoded><![CDATA[
arXiv:2505.23207v1 Announce Type: cross 
Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Generator Matching for Time Series</title>
<link>https://arxiv.org/abs/2505.23215</link>
<guid>https://arxiv.org/abs/2505.23215</guid>
<content:encoded><![CDATA[
arXiv:2505.23215v1 Announce Type: cross 
Abstract: Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge. In this paper, we leverage ideas from generative modeling of image data to push the boundary of time series generation. For this, we find new generators of SDEs and jump processes, inspired by trajectory flow matching, that have the marginal distributions of the time series of interest. Specifically, we can handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled Gaussians that allow for closed form formulas of the corresponding Kullback-Leibler divergence in the loss. Unlike most other approaches, we are able to handle irregularly sampled time series.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint estimation of smooth graph signals from partial linear measurements</title>
<link>https://arxiv.org/abs/2505.23240</link>
<guid>https://arxiv.org/abs/2505.23240</guid>
<content:encoded><![CDATA[
arXiv:2505.23240v1 Announce Type: cross 
Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given partial linear measurements of the signals, for a potentially small subset of the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are smooth w.r.t $G$, in the sense that the quadratic variation of the signals over the graph is small, we obtain non-asymptotic bounds on the mean squared error for jointly recovering $x_t$'s, for the smoothness penalized least squares estimator. In particular, this implies for certain choices of $G$ that this estimator is weakly consistent (as $T \rightarrow \infty$) under potentially very stringent sampling, where only one coordinate is measured per vertex for a vanishingly small fraction of the vertices. The results are extended to a ``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths of a collection of $n$ items, and noisy pairwise difference measurements are obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency is established for certain choices of $G$ even when the individual $G_t$'s are very sparse and disconnected.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Thompson Sampling: Valid Inference via Variance Inflation</title>
<link>https://arxiv.org/abs/2505.23260</link>
<guid>https://arxiv.org/abs/2505.23260</guid>
<content:encoded><![CDATA[
arXiv:2505.23260v1 Announce Type: cross 
Abstract: We consider the problem of statistical inference when the data is collected via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known to be both asymptotically optimal and empirically effective, its adaptive sampling scheme poses challenges for constructing confidence intervals for model parameters. We propose and analyze a variant of TS, called Stable Thompson Sampling, in which the posterior variance is inflated by a logarithmic factor. We show that this modification leads to asymptotically normal estimates of the arm means, despite the non-i.i.d. nature of the data. Importantly, this statistical benefit comes at a modest cost: the variance inflation increases regret by only a logarithmic factor compared to standard TS. Our results reveal a principled trade-off: by paying a small price in regret, one can enable valid statistical inference for adaptive decision-making algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LADA: Scalable Label-Specific CLIP Adapter for Continual Learning</title>
<link>https://arxiv.org/abs/2505.23271</link>
<guid>https://arxiv.org/abs/2505.23271</guid>
<content:encoded><![CDATA[
arXiv:2505.23271v1 Announce Type: cross 
Abstract: Continual learning with vision-language models like CLIP offers a pathway toward scalable machine learning systems by leveraging its transferable representations. Existing CLIP-based methods adapt the pre-trained image encoder by adding multiple sets of learnable parameters, with each task using a partial set of parameters. This requires selecting the expected parameters for input images during inference, which is prone to error that degrades performance. To address this problem, we introduce LADA (Label-specific ADApter). Instead of partitioning parameters across tasks, LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. To prevent catastrophic forgetting, LADA employs feature distillation for seen classes, preventing their features from being interfered with by new classes. Positioned after the image encoder, LADA prevents gradient flow to the frozen CLIP parameters, ensuring efficient training. Extensive results show that LADA achieves state-of-the-art performance in continual learning settings. The implementation code is available at https://github.com/MaolinLuo/LADA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does Response Length Affect Long-Form Factuality</title>
<link>https://arxiv.org/abs/2505.23295</link>
<guid>https://arxiv.org/abs/2505.23295</guid>
<content:encoded><![CDATA[
arXiv:2505.23295v1 Announce Type: cross 
Abstract: Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction</title>
<link>https://arxiv.org/abs/2505.23305</link>
<guid>https://arxiv.org/abs/2505.23305</guid>
<content:encoded><![CDATA[
arXiv:2505.23305v1 Announce Type: cross 
Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous music generation, source imputation, and query-driven source separation. Unlike prior approaches constrained to fixed instrument classes, MGE-LDM learns a joint distribution over full mixtures, submixtures, and individual stems within a single compact latent diffusion model. At inference, MGE-LDM enables (1) complete mixture generation, (2) partial generation (i.e., source imputation), and (3) text-conditioned extraction of arbitrary sources. By formulating both separation and imputation as conditional inpainting tasks in the latent space, our approach supports flexible, class-agnostic manipulation of arbitrary instrument sources. Notably, MGE-LDM can be trained jointly across heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without relying on predefined instrument categories. Audio samples are available at our project page: https://yoongi43.github.io/MGELDM_Samples/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition</title>
<link>https://arxiv.org/abs/2505.23313</link>
<guid>https://arxiv.org/abs/2505.23313</guid>
<content:encoded><![CDATA[
arXiv:2505.23313v1 Announce Type: cross 
Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in human-centered research and has made great progress in recent years with the development of deep neural networks. However, the potential vulnerability and anti-interference ability have still not been fully explored. To bridge this gap, this paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition. Specifically, we exploit both global- and patch-level attacks on the pedestrian images, based on the pre-trained CLIP-based PAR framework. It first divides the input pedestrian image into non-overlapping patches and embeds them into feature embeddings using a projection layer. Meanwhile, the attribute set is expanded into sentences using prompts and embedded into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer is adopted to fuse the obtained vision and text tokens, and a feed-forward network is utilized for attribute recognition. Based on the aforementioned PAR framework, we adopt the adversarial semantic and label-perturbation to generate the adversarial noise, termed ASL-PAR. We also design a semantic offset defense strategy to suppress the influence of adversarial attacks. Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of our proposed adversarial attack and defense strategies for the pedestrian attribute recognition. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments</title>
<link>https://arxiv.org/abs/2505.23315</link>
<guid>https://arxiv.org/abs/2505.23315</guid>
<content:encoded><![CDATA[
arXiv:2505.23315v1 Announce Type: cross 
Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that scores are only released when they meet high reliability standards. Confidence modelling addresses this by assigning a reliability estimate measure, in the form of a confidence score, to each automated score. In this study, we frame confidence estimation as a classification task: predicting whether an AES-generated score correctly places a candidate in the appropriate CEFR level. While this is a binary decision, we leverage the inherent granularity of the scoring domain in two ways. First, we reformulate the task as an n-ary classification problem using score binning. Second, we introduce a set of novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that incorporate the ordinal structure of CEFR labels. Our best-performing model achieves an F1 score of 0.97, and enables the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to approximately 92% (approx.) CEFR agreement from the standalone AES model where we release all AM predicted scores.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.23331</link>
<guid>https://arxiv.org/abs/2505.23331</guid>
<content:encoded><![CDATA[
arXiv:2505.23331v1 Announce Type: cross 
Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors</title>
<link>https://arxiv.org/abs/2505.23344</link>
<guid>https://arxiv.org/abs/2505.23344</guid>
<content:encoded><![CDATA[
arXiv:2505.23344v1 Announce Type: cross 
Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical and photophysical processes with methods such as the widely used fewest-switches surface hopping (FSSH). There is therefore a strong incentive to machine learn NACs for accelerating simulations. However, this is challenging due to NACs' vectorial, double-valued character and the singularity near a conical intersection seam. For the first time, we design NAC-specific descriptors based on our domain expertise and show that they allow learning NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to success is also our new ML phase-correction procedure. We demonstrate the efficiency and robustness of our approach on a prototypical example of fully ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic structure level. This ML-FSSH dynamics leads to an accurate description of $S_1$ decay while reducing error bars by allowing the execution of a large ensemble of trajectories. Our implementations are available in open-source MLatom.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference</title>
<link>https://arxiv.org/abs/2505.23389</link>
<guid>https://arxiv.org/abs/2505.23389</guid>
<content:encoded><![CDATA[
arXiv:2505.23389v1 Announce Type: cross 
Abstract: Quantum sensing exploits non-classical effects to overcome limitations of classical sensors, with applications ranging from gravitational-wave detection to nanoscale imaging. However, practical quantum sensors built on noisy intermediate-scale quantum (NISQ) devices face significant noise and sampling constraints, and current variational quantum sensing (VQS) methods lack rigorous performance guarantees. This paper proposes an online control framework for VQS that dynamically updates the variational parameters while providing deterministic error bars on the estimates. By leveraging online conformal inference techniques, the approach produces sequential estimation sets with a guaranteed long-term risk level. Experiments on a quantum magnetometry task confirm that the proposed dynamic VQS approach maintains the required reliability over time, while still yielding precise estimates. The results demonstrate the practical benefits of combining variational quantum algorithms with online conformal inference to achieve reliable quantum sensing on NISQ devices.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Editing for Audio-Visual Dubbing</title>
<link>https://arxiv.org/abs/2505.23406</link>
<guid>https://arxiv.org/abs/2505.23406</guid>
<content:encoded><![CDATA[
arXiv:2505.23406v1 Announce Type: cross 
Abstract: Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</title>
<link>https://arxiv.org/abs/2505.23416</link>
<guid>https://arxiv.org/abs/2505.23416</guid>
<content:encoded><![CDATA[
arXiv:2505.23416v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves</title>
<link>https://arxiv.org/abs/2505.23431</link>
<guid>https://arxiv.org/abs/2505.23431</guid>
<content:encoded><![CDATA[
arXiv:2505.23431v1 Announce Type: cross 
Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel dissimilarity measure for polygonal curves. $k$-DTW has stronger metric properties than Dynamic Time Warping (DTW) and is more robust to outliers than the Fr\'{e}chet distance, which are the two gold standards of dissimilarity measures for polygonal curves. We show interesting properties of $k$-DTW and give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm for $k$-DTW by a parametric search for the $k$-th largest matched distance. We prove the first dimension-free learning bounds for curves and further learning theoretic results. $k$-DTW not only admits smaller sample size than DTW for the problem of learning the median of curves, where some factors depending on the curves' complexity $m$ are replaced by $k$, but we also show a surprising separation on the associated Rademacher and Gaussian complexities: $k$-DTW admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$ when $k\ll m$. We complement our theoretical findings with an experimental illustration of the benefits of using $k$-DTW for clustering and nearest neighbor classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
arXiv:2505.23436v1 Announce Type: cross 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation</title>
<link>https://arxiv.org/abs/2505.23445</link>
<guid>https://arxiv.org/abs/2505.23445</guid>
<content:encoded><![CDATA[
arXiv:2505.23445v1 Announce Type: cross 
Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a measure becomes a target, it ceases to be a good measure''. As machine learning models and the optimisation capacity to train them grow, growing empirical evidence reinforced the belief in the validity of this law without however being formalised. Recently, a few attempts were made to formalise Goodhart's law, either by categorising variants of it, or by looking at how optimising a proxy metric affects the optimisation of an intended goal. In this work, we alleviate the simplifying independence assumption, made in previous works, and the assumption on the learning paradigm made in most of them, to study the effect of the coupling between the proxy metric and the intended goal on Goodhart's law. Our results show that in the case of light tailed goal and light tailed discrepancy, dependence does not change the nature of Goodhart's effect. However, in the light tailed goal and heavy tailed discrepancy case, we exhibit an example where over-optimisation occurs at a rate inversely proportional to the heavy tailedness of the discrepancy between the goal and the metric. %
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning</title>
<link>https://arxiv.org/abs/2505.23475</link>
<guid>https://arxiv.org/abs/2505.23475</guid>
<content:encoded><![CDATA[
arXiv:2505.23475v1 Announce Type: cross 
Abstract: Fast and scalable alignment of time series is a fundamental challenge in many domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor scalability and sensitivity to noise. We introduce TimePoint, a self-supervised method that dramatically accelerates DTW-based alignment while typically improving alignment accuracy by learning keypoints and descriptors from synthetic data. Inspired by 2D keypoint detection but carefully adapted to the unique challenges of 1D signals, TimePoint leverages efficient 1D diffeomorphisms, which effectively model nonlinear time warping, to generate realistic training data. This approach, along with fully convolutional and wavelet convolutional architectures, enables the extraction of informative keypoints and descriptors. Applying DTW to these sparse representations yield major speedups and typically higher alignment accuracy than standard DTW applied to the full signals. TimePoint demonstrates strong generalization to real-world time series when trained solely on synthetic data, and further improves with fine-tuning on real data. Extensive experiments demonstrate that TimePoint consistently achieves faster and more accurate alignments than standard DTW, making it a scalable solution for time-series analysis. Our code is available at https://github.com/BGU-CS-VIL/TimePoint
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v1 Announce Type: cross 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</title>
<link>https://arxiv.org/abs/2505.23509</link>
<guid>https://arxiv.org/abs/2505.23509</guid>
<content:encoded><![CDATA[
arXiv:2505.23509v1 Announce Type: cross 
Abstract: Audio DNNs have demonstrated impressive performance on various machine listening tasks; however, most of their representations are computationally costly and uninterpretable, leaving room for optimization. Here, we propose a novel approach centered on spectrotemporal modulation (STM) features, a signal processing method that mimics the neurophysiological representation in the human auditory cortex. The classification performance of our STM-based model, without any pretraining, is comparable to that of pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds, which are essential categories for both human cognition and machine perception. These results show that STM is an efficient and interpretable feature representation for audio classification, advancing the development of machine listening and unlocking exciting new possibilities for basic understanding of speech and auditory sciences, as well as developing audio BCI and cognitive computing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration</title>
<link>https://arxiv.org/abs/2505.23515</link>
<guid>https://arxiv.org/abs/2505.23515</guid>
<content:encoded><![CDATA[
arXiv:2505.23515v1 Announce Type: cross 
Abstract: In this work, we propose a full-band real-time speech enhancement system with GAN-based stochastic regeneration. Predictive models focus on estimating the mean of the target distribution, whereas generative models aim to learn the full distribution. This behavior of predictive models may lead to over-suppression, i.e. the removal of speech content. In the literature, it was shown that combining a predictive model with a generative one within the stochastic regeneration framework can reduce the distortion in the output. We use this framework to obtain a real-time speech enhancement system. With 3.58M parameters and a low latency, our system is designed for real-time streaming with a lightweight architecture. Experiments show that our system improves over the first stage in terms of NISQA-MOS metric. Finally, through an ablation study, we show the importance of noisy conditioning in our system. We participated in 2025 Urgent Challenge with our model and later made further improvements.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data</title>
<link>https://arxiv.org/abs/2505.23522</link>
<guid>https://arxiv.org/abs/2505.23522</guid>
<content:encoded><![CDATA[
arXiv:2505.23522v1 Announce Type: cross 
Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical limitations in systematic coverage of geosystem components and cross-sphere interactions, often constrained to isolated subsystems (only in Human-activities sphere or atmosphere) with limited evaluation dimensions (less than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first comprehensive multimodal benchmark spanning all six Earth science spheres (atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and Human-activities sphere) and cross-spheres with one hundred expert-curated evaluation dimensions. Leveraging observational data from satellite sensors and in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four tiers: perception, general reasoning, scientific knowledge reasoning and chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per sphere to establish authoritative evaluation dimensions and curate relevant observational datasets, 40 crowd-sourcing annotators to assist experts for annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets a new standard for geosystem-aware AI, advancing both scientific discovery and practical applications in environmental monitoring and disaster prediction. The dataset, source code, and trained models were released.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters</title>
<link>https://arxiv.org/abs/2505.23554</link>
<guid>https://arxiv.org/abs/2505.23554</guid>
<content:encoded><![CDATA[
arXiv:2505.23554v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Parametric Distributions from Samples and Preferences</title>
<link>https://arxiv.org/abs/2505.23557</link>
<guid>https://arxiv.org/abs/2505.23557</guid>
<content:encoded><![CDATA[
arXiv:2505.23557v1 Announce Type: cross 
Abstract: Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preference-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of $\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$ rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to dimension and problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information</title>
<link>https://arxiv.org/abs/2505.23558</link>
<guid>https://arxiv.org/abs/2505.23558</guid>
<content:encoded><![CDATA[
arXiv:2505.23558v1 Announce Type: cross 
Abstract: Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: https://github.com/Liar406/Look_Again.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring</title>
<link>https://arxiv.org/abs/2505.23575</link>
<guid>https://arxiv.org/abs/2505.23575</guid>
<content:encoded><![CDATA[
arXiv:2505.23575v1 Announce Type: cross 
Abstract: As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that CoT monitoring improves detection by up to 27 percentage points in scenarios where action-only monitoring fails to reliably identify sabotage. However, CoT traces can also contain misleading rationalizations that deceive the monitor, reducing performance in more obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores both reasoning and final outputs and combines them using a weighted average. This hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates over four times higher than action-only monitoring for subtle deception scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation</title>
<link>https://arxiv.org/abs/2505.23587</link>
<guid>https://arxiv.org/abs/2505.23587</guid>
<content:encoded><![CDATA[
arXiv:2505.23587v1 Announce Type: cross 
Abstract: In medical image segmentation, limited external validity remains a critical obstacle when models are deployed across unseen datasets, an issue particularly pronounced in the ultrasound image domain. Existing solutions-such as domain adaptation and GAN-based style transfer-while promising, often fall short in the medical domain where datasets are typically small and diverse. This paper presents a novel application of principal component analysis (PCA) to address this limitation. PCA preprocessing reduces noise and emphasizes essential features by retaining approximately 90\% of the dataset variance. We evaluate our approach across six diverse breast tumor ultrasound datasets comprising 3,983 B-mode images and corresponding expert tumor segmentation masks. For each dataset, a corresponding dimensionality reduced PCA-dataset is created and U-Net-based segmentation models are trained on each of the twelve datasets. Each model trained on an original dataset was inferenced on the remaining five out-of-domain original datasets (baseline results), while each model trained on a PCA dataset was inferenced on five out-of-domain PCA datasets. Our experimental results indicate that using PCA reconstructed datasets, instead of original images, improves the model's recall and Dice scores, particularly for model-dataset pairs where baseline performance was lowest, achieving statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05, $p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p = 0.03$). Our method reduced the decline in recall values due to external validation by $33\%$. These findings underscore the potential of PCA reconstruction as a safeguard to mitigate declines in segmentation performance, especially in challenging cases, with implications for enhancing external validity in real-world medical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilook Coherent Imaging: Theoretical Guarantees and Algorithms</title>
<link>https://arxiv.org/abs/2505.23594</link>
<guid>https://arxiv.org/abs/2505.23594</guid>
<content:encoded><![CDATA[
arXiv:2505.23594v1 Announce Type: cross 
Abstract: Multilook coherent imaging is a widely used technique in applications such as digital holography, ultrasound imaging, and synthetic aperture radar. A central challenge in these systems is the presence of multiplicative noise, commonly known as speckle, which degrades image quality. Despite the widespread use of coherent imaging systems, their theoretical foundations remain relatively underexplored. In this paper, we study both the theoretical and algorithmic aspects of likelihood-based approaches for multilook coherent imaging, providing a rigorous framework for analysis and method development. Our theoretical contributions include establishing the first theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our results capture the dependence of MSE on the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we employ projected gradient descent (PGD) as an efficient method for computing the maximum likelihood solution. Furthermore, we introduce two key ideas to enhance the practical performance of PGD. First, we incorporate the Newton-Schulz algorithm to compute matrix inverses within the PGD iterations, significantly reducing computational complexity. Second, we develop a bagging strategy to mitigate projection errors introduced during PGD updates. We demonstrate that combining these techniques with PGD yields state-of-the-art performance. Our code is available at https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</title>
<link>https://arxiv.org/abs/2505.23617</link>
<guid>https://arxiv.org/abs/2505.23617</guid>
<content:encoded><![CDATA[
arXiv:2505.23617v1 Announce Type: cross 
Abstract: Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.23619</link>
<guid>https://arxiv.org/abs/2505.23619</guid>
<content:encoded><![CDATA[
arXiv:2505.23619v1 Announce Type: cross 
Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice cloning, have intensified the demand for adaptable and efficient deepfake detection methods. As TTS systems continue to evolve, detection models must be able to efficiently adapt to previously unseen generation models with minimal data. This paper introduces ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show how the combination of a powerful deep embedding model with the Gaussian processes flexibility can achieve strong performance and adaptability. Additionally, we show this approach can also be used for personalized detection, with greater robustness to new TTS models and one-shot adaptability. To support our evaluation, a benchmark dataset is constructed for this task using new state-of-the-art voice cloning models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Optimality for Private KL Distribution Estimation</title>
<link>https://arxiv.org/abs/2505.23620</link>
<guid>https://arxiv.org/abs/2505.23620</guid>
<content:encoded><![CDATA[
arXiv:2505.23620v1 Announce Type: cross 
Abstract: We study the fundamental problem of estimating an unknown discrete distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances $p$ and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on $p$ is compared to the minimum achievable estimation error over a small local neighborhood of $p$. Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Reasoning Models More Prone to Hallucination?</title>
<link>https://arxiv.org/abs/2505.23646</link>
<guid>https://arxiv.org/abs/2505.23646</guid>
<content:encoded><![CDATA[
arXiv:2505.23646v1 Announce Type: cross 
Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization-Free Diffusion Model -- A Perturbation Theory Approach</title>
<link>https://arxiv.org/abs/2505.23652</link>
<guid>https://arxiv.org/abs/2505.23652</guid>
<content:encoded><![CDATA[
arXiv:2505.23652v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a powerful framework in generative modeling, typically relying on optimizing neural networks to estimate the score function via forward SDE simulations. In this work, we propose an alternative method that is both optimization-free and forward SDE-free. By expanding the score function in a sparse set of eigenbasis of the backward Kolmogorov operator associated with the diffusion process, we reformulate score estimation as the solution to a linear system, avoiding iterative optimization and time-dependent sample generation. We analyze the approximation error using perturbation theory and demonstrate the effectiveness of our method on high-dimensional Boltzmann distributions and real-world datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation</title>
<link>https://arxiv.org/abs/2505.23657</link>
<guid>https://arxiv.org/abs/2505.23657</guid>
<content:encoded><![CDATA[
arXiv:2505.23657v1 Announce Type: cross 
Abstract: Recent decoding methods improve the factuality of large language models~(LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Perspective on Memorization and Reconstruction</title>
<link>https://arxiv.org/abs/2505.23658</link>
<guid>https://arxiv.org/abs/2505.23658</guid>
<content:encoded><![CDATA[
arXiv:2505.23658v1 Announce Type: cross 
Abstract: We introduce a new Bayesian perspective on the concept of data reconstruction, and leverage this viewpoint to propose a new security definition that, in certain settings, provably prevents reconstruction attacks. We use our paradigm to shed new light on one of the most notorious attacks in the privacy and memorization literature - fingerprinting code attacks (FPC). We argue that these attacks are really a form of membership inference attacks, rather than reconstruction attacks. Furthermore, we show that if the goal is solely to prevent reconstruction (but not membership inference), then in some cases the impossibility results derived from FPC no longer apply.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoLA: Low-Rank Linear Attention With Sparse Caching</title>
<link>https://arxiv.org/abs/2505.23666</link>
<guid>https://arxiv.org/abs/2505.23666</guid>
<content:encoded><![CDATA[
arXiv:2505.23666v1 Announce Type: cross 
Abstract: Transformer-based large language models suffer from quadratic complexity at inference on long sequences. Linear attention methods are efficient alternatives, however, they fail to provide an accurate approximation of softmax attention. By additionally incorporating sliding window attention into each linear attention head, this gap can be closed for short context-length tasks. Unfortunately, these approaches cannot recall important information from long contexts due to "memory collisions". In this paper , we propose LoLA: Low-rank Linear Attention with sparse caching. LoLA separately stores additional key-value pairs that would otherwise interfere with past associative memories. Moreover, LoLA further closes the gap between linear attention models and transformers by distributing past key-value pairs into three forms of memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. As an inference-only strategy, LoLA enables pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to 97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1 8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an extremely lightweight approach: Nearly all of our results can be reproduced on a single consumer GPU.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
arXiv:2505.23671v1 Announce Type: cross 
Abstract: Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobi-$\pi$: Mobilizing Your Robot Learning Policy</title>
<link>https://arxiv.org/abs/2505.23692</link>
<guid>https://arxiv.org/abs/2505.23692</guid>
<content:encoded><![CDATA[
arXiv:2505.23692v1 Announce Type: cross 
Abstract: Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.23709</link>
<guid>https://arxiv.org/abs/2505.23709</guid>
<content:encoded><![CDATA[
arXiv:2505.23709v1 Announce Type: cross 
Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering</title>
<link>https://arxiv.org/abs/2505.23723</link>
<guid>https://arxiv.org/abs/2505.23723</guid>
<content:encoded><![CDATA[
arXiv:2505.23723v1 Announce Type: cross 
Abstract: The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence Analysis of Muon</title>
<link>https://arxiv.org/abs/2505.23737</link>
<guid>https://arxiv.org/abs/2505.23737</guid>
<content:encoded><![CDATA[
arXiv:2505.23737v1 Announce Type: cross 
Abstract: The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent (GD). We further characterize the conditions under which Muon can outperform GD. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trust Or Not To Trust Your Vision-Language Model's Prediction</title>
<link>https://arxiv.org/abs/2505.23745</link>
<guid>https://arxiv.org/abs/2505.23745</guid>
<content:encoded><![CDATA[
arXiv:2505.23745v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.23747</link>
<guid>https://arxiv.org/abs/2505.23747</guid>
<content:encoded><![CDATA[
arXiv:2505.23747v1 Announce Type: cross 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
arXiv:2505.23759v1 Announce Type: cross 
Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Chat Logs to Collective Insights: Aggregative Question Answering</title>
<link>https://arxiv.org/abs/2505.23765</link>
<guid>https://arxiv.org/abs/2505.23765</guid>
<content:encoded><![CDATA[
arXiv:2505.23765v1 Announce Type: cross 
Abstract: Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2301.08028</link>
<guid>https://arxiv.org/abs/2301.08028</guid>
<content:encoded><![CDATA[
arXiv:2301.08028v4 Announce Type: replace 
Abstract: While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug In and Learn: Federated Intelligence over a Smart Grid of Models</title>
<link>https://arxiv.org/abs/2302.04363</link>
<guid>https://arxiv.org/abs/2302.04363</guid>
<content:encoded><![CDATA[
arXiv:2302.04363v4 Announce Type: replace 
Abstract: We present a model-agnostic federated learning method that mirrors the operation of a smart power grid: diverse local models, like energy prosumers, train independently on their own data while exchanging lightweight signals to coordinate with statistically similar peers. This coordination is governed by a graph-based regularizer that encourages connected models to produce similar predictions on a shared, public unlabeled dataset. The resulting method is a flexible instance of regularized empirical risk minimization and supports a wide variety of local models - both parametric and non-parametric - provided they can be trained via regularized loss minimization. Such training is readily supported by standard ML libraries including scikit-learn, Keras, and PyTorch.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Collaborative Filtering is not Collaborative: Unfairness of PCA for Recommendations</title>
<link>https://arxiv.org/abs/2310.09687</link>
<guid>https://arxiv.org/abs/2310.09687</guid>
<content:encoded><![CDATA[
arXiv:2310.09687v2 Announce Type: replace 
Abstract: We study the fairness of dimensionality reduction methods for recommendations. We focus on the fundamental method of principal component analysis (PCA), which identifies latent components and produces a low-rank approximation via the leading components while discarding the trailing components. Prior works have defined notions of "fair PCA"; however, these definitions do not answer the following question: why is PCA unfair? We identify two underlying popularity mechanisms that induce item unfairness in PCA. The first negatively impacts less popular items because less popular items rely on trailing latent components to recover their values. The second negatively impacts highly popular items, since the leading PCA components specialize in individual popular items instead of capturing similarities between items. To address these issues, we develop a polynomial-time algorithm, Item-Weighted PCA, that flexibly up-weights less popular items when optimizing for leading principal components. We theoretically show that PCA, in all cases, and Normalized PCA, in cases of block-diagonal matrices, are instances of Item-Weighted PCA. We empirically show that there exist datasets for which Item-Weighted PCA yields the optimal solution while the baselines do not. In contrast to past dimensionality reduction re-weighting techniques, Item-Weighted PCA solves a convex optimization problem and enforces a hard rank constraint. Our evaluations on real-world datasets show that Item-Weighted PCA not only mitigates both unfairness mechanisms, but also produces recommendations that outperform those of PCA baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hijacking Large Language Models via Adversarial In-Context Learning</title>
<link>https://arxiv.org/abs/2311.09948</link>
<guid>https://arxiv.org/abs/2311.09948</guid>
<content:encoded><![CDATA[
arXiv:2311.09948v3 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the preconditioned prompts. Despite its promising performance, crafted adversarial attacks pose a notable threat to the robustness of LLMs. Existing attacks are either easy to detect, require a trigger in user input, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable prompt injection attack against ICL, aiming to hijack LLMs to generate the target output or elicit harmful responses. In our threat model, the hacker acts as a model publisher who leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos via prompt injection. We also propose effective defense strategies using a few shots of clean demos, enhancing the robustness of LLMs during ICL. Extensive experimental results across various classification and jailbreak tasks demonstrate the effectiveness of the proposed attack and defense strategies. This work highlights the significant security vulnerabilities of LLMs during ICL and underscores the need for further in-depth studies.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical guarantees on the best-of-n alignment policy</title>
<link>https://arxiv.org/abs/2401.01879</link>
<guid>https://arxiv.org/abs/2401.01879</guid>
<content:encoded><![CDATA[
arXiv:2401.01879v3 Announce Type: replace 
Abstract: A simple and effective method for the inference-time alignment and scaling test-time compute of generative models is best-of-$n$ sampling, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n < 1000$.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks</title>
<link>https://arxiv.org/abs/2401.13330</link>
<guid>https://arxiv.org/abs/2401.13330</guid>
<content:encoded><![CDATA[
arXiv:2401.13330v2 Announce Type: replace 
Abstract: Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Sufficient Views: A DNN model making predictions with more evidence has higher accuracy</title>
<link>https://arxiv.org/abs/2402.01095</link>
<guid>https://arxiv.org/abs/2402.01095</guid>
<content:encoded><![CDATA[
arXiv:2402.01095v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) exhibit high performance in image recognition; however, the reasons for their strong generalization abilities remain unclear. A plausible hypothesis is that DNNs achieve robust and accurate predictions by identifying multiple pieces of evidence from images. Thus, to test this hypothesis, this study proposed minimal sufficient views (MSVs). MSVs is defined as a set of minimal regions within an input image that are sufficient to preserve the prediction of DNNs, thus representing the evidence discovered by the DNN. We empirically demonstrated a strong correlation between the number of MSVs (i.e., the number of pieces of evidence) and the generalization performance of the DNN models. Remarkably, this correlation was found to hold within a single DNN as well as between different DNNs, including convolutional and transformer models. This suggested that a DNN model that makes its prediction based on more evidence has a higher generalization performance. We proposed a metric based on MSVs for DNN model selection that did not require label information. Consequently, we empirically showed that the proposed metric was less dependent on the degree of overfitting, rendering it a more reliable indicator of model performance than existing metrics, such as average confidence.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Poison Large Language Models for Downstream Manipulation</title>
<link>https://arxiv.org/abs/2402.13459</link>
<guid>https://arxiv.org/abs/2402.13459</guid>
<content:encoded><![CDATA[
arXiv:2402.13459v3 Announce Type: replace 
Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where the adversary inserts backdoor triggers into training data to manipulate outputs. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the supervised fine-tuning (SFT) process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various language model tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during SFT of LLMs and the necessity of safeguarding LLMs against data poisoning attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniArch: Building Foundation Model For Scientific Computing</title>
<link>https://arxiv.org/abs/2402.16014</link>
<guid>https://arxiv.org/abs/2402.16014</guid>
<content:encoded><![CDATA[
arXiv:2402.16014v3 Announce Type: replace 
Abstract: Foundation models have revolutionized language modeling, while whether this success is replicated in scientific computing remains unexplored. We present OmniArch, the first prototype aiming at solving multi-scale and multi-physics scientific computing problems with physical alignment. We addressed all three challenges with one unified architecture. Its pre-training stage contains a Fourier Encoder-decoder fading out the disharmony across separated dimensions and a Transformer backbone integrating quantities through temporal dynamics, and the novel PDE-Aligner performs physics-informed fine-tuning under flexible conditions. As far as we know, we first conduct 1D-2D-3D united pre-training on the PDEBench, and it sets not only new performance benchmarks for 1D, 2D, and 3D PDEs but also demonstrates exceptional adaptability to new physics via in-context and zero-shot learning approaches, which supports realistic engineering applications and foresight physics discovery.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates</title>
<link>https://arxiv.org/abs/2402.17390</link>
<guid>https://arxiv.org/abs/2402.17390</guid>
<content:encoded><![CDATA[
arXiv:2402.17390v2 Announce Type: replace 
Abstract: Machine-learning models demand periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly updated model may commit mistakes the previous model did not make. Such misclassifications are referred to as negative flips, experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, previously ineffective adversarial attacks on some inputs may become successful, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the samples for which no adversarial example was found before the update. We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators. Our experiments on robust models for computer vision confirm that both accuracy and robustness, even if improved after model update, can be affected by negative flips, and our robustness-congruent adversarial training can mitigate the problem, outperforming competing baseline methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers</title>
<link>https://arxiv.org/abs/2403.07404</link>
<guid>https://arxiv.org/abs/2403.07404</guid>
<content:encoded><![CDATA[
arXiv:2403.07404v4 Announce Type: replace 
Abstract: Continual learning is crucial for applying machine learning in challenging, dynamic, and often resource-constrained environments. However, catastrophic forgetting - overwriting previously learned knowledge when new information is acquired - remains a major challenge. In this work, we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting, highlighting their potential to accelerate computation. Motivated by these findings, we propose to use auxiliary classifiers(ACs) to enhance performance and demonstrate that integrating ACs into various continual learning methods consistently improves accuracy across diverse evaluation settings, yielding an average 10% relative gain. We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers. Our approach provides a scalable and efficient solution for continual learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2404.05019</link>
<guid>https://arxiv.org/abs/2404.05019</guid>
<content:encoded><![CDATA[
arXiv:2404.05019v3 Announce Type: replace 
Abstract: Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition</title>
<link>https://arxiv.org/abs/2404.08008</link>
<guid>https://arxiv.org/abs/2404.08008</guid>
<content:encoded><![CDATA[
arXiv:2404.08008v2 Announce Type: replace 
Abstract: Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive. Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition. Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses. Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating. We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation. Experimental results show that our sample-efficient evaluation method recovers "gold-standard" model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development. Code is available at https://github.com/weiji-Feng/MAD-Eval .
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model</title>
<link>https://arxiv.org/abs/2405.14457</link>
<guid>https://arxiv.org/abs/2405.14457</guid>
<content:encoded><![CDATA[
arXiv:2405.14457v3 Announce Type: replace 
Abstract: Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we focus on a threat model where the adversary has access only to the final model, with no visibility into intermediate updates. In the literature, this hidden state threat model exhibits a significant gap between the lower bound from empirical privacy auditing and the theoretical upper bound provided by privacy accounting. To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence designed to maximize the privacy loss of the final model without relying on intermediate updates. Our experiments show that this approach consistently outperforms previous attempts at auditing the hidden state model. Furthermore, our results advance the understanding of achievable privacy guarantees within this threat model. Specifically, when the crafted gradient is inserted at every optimization step, we show that concealing the intermediate model updates in DP-SGD does not enhance the privacy guarantees. The situation is more complex when the crafted gradient is not inserted at every step: our auditing lower bound matches the privacy upper bound only for an adversarially-chosen loss landscape and a sufficiently large batch size. This suggests that existing privacy upper bounds can be improved in certain regimes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-form Solutions: A New Perspective on Solving Differential Equations</title>
<link>https://arxiv.org/abs/2405.14620</link>
<guid>https://arxiv.org/abs/2405.14620</guid>
<content:encoded><![CDATA[
arXiv:2405.14620v3 Announce Type: replace 
Abstract: The quest for analytical solutions to differential equations has traditionally been constrained by the need for extensive mathematical expertise. Machine learning methods like genetic algorithms have shown promise in this domain, but are hindered by significant computational time and the complexity of their derived solutions. This paper introduces SSDE (Symbolic Solver for Differential Equations), a novel reinforcement learning-based approach that derives symbolic closed-form solutions for various differential equations. Evaluations across a diverse set of ordinary and partial differential equations demonstrate that SSDE outperforms existing machine learning methods, delivering superior accuracy and efficiency in obtaining analytical solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels</title>
<link>https://arxiv.org/abs/2405.17642</link>
<guid>https://arxiv.org/abs/2405.17642</guid>
<content:encoded><![CDATA[
arXiv:2405.17642v2 Announce Type: replace 
Abstract: The growing complexity of AI systems has intensified the need for transparency through Explainable AI (XAI). Counterfactual explanations (CFs) offer actionable "what-if" scenarios on three levels: Local CFs providing instance-specific insights, Global CFs addressing broader trends, and Group-wise CFs (GWCFs) striking a balance and revealing patterns within cohesive groups. Despite the availability of methods for each granularity level, the field lacks a unified method that integrates these complementary approaches. We address this limitation by proposing a gradient-based optimization method for differentiable models that generates Local, Global, and Group-wise Counterfactual Explanations in a unified manner. We especially enhance GWCF generation by combining instance grouping and counterfactual generation into a single efficient process, replacing traditional two-step methods. Moreover, to ensure trustworthiness, we innovatively introduce the integration of plausibility criteria into the GWCF domain, making explanations both valid and realistic. Our results demonstrate the method's effectiveness in balancing validity, proximity, and plausibility while optimizing group granularity, with practical utility validated through practical use cases.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proper Dataset Valuation by Pointwise Mutual Information</title>
<link>https://arxiv.org/abs/2405.18253</link>
<guid>https://arxiv.org/abs/2405.18253</guid>
<content:encoded><![CDATA[
arXiv:2405.18253v3 Announce Type: replace 
Abstract: Data plays a central role in advancements in modern artificial intelligence, with high-quality data emerging as a key driver of model performance. This has prompted the development of principled and effective data curation methods in recent years. However, existing methods largely rely on heuristics, and whether they are truly effective remains unclear. For instance, standard evaluation methods that assess a trained model's performance on specific benchmarks may incentivize assigning high scores to data that merely resembles the test set. This issue exemplifies Goodhart's law: when a measure becomes a target, it ceases to be a good measure. To address this issue, we propose an information-theoretic framework for evaluating data curation methods. We define dataset quality in terms of its informativeness about the true model parameters, formalized using the Blackwell ordering of informativeness. Under this ordering, Blackwell's theorem ensures that more informative data yields optimal models with lower expected loss on the true underlying distribution. To measure informativeness, we show that the Blackwell order can be determined by the Shannon mutual information between the curated data and the test data. To estimate this mutual information, we introduce a novel method that trains Bayesian models on embedded datasets and computes mutual information from the posteriors of model parameters. Experiments on real-world data demonstrate that our mutual information-based evaluation assigns appropriately lower scores to data curation strategies that reduce dataset informativeness, while traditional test score-based evaluation methods may favor data curation strategies that overfit to the test set but compromise the training data's informativeness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Diffusion: A Diffusion Based Model for Stochastic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2406.02827</link>
<guid>https://arxiv.org/abs/2406.02827</guid>
<content:encoded><![CDATA[
arXiv:2406.02827v2 Announce Type: replace 
Abstract: Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning</title>
<link>https://arxiv.org/abs/2406.09187</link>
<guid>https://arxiv.org/abs/2406.09187</guid>
<content:encoded><![CDATA[
arXiv:2406.09187v3 Announce Type: replace 
Abstract: The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data</title>
<link>https://arxiv.org/abs/2407.11500</link>
<guid>https://arxiv.org/abs/2407.11500</guid>
<content:encoded><![CDATA[
arXiv:2407.11500v2 Announce Type: replace 
Abstract: The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA) ordinal grading systems has been a subject of on-going debate and concern. Existing automated solutions are trained to emulate these imperfect systems, whilst also being reliant on large annotated databases for fully-supervised training. This work proposes a three stage approach for automated continuous grading of knee OA that is built upon the principles of Anomaly Detection (AD); learning a robust representation of healthy knee X-rays and grading disease severity based on its distance to the centre of normality. In the first stage, SS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal' representation, requiring only examples of healthy subjects and <3% of the labels that existing methods require. In the second stage, this model is used to pseudo label a subset of unlabelled data as 'normal' or 'anomalous', followed by denoising of pseudo labels with CLIP. The final stage involves retraining on labelled and pseudo labelled data using the proposed Dual Centre Representation Learning (DCRL) which learns the centres of two representation spaces; normal and anomalous. Disease severity is then graded based on the distance to the learned centres. The proposed methodology outperforms existing techniques by margins of up to 24% in terms of OA detection and the disease severity scores correlate with the Kellgren-Lawrence grading system at the same level as human expert performance. Code available at https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.12307</link>
<guid>https://arxiv.org/abs/2408.12307</guid>
<content:encoded><![CDATA[
arXiv:2408.12307v3 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep Everyone Happy: Online Fair Division of Numerous Items with Few Copies</title>
<link>https://arxiv.org/abs/2408.12845</link>
<guid>https://arxiv.org/abs/2408.12845</guid>
<content:encoded><![CDATA[
arXiv:2408.12845v2 Announce Type: replace 
Abstract: This paper considers a novel variant of the online fair division problem involving multiple agents in which a learner sequentially observes an indivisible item that has to be irrevocably allocated to one of the agents while satisfying a fairness and efficiency constraint. Existing algorithms assume a small number of items with a sufficiently large number of copies, which ensures a good utility estimation for all item-agent pairs from noisy bandit feedback. However, this assumption may not hold in many real-life applications, for example, an online platform that has a large number of users (items) who use the platform's service providers (agents) only a few times (a few copies of items), which makes it difficult to accurately estimate utilities for all item-agent pairs. To address this, we assume utility is an unknown function of item-agent features. We then propose algorithms that model online fair division as a contextual bandit problem, with sub-linear regret guarantees. Our experimental results further validate the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2408.14774</link>
<guid>https://arxiv.org/abs/2408.14774</guid>
<content:encoded><![CDATA[
arXiv:2408.14774v4 Announce Type: replace 
Abstract: We introduce Instruct-SkillMix, an automated approach for creating diverse, high quality SFT data for instruction-following. The pipeline involves two stages, each leveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to extract core "skills" for instruction-following by directly prompting the model. This is inspired by ``LLM metacognition'' of Didolkar et al. (2024); (2) Data generation: uses the powerful LLM to generate (instruction, response) data that exhibit a randomly chosen pair of these skills. Here, the use of random skill combinations promotes diversity and difficulty. The estimated cost of creating the dataset is under $600.
  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from Instruct-SkillMix leads to strong gains on instruction following benchmarks such as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples, LLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0, a level similar to frontier models like Claude 3 Opus and LLaMA-3.1-405B-Instruct. Ablation studies also suggest plausible reasons for why creating open instruction-tuning datasets via naive crowd-sourcing has proved difficult. In our dataset, adding 20% low quality answers (``shirkers'') causes a noticeable degradation in performance. The Instruct-SkillMix pipeline seems flexible and adaptable to other settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Data, My Model: Learning Who Really Helps in Federated Learning</title>
<link>https://arxiv.org/abs/2409.02064</link>
<guid>https://arxiv.org/abs/2409.02064</guid>
<content:encoded><![CDATA[
arXiv:2409.02064v3 Announce Type: replace 
Abstract: Many important machine learning applications involve networks of devices-such as wearables or smartphones-that generate local data and train personalized models. A key challenge is determining which peers are most beneficial for collaboration. We propose a simple and privacy-preserving method to select relevant collaborators by evaluating how much a model improves after a single gradient step using another devices data-without sharing raw data. This method naturally extends to non-parametric models by replacing the gradient step with a non-parametric generalization. Our approach enables model-agnostic, data-driven peer selection for personalized federated learning (PersFL).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Filtration for RLHF to Mitigate Noise in Reward Models</title>
<link>https://arxiv.org/abs/2409.06957</link>
<guid>https://arxiv.org/abs/2409.06957</guid>
<content:encoded><![CDATA[
arXiv:2409.06957v4 Announce Type: replace 
Abstract: While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (R2) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/DtYXs/verl/tree/pf-ppo.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists</title>
<link>https://arxiv.org/abs/2409.13931</link>
<guid>https://arxiv.org/abs/2409.13931</guid>
<content:encoded><![CDATA[
arXiv:2409.13931v4 Announce Type: replace 
Abstract: On-device LLMs have gained increasing attention for their ability to enhance privacy and provide a personalized user experience. To facilitate private learning with scarce data, Federated Learning has become a standard approach. However, it faces challenges such as computational resource heterogeneity and data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and $\textbf{S}$pecialists), the first approach to address both challenges. A key innovation of our method is the bi-level optimization formulation of the Mixture-of-Experts learning objective, where the router is optimized using a separate validation set to ensure alignment with the target distribution. We solve our objective with alternating minimization, for which we provide a theoretical analysis. Our method shares generalist experts across users while localizing a varying number of specialist experts, thereby adapting to users' computational resources and preserving privacy. Through extensive experiments, we show CoMiGS effectively balances general and personalized knowledge for each token generation. We demonstrate that CoMiGS remains robust against overfitting-due to the generalists' regularizing effect-while adapting to local data through specialist expertise. We open source our codebase for collaborative LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Protocols for Continual Learning via Statistical Physics and Control Theory</title>
<link>https://arxiv.org/abs/2409.18061</link>
<guid>https://arxiv.org/abs/2409.18061</guid>
<content:encoded><![CDATA[
arXiv:2409.18061v3 Announce Type: replace 
Abstract: Artificial neural networks often struggle with catastrophic forgetting when learning multiple tasks sequentially, as training on new tasks degrades the performance on previously learned tasks. Recent theoretical work has addressed this issue by analysing learning curves in synthetic frameworks under predefined training protocols. However, these protocols relied on heuristics and lacked a solid theoretical foundation assessing their optimality. In this paper, we fill this gap by combining exact equations for training dynamics, derived using statistical physics techniques, with optimal control methods. We apply this approach to teacher-student models for continual learning and multi-task problems, obtaining a theory for task-selection protocols maximising performance while minimising forgetting. Our theoretical analysis offers non-trivial yet interpretable strategies for mitigating catastrophic forgetting, shedding light on how optimal learning protocols modulate established effects, such as the influence of task similarity on forgetting. Finally, we validate our theoretical findings with experiments on real-world data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.02681</link>
<guid>https://arxiv.org/abs/2410.02681</guid>
<content:encoded><![CDATA[
arXiv:2410.02681v4 Announce Type: replace 
Abstract: Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes: the cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy. Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. In particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes. Extensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</title>
<link>https://arxiv.org/abs/2410.03348</link>
<guid>https://arxiv.org/abs/2410.03348</guid>
<content:encoded><![CDATA[
arXiv:2410.03348v4 Announce Type: replace 
Abstract: Neurosymbolic learning enables the integration of symbolic reasoning with deep learning but faces significant challenges in scaling to complex symbolic programs, large datasets, or both. We introduce DOLPHIN, a framework that tackles these challenges by supporting neurosymbolic programs in Python, executing complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU. Across 13 benchmarks spanning tasks over text, image, and video data, with symbolic reasoning features like recursion and black-box functions, DOLPHIN converges to state-of-the-art accuracies on the more complex benchmarks while existing frameworks such as Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On simpler benchmarks, DOLPHIN matches their performance, while achieving these results 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency and convergence on difficult benchmarks where existing frameworks struggle. The code is published at https://github.com/Dolphin-NeSy/Dolphin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2410.03972</link>
<guid>https://arxiv.org/abs/2410.03972</guid>
<content:encoded><![CDATA[
arXiv:2410.03972v2 Announce Type: replace 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions-a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks-flip-flop memory, sine wave generation, delayed discrimination, and path integration-while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers aiming to tailor RNN solutions-whether to uncover shared neural mechanisms or to model individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints</title>
<link>https://arxiv.org/abs/2410.05612</link>
<guid>https://arxiv.org/abs/2410.05612</guid>
<content:encoded><![CDATA[
arXiv:2410.05612v2 Announce Type: replace 
Abstract: Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers. These models are first pretrained on vast and diverse datasets and then adapted to specific downstream tasks, often with significantly less data. However, the mechanisms behind the success of this ubiquitous pretrain-then-adapt paradigm remain underexplored, particularly the characteristics of pretraining checkpoints that enhance downstream adaptation. We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task. We demonstrate that this Bayesian model selection criterion can be effectively implemented without access to the downstream data or prior knowledge of the downstream task. Furthermore, we provide empirical evidence that the criterion reliably correlates with improved finetuning performance, offering a principled approach to predicting model adaptability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression</title>
<link>https://arxiv.org/abs/2410.09615</link>
<guid>https://arxiv.org/abs/2410.09615</guid>
<content:encoded><![CDATA[
arXiv:2410.09615v3 Announce Type: replace 
Abstract: Conventional model compression techniques for LLMs address high memory consumption and slow inference challenges but typically require computationally expensive retraining to preserve accuracy. In contrast, one-shot compression methods eliminate retraining cost, but struggle to achieve accuracy comparable to dense models. This paper presents SLIM, a new one-shot compression framework that holistically integrates hardware-friendly quantization, sparsity, and low-rank approximation into a unified process. First, we formulate the quantization process using a probabilistic approach (SLIM-Quant) that enables us to apply uniform quantization. Then, we use an existing one-shot pruning method to apply semi-structured sparsity on top of the quantized weights. Finally, to compensate for the introduced aggregated quantization and sparsity error, we use a novel saliency function with unique invertible and additive features that enables us to mathematically compute the value of low-rank adapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4 sparsity with 4-bit weight quantization, outperforming prior methods. Models compressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100 GPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory reduction in comparison to their dense counterparts. We also propose an optional PEFT recipe that further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM without fine-tuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.09754</link>
<guid>https://arxiv.org/abs/2410.09754</guid>
<content:encoded><![CDATA[
arXiv:2410.09754v2 Announce Type: replace 
Abstract: Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2410.11778</link>
<guid>https://arxiv.org/abs/2410.11778</guid>
<content:encoded><![CDATA[
arXiv:2410.11778v3 Announce Type: replace 
Abstract: Although transformers have demonstrated impressive capabilities for in-context learning (ICL) in practice, theoretical understanding of the underlying mechanism that allows transformers to perform ICL is still in its infancy. This work aims to theoretically study the training dynamics of transformers for in-context classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain assumptions, a single-layer transformer trained via gradient descent converges to a globally optimal model at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL inference error of the trained transformer. We show that when the lengths of training and testing prompts are sufficiently large, the prediction of the trained transformer approaches the ground truth distribution of the labels. Experimental results corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2410.13974</link>
<guid>https://arxiv.org/abs/2410.13974</guid>
<content:encoded><![CDATA[
arXiv:2410.13974v2 Announce Type: replace 
Abstract: Graph Prompt Learning (GPL) has been introduced as a promising approach that uses prompts to adapt pre-trained GNN models to specific downstream tasks without requiring fine-tuning of the entire model. Despite the advantages of GPL, little attention has been given to its vulnerability to backdoor attacks, where an adversary can manipulate the model's behavior by embedding hidden triggers. Existing graph backdoor attacks rely on modifying model parameters during training, but this approach is impractical in GPL as GNN encoder parameters are frozen after pre-training. Moreover, downstream users may fine-tune their own task models on clean datasets, further complicating the attack. In this paper, we propose TGPA, a backdoor attack framework designed specifically for GPL. TGPA injects backdoors into graph prompts without modifying pre-trained GNN encoders and ensures high attack success rates and clean accuracy. To address the challenge of model fine-tuning by users, we introduce a finetuning-resistant poisoning approach that maintains the effectiveness of the backdoor even after downstream model adjustments. Extensive experiments on multiple datasets under various settings demonstrate the effectiveness of TGPA in compromising GPL models with fixed GNN encoders.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures</title>
<link>https://arxiv.org/abs/2410.14667</link>
<guid>https://arxiv.org/abs/2410.14667</guid>
<content:encoded><![CDATA[
arXiv:2410.14667v2 Announce Type: replace 
Abstract: Inverse problems aim to reconstruct unseen data from corrupted or perturbed measurements. While most work focuses on improving reconstruction quality, generalization accuracy and robustness are equally important, especially for safety-critical applications. Model-based architectures (MBAs), such as loop unrolling methods, are considered more interpretable and achieve better reconstructions. Empirical evidence suggests that MBAs are more robust to perturbations than black-box solvers, but the accuracy-robustness tradeoff in MBAs remains underexplored. In this work, we propose a simple yet effective training scheme for MBAs, called SGD jittering, which injects noise iteration-wise during reconstruction. We theoretically demonstrate that SGD jittering not only generalizes better than the standard mean squared error training but is also more robust to average-case attacks. We validate SGD jittering using denoising toy examples, seismic deconvolution, and single-coil MRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner reconstructions for out-of-distribution data and demonstrates enhanced robustness against adversarial attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphNarrator: Generating Textual Explanations for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2410.15268</link>
<guid>https://arxiv.org/abs/2410.15268</guid>
<content:encoded><![CDATA[
arXiv:2410.15268v2 Announce Type: replace 
Abstract: Graph representation learning has garnered significant attention due to its broad applications in various domains, such as recommendation systems and social network analysis. Despite advancements in graph learning methods, challenges still remain in explainability when graphs are associated with semantic features. In this paper, we present GraphNarrator, the first method designed to generate natural language explanations for Graph Neural Networks. GraphNarrator employs a generative language model that maps input-output pairs to explanations reflecting the model's decision-making process. To address the lack of ground truth explanations to train the model, we propose first generating pseudo-labels that capture the model's decisions from saliency-based explanations, then using Expert Iteration to iteratively train the pseudo-label generator based on training objectives on explanation quality. The high-quality pseudo-labels are finally utilized to train an end-to-end explanation generator model. Extensive experiments are conducted to demonstrate the effectiveness of GraphNarrator in producing faithful, concise, and human-preferred natural language explanations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces</title>
<link>https://arxiv.org/abs/2410.15625</link>
<guid>https://arxiv.org/abs/2410.15625</guid>
<content:encoded><![CDATA[
arXiv:2410.15625v3 Announce Type: replace 
Abstract: Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing</title>
<link>https://arxiv.org/abs/2410.17194</link>
<guid>https://arxiv.org/abs/2410.17194</guid>
<content:encoded><![CDATA[
arXiv:2410.17194v4 Announce Type: replace 
Abstract: Knowledge Editing (KE) algorithms alter models' weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. However, recent work has shown that applying KE can adversely affect models' broader factual recall accuracy and diminish their reasoning abilities. Although these studies give insights into the potential harms of KE algorithms, e.g., performance evaluations on benchmarks, little is understood about why such destructive failures occur. Motivated by this, we define a novel synthetic task in which a Transformer is trained from scratch to internalize a "structured" knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has "trickling effects" on other entities (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models on this task, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it degrades models' factual recall and reasoning performance. We further corroborate our findings in naturalistic settings with pre-trained Llama and Mamba models as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model abilities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Linear Bandits with Blocking Constraints</title>
<link>https://arxiv.org/abs/2410.20041</link>
<guid>https://arxiv.org/abs/2410.20041</guid>
<content:encoded><![CDATA[
arXiv:2410.20041v2 Announce Type: replace 
Abstract: We investigate the high-dimensional sparse linear bandits problem in a data-poor regime where the time horizon is much smaller than the ambient dimension and number of arms. We study the setting under the additional blocking constraint where each unique arm can be pulled only once. The blocking constraint is motivated by practical applications in personalized content recommendation and identification of data points to improve annotation efficiency for complex learning tasks. With mild assumptions on the arms, our proposed online algorithm (BSLB) achieves a regret guarantee of $\widetilde{\mathsf{O}}((1+\beta_k)^2k^{\frac{2}{3}} \mathsf{T}^{\frac{2}{3}})$ where the parameter vector has an (unknown) relative tail $\beta_k$ -- the ratio of $\ell_1$ norm of the top-$k$ and remaining entries of the parameter vector. To this end, we show novel offline statistical guarantees of the lasso estimator for the linear model that is robust to the sparsity modeling assumption. Finally, we propose a meta-algorithm (C-BSLB) based on corralling that does not need knowledge of optimal sparsity parameter $k$ at minimal cost to regret. Our experiments on multiple real-world datasets demonstrate the validity of our algorithms and theoretical framework.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GWQ: Gradient-Aware Weight Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2411.00850</link>
<guid>https://arxiv.org/abs/2411.00850</guid>
<content:encoded><![CDATA[
arXiv:2411.00850v4 Announce Type: replace 
Abstract: Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters presents significant challenges for the deployment. So, compressing LLMs to low bits can enable to deploy on resource-constrained devices. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the top 1\% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ on different task include language modeling, grounding detection, massive multitask language understanding and vision-language question and answering. Results show that models quantified by GWQ performs better than other quantization method. During quantization process, GWQ only need one calibration set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Sampling Correction via Approximately 10 Parameters</title>
<link>https://arxiv.org/abs/2411.06503</link>
<guid>https://arxiv.org/abs/2411.06503</guid>
<content:encoded><![CDATA[
arXiv:2411.06503v3 Announce Type: replace 
Abstract: While powerful for generation, Diffusion Probabilistic Models (DPMs) face slow sampling challenges, for which various distillation-based methods have been proposed. However, they typically require significant additional training costs and model parameter storage, limiting their practicality. In this work, we propose PCA-based Adaptive Search (PAS), which optimizes existing solvers for DPMs with minimal additional costs. Specifically, we first employ PCA to obtain a few basis vectors to span the high-dimensional sampling space, which enables us to learn just a set of coordinates to correct the sampling direction; furthermore, based on the observation that the cumulative truncation error exhibits an ``S"-shape, we design an adaptive search strategy that further enhances the sampling efficiency and reduces the number of stored parameters to approximately 10. Extensive experiments demonstrate that PAS can significantly enhance existing fast solvers in a plug-and-play manner with negligible costs. E.g., on CIFAR10, PAS optimizes DDIM's FID from 15.69 to 4.37 (NFE=10) using only 12 parameters and sub-minute training on a single A100 GPU. Code is available at https://github.com/onefly123/PAS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers</title>
<link>https://arxiv.org/abs/2411.17296</link>
<guid>https://arxiv.org/abs/2411.17296</guid>
<content:encoded><![CDATA[
arXiv:2411.17296v3 Announce Type: replace 
Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs). However, self--attention, the core module of GTs, preserves only low-frequency signals in graph features, leading to ineffectiveness in capturing other important signals like high-frequency ones. Some recent GT models help alleviate this issue, but their flexibility and expressiveness are still limited since the filters they learn are fixed on predefined graph spectrum or spectral order. To tackle this challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer (GrokFormer), a novel GT model that learns highly expressive spectral filters with adaptive graph spectrum and spectral order through a Fourier series modeling over learnable activation functions. We demonstrate theoretically and empirically that the proposed GrokFormer filter offers better expressiveness than other spectral methods. Comprehensive experiments on 10 real-world node classification datasets across various domains, scales, and graph properties, as well as 5 graph classification datasets, show that GrokFormer outperforms state-of-the-art GTs and GNNs. Our code is available at https://github.com/GGA23/GrokFormer
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot Navigation in Digital Twins</title>
<link>https://arxiv.org/abs/2411.18212</link>
<guid>https://arxiv.org/abs/2411.18212</guid>
<content:encoded><![CDATA[
arXiv:2411.18212v2 Announce Type: replace 
Abstract: Path planning under wireless performance constraints is a complex challenge in robot navigation. However, naively incorporating such constraints into classical planning algorithms often incurs prohibitive search costs. In this paper, we propose SCoTT, a wireless-aware path planning framework that leverages vision-language models (VLMs) to co-optimize average path gains and trajectory length using wireless heatmap images and ray-tracing data from a digital twin (DT). At the core of our framework is Strategic Chain-of-Thought Tasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive search problem into structured subtasks, each solved via chain-of-thought prompting. To establish strong baselines, we compare classical A* and wireless-aware extensions of it, and derive DP-WA*, an optimal, iterative dynamic programming algorithm that incorporates all path gains and distance metrics from the DT, but at significant computational cost. In extensive experiments, we show that SCoTT achieves path gains within 2% of DP-WA* while consistently generating shorter trajectories. Moreover, SCoTT's intermediate outputs can be used to accelerate DP-WA* by reducing its search space, saving up to 62% in execution time. We validate our framework using four VLMs, demonstrating effectiveness across both large and small models, thus making it applicable to a wide range of compact models at low inference cost. We also show the practical viability of our approach by deploying SCoTT as a ROS node within Gazebo simulations. Finally, we discuss data acquisition pipelines, compute requirements, and deployment considerations for VLMs in 6G-enabled DTs, underscoring the potential of natural language interfaces for wireless-aware navigation in real-world applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs</title>
<link>https://arxiv.org/abs/2412.00315</link>
<guid>https://arxiv.org/abs/2412.00315</guid>
<content:encoded><![CDATA[
arXiv:2412.00315v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool to capture intricate network patterns, achieving success across different domains. However, existing GNNs require careful domain-specific architecture designs and training from scratch on each dataset, leading to an expertise-intensive process with difficulty in generalizing across graphs from different domains. Therefore, it can be hard for practitioners to infer which GNN model can generalize well to graphs from their domains. To address this challenge, we propose a novel cross-domain pretraining framework, "one model for one graph," which overcomes the limitations of previous approaches that failed to use a single GNN to capture diverse graph patterns across domains with significant gaps. Specifically, we pretrain a bank of expert models, with each one corresponding to a specific dataset. When inferring to a new graph, gating functions choose a subset of experts to effectively integrate prior model knowledge while avoiding negative transfer. Extensive experiments consistently demonstrate the superiority of our proposed method on both link prediction and node classification tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Rising Bandit</title>
<link>https://arxiv.org/abs/2412.00798</link>
<guid>https://arxiv.org/abs/2412.00798</guid>
<content:encoded><![CDATA[
arXiv:2412.00798v3 Announce Type: replace 
Abstract: Combinatorial online learning is a fundamental task for selecting the optimal action (or super arm) as a combination of base arms in sequential interactions with systems providing stochastic rewards. It is applicable to diverse domains such as robotics, social advertising, network routing, and recommendation systems. In many real-world scenarios, we often encounter rising rewards, where playing a base arm not only provides an instantaneous reward but also contributes to the enhancement of future rewards, e.g., robots enhancing proficiency through practice and social influence strengthening in the history of successful recommendations. Moreover, the enhancement of a single base arm may affect multiple super arms that include it, introducing complex dependencies that are not captured by existing rising bandit models. To address this, we introduce the Combinatorial Rising Bandit (CRB) framework and propose a provably efficient algorithm, Combinatorial Rising Upper Confidence Bound (CRUCB). We establish an upper bound on regret CRUCB and show that it is nearly tight by deriving a matching lower bound. In addition, we empirically demonstrate the effectiveness of CRUCB not only in synthetic environments but also in realistic applications of deep reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INRFlow: Flow Matching for INRs in Ambient Space</title>
<link>https://arxiv.org/abs/2412.03791</link>
<guid>https://arxiv.org/abs/2412.03791</guid>
<content:encoded><![CDATA[
arXiv:2412.03791v2 Announce Type: replace 
Abstract: Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on irregular or unstructured data like 3D point clouds or even protein structures. These models are commonly trained in two stages: first, a data compressor is trained, and in a subsequent training stage a flow matching generative model is trained in the latent space of the data compressor. This two-stage paradigm sets obstacles for unifying models across data domains, as hand-crafted compressors architectures are used for different data modalities. To this end, we introduce INRFlow, a domain-agnostic approach to learn flow matching transformers directly in ambient space. Drawing inspiration from INRs, we introduce a conditionally independent point-wise training objective that enables INRFlow to make predictions continuously in coordinate space. Our empirical results demonstrate that INRFlow effectively handles different data modalities such as images, 3D point clouds and protein structure data, achieving strong performance in different domains and outperforming comparable approaches. INRFlow is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition</title>
<link>https://arxiv.org/abs/2412.17333</link>
<guid>https://arxiv.org/abs/2412.17333</guid>
<content:encoded><![CDATA[
arXiv:2412.17333v2 Announce Type: replace 
Abstract: Shock waves caused by earthquakes can be devastating. Generating realistic earthquake-caused ground motion waveforms help reducing losses in lives and properties, yet generative models for the task tend to generate subpar waveforms. We present High-fidelity Earthquake Groundmotion Generation System (HEGGS) and demonstrate its superior performance using earthquakes from North American, East Asian, and European regions. HEGGS exploits the intrinsic characteristics of earthquake dataset and learns the waveforms using an end-to-end differentiable generator containing conditional latent diffusion model and hi-fidelity waveform construction model. We show the learning efficiency of HEGGS by training it on a single GPU machine and validate its performance using earthquake databases from North America, East Asia, and Europe, using diverse criteria from waveform generation tasks and seismology. Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE analysis, frequency content analysis, and section plot analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms</title>
<link>https://arxiv.org/abs/2412.18202</link>
<guid>https://arxiv.org/abs/2412.18202</guid>
<content:encoded><![CDATA[
arXiv:2412.18202v5 Announce Type: replace 
Abstract: This paper leverages machine learning algorithms to forecast and analyze financial time series. The process begins with a denoising autoencoder to filter out random noise fluctuations from the main contract price data. Then, one-dimensional convolution reduces the dimensionality of the filtered data and extracts key information. The filtered and dimensionality-reduced price data is fed into a GANs network, and its output serve as input of a fully connected network. Through cross-validation, a model is trained to capture features that precede large price fluctuations. The model predicts the likelihood and direction of significant price changes in real-time price sequences, placing trades at moments of high prediction accuracy. Empirical results demonstrate that using autoencoders and convolution to filter and denoise financial data, combined with GANs, achieves a certain level of predictive performance, validating the capabilities of machine learning algorithms to discover underlying patterns in financial sequences. Keywords - CNN;GANs; Cryptocurrency; Prediction.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2501.09620</link>
<guid>https://arxiv.org/abs/2501.09620</guid>
<content:encoded><![CDATA[
arXiv:2501.09620v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination-that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causality to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credit Risk Identification in Supply Chains Using Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2501.10348</link>
<guid>https://arxiv.org/abs/2501.10348</guid>
<content:encoded><![CDATA[
arXiv:2501.10348v4 Announce Type: replace 
Abstract: Credit risk management within supply chains has emerged as a critical research area due to its significant implications for operational stability and financial sustainability. The intricate interdependencies among supply chain participants mean that credit risks can propagate across networks, with impacts varying by industry. This study explores the application of Generative Adversarial Networks (GANs) to enhance credit risk identification in supply chains. GANs enable the generation of synthetic credit risk scenarios, addressing challenges related to data scarcity and imbalanced datasets. By leveraging GAN-generated data, the model improves predictive accuracy while effectively capturing dynamic and temporal dependencies in supply chain data. The research focuses on three representative industries-manufacturing (steel), distribution (pharmaceuticals), and services (e-commerce) to assess industry-specific credit risk contagion. Experimental results demonstrate that the GAN-based model outperforms traditional methods, including logistic regression, decision trees, and neural networks, achieving superior accuracy, recall, and F1 scores. The findings underscore the potential of GANs in proactive risk management, offering robust tools for mitigating financial disruptions in supply chains. Future research could expand the model by incorporating external market factors and supplier relationships to further enhance predictive capabilities. Keywords- Generative Adversarial Networks (GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data Augmentation
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Granger Causality Learning for Interdependent Clients with State Space Representation</title>
<link>https://arxiv.org/abs/2501.13890</link>
<guid>https://arxiv.org/abs/2501.13890</guid>
<content:encoded><![CDATA[
arXiv:2501.13890v4 Announce Type: replace 
Abstract: Advanced sensors and IoT devices have improved the monitoring and control of complex industrial enterprises. They have also created an interdependent fabric of geographically distributed process operations (clients) across these enterprises. Granger causality is an effective approach to detect and quantify interdependencies by examining how one client's state affects others over time. Understanding these interdependencies captures how localized events, such as faults and disruptions, can propagate throughout the system, possibly causing widespread operational impacts. However, the large volume and complexity of industrial data pose challenges in modeling these interdependencies. This paper develops a federated approach to learning Granger causality. We utilize a linear state space system framework that leverages low-dimensional state estimates to analyze interdependencies. This addresses bandwidth limitations and the computational burden commonly associated with centralized data processing. We propose augmenting the client models with the Granger causality information learned by the server through a Machine Learning (ML) function. We examine the co-dependence between the augmented client and server models and reformulate the framework as a standalone ML algorithm providing conditions for its sublinear and linear convergence rates. We also study the convergence of the framework to a centralized oracle model. Moreover, we include a differential privacy analysis to ensure data security while preserving causal insights. Using synthetic data, we conduct comprehensive experiments to demonstrate the robustness of our approach to perturbations in causality, the scalability to the size of communication, number of clients, and the dimensions of raw data. We also evaluate the performance on two real-world industrial control system datasets by reporting the volume of data saved by decentralization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2501.18887</link>
<guid>https://arxiv.org/abs/2501.18887</guid>
<content:encoded><![CDATA[
arXiv:2501.18887v3 Announce Type: replace 
Abstract: The increasing complexity of AI systems has made understanding their behavior critical. Numerous interpretability methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components, which emerged from explainable AI, data-centric AI, and mechanistic interpretability, respectively. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of methods and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and a unified view of them benefits both interpretability and broader AI research. To this end, we first analyze popular methods for these three types of attributions and present a unified view demonstrating that these seemingly distinct methods employ similar techniques (such as perturbations, gradients, and linear approximations) over different aspects and thus differ primarily in their perspectives rather than techniques. Then, we demonstrate how this unified view enhances understanding of existing attribution methods, highlights shared concepts and evaluation criteria among these methods, and leads to new research directions both in interpretability research, by addressing common challenges and facilitating cross-attribution innovation, and in AI more broadly, with applications in model editing, steering, and regulation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Norm-Bounded Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2501.19050</link>
<guid>https://arxiv.org/abs/2501.19050</guid>
<content:encoded><![CDATA[
arXiv:2501.19050v3 Announce Type: replace 
Abstract: In this work, we propose norm-bounded low-rank adaptation (NB-LoRA) for parameter-efficient fine tuning. NB-LoRA is a novel parameterization of low-rank weight adaptations that admits explicit bounds on each singular value of the adaptation matrix, which can thereby satisfy any prescribed unitarily invariant norm bound, including the Schatten norms (e.g., nuclear, Frobenius, spectral norm). The proposed parameterization is unconstrained, smooth, and complete, i.e. it covers all matrices satisfying the prescribed rank and singular-value bounds. Comparative experiments on large language models show that NB-LoRA achieves superior adaptation performance and faster training over a range of models, tasks and ranks. Vision fine-tuning experiments show that NB-LoRA can achieve strong adaptation performance while avoiding model catastrophic forgetting, and compared to existing approaches it is substantially more robust to a hyper-parameters such as including adaptation rank, learning rate and number of training epochs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Learning for Combinatorial Multi-armed Bandits</title>
<link>https://arxiv.org/abs/2501.19300</link>
<guid>https://arxiv.org/abs/2501.19300</guid>
<content:encoded><![CDATA[
arXiv:2501.19300v2 Announce Type: replace 
Abstract: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion</title>
<link>https://arxiv.org/abs/2502.00264</link>
<guid>https://arxiv.org/abs/2502.00264</guid>
<content:encoded><![CDATA[
arXiv:2502.00264v2 Announce Type: replace 
Abstract: Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network</title>
<link>https://arxiv.org/abs/2502.00288</link>
<guid>https://arxiv.org/abs/2502.00288</guid>
<content:encoded><![CDATA[
arXiv:2502.00288v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) for continuous control often requires large amounts of online interaction data. Value-based RL methods can mitigate this burden by offering relatively high sample efficiency. Some studies further enhance sample efficiency by incorporating offline demonstration data to "kick-start" training, achieving promising results in continuous control. However, they typically compute the Q-function independently for each action dimension, neglecting interdependencies and making it harder to identify optimal actions when learning from suboptimal data, such as non-expert demonstration and online-collected data during the training process. To address these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner. First, ARSQ decomposes the continuous action space into discrete spaces in a coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained continuous control tasks. Next, it auto-regressively predicts dimensional action advantages within each decision step, enabling more effective decision-making in continuous control tasks. We evaluate ARSQ on two continuous control benchmarks, RLBench and D4RL, integrating demonstration data into online training. On D4RL, which includes non-expert demonstrations, ARSQ achieves an average $1.62\times$ performance improvement over SOTA value-based baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses various baselines, demonstrating its effectiveness in learning from suboptimal online-collected data. Project page is at https://sites.google.com/view/ar-soft-q
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn Singularly Perturbed Solutions via Homotopy Dynamics</title>
<link>https://arxiv.org/abs/2502.00488</link>
<guid>https://arxiv.org/abs/2502.00488</guid>
<content:encoded><![CDATA[
arXiv:2502.00488v3 Announce Type: replace 
Abstract: Solving partial differential equations (PDEs) using neural networks has become a central focus in scientific machine learning. Training neural networks for singularly perturbed problems is particularly challenging due to certain parameters in the PDEs that introduce near-singularities in the loss function. In this study, we overcome this challenge by introducing a novel method based on homotopy dynamics to effectively manipulate these parameters. From a theoretical perspective, we analyze the effects of these parameters on training difficulty in these singularly perturbed problems and establish the convergence of the proposed homotopy dynamics method. Experimentally, we demonstrate that our approach significantly accelerates convergence and improves the accuracy of these singularly perturbed problems. These findings present an efficient optimization strategy leveraging homotopy dynamics, offering a robust framework to extend the applicability of neural networks for solving singularly perturbed differential equations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers</title>
<link>https://arxiv.org/abs/2502.01310</link>
<guid>https://arxiv.org/abs/2502.01310</guid>
<content:encoded><![CDATA[
arXiv:2502.01310v2 Announce Type: replace 
Abstract: Neural network-based optimal transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing OT approaches, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural nets). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for general OT case, paving the promising direction for future research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection</title>
<link>https://arxiv.org/abs/2502.01699</link>
<guid>https://arxiv.org/abs/2502.01699</guid>
<content:encoded><![CDATA[
arXiv:2502.01699v2 Announce Type: replace 
Abstract: Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.02410</link>
<guid>https://arxiv.org/abs/2502.02410</guid>
<content:encoded><![CDATA[
arXiv:2502.02410v2 Announce Type: replace 
Abstract: Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time-series-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Exploration for Multi-Reward Multi-Policy Evaluation</title>
<link>https://arxiv.org/abs/2502.02516</link>
<guid>https://arxiv.org/abs/2502.02516</guid>
<content:encoded><![CDATA[
arXiv:2502.02516v2 Announce Type: replace 
Abstract: We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions must be evaluated simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC perspective to achieve $\epsilon$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of this adaptive exploration scheme.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRAX: Medical Reasoning Agent for Chest X-ray</title>
<link>https://arxiv.org/abs/2502.02673</link>
<guid>https://arxiv.org/abs/2502.02673</guid>
<content:encoded><![CDATA[
arXiv:2502.02673v2 Announce Type: replace 
Abstract: Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.04778</link>
<guid>https://arxiv.org/abs/2502.04778</guid>
<content:encoded><![CDATA[
arXiv:2502.04778v2 Announce Type: replace 
Abstract: Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning (RL) to manage the risk of hazardous exploitation of unseen actions. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling</title>
<link>https://arxiv.org/abs/2502.05743</link>
<guid>https://arxiv.org/abs/2502.05743</guid>
<content:encoded><![CDATA[
arXiv:2502.05743v2 Announce Type: replace 
Abstract: Diffusion models, though originally designed for generative tasks, have demonstrated impressive self-supervised representation learning capabilities. A particularly intriguing phenomenon in these models is the emergence of unimodal representation dynamics, where the quality of learned features peaks at an intermediate noise level. In this work, we conduct a comprehensive theoretical and empirical investigation of this phenomenon. Leveraging the inherent low-dimensionality structure of image data, we theoretically demonstrate that the unimodal dynamic emerges when the diffusion model successfully captures the underlying data distribution. The unimodality arises from an interplay between denoising strength and class confidence across noise scales. Empirically, we further show that, in classification tasks, the presence of unimodal dynamics reliably indicates generalization: it emerges when the model generalizes and gradually transitions to a monotonically decreasing curve as the model begins to memorize the training data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Sequence Preconditioning</title>
<link>https://arxiv.org/abs/2502.06545</link>
<guid>https://arxiv.org/abs/2502.06545</guid>
<content:encoded><![CDATA[
arXiv:2502.06545v2 Announce Type: replace 
Abstract: We study the problem of preconditioning in the setting of sequential prediction. From the theoretical lens of linear dynamical systems, we show that applying a convolution to the input sequence translates to applying a polynomial to the unknown transition matrix in the hidden space. With this insight, we develop a novel preconditioning method that convolves the input sequence with the coefficients of the Chebyshev or Legendre polynomials. We formally prove that this improves the regret of two distinct prediction methods. Moreover, using this preconditioning technique on either method gives the first sublinear regret bounds that are also hidden dimension independent (up to logarithmic factors) even when the hidden transition matrix is asymmetric. From rigorous experiments on synthetic data we show that our simple preconditioning method generalizes to both 1) settings where the data is not from a linear dynamical system and 2) a broad range of learning algorithms, including recurrent neural networks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Dropout on Performance in Long-range Graph Learning Tasks</title>
<link>https://arxiv.org/abs/2502.07364</link>
<guid>https://arxiv.org/abs/2502.07364</guid>
<content:encoded><![CDATA[
arXiv:2502.07364v2 Announce Type: replace 
Abstract: Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks (GNNs) that propagate information across the graph via local neighborhoods. The scheme gives rise to two key challenges: over-smoothing and over-squashing. While several Dropout-style algorithms, such as DropEdge and DropMessage, have successfully addressed over-smoothing, their impact on over-squashing remains largely unexplored. This represents a critical gap in the literature, as failure to mitigate over-squashing would make these methods unsuitable for long-range tasks -- the intended use case of deep MPNNs. In this work, we study the aforementioned algorithms, and closely related edge-dropping algorithms -- DropNode, DropAgg and DropGNN -- in the context of over-squashing. We present theoretical results showing that DropEdge-variants reduce sensitivity between distant nodes, limiting their suitability for long-range tasks. To address this, we introduce DropSens, a sensitivity-aware variant of DropEdge that explicitly controls the proportion of information lost due to edge-dropping, thereby increasing sensitivity to distant nodes despite dropping the same number of edges. Our experiments on long-range synthetic and real-world datasets confirm the predicted limitations of existing edge-dropping and feature-dropping methods. Moreover, DropSens consistently outperforms graph rewiring techniques designed to mitigate over-squashing, suggesting that simple, targeted modifications can substantially improve a model's ability to capture long-range interactions. Our conclusions highlight the need to re-evaluate and re-design existing methods for training deep GNNs, with a renewed focus on modelling long-range interactions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View</title>
<link>https://arxiv.org/abs/2502.11256</link>
<guid>https://arxiv.org/abs/2502.11256</guid>
<content:encoded><![CDATA[
arXiv:2502.11256v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Signed Graph Approach to Understanding and Mitigating Oversmoothing in GNNs</title>
<link>https://arxiv.org/abs/2502.11394</link>
<guid>https://arxiv.org/abs/2502.11394</guid>
<content:encoded><![CDATA[
arXiv:2502.11394v2 Announce Type: replace 
Abstract: Deep graph neural networks (GNNs) often suffer from oversmoothing, where node representations become overly homogeneous with increasing depth. While techniques like normalization, residual connections, and edge dropout have been proposed to mitigate oversmoothing, they are typically developed independently, with limited theoretical understanding of their underlying mechanisms. In this work, we present a unified theoretical perspective based on the framework of signed graphs, showing that many existing strategies implicitly introduce negative edges that alter message-passing to resist oversmoothing. However, we show that merely adding negative edges in an unstructured manner is insufficient-the asymptotic behavior of signed propagation depends critically on the strength and organization of positive and negative edges. To address this limitation, we leverage the theory of structural balance, which promotes stable, cluster-preserving dynamics by connecting similar nodes with positive edges and dissimilar ones with negative edges. We propose Structural Balanced Propagation (SBP), a plug-and-play method that assigns signed edges based on either labels or feature similarity to explicitly enhance structural balance in the constructed signed graphs. Experiments on nine benchmarks across both homophilic and heterophilic settings demonstrate that SBP consistently improves classification accuracy and mitigates oversmoothing, even at depths of up to 300 layers. Our results provide a principled explanation for prior oversmoothing remedies and introduce a new direction for signed message-passing design in deep GNNs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Semi-supervised Learning with Zero-shot Pseudolabels</title>
<link>https://arxiv.org/abs/2502.12584</link>
<guid>https://arxiv.org/abs/2502.12584</guid>
<content:encoded><![CDATA[
arXiv:2502.12584v2 Announce Type: replace 
Abstract: The high cost of data labeling presents a major barrier to deploying machine learning systems at scale. Semi-supervised learning (SSL) mitigates this challenge by utilizing unlabeled data alongside limited labeled examples, while the emergence of foundation models (FMs) offers powerful zero-shot capabilities that can further reduce labeling cost. However, directly fine-tuning large FMs is often impractical in resource-constrained settings, and na\"ively using their pseudo-labels for unlabeled data can degrade performance due to its unreliablity or domain mismatch with target task. In this work, we introduce ZeroMatch, a novel SSL framework that integrates knowledge distillation with consistency-based learning to jointly leverage labeled data, unlabeled data, and pseudo-labels from FMs. ZeroMatch enables training compact student models using only FM inference, making it suitable for low-resource environments such as personal devices with limited compute. Experiments on six vision and language classification benchmarks show that ZeroMatch consistently outperforms standard SSL and zero-shot augmented methods, demonstrating its effectiveness and robustness across a range of foundation model qualities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning</title>
<link>https://arxiv.org/abs/2502.12913</link>
<guid>https://arxiv.org/abs/2502.12913</guid>
<content:encoded><![CDATA[
arXiv:2502.12913v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to BF16-based fine-tuning while significantly reducing 1.85x memory usage. Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction</title>
<link>https://arxiv.org/abs/2502.13344</link>
<guid>https://arxiv.org/abs/2502.13344</guid>
<content:encoded><![CDATA[
arXiv:2502.13344v3 Announce Type: replace 
Abstract: Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable the prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</title>
<link>https://arxiv.org/abs/2502.14276</link>
<guid>https://arxiv.org/abs/2502.14276</guid>
<content:encoded><![CDATA[
arXiv:2502.14276v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspherical Normalization for Scalable Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.15280</link>
<guid>https://arxiv.org/abs/2502.15280</guid>
<content:encoded><![CDATA[
arXiv:2502.15280v2 Announce Type: replace 
Abstract: Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization. In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains. The code is available at https://dojeon-ai.github.io/SimbaV2.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason from Feedback at Test-Time</title>
<link>https://arxiv.org/abs/2502.15771</link>
<guid>https://arxiv.org/abs/2502.15771</guid>
<content:encoded><![CDATA[
arXiv:2502.15771v2 Announce Type: replace 
Abstract: Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Joint DNN Model Deployment and Partitioning Optimization for Collaborative Edge Inference Services</title>
<link>https://arxiv.org/abs/2502.16091</link>
<guid>https://arxiv.org/abs/2502.16091</guid>
<content:encoded><![CDATA[
arXiv:2502.16091v3 Announce Type: replace 
Abstract: Edge inference (EI) has emerged as a promising paradigm to address the growing limitations of cloud-based Deep Neural Network (DNN) inference services, such as high response latency, limited scalability, and severe data privacy exposure. However, deploying DNN models on resource-constrained edge devices introduces additional challenges, including limited computation/storage resources, dynamic service demands, and heightened privacy risks. To tackle these issues, this paper presents a novel privacy-aware optimization framework that jointly addresses DNN model deployment, user-server association, and model partitioning, with the goal of minimizing long-term average inference delay under resource and privacy constraints. The problem is formulated as a complex, NP-hard stochastic optimization. To efficiently handle system dynamics and computational complexity, we employ a Lyapunov-based approach to transform the long-term objective into tractable per-slot decisions. Furthermore, we introduce a coalition formation game to enable adaptive user-server association and design a greedy algorithm for model deployment within each coalition. Extensive simulations demonstrate that the proposed algorithm significantly reduces inference delay and consistently satisfies privacy constraints, outperforming state-of-the-art baselines across diverse scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields</title>
<link>https://arxiv.org/abs/2503.08674</link>
<guid>https://arxiv.org/abs/2503.08674</guid>
<content:encoded><![CDATA[
arXiv:2503.08674v2 Announce Type: replace 
Abstract: Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?</title>
<link>https://arxiv.org/abs/2503.10632</link>
<guid>https://arxiv.org/abs/2503.10632</guid>
<content:encoded><![CDATA[
arXiv:2503.10632v2 Announce Type: replace 
Abstract: Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep networks, including advanced architectures such as vision Transformers (ViTs). This work asks whether a similar replacement in the attention can bring benefits. In this paper, we design the first learnable attention called Kolmogorov-Arnold Attention (KArAt) for ViTs that can operate on any basis, ranging from Fourier, Wavelets, Splines, to Rational Functions. However, learnable activations in attention cause a memory explosion. To remedy this, we propose a modular version of KArAt that uses a low-rank approximation. By adopting the Fourier basis, Fourier-KArAt and its variants, in some cases, outperform their traditional softmax counterparts, or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We also deploy Fourier KArAt to ConViT and Swin-Transformer, and use it in detection and segmentation with ViT-Det. We dissect these architectures' performance by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and transferability to other datasets. KArAt's learnable activation shows a better attention score across all ViTs, indicating better token-to-token interactions, contributing to better inference. Still, its generalizability does not scale with larger ViTs. However, many factors, including the present computing interface, affect the performance of parameter- and memory-heavy KArAts. We note that the goal of this paper is not to produce efficient attention or challenge the traditional activations; by designing KArAt, we are the first to show that attention can be learned and encourage researchers to explore KArAt in conjunction with more advanced architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-regularized Gradient Estimators for Approximate Bayesian Inference</title>
<link>https://arxiv.org/abs/2503.11964</link>
<guid>https://arxiv.org/abs/2503.11964</guid>
<content:encoded><![CDATA[
arXiv:2503.11964v3 Announce Type: replace 
Abstract: Effective uncertainty quantification is important for training modern predictive models with limited data, enhancing both accuracy and robustness. While Bayesian methods are effective for this purpose, they can be challenging to scale. When employing approximate Bayesian inference, ensuring the quality of samples from the posterior distribution in a computationally efficient manner is essential. This paper addresses the estimation of the Bayesian posterior to generate diverse samples by approximating the gradient flow of the Kullback-Leibler (KL) divergence and the cross entropy of the target approximation under the metric induced by the Stein Operator. It presents empirical evaluations on classification tasks to assess the method's performance and discuss its effectiveness for Model-Based Reinforcement Learning that uses uncertainty-aware network dynamics models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation</title>
<link>https://arxiv.org/abs/2503.12600</link>
<guid>https://arxiv.org/abs/2503.12600</guid>
<content:encoded><![CDATA[
arXiv:2503.12600v2 Announce Type: replace 
Abstract: The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in evaluating research ideas within academic settings. Existing solutions primarily rely on prompt-based LLM methods or fine-tuned lightweight language models for idea evaluation. However, these methods are often unstable and struggle to comprehend the complex semantic information embedded in the ideas, impeding their ability to perform high-quality evaluations. To address the above challenges, we propose GraphEval, a lightweight graph-based LLM framework for idea evaluation. Our insight is that a complex idea can be broken down into comprehensible viewpoint nodes using prompts from small LLMs. These viewpoint nodes can then be linked together through edges created from LLM-based relation extraction and/or BERT similarity scores. The created viewpoint-graph can be used to conveniently propagate scores across view-nodes to improve the robustness of the idea evaluations. In particular, we propose two lightweight graph-based methods for idea evaluation: (1) GraphEval-LP: a training-free label propagation algorithm that propagates evaluation scores from known view-nodes to unknown nodes; (2) GraphEval-GNN: a Graph Neural Networks (GNN) that is trained to predict the evaluation scores given the observed graph with minimal computation resources. Moreover, to overcome LLM's limitation in objectively assessing the novelty of ideas, we further propose a novelty detection model to GraphEval-GNN to enhance its capability in judging idea novelty. Experiments on two datasets show GraphEval improves F1 scores by at least 14% with low computation and API costs. Additionally, GraphEval can effectively detect plagiarized ideas.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Bounds for Adversarial Constrained Online Convex Optimization</title>
<link>https://arxiv.org/abs/2503.13366</link>
<guid>https://arxiv.org/abs/2503.13366</guid>
<content:encoded><![CDATA[
arXiv:2503.13366v4 Announce Type: replace 
Abstract: Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\tilde{O} \left( \sqrt{T} \right)$ for the regret and CCV, respectively. Based on a new surrogate loss function enforcing a minimum penalty on the constraint function, we demonstrate that both the Follow-the-Regularized-Leader and the Online Gradient Descent achieve the optimal bounds.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification</title>
<link>https://arxiv.org/abs/2503.15578</link>
<guid>https://arxiv.org/abs/2503.15578</guid>
<content:encoded><![CDATA[
arXiv:2503.15578v2 Announce Type: replace 
Abstract: Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While existing transformer-based models have shown promise in time series analysis, they mainly focus on forecasting and fail to fully exploit the distinctive characteristics of MedTS data. In this paper, we introduce Sparseformer, a transformer specifically designed for MedTS classification. We propose a sparse token-based dual-attention mechanism that enables global modeling and token compression, allowing dynamic focus on the most informative tokens while distilling redundant features. This mechanism is then applied to the multi-granularity, cross-channel encoding of medical signals, capturing intra- and inter-granularity correlations and inter-channel connections. The sparsification design allows our model to handle heterogeneous inputs of varying lengths and channels directly. Further, we introduce an adaptive label encoder to address label space misalignment across datasets, equipping our model with cross-dataset transferability to alleviate the medical label scarcity issue. Our model outperforms 12 baselines across seven medical datasets under supervised learning. In the few-shot learning experiments, our model also achieves superior average results. In addition, the in-domain and cross-domain experiments among three diagnostic scenarios demonstrate our model's zero-shot learning capability. Collectively, these findings underscore the robustness and transferability of our model in various medical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Bias Reinforcement in LLM Agents Debate</title>
<link>https://arxiv.org/abs/2503.16814</link>
<guid>https://arxiv.org/abs/2503.16814</guid>
<content:encoded><![CDATA[
arXiv:2503.16814v2 Announce Type: replace 
Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty</title>
<link>https://arxiv.org/abs/2503.18314</link>
<guid>https://arxiv.org/abs/2503.18314</guid>
<content:encoded><![CDATA[
arXiv:2503.18314v4 Announce Type: replace 
Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model up to an information-theoretic bound, mitigating its over-confidence stemming from data memorization. We evaluate LoTUS on Transformer and ResNet18 models against eight baselines across five public datasets. Beyond established MU benchmarks, we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining is impractical, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. The experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: https://github.com/cspartalis/LoTUS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?</title>
<link>https://arxiv.org/abs/2504.00186</link>
<guid>https://arxiv.org/abs/2504.00186</guid>
<content:encoded><![CDATA[
arXiv:2504.00186v2 Announce Type: replace 
Abstract: Spurious correlations are unstable statistical associations that hinder robust decision-making. Conventional wisdom suggests that models relying on such correlations will fail to generalize out-of-distribution (OOD), especially under strong distribution shifts. However, empirical evidence challenges this view as naive in-distribution empirical risk minimizers often achieve the best OOD accuracy across popular OOD generalization benchmarks. In light of these results, we propose a different perspective: many widely used benchmarks for evaluating robustness to spurious correlations are misspecified. Specifically, they fail to include shifts in spurious correlations that meaningfully impact OOD generalization, making them unsuitable for evaluating the benefit of removing such correlations. We establish conditions under which a distribution shift can reliably assess a model's reliance on spurious correlations. Crucially, under these conditions, we should not observe a strong positive correlation between in-distribution and OOD accuracy, often called "accuracy on the line." Yet, most state-of-the-art benchmarks exhibit this pattern, suggesting they do not effectively assess robustness. Our findings expose a key limitation in current benchmarks used to evaluate domain generalization algorithms, that is, models designed to avoid spurious correlations. We highlight the need to rethink how robustness to spurious correlations is assessed, identify well-specified benchmarks the field should prioritize, and enumerate strategies for designing future benchmarks that meaningfully reflect robustness under distribution shift.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2504.02298</link>
<guid>https://arxiv.org/abs/2504.02298</guid>
<content:encoded><![CDATA[
arXiv:2504.02298v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose SPike-Aware Consistency Enhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets. Furthermore, SPACE exhibits robust generalization across diverse network architectures, consistently enhancing the performance of SNNs on CNNs (such as VGG and ResNet), Transformer models, and ConvLSTM architectures. Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BECAME: BayEsian Continual Learning with Adaptive Model MErging</title>
<link>https://arxiv.org/abs/2504.02666</link>
<guid>https://arxiv.org/abs/2504.02666</guid>
<content:encoded><![CDATA[
arXiv:2504.02666v2 Announce Type: replace 
Abstract: Continual Learning (CL) strives to learn incrementally across tasks while mitigating catastrophic forgetting. A key challenge in CL is balancing stability (retaining prior knowledge) and plasticity (learning new tasks). While representative gradient projection methods ensure stability, they often limit plasticity. Model merging techniques offer promising solutions, but prior methods typically rely on empirical assumptions and carefully selected hyperparameters. In this paper, we explore the potential of model merging to enhance the stability-plasticity trade-off, providing theoretical insights that underscore its benefits. Specifically, we reformulate the merging mechanism using Bayesian continual learning principles and derive a closed-form solution for the optimal merging coefficient that adapts to the diverse characteristics of tasks. To validate our approach, we introduce a two-stage framework named BECAME, which synergizes the expertise of gradient projection and adaptive merging. Extensive experiments show that our approach outperforms state-of-the-art CL methods and existing merging strategies.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism</title>
<link>https://arxiv.org/abs/2504.11558</link>
<guid>https://arxiv.org/abs/2504.11558</guid>
<content:encoded><![CDATA[
arXiv:2504.11558v2 Announce Type: replace 
Abstract: We introduce Error Broadcast and Decorrelation (EBD), a novel learning framework for neural networks that addresses credit assignment by directly broadcasting output errors to individual layers, circumventing weight transport of backpropagation. EBD is rigorously grounded in the stochastic orthogonality property of Minimum Mean Square Error estimators. This fundamental principle states that the error of an optimal estimator is orthogonal to functions of the input. Guided by this insight, EBD defines layerwise loss functions that directly penalize correlations between layer activations and output errors, thereby establishing a principled foundation for error broadcasting. This theoretically sound mechanism naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate EBD's competitive or better performance against other error-broadcast methods on benchmark datasets. Our findings establish EBD as an efficient, biologically plausible, and principled alternative for neural network training.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization</title>
<link>https://arxiv.org/abs/2504.14762</link>
<guid>https://arxiv.org/abs/2504.14762</guid>
<content:encoded><![CDATA[
arXiv:2504.14762v2 Announce Type: replace 
Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling training as a random walk over a high-dimensional graph of binary subnetworks. Each node represents a masked version of the network, and dropout induces stochastic traversal across this space. We define a subnetwork contribution score that quantifies generalization and show that it varies smoothly over the graph. Using tools from spectral graph theory, PAC-Bayes analysis, and combinatorics, we prove that generalizing subnetworks form large, connected, low-resistance clusters, and that their number grows exponentially with network width. This reveals dropout as a mechanism for sampling from a robust, structured ensemble of well-generalizing subnetworks with built-in redundancy. Extensive experiments validate every theoretical claim across diverse architectures. Together, our results offer a unified foundation for understanding dropout and suggest new directions for mask-guided regularization and subnetwork optimization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v4 Announce Type: replace 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias</title>
<link>https://arxiv.org/abs/2505.00926</link>
<guid>https://arxiv.org/abs/2505.00926</guid>
<content:encoded><![CDATA[
arXiv:2505.00926v3 Announce Type: replace 
Abstract: Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</title>
<link>https://arxiv.org/abs/2505.03793</link>
<guid>https://arxiv.org/abs/2505.03793</guid>
<content:encoded><![CDATA[
arXiv:2505.03793v2 Announce Type: replace 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dequantified Diffusion-Schr{\"o}dinger Bridge for Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2505.05034</link>
<guid>https://arxiv.org/abs/2505.05034</guid>
<content:encoded><![CDATA[
arXiv:2505.05034v3 Announce Type: replace 
Abstract: Density ratio estimation is fundamental to tasks involving f-divergences, yet existing methods often fail under significantly different distributions or inadequately overlapping supports -- the density-chasm and the support-chasm problems. Additionally, prior approaches yield divergent time scores near boundaries, leading to instability. We design $\textbf{D}^3\textbf{RE}$, a unified framework for robust, stable and efficient density ratio estimation. We propose the dequantified diffusion bridge interpolant (DDBI), which expands support coverage and stabilizes time scores via diffusion bridges and Gaussian dequantization. Building on DDBI, the proposed dequantified Schr{\"o}dinger bridge interpolant (DSBI) incorporates optimal transport to solve the Schr{\"o}dinger bridge problem, enhancing accuracy and efficiency. Our method offers uniform approximation and bounded time scores in theory, and outperforms baselines empirically in mutual information and density estimation tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Survival Modeling in the Age of Foundation Models</title>
<link>https://arxiv.org/abs/2505.07683</link>
<guid>https://arxiv.org/abs/2505.07683</guid>
<content:encoded><![CDATA[
arXiv:2505.07683v2 Announce Type: replace 
Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and image data. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model</title>
<link>https://arxiv.org/abs/2505.10438</link>
<guid>https://arxiv.org/abs/2505.10438</guid>
<content:encoded><![CDATA[
arXiv:2505.10438v2 Announce Type: replace 
Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems. Deriving their physics-based models can be challenging as it requires performance characteristics, that are not always available, and one often has to make many simplifying assumptions. In this paper, the limitations of conventional experimental methods used to derive component-level and locally linear parameter-varying models are discussed and addressed by employing identification techniques based on data collected from standard engine operation under closed-loop control. The rotor dynamics were estimated using the sparse identification of nonlinear dynamics. Subsequently, the autonomous part of the dynamics was mapped into an optimally constructed Koopman eigenfunction space. The process included eigenvalue optimization using metaheuristic algorithms and temporal projection, followed by gradient-based eigenfunction identification. The resulting Koopman model was validated against an in-house reference component-level model. A globally optimal nonlinear feedback controller and a Kalman estimator were then designed in the eigenfunction space and compared to the classical and gain-scheduled proportional-integral controllers, as well as a proposed internal model control approach. The eigenmode structure allowed targeting individual modes during the optimization process, resulting in a better performance tuning. The results showed that the Koopman-based controller outperformed the other benchmark controllers in both reference tracking and disturbance rejection, under sea-level and varying flight conditions, due to its global nature.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks</title>
<link>https://arxiv.org/abs/2505.11461</link>
<guid>https://arxiv.org/abs/2505.11461</guid>
<content:encoded><![CDATA[
arXiv:2505.11461v2 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) methods typically require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and policy gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for extensions to additional problems in wireless communications and radar networks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of ReLU Networks through the ReLU Transition Graph</title>
<link>https://arxiv.org/abs/2505.11692</link>
<guid>https://arxiv.org/abs/2505.11692</guid>
<content:encoded><![CDATA[
arXiv:2505.11692v2 Announce Type: replace 
Abstract: We develop a novel theoretical framework for analyzing ReLU neural networks through the lens of a combinatorial object we term the ReLU Transition Graph (RTG). In this graph, each node corresponds to a linear region induced by the network's activation patterns, and edges connect regions that differ by a single neuron flip. Building on this structure, we derive a suite of new theoretical results connecting RTG geometry to expressivity, generalization, and robustness. Our contributions include tight combinatorial bounds on RTG size and diameter, a proof of RTG connectivity, and graph-theoretic interpretations of VC-dimension. We also relate entropy and average degree of the RTG to generalization error. Each theoretical result is rigorously validated via carefully controlled experiments across varied network depths, widths, and data regimes. This work provides the first unified treatment of ReLU network structure via graph theory and opens new avenues for compression, regularization, and complexity control rooted in RTG analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory</title>
<link>https://arxiv.org/abs/2505.11694</link>
<guid>https://arxiv.org/abs/2505.11694</guid>
<content:encoded><![CDATA[
arXiv:2505.11694v2 Announce Type: replace 
Abstract: We present a complete theoretical and empirical framework establishing feedforward neural networks as universal finite-state machines (N-FSMs). Our results prove that finite-depth ReLU and threshold networks can exactly simulate deterministic finite automata (DFAs) by unrolling state transitions into depth-wise neural layers, with formal characterizations of required depth, width, and state compression. We demonstrate that DFA transitions are linearly separable, binary threshold activations allow exponential compression, and Myhill-Nerode equivalence classes can be embedded into continuous latent spaces while preserving separability. We also formalize the expressivity boundary: fixed-depth feedforward networks cannot recognize non-regular languages requiring unbounded memory. Unlike prior heuristic or probing-based studies, we provide constructive proofs and design explicit DFA-unrolled neural architectures that empirically validate every claim. Our results bridge deep learning, automata theory, and neural-symbolic computation, offering a rigorous blueprint for how discrete symbolic processes can be realized in continuous neural systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption</title>
<link>https://arxiv.org/abs/2505.12147</link>
<guid>https://arxiv.org/abs/2505.12147</guid>
<content:encoded><![CDATA[
arXiv:2505.12147v2 Announce Type: replace 
Abstract: The rapid increase in computing power and the ability to store Big Data in the infrastructure has enabled predictions in a large variety of domains by Machine Learning. However, in many cases, existing Machine Learning tools are considered insufficient or incorrect since they exploit only probabilistic dependencies rather than inference logic. Causal Machine Learning methods seem to close this gap. In this paper, two prevalent tools based on Causal Machine Learning methods are compared, as well as their mathematical underpinning background. The operation of the tools is demonstrated by examining their response to 18 queries, based on the IDEAL Household Energy Dataset, published by the University of Edinburgh. First, it was important to evaluate the causal relations assumption that allowed the use of this approach; this was based on the preexisting scientific knowledge of the domain and was implemented by use of the in-built validation tools. Results were encouraging and may easily be extended to other domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents</title>
<link>https://arxiv.org/abs/2505.12204</link>
<guid>https://arxiv.org/abs/2505.12204</guid>
<content:encoded><![CDATA[
arXiv:2505.12204v2 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive capabilities in complex decision-making tasks. This progress raises a natural question: how do these artificial systems compare to biological agents, which have been shaped by millions of years of evolution? To help answer this question, we undertake a comparative study of biological mice and RL agents in a predator-avoidance maze environment. Through this analysis, we identify a striking disparity: RL agents consistently demonstrate a lack of self-preservation instinct, readily risking ``death'' for marginal efficiency gains. These risk-taking strategies are in contrast to biological agents, which exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging this gap between the biological and artificial, we propose two novel mechanisms that encourage more naturalistic risk-avoidance behaviors in RL agents. Our approach leads to the emergence of naturalistic behaviors, including strategic environment assessment, cautious path planning, and predator avoidance patterns that closely mirror those observed in biological systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the definition and importance of interpretability in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.13510</link>
<guid>https://arxiv.org/abs/2505.13510</guid>
<content:encoded><![CDATA[
arXiv:2505.13510v2 Announce Type: replace 
Abstract: Though neural networks trained on large datasets have been successfully used to describe and predict many physical phenomena, there is a sense among scientists that, unlike traditional scientific models comprising simple mathematical expressions, their findings cannot be integrated into the body of scientific knowledge. Critics of machine learning's inability to produce human-understandable relationships have converged on the concept of "interpretability" as its point of departure from more traditional forms of science. As the growing interest in interpretability has shown, researchers in the physical sciences seek not just predictive models, but also to uncover the fundamental principles that govern a system of interest. However, clarity around a definition of interpretability and the precise role that it plays in science is lacking in the literature. In this work, we argue that researchers in equation discovery and symbolic regression tend to conflate the concept of sparsity with interpretability. We review key papers on interpretable machine learning from outside the scientific community and argue that, though the definitions and methods they propose can inform questions of interpretability for scientific machine learning (SciML), they are inadequate for this new purpose. Noting these deficiencies, we propose an operational definition of interpretability for the physical sciences. Our notion of interpretability emphasizes understanding of the mechanism over mathematical sparsity. Innocuous though it may seem, this emphasis on mechanism shows that sparsity is often unnecessary. It also questions the possibility of interpretable scientific discovery when prior knowledge is lacking. We believe a precise and philosophically informed definition of interpretability in SciML will help focus research efforts toward the most significant obstacles to realizing a data-driven scientific future.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
arXiv:2505.14669v2 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an "optimal" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach</title>
<link>https://arxiv.org/abs/2505.16403</link>
<guid>https://arxiv.org/abs/2505.16403</guid>
<content:encoded><![CDATA[
arXiv:2505.16403v2 Announce Type: replace 
Abstract: Manipulation of local training data and local updates, i.e., the poisoning attack, is the main threat arising from the collaborative nature of the federated learning (FL) paradigm. Most existing poisoning attacks aim to manipulate local data/models in a way that causes denial-of-service (DoS) issues. In this paper, we introduce a novel attack method, named Federated Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the extent of poisoning in a subtle controlled manner. It operates with a predefined objective, such as reducing global model's prediction accuracy by 10%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC) theory with model poisoning attacks. It can manipulate the updates from malicious clients to drive the global model towards a compromised state, achieving this at a controlled and inconspicuous rate. Additionally, leveraging the robust control properties of FedSA allows precise control over the convergence bounds, enabling the attacker to set the global accuracy of the poisoned model to any desired level. Experimental results demonstrate that FedSA can accurately achieve a predefined global accuracy with fewer malicious clients while maintaining a high level of stealth and adjustable learning rates.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved and Oracle-Efficient Online $\ell_1$-Multicalibration</title>
<link>https://arxiv.org/abs/2505.17365</link>
<guid>https://arxiv.org/abs/2505.17365</guid>
<content:encoded><![CDATA[
arXiv:2505.17365v2 Announce Type: replace 
Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated predictions across multiple groups in adversarial settings, across $T$ rounds. Although online calibration is typically studied in the $\ell_1$ norm, prior approaches to online multicalibration have taken the indirect approach of obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then transferred these guarantees to $\ell_1$ at additional loss. In contrast, we propose a direct method that achieves improved and oracle-efficient rates of $\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$ respectively, for online $\ell_1$-multicalibration. Our key insight is a novel reduction of online \(\ell_1\)-multicalibration to an online learning problem with product-based rewards, which we refer to as \emph{online linear-product optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm for this linearized problem. Although this method guarantees the desired sublinear rate (nearly matching the best rate for online calibration), it is computationally expensive when the group family \(\mathcal{H}\) is large or infinite, since it enumerates all possible groups. To address scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a polynomial number of calls to an offline optimization (\emph{multicalibration evaluation}) oracle, resulting in \emph{oracle-efficient} online \(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$. Our framework also extends to certain infinite families of groups (e.g., all linear functions on the context space) by exploiting a $1$-Lipschitz property of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[
arXiv:2505.17652v2 Announce Type: replace 
Abstract: Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces $\textbf{C}$ompetence-$\textbf{D}$ifficulty $\textbf{A}$lignment $\textbf{S}$ampling ($\textbf{CDAS}$), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network</title>
<link>https://arxiv.org/abs/2505.19423</link>
<guid>https://arxiv.org/abs/2505.19423</guid>
<content:encoded><![CDATA[
arXiv:2505.19423v2 Announce Type: replace 
Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated enhanced exploration capabilities and greater robustness than using traditional policy gradient. However, ERL suffers from the high computational costs and low search efficiency, as EAs require evaluating numerous candidate policies with expensive simulations, many of which are ineffective and do not contribute meaningfully to the training. One intuitive way to reduce the ineffective evaluations is to adopt the surrogates. Unfortunately, existing ERL policies are often modeled as deep neural networks (DNNs) and thus naturally represented as high-dimensional vectors containing millions of weights, which makes the building of effective surrogates for ERL policies extremely challenging. This paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE) and Hyperbolic Neural Networks (HNN). Specifically, AE compresses high-dimensional policies into low-dimensional representations while extracting key features as the inputs for the surrogate. HNN, functioning as a classification-based surrogate model, can learn complex nonlinear relationships from sampled data and enable more accurate pre-selection of the sampled policies without real evaluations. The experiments on 10 Atari and 4 Mujoco games have verified that the proposed method outperforms previous approaches significantly. The search trajectories guided by AE and HNN are also visually demonstrated to be more effective, in terms of both exploration and convergence. This paper not only presents the first learnable policy embedding and surrogate-modeling modules for high-dimensional ERL policies, but also empirically reveals when and why they can be successful.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Wave Networks</title>
<link>https://arxiv.org/abs/2505.20034</link>
<guid>https://arxiv.org/abs/2505.20034</guid>
<content:encoded><![CDATA[
arXiv:2505.20034v2 Announce Type: replace 
Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing (MP) of graph neural networks (GNNs). Existing methods consider MP between nodes as a heat diffusion process, and leverage heat equation to model the temporal evolution of nodes in the embedding space. However, heat equation can hardly depict the wave nature of graph signals in graph signal processing. Besides, heat equation is essentially a partial differential equation (PDE) involving a first partial derivative of time, whose numerical solution usually has low stability, and leads to inefficient model training. In this paper, we would like to depict more wave details in MP, since graph signals are essentially wave signals that can be seen as a superposition of a series of waves in the form of eigenvector. This motivates us to consider MP as a wave propagation process to capture the temporal evolution of wave signals in the space. Based on wave equation in physics, we innovatively develop a graph wave equation to leverage the wave propagation on graphs. In details, we demonstrate that the graph wave equation can be connected to traditional spectral GNNs, facilitating the design of graph wave networks based on various Laplacians and enhancing the performance of the spectral GNNs. Besides, the graph wave equation is particularly a PDE involving a second partial derivative of time, which has stronger stability on graphs than the heat equation that involves a first partial derivative of time. Additionally, we theoretically prove that the numerical solution derived from the graph wave equation are constantly stable, enabling to significantly enhance model efficiency while ensuring its performance. Extensive experiments show that GWNs achieve SOTA and efficient performance on benchmark datasets, and exhibit outstanding performance in addressing challenging graph problems, such as over-smoothing and heterophily.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring User-side Fairness in Dynamic Recommender Systems</title>
<link>https://arxiv.org/abs/2308.15651</link>
<guid>https://arxiv.org/abs/2308.15651</guid>
<content:encoded><![CDATA[
arXiv:2308.15651v3 Announce Type: replace-cross 
Abstract: User-side group fairness is crucial for modern recommender systems, aiming to alleviate performance disparities among user groups defined by sensitive attributes like gender, race, or age. In the ever-evolving landscape of user-item interactions, continual adaptation to newly collected data is crucial for recommender systems to stay aligned with the latest user preferences. However, we observe that such continual adaptation often exacerbates performance disparities. This necessitates a thorough investigation into user-side fairness in dynamic recommender systems, an area that has been unexplored in the literature. This problem is challenging due to distribution shifts, frequent model updates, and non-differentiability of ranking metrics. To our knowledge, this paper presents the first principled study on ensuring user-side fairness in dynamic recommender systems. We start with theoretical analyses on fine-tuning v.s. retraining, showing that the best practice is incremental fine-tuning with restart. Guided by our theoretical analyses, we propose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to dynamically ensure user-side fairness over time. To overcome the non-differentiability of recommendation metrics in the fairness loss, we further introduce Differentiable Hit (DH) as an improvement over the recent NeuralNDCG method, not only alleviating its gradient vanishing issue but also achieving higher efficiency. Besides that, we also address the instability issue of the fairness loss by leveraging the competing nature between the recommendation loss and the fairness loss. Through extensive experiments on real-world datasets, we demonstrate that FADE effectively and efficiently reduces performance disparities with little sacrifice in the overall recommendation performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</title>
<link>https://arxiv.org/abs/2310.18940</link>
<guid>https://arxiv.org/abs/2310.18940</guid>
<content:encoded><![CDATA[
arXiv:2310.18940v4 Announce Type: replace-cross 
Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Straightforward Gradient-Based Approach for High-Tc Superconductor Design: Leveraging Domain Knowledge via Adaptive Constraints</title>
<link>https://arxiv.org/abs/2403.13627</link>
<guid>https://arxiv.org/abs/2403.13627</guid>
<content:encoded><![CDATA[
arXiv:2403.13627v2 Announce Type: replace-cross 
Abstract: Materials design aims to discover novel compounds with desired properties. However, prevailing strategies face critical trade-offs. Conventional element-substitution approaches readily and adaptively incorporate various domain knowledge but remain confined to a narrow search space. In contrast, deep generative models efficiently explore vast compositional landscapes, yet they struggle to flexibly integrate domain knowledge. To address these trade-offs, we propose a gradient-based material design framework that combines these strengths, offering both efficiency and adaptability. In our method, chemical compositions are optimised to achieve target properties by using property prediction models and their gradients. In order to seamlessly enforce diverse constraints, including those reflecting domain insights such as oxidation states, discretised compositional ratios, types of elements, and their abundance, we apply masks and employ a special loss function, namely the integer loss. Furthermore, we initialise the optimisation using promising candidates from existing dataset, effectively guiding the search away from unfavourable regions and thus helping to avoid poor solutions. Our approach demonstrates a more efficient exploration of superconductor candidates, uncovering candidate materials with higher critical temperature than conventional element-substitution and generative models. Importantly, it could propose new compositions beyond those found in existing databases, including new hydride superconductors absent from the training dataset but which share compositional similarities with materials found in literature. This synergy of domain knowledge and machine-learning-based scalability provides a robust foundation for rapid, adaptive, and comprehensive materials design for superconductors and beyond.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the Interplay between Carryover Effects and Reward Autocorrelations in Switchback Experiments</title>
<link>https://arxiv.org/abs/2403.17285</link>
<guid>https://arxiv.org/abs/2403.17285</guid>
<content:encoded><![CDATA[
arXiv:2403.17285v3 Announce Type: replace-cross 
Abstract: A/B testing has become the gold standard for policy evaluation in modern technological industries. Motivated by the widespread use of switchback experiments in A/B testing, this paper conducts a comprehensive comparative analysis of various switchback designs in Markovian environments. Unlike many existing works which derive the optimal design based on specific and relatively simple estimators, our analysis covers a range of state-of-the-art estimators developed in the reinforcement learning (RL) literature. It reveals that the effectiveness of different switchback designs depends crucially on (i) the size of the carryover effect and (ii) the auto-correlations among reward errors over time. Meanwhile, these findings are estimator-agnostic, i.e., they apply to most RL estimators. Based on these insights, we provide a workflow to offer guidelines for practitioners on designing switchback experiments in A/B testing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatHuman: Chatting about 3D Humans with Tools</title>
<link>https://arxiv.org/abs/2405.04533</link>
<guid>https://arxiv.org/abs/2405.04533</guid>
<content:encoded><![CDATA[
arXiv:2405.04533v2 Announce Type: replace-cross 
Abstract: Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including 3D pose, shape, contact, human-object interaction, and emotion. While widely applicable in vision and other areas, such methods require expert knowledge to select, use, and interpret the results. To address this, we introduce ChatHuman, a language-driven system that integrates the capabilities of specialized methods into a unified framework. ChatHuman functions as an assistant proficient in utilizing, analyzing, and interacting with tools specific to 3D human tasks, adeptly discussing and resolving related challenges. Built on a Large Language Model (LLM) framework, ChatHuman is trained to autonomously select, apply, and interpret a diverse set of tools in response to user inputs. Our approach overcomes significant hurdles in adapting LLMs to 3D human tasks, including the need for domain-specific knowledge and the ability to interpret complex 3D outputs. The innovations of ChatHuman include leveraging academic publications to instruct the LLM on tool usage, employing a retrieval-augmented generation model to create in-context learning examples for managing new tools, and effectively discriminating between and integrating tool results by transforming specialized 3D outputs into comprehensible formats. Experiments demonstrate that ChatHuman surpasses existing models in both tool selection accuracy and overall performance across various 3D human tasks, and it supports interactive chatting with users. ChatHuman represents a significant step toward consolidating diverse analytical methods into a unified, robust system for 3D human tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Thompson Sampling for Learning Linear Quadratic Regulators with $O(\sqrt{T})$ Regret</title>
<link>https://arxiv.org/abs/2405.19380</link>
<guid>https://arxiv.org/abs/2405.19380</guid>
<content:encoded><![CDATA[
arXiv:2405.19380v2 Announce Type: replace-cross 
Abstract: We propose a novel Thompson sampling algorithm that learns linear quadratic regulators (LQR) with a Bayesian regret bound of $O(\sqrt{T})$. Our method leverages Langevin dynamics with a carefully designed preconditioner and incorporates a simple excitation mechanism. We show that the excitation signal drives the minimum eigenvalue of the preconditioner to grow over time, thereby accelerating the approximate posterior sampling process. Furthermore, we establish nontrivial concentration properties of the approximate posteriors generated by our algorithm. These properties enable us to bound the moments of the system state and attain an $O(\sqrt{T})$ regret bound without relying on the restrictive assumptions that are often used in the literature.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JANET: Joint Adaptive predictioN-region Estimation for Time-series</title>
<link>https://arxiv.org/abs/2407.06390</link>
<guid>https://arxiv.org/abs/2407.06390</guid>
<content:encoded><![CDATA[
arXiv:2407.06390v2 Announce Type: replace-cross 
Abstract: Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models</title>
<link>https://arxiv.org/abs/2408.04556</link>
<guid>https://arxiv.org/abs/2408.04556</guid>
<content:encoded><![CDATA[
arXiv:2408.04556v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, the extended experiments demonstrate that our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Empirical Mean is Minimax Optimal for Local Glivenko-Cantelli</title>
<link>https://arxiv.org/abs/2410.02835</link>
<guid>https://arxiv.org/abs/2410.02835</guid>
<content:encoded><![CDATA[
arXiv:2410.02835v2 Announce Type: replace-cross 
Abstract: We revisit the recently introduced Local Glivenko-Cantelli setting, which studies distribution-dependent uniform convergence rates of the Empirical Mean Estimator (EME). In this work, we investigate generalizations of this setting where arbitrary estimators are allowed rather than just the EME. Can a strictly larger class of measures be learned? Can better risk decay rates be obtained? We provide exhaustive answers to these questions, which are both negative, provided the learner is barred from exploiting some infinite-dimensional pathologies. On the other hand, allowing such exploits does lead to a strictly larger class of learnable measures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes</title>
<link>https://arxiv.org/abs/2410.06678</link>
<guid>https://arxiv.org/abs/2410.06678</guid>
<content:encoded><![CDATA[
arXiv:2410.06678v3 Announce Type: replace-cross 
Abstract: We propose M3Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M3Bench requires an embodied agent to reason about its configuration, environmental constraints, and task objectives to generate coordinated whole-body motion trajectories for object rearrangement. M3Bench features 30,000 object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M3BenchMaker, an automatic data generation tool that produces whole-body motion trajectories from high-level task instructions using only basic scene and robot information. Our benchmark includes various task splits to evaluate generalization across different dimensions and leverages realistic physics simulation for trajectory assessment. Extensive evaluation analysis reveals that state-of-the-art models struggle with coordinating base-arm motion while adhering to environmental and task-specific constraints, underscoring the need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker we aim to advance robotics research toward more adaptive and capable mobile manipulation in diverse, real-world environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Gradient-Based Methods: Multi-Property Materials Design Beyond Differentiable Targets</title>
<link>https://arxiv.org/abs/2410.08562</link>
<guid>https://arxiv.org/abs/2410.08562</guid>
<content:encoded><![CDATA[
arXiv:2410.08562v4 Announce Type: replace-cross 
Abstract: Gradient-based methods offer a simple, efficient strategy for materials design by directly optimizing candidates using gradients from pretrained property predictors. However, their use in crystal structure optimization is hindered by two key challenges: handling non-differentiable constraints, such as charge neutrality and structural fidelity, and susceptibility to poor local minima. We revisit and extend the gradient-based methods to address these issues. We propose Simultaneous Multi-property Optimization using Adaptive Crystal Synthesizer (SMOACS), which integrates oxidation-number masks and template-based initialization to enforce non-differentiable constraints, avoid poor local minima, and flexibly incorporate additional constraints without retraining. SMOACS enables multi-property optimization. including exceptional targets such as high-temperature superconductivity, and scales to large crystal systems, both persistent challenges for generative models, even those enhanced with gradient-based guidance from property predictors. In experiments on five target properties and three datasets, SMOACS outperforms generative models and Bayesian optimization methods, successfully designing 135-atom perovskite structures that satisfy multiple property targets and constraints, a task at which the other methods fail entirely.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs</title>
<link>https://arxiv.org/abs/2410.11001</link>
<guid>https://arxiv.org/abs/2410.11001</guid>
<content:encoded><![CDATA[
arXiv:2410.11001v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose $\textit{graph of records}$ ($\textbf{GoR}$), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the $\textit{retrieve-then-generate}$ paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR features a $\textit{graph neural network}$ and an elaborately designed $\textit{BERTScore}$-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance ($\textit{e.g.}$, 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-dependent Convergence Theory for Diffusion Models</title>
<link>https://arxiv.org/abs/2410.13738</link>
<guid>https://arxiv.org/abs/2410.13738</guid>
<content:encoded><![CDATA[
arXiv:2410.13738v2 Announce Type: replace-cross 
Abstract: Score-based diffusion models have demonstrated outstanding empirical performance in machine learning and artificial intelligence, particularly in generating high-quality new samples from complex probability distributions. Improving the theoretical understanding of diffusion models, with a particular focus on the convergence analysis, has attracted significant attention. In this work, we develop a convergence rate that is adaptive to the smoothness of different target distributions, referred to as instance-dependent bound. Specifically, we establish an iteration complexity of $\min\{d,d^{2/3}L^{1/3},d^{1/3}L\}\varepsilon^{-2/3}$ (up to logarithmic factors), where $d$ denotes the data dimension, and $\varepsilon$ quantifies the output accuracy in terms of total variation (TV) distance. In addition, $L$ represents a relaxed Lipschitz constant, which, in the case of Gaussian mixture models, scales only logarithmically with the number of components, the dimension and iteration number, demonstrating broad applicability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guarantees of a Preconditioned Subgradient Algorithm for Overparameterized Asymmetric Low-rank Matrix Recovery</title>
<link>https://arxiv.org/abs/2410.16826</link>
<guid>https://arxiv.org/abs/2410.16826</guid>
<content:encoded><![CDATA[
arXiv:2410.16826v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on a matrix factorization-based approach to recover low-rank {\it asymmetric} matrices from corrupted measurements. We propose an {\it Overparameterized Preconditioned Subgradient Algorithm (OPSA)} and provide, for the first time in the literature, linear convergence rates independent of the rank of the sought asymmetric matrix in the presence of gross corruptions. Our work goes beyond existing results in preconditioned-type approaches addressing their current limitation, i.e., the lack of convergence guarantees in the case of {\it asymmetric matrices of unknown rank}. By applying our approach to (robust) matrix sensing, we highlight its merits when the measurement operator satisfies a mixed-norm restricted isometry property. Lastly, we present extensive numerical experiments that validate our theoretical results and demonstrate the effectiveness of our approach for different levels of overparameterization and outlier corruptions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Positive Pairs in Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.18200</link>
<guid>https://arxiv.org/abs/2410.18200</guid>
<content:encoded><![CDATA[
arXiv:2410.18200v2 Announce Type: replace-cross 
Abstract: The training methods in AI do involve semantically distinct pairs of samples. However, their role typically is to enhance the between class separability. The actual notion of similarity is normally learned from semantically identical pairs. This paper presents SimLAP: a simple framework for learning visual representation from arbitrary pairs. SimLAP explores the possibility of learning similarity from semantically distinct sample pairs. The approach is motivated by the observation that for any pair of classes there exists a subspace in which semantically distinct samples exhibit similarity. This phenomenon can be exploited for a novel method of learning, which optimises the similarity of an arbitrary pair of samples, while simultaneously learning the enabling subspace. The feasibility of the approach will be demonstrated experimentally and its merits discussed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Cloud Synthesis Using Inner Product Transforms</title>
<link>https://arxiv.org/abs/2410.18987</link>
<guid>https://arxiv.org/abs/2410.18987</guid>
<content:encoded><![CDATA[
arXiv:2410.18987v3 Announce Type: replace-cross 
Abstract: Point-cloud synthesis, i.e. the generation of novel point clouds from an input distribution, remains a challenging task, for which numerous complex machine-learning models have been devised. We develop a novel method that encodes geometrical-topological characteristics of point clouds using inner products, leading to a highly-efficient point cloud representation with provable expressivity properties. Integrated into deep learning models, our encoding exhibits high quality in typical tasks like reconstruction, generation, and interpolation, with inference times orders of magnitude faster than existing methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</title>
<link>https://arxiv.org/abs/2411.04999</link>
<guid>https://arxiv.org/abs/2411.04999</guid>
<content:encoded><![CDATA[
arXiv:2411.04999v2 Announce Type: replace-cross 
Abstract: Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraints and Variables Reduction for Optimal Power Flow Using Hierarchical Graph Neural Networks with Virtual Node-Splitting</title>
<link>https://arxiv.org/abs/2411.06268</link>
<guid>https://arxiv.org/abs/2411.06268</guid>
<content:encoded><![CDATA[
arXiv:2411.06268v2 Announce Type: replace-cross 
Abstract: Power system networks are often modeled as homogeneous graphs, which limits the ability of graph neural network (GNN) to capture individual generator features at the same nodes. By introducing the proposed virtual node-splitting strategy, generator-level attributes like costs, limits, and ramp rates can be fully captured by GNN models, improving GNN's learning capacity and prediction accuracy. Optimal power flow (OPF) problem is used for real-time grid operations. Limited timeframe motivates studies to create size-reduced OPF (ROPF) models to relieve the computational complexity. In this paper, with virtual node-splitting, a novel two-stage adaptive hierarchical GNN is developed to (i) predict critical lines that would be congested, and then (ii) predict base generators that would operate at the maximum capacity. This will substantially reduce the constraints and variables needed for OPF, creating the proposed ROPFLG model with reduced monitor lines and reduced generator-specific variables and constraints. Two ROPF models, ROPFL and ROPFG, with just reduced lines or generators respectively, are also implemented as additional benchmark models. Case studies show that the proposed ROPFLG consistently outperforms the benchmark full OPF (FOPF) and the other two ROPF methods, achieving significant computational time savings while reliably finding optimal solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains</title>
<link>https://arxiv.org/abs/2411.06426</link>
<guid>https://arxiv.org/abs/2411.06426</guid>
<content:encoded><![CDATA[
arXiv:2411.06426v3 Announce Type: replace-cross 
Abstract: As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surveying the space of descriptions of a composite system with machine learning</title>
<link>https://arxiv.org/abs/2411.18579</link>
<guid>https://arxiv.org/abs/2411.18579</guid>
<content:encoded><![CDATA[
arXiv:2411.18579v2 Announce Type: replace-cross 
Abstract: Multivariate information theory provides a general and principled framework for understanding how the components of a complex system are connected. Existing analyses are coarse in nature -- built up from characterizations of discrete subsystems -- and can be computationally prohibitive. In this work, we propose to study the continuous space of possible descriptions of a composite system as a window into its organizational structure. A description consists of specific information conveyed about each of the components, and the space of possible descriptions is equivalent to the space of lossy compression schemes of the components. We introduce a machine learning framework to optimize descriptions that extremize key information theoretic quantities used to characterize organization, such as total correlation and O-information. Through case studies on spin systems, sudoku boards, and letter sequences from natural language, we identify extremal descriptions that reveal how system-wide variation emerges from individual components. By integrating machine learning into a fine-grained information theoretic analysis of composite random variables, our framework opens a new avenues for probing the structure of real-world complex systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery</title>
<link>https://arxiv.org/abs/2411.19093</link>
<guid>https://arxiv.org/abs/2411.19093</guid>
<content:encoded><![CDATA[
arXiv:2411.19093v2 Announce Type: replace-cross 
Abstract: Clean water and sanitation are essential for health, well-being, and sustainable development, yet significant global disparities persist. Although the United Nations' Sustainable Development Goal (SDG) 6 clearly defines targets for universal access to clean water and sanitation, limitations in data coverage and openness impede accurate tracking of progress in many countries. To bridge these gaps, this study integrates Afrobarometer survey data, satellite imagery from Landsat 8 and Sentinel-2, and advanced deep learning techniques using Meta's self-supervised Distillation with No Labels (DINO) model to develop a modeling framework for evaluating access to piped water and sewage system across diverse African regions. The modeling framework achieved notable accuracy, with over 96% for piped water and 97% for sewage system access classification. When combined with geospatial population data, validation against official statistics from the United Nations Joint Monitoring Program demonstrated high concordance at the national scale (R2 of 0.95 for piped water access and R2 of 0.85 for sewage system access). The national-level estimates can represent SDG Indicators 6.1.1 and 6.2.1. This approach provides policymakers and stakeholders with an effective, scalable, and cost-efficient tool to pinpoint underserved areas requiring targeted intervention. The methodology developed herein can be adapted for assessing other infrastructure-related SDGs, promoting enhanced monitoring and informed decision-making towards achieving global sustainability objectives.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Tensor Motion Planning</title>
<link>https://arxiv.org/abs/2411.19393</link>
<guid>https://arxiv.org/abs/2411.19393</guid>
<content:encoded><![CDATA[
arXiv:2411.19393v3 Announce Type: replace-cross 
Abstract: Batch planning is increasingly necessary to quickly produce diverse and quality motion plans for downstream learning applications, such as distillation and imitation learning. This paper presents Global Tensor Motion Planning (GTMP) -- a sampling-based motion planning algorithm comprising only tensor operations. We introduce a novel discretization structure represented as a random multipartite graph, enabling efficient vectorized sampling, collision checking, and search. We provide a theoretical investigation showing that GTMP exhibits probabilistic completeness while supporting modern GPU/TPU. Additionally, by incorporating smooth structures into the multipartite graph, GTMP directly plans smooth splines without requiring gradient-based optimization. Experiments on lidar-scanned occupancy maps and the MotionBenchMarker dataset demonstrate GTMP's computation efficiency in batch planning compared to baselines, underscoring GTMP's potential as a robust, scalable planner for diverse applications and large-scale robot learning tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning</title>
<link>https://arxiv.org/abs/2412.00175</link>
<guid>https://arxiv.org/abs/2412.00175</guid>
<content:encoded><![CDATA[
arXiv:2412.00175v3 Announce Type: replace-cross 
Abstract: Good datasets are essential for developing and benchmarking any machine learning system. Their importance is even more extreme for safety critical applications such as deepfake detection - the focus of this paper. Here we reveal that two of the most widely used audio-video deepfake datasets suffer from a previously unidentified spurious feature: the leading silence. Fake videos start with a very brief moment of silence and based on this feature alone, we can separate the real and fake samples almost perfectly. As such, previous audio-only and audio-video models exploit the presence of silence in the fake videos and consequently perform worse when the leading silence is removed. To circumvent latching on such unwanted artifact and possibly other unrevealed ones we propose a shift from supervised to unsupervised learning by training models exclusively on real data. We show that by aligning self-supervised audio-video representations we remove the risk of relying on dataset-specific biases and improve robustness in deepfake detection.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title>
<link>https://arxiv.org/abs/2412.13670</link>
<guid>https://arxiv.org/abs/2412.13670</guid>
<content:encoded><![CDATA[
arXiv:2412.13670v2 Announce Type: replace-cross 
Abstract: Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Representation Learning for fMRI-based Neurological Disorder Identification</title>
<link>https://arxiv.org/abs/2412.16197</link>
<guid>https://arxiv.org/abs/2412.16197</guid>
<content:encoded><![CDATA[
arXiv:2412.16197v2 Announce Type: replace-cross 
Abstract: Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping</title>
<link>https://arxiv.org/abs/2412.19529</link>
<guid>https://arxiv.org/abs/2412.19529</guid>
<content:encoded><![CDATA[
arXiv:2412.19529v4 Announce Type: replace-cross 
Abstract: Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and true gradient) is considered to have only a finite $\mathfrak{p}$-th moment where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different algorithms and proved them to converge at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate for smooth objectives after $T$ iterations. Notably, all these new-designed algorithms are based on the same technique - gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, we prove that Batched NSGDM can achieve the optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the case where the tail index $\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An unsupervised method for MRI recovery: Deep image prior with structured sparsity</title>
<link>https://arxiv.org/abs/2501.01482</link>
<guid>https://arxiv.org/abs/2501.01482</guid>
<content:encoded><![CDATA[
arXiv:2501.01482v3 Announce Type: replace-cross 
Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRAG: Retrieval-Augmented Generation over Video Corpus</title>
<link>https://arxiv.org/abs/2501.05874</link>
<guid>https://arxiv.org/abs/2501.05874</guid>
<content:encoded><![CDATA[
arXiv:2501.05874v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available at https://github.com/starsuzi/VideoRAG.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Product Attention Is All You Need</title>
<link>https://arxiv.org/abs/2501.06425</link>
<guid>https://arxiv.org/abs/2501.06425</guid>
<content:encoded><![CDATA[
arXiv:2501.06425v4 Announce Type: replace-cross 
Abstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines, including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at the decoding stage enable processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Localization and Activation Editing for Low-Resource Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.01179</link>
<guid>https://arxiv.org/abs/2502.01179</guid>
<content:encoded><![CDATA[
arXiv:2502.01179v4 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. Due to their extremely small parameter counts, these methods show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Large Language Model Collaborative Decoding via Speculation</title>
<link>https://arxiv.org/abs/2502.01662</link>
<guid>https://arxiv.org/abs/2502.01662</guid>
<content:encoded><![CDATA[
arXiv:2502.01662v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) collaborative decoding techniques improve output quality by combining the outputs of multiple models at each generation step, but they incur high computational costs. In this paper, we introduce Collaborative decoding via Speculation (CoS), a novel framework that accelerates collaborative decoding without compromising performance. Inspired by Speculative Decoding--where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel, our approach builds on two key insights: (1) the verification distribution can be the combined distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to collaboration among n models and theoretically prove that CoS is never slower than standard collaborative decoding, typically achieving faster speed. Extensive experiments demonstrate CoS is 1.11x-2.23x faster than standard collaborative decoding without compromising generation quality. Our code is available at https://github.com/Kamichanw/CoS/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wake-Informed 3D Path Planning for Autonomous Underwater Vehicles Using A* and Neural Network Approximations</title>
<link>https://arxiv.org/abs/2502.01918</link>
<guid>https://arxiv.org/abs/2502.01918</guid>
<content:encoded><![CDATA[
arXiv:2502.01918v2 Announce Type: replace-cross 
Abstract: Autonomous Underwater Vehicles (AUVs) encounter significant energy, control and navigation challenges in complex underwater environments, particularly during close-proximity operations, such as launch and recovery (LAR), where fluid interactions and wake effects present additional navigational and energy challenges. Traditional path planning methods fail to incorporate these detailed wake structures, resulting in increased energy consumption, reduced control stability, and heightened safety risks. This paper presents a novel wake-informed, 3D path planning approach that fully integrates localized wake effects and global currents into the planning algorithm. Two variants of the A* algorithm - a current-informed planner and a wake-informed planner - are created to assess its validity and two neural network models are then trained to approximate these planners for real-time applications. Both the A* planners and NN models are evaluated using important metrics such as energy expenditure, path length, and encounters with high-velocity and turbulent regions. The results demonstrate a wake-informed A* planner consistently achieves the lowest energy expenditure and minimizes encounters with high-velocity regions, reducing energy consumption by up to 11.3%. The neural network models are observed to offer computational speedup of 6 orders of magnitude, but exhibit 4.51 - 19.79% higher energy expenditures and 9.81 - 24.38% less optimal paths. These findings underscore the importance of incorporating detailed wake structures into traditional path planning algorithms and the benefits of neural network approximations to enhance energy efficiency and operational safety for AUVs in complex 3D domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance</title>
<link>https://arxiv.org/abs/2502.04350</link>
<guid>https://arxiv.org/abs/2502.04350</guid>
<content:encoded><![CDATA[
arXiv:2502.04350v2 Announce Type: replace-cross 
Abstract: Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-turn supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0 and https://huggingface.co/yongchao98.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives</title>
<link>https://arxiv.org/abs/2502.04358</link>
<guid>https://arxiv.org/abs/2502.04358</guid>
<content:encoded><![CDATA[
arXiv:2502.04358v2 Announce Type: replace-cross 
Abstract: Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regression and Forecasting of U.S. Stock Returns Based on LSTM</title>
<link>https://arxiv.org/abs/2502.05210</link>
<guid>https://arxiv.org/abs/2502.05210</guid>
<content:encoded><![CDATA[
arXiv:2502.05210v3 Announce Type: replace-cross 
Abstract: This paper analyses the investment returns of three stock sectors, Manuf, Hitec, and Other, in the U.S. stock market, based on the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model, in order to test the validity of the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model for the three sectors of the market. French five-factor model for the three sectors of the market. Also, the LSTM model is used to explore the additional factors affecting stock returns. The empirical results show that the Fama-French five-factor model has better validity for the three segments of the market under study, and the LSTM model has the ability to capture the factors affecting the returns of certain industries, and can better regress and predict the stock returns of the relevant industries. Keywords- Fama-French model; Carhart model; Factor model; LSTM model.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Differentially Private Bayesian Optimization</title>
<link>https://arxiv.org/abs/2502.06044</link>
<guid>https://arxiv.org/abs/2502.06044</guid>
<content:encoded><![CDATA[
arXiv:2502.06044v2 Announce Type: replace-cross 
Abstract: In recent years, there has been much work on scaling Bayesian Optimization to high-dimensional problems, for example hyperparameter tuning in large machine learning models. These scalable methods have been successful, finding high objective values much more quickly than traditional global Bayesian Optimization or random search-based methods. At the same time, these large models often use sensitive data, but preservation of Differential Privacy has not scaled alongside these modern Bayesian Optimization procedures. Here we develop a method to privately optimize potentially high-dimensional parameter spaces using privatized Gradient Informative Bayesian Optimization. Our theoretical results show that under suitable conditions, our method converges exponentially fast to a locally optimal parameter configuration, up to a natural privacy error. Moreover, regardless of whether the assumptions are satisfied, we prove that our algorithm maintains privacy and empirically display superior performance to existing methods in the high-dimensional hyperparameter setting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Information in Human-AI Decision-making</title>
<link>https://arxiv.org/abs/2502.06152</link>
<guid>https://arxiv.org/abs/2502.06152</guid>
<content:encoded><![CDATA[
arXiv:2502.06152v4 Announce Type: replace-cross 
Abstract: Multiple agents -- including humans and AI models -- are increasingly combined to make decisions with the expectation of achieving complementary performance, where the decisions they make together outperform those made individually. However, knowing how to improve the performance of collaborating agents is often difficult without knowing more about what particular information and strategies each agent employs. With a focus on human-AI pairings, we contribute a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflows. We present a novel explanation technique (ILIV-SHAP) that adapts SHAP explanations to highlight human-complementing information. We validate the effectiveness of the framework and ILIV-SHAP through a study of human-AI decision-making. We show that our measure of complementary information can be used to identify which AI model will best complement human decisions. We also find that presenting ILIV-SHAP with AI predictions leads to reliably greater reductions in error over non-AI assisted decisions more than vanilla SHAP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms</title>
<link>https://arxiv.org/abs/2502.08166</link>
<guid>https://arxiv.org/abs/2502.08166</guid>
<content:encoded><![CDATA[
arXiv:2502.08166v2 Announce Type: replace-cross 
Abstract: When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the reporting database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Mental Health: Exploring College Students' Well-being through Smartphone Behaviors</title>
<link>https://arxiv.org/abs/2502.08766</link>
<guid>https://arxiv.org/abs/2502.08766</guid>
<content:encoded><![CDATA[
arXiv:2502.08766v2 Announce Type: replace-cross 
Abstract: The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellFlux: Simulating Cellular Morphology Changes via Flow Matching</title>
<link>https://arxiv.org/abs/2502.09775</link>
<guid>https://arxiv.org/abs/2502.09775</guid>
<content:encoded><![CDATA[
arXiv:2502.09775v2 Announce Type: replace-cross 
Abstract: Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlux, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlux models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlux generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlux enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research. Project page: https://yuhui-zh15.github.io/CellFlux/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Refined Analysis of UCBVI</title>
<link>https://arxiv.org/abs/2502.17370</link>
<guid>https://arxiv.org/abs/2502.17370</guid>
<content:encoded><![CDATA[
arXiv:2502.17370v2 Announce Type: replace-cross 
Abstract: In this work, we provide a refined analysis of the UCBVI algorithm (Azar et al., 2017), improving both the bonus terms and the regret analysis. Additionally, we compare our version of UCBVI with both its original version and the state-of-the-art MVP algorithm. Our empirical validation demonstrates that improving the multiplicative constants in the bounds has significant positive effects on the empirical performance of the algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Neuro-Symbolic Decision Transformer</title>
<link>https://arxiv.org/abs/2503.07148</link>
<guid>https://arxiv.org/abs/2503.07148</guid>
<content:encoded><![CDATA[
arXiv:2503.07148v3 Announce Type: replace-cross 
Abstract: We present a hierarchical neuro-symbolic control framework that tightly couples a classical symbolic planner with a transformer-based policy to address long-horizon decision-making under uncertainty. At the high level, the planner assembles an interpretable sequence of operators that guarantees logical coherence with task constraints, while at the low level each operator is rendered as a sub-goal token that conditions a decision transformer to generate fine-grained actions directly from raw observations. This bidirectional interface preserves the combinatorial efficiency and explainability of symbolic reasoning without sacrificing the adaptability of deep sequence models, and it permits a principled analysis that tracks how approximation errors from both planning and execution accumulate across the hierarchy. Empirical studies in stochastic grid-world domains demonstrate that the proposed method consistently surpasses purely symbolic, purely neural and existing hierarchical baselines in both success and efficiency, highlighting its robustness for sequential tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v4 Announce Type: replace-cross 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Cascade Ranking as One Network</title>
<link>https://arxiv.org/abs/2503.09492</link>
<guid>https://arxiv.org/abs/2503.09492</guid>
<content:encoded><![CDATA[
arXiv:2503.09492v2 Announce Type: replace-cross 
Abstract: Cascade Ranking is a prevalent architecture in large-scale top-k selection systems like recommendation and advertising platforms. Traditional training methods focus on single-stage optimization, neglecting interactions between stages. Recent advances have introduced interaction-aware training paradigms, but still struggle to 1) align training objectives with the goal of the entire cascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn effective collaboration patterns for different stages. To address these challenges, we propose LCRON, which introduces a novel surrogate loss function derived from the lower bound probability that ground truth items are selected by cascade ranking, ensuring alignment with the overall objective of the system. According to the properties of the derived bound, we further design an auxiliary loss for each stage to drive the reduction of this bound, leading to a more robust and effective top-k selection. LCRON enables end-to-end training of the entire cascade ranking system as a unified network. Experimental results demonstrate that LCRON achieves significant improvement over existing methods on public benchmarks and industrial applications, addressing key limitations in cascade ranking training and significantly enhancing system performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2503.12941</link>
<guid>https://arxiv.org/abs/2503.12941</guid>
<content:encoded><![CDATA[
arXiv:2503.12941v2 Announce Type: replace-cross 
Abstract: Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at https://github.com/Ghy0501/HiDe-LLaVA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article</title>
<link>https://arxiv.org/abs/2503.16561</link>
<guid>https://arxiv.org/abs/2503.16561</guid>
<content:encoded><![CDATA[
arXiv:2503.16561v2 Announce Type: replace-cross 
Abstract: The future work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from key sections of a scientific article alongside related papers and analyze how the trends have evolved. We experimented with various Large Language Models (LLMs) and integrated Retrieval-Augmented Generation (RAG) to enhance the generation process. We incorporate a LLM feedback mechanism to improve the quality of the generated content and propose an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAG-based approach with LLM feedback outperforms other methods evaluated through qualitative and quantitative metrics. Moreover, we conduct a human evaluation to assess the LLM as an extractor and judge. The code and dataset for this project are here, code: HuggingFace
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach</title>
<link>https://arxiv.org/abs/2503.18085</link>
<guid>https://arxiv.org/abs/2503.18085</guid>
<content:encoded><![CDATA[
arXiv:2503.18085v2 Announce Type: replace-cross 
Abstract: Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Framing Analysis of News</title>
<link>https://arxiv.org/abs/2503.20960</link>
<guid>https://arxiv.org/abs/2503.20960</guid>
<content:encoded><![CDATA[
arXiv:2503.20960v3 Announce Type: replace-cross 
Abstract: Automated frame analysis of political communication is a popular task in computational social science that is used to study how authors select aspects of a topic to frame its reception. So far, such studies have been narrow, in that they use a fixed set of pre-defined frames and focus only on the text, ignoring the visual contexts in which those texts appear. Especially for framing in the news, this leaves out valuable information about editorial choices, which include not just the written article but also accompanying photographs. To overcome such limitations, we present a method for conducting multi-modal, multi-label framing analysis at scale using large (vision-) language models. Grounding our work in framing theory, we extract latent meaning embedded in images used to convey a certain point and contrast that to the text by comparing the respective frames used. We also identify highly partisan framing of topics with issue-specific frame analysis found in prior qualitative work. We demonstrate a method for doing scalable integrative framing analysis of both text and image in news, providing a more complete picture for understanding media bias.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data</title>
<link>https://arxiv.org/abs/2503.24129</link>
<guid>https://arxiv.org/abs/2503.24129</guid>
<content:encoded><![CDATA[
arXiv:2503.24129v2 Announce Type: replace-cross 
Abstract: The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or "blind", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Knowledgeable Self-awareness</title>
<link>https://arxiv.org/abs/2504.03553</link>
<guid>https://arxiv.org/abs/2504.03553</guid>
<content:encoded><![CDATA[
arXiv:2504.03553v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>am-ELO: A Stable Framework for Arena-based LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.03475</link>
<guid>https://arxiv.org/abs/2505.03475</guid>
<content:encoded><![CDATA[
arXiv:2505.03475v2 Announce Type: replace-cross 
Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay</title>
<link>https://arxiv.org/abs/2505.04209</link>
<guid>https://arxiv.org/abs/2505.04209</guid>
<content:encoded><![CDATA[
arXiv:2505.04209v2 Announce Type: replace-cross 
Abstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</title>
<link>https://arxiv.org/abs/2505.12864</link>
<guid>https://arxiv.org/abs/2505.12864</guid>
<content:encoded><![CDATA[
arXiv:2505.12864v2 Announce Type: replace-cross 
Abstract: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.14020</link>
<guid>https://arxiv.org/abs/2505.14020</guid>
<content:encoded><![CDATA[
arXiv:2505.14020v2 Announce Type: replace-cross 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
arXiv:2505.16223v2 Announce Type: replace-cross 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary</title>
<link>https://arxiv.org/abs/2505.18325</link>
<guid>https://arxiv.org/abs/2505.18325</guid>
<content:encoded><![CDATA[
arXiv:2505.18325v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries-a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models'safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios.We have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets will be released at https://anonymous.4open.science/r/RASS-80D3.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training</title>
<link>https://arxiv.org/abs/2505.18377</link>
<guid>https://arxiv.org/abs/2505.18377</guid>
<content:encoded><![CDATA[
arXiv:2505.18377v2 Announce Type: replace-cross 
Abstract: DONNs leverage light propagation for efficient analog AI and signal processing. Advances in nanophotonic fabrication and metasurface-based wavefront engineering have opened new pathways to realize high-capacity DONNs across various spectral regimes. Training such DONN systems to determine the metasurface structures remains challenging. Heuristic methods are fast but oversimplify metasurfaces modulation, often resulting in physically unrealizable designs and significant performance degradation. Simulation-in-the-loop optimizes implementable metasurfaces via adjoint methods, but is computationally prohibitive and unscalable. To address these limitations, we propose SP2RINT, a spatially decoupled, progressive training framework that formulates DONN training as a PDE-constrained learning problem. Metasurface responses are first relaxed into freely trainable transfer matrices with a banded structure. We then progressively enforce physical constraints by alternating between transfer matrix training and adjoint-based inverse design, avoiding per-iteration PDE solves while ensuring final physical realizability. To further reduce runtime, we introduce a physics-inspired, spatially decoupled inverse design strategy based on the natural locality of field interactions. This approach partitions the metasurface into independently solvable patches, enabling scalable and parallel inverse design with system-level calibration. Evaluated across diverse DONN training tasks, SP2RINT achieves digital-comparable accuracy while being 1825 times faster than simulation-in-the-loop approaches. By bridging the gap between abstract DONN models and implementable photonic hardware, SP2RINT enables scalable, high-performance training of physically realizable meta-optical neural systems. Our code is available at https://github.com/ScopeX-ASU/SP2RINT
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model</title>
<link>https://arxiv.org/abs/2505.18486</link>
<guid>https://arxiv.org/abs/2505.18486</guid>
<content:encoded><![CDATA[
arXiv:2505.18486v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely explored for automated scoring in low-stakes assessment to facilitate learning and instruction. Empirical evidence related to which LLM produces the most reliable scores and induces least rater effects needs to be collected before the use of LLMs for automated scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini 2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in scoring two types of writing tasks. The accuracy of the holistic and analytic scores from LLMs compared with human raters was evaluated in terms of Quadratic Weighted Kappa. Intra-rater consistency across prompts was compared in terms of Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human raters using the Many-Facet Rasch model. The results in general supported the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring accuracy, better rater reliability, and less rater effects.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations</title>
<link>https://arxiv.org/abs/2505.19164</link>
<guid>https://arxiv.org/abs/2505.19164</guid>
<content:encoded><![CDATA[
arXiv:2505.19164v2 Announce Type: replace-cross 
Abstract: In the domain of sponsored search advertising, the focus of Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.3 billion items.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change</title>
<link>https://arxiv.org/abs/2505.19328</link>
<guid>https://arxiv.org/abs/2505.19328</guid>
<content:encoded><![CDATA[
arXiv:2505.19328v2 Announce Type: replace-cross 
Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can play a critical role in the personalization and effectiveness of digital behaviour change interventions. These subtle and conflicting emotions are manifested by a discord between multiple modalities, such as facial and vocal expressions, and body language. Although experts can be trained to identify A/H, integrating them into digital interventions is costly and less effective. Automatic learning systems provide a cost-effective alternative that can adapt to individual users, and operate seamlessly within real-time, and resource-limited environments. However, there are currently no datasets available for the design of ML models to recognize A/H. This paper introduces a first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for subject-based multimodal recognition of A/H in videos. It contains videos from 224 participants captured across 9 provinces in Canada, with different age, and ethnicity. Through our web platform, we recruited participants to answer 7 questions, some of which were designed to elicit A/H while recording themselves via webcam with microphone. BAH amounts to 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp segments to indicate where A/H occurs, and provide frame- and video-level annotations with the A/H cues. Video transcripts and their timestamps are also included, along with cropped and aligned faces in each frame, and a variety of participants meta-data. We include results baselines for BAH at frame- and video-level recognition in multi-modal setups, in addition to zero-shot prediction, and for personalization using unsupervised domain adaptation. The limited performance of baseline models highlights the challenges of recognizing A/H in real-world videos. The data, code, and pretrained weights are available.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2505.19205</link>
<guid>https://arxiv.org/abs/2505.19205</guid>
<content:encoded><![CDATA[
<div> Google, hyperparameter optimization, OptiMindTune, multi-agent framework, machine learning

Summary: 
The paper presents OptiMindTune, a multi-agent framework utilizing three specialized AI agents - Recommender Agent, Evaluator Agent, and Decision Agent - powered by Google's Gemini models for hyperparameter optimization. By enabling collaborative intelligence among these agents, OptiMindTune aims to efficiently optimize hyperparameters by addressing various aspects of the HPO problem such as model selection, hyperparameter suggestions, evaluation, and decision-making. The framework integrates principles from advanced large language models and adaptive search to achieve scalable and intelligent AutoML. It aims to converge to optimal hyperparameter configurations quickly and robustly compared to traditional methods. The multi-agent paradigm opens up new possibilities for tackling the complexities of modern machine learning model tuning. <div>
arXiv:2505.19205v2 Announce Type: replace 
Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of machine learning model development, significantly impacting model performance and generalization. Traditional HPO methods often struggle with high dimensionality, complex interdependencies, and computational expense. This paper introduces OptiMindTune, a novel multi-agent framework designed to intelligently and efficiently optimize hyperparameters. OptiMindTune leverages the collaborative intelligence of three specialized AI agents -- a Recommender Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's Gemini models. These agents address distinct facets of the HPO problem, from model selection and hyperparameter suggestion to robust evaluation and strategic decision-making. By fostering dynamic interactions and knowledge sharing, OptiMindTune aims to converge to optimal hyperparameter configurations more rapidly and robustly than existing single-agent or monolithic approaches. Our framework integrates principles from advanced large language models, and adaptive search to achieve scalable and intelligent AutoML. We posit that this multi-agent paradigm offers a promising avenue for tackling the increasing complexity of modern machine learning model tuning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</title>
<link>https://arxiv.org/abs/2505.19255</link>
<guid>https://arxiv.org/abs/2505.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning Finetuning, large language models, multimodal reasoning, Visual Sketchpad, VTool-R1 <br />
Summary: 
The article introduces VTool-R1, a framework that enhances the reasoning capabilities of large language models (LLMs) by incorporating visual reasoning steps. By interleaving text and visual steps during training, VTool-R1 enables LLMs to generate multimodal chains of thought. This approach integrates Python-based visual editing tools into the Reinforcement Learning Finetuning process, allowing LLMs to strategically utilize visual reasoning in their responses. The framework is trained using outcome-based rewards tied to task accuracy, enabling LLMs to "think with images" and improve reasoning performance. Experiments conducted on structured visual question answering tasks demonstrate that VTool-R1 significantly enhances the ability of LLMs to generate multimodal chains of thoughts by utilizing visual reasoning tools effectively. <div>
arXiv:2505.19255v2 Announce Type: replace 
Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use. While recent works attempt to extend RFT to vision-language models (VLMs), these efforts largely produce text-only reasoning conditioned on static image inputs, falling short of true multimodal reasoning in the response. In contrast, test-time methods like Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate multimodal chains of thought by interleaving text and intermediate visual reasoning steps. VTool-R1 integrates Python-based visual editing tools into the RFT process, enabling VLMs to learn when and how to generate visual reasoning steps that benefit final reasoning. Trained with outcome-based rewards tied to task accuracy, our approach elicits strategic visual tool use for reasoning without relying on process-based supervision. Experiments on structured visual question answering over charts and tables show that VTool-R1 enhances reasoning performance by teaching VLMs to "think with images" and generate multimodal chain of thoughts with tools.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large Reasoning Models for Agriculture</title>
<link>https://arxiv.org/abs/2505.19259</link>
<guid>https://arxiv.org/abs/2505.19259</guid>
<content:encoded><![CDATA[
<div> Keywords: Agricultural reasoning, Large reasoning models, AgReason, AgThoughts, AgThinker

Summary:<br /><br />Agricultural decision-making is a complex process that relies on various factors such as geography, climate, and economics. Traditional large language models struggle to navigate this complexity, leading researchers to explore the use of large reasoning models. The AgReason project introduces an expert-curated benchmark with 100 questions for agricultural reasoning, showing that large reasoning models outperform conventional ones. The AgThoughts dataset, with 44.6K question-answer pairs, is presented to improve agricultural reasoning abilities in language models. The AgThinker suite of small reasoning models, designed for consumer-grade GPUs, demonstrates the effectiveness of the AgThoughts dataset in enhancing agricultural reasoning capabilities in language models. This research highlights the potential of using large reasoning models in agricultural decision-making processes. <br /><br />Summary: <div>
arXiv:2505.19259v2 Announce Type: replace 
Abstract: Agricultural decision-making involves complex, context-specific reasoning, where choices about crops, practices, and interventions depend heavily on geographic, climatic, and economic conditions. Traditional large language models (LLMs) often fall short in navigating this nuanced problem due to limited reasoning capacity. We hypothesize that recent advances in large reasoning models (LRMs) can better handle such structured, domain-specific inference. To investigate this, we introduce AgReason, the first expert-curated open-ended science benchmark with 100 questions for agricultural reasoning. Evaluations across thirteen open-source and proprietary models reveal that LRMs outperform conventional ones, though notable challenges persist, with the strongest Gemini-based baseline achieving 36% accuracy. We also present AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with human oversight and equipped with synthetically generated reasoning traces. Using AgThoughts, we develop AgThinker, a suite of small reasoning models that can be run on consumer-grade GPUs, and show that our dataset can be effective in unlocking agricultural reasoning abilities in LLMs. Our project page is here: https://baskargroup.github.io/Ag_reasoning/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19698</link>
<guid>https://arxiv.org/abs/2505.19698</guid>
<content:encoded><![CDATA[
<div> Keywords: model-based reinforcement learning, diffusion world models, Atari100k benchmark, Joint Embedding Diffusion, human-level performance

Summary:
Recent advancements in model-based reinforcement learning have led to super-human performance on the Atari100k benchmark. However, there exists a performance asymmetry where MBRL agents excel in some tasks but underperform in others, skewing aggregate metrics. Pixel-based agents trained with diffusion world models exhibit this asymmetry more prominently. To address this issue, tasks are categorized as Agent-Optimal or Human-Optimal, highlighting the need for balanced metrics. The lack of temporally-structured latent space in pixel-based methods is identified as a contributor to the asymmetry. A new model, Joint Embedding Diffusion (JEDI), is proposed to tackle this problem. JEDI outperforms current models in human-optimal tasks, remains competitive in the Atari100k benchmark, and boasts improved efficiency in terms of speed and memory usage. This work challenges the traditional definition of crossing human-level performance in the Atari100k domain.<br /><br />Summary: <div>
arXiv:2505.19698v2 Announce Type: replace 
Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning</title>
<link>https://arxiv.org/abs/2505.20135</link>
<guid>https://arxiv.org/abs/2505.20135</guid>
<content:encoded><![CDATA[
<div> Keywords: Replay-based continual learning, memory buffer, dataset distillation, global information, forgetting mitigation

Summary:
Replay-based continual learning methods often struggle with consolidating past knowledge due to limited memory buffer capacity and heuristic data selection. To address this challenge, a new dataset distillation framework for continual learning is proposed in this study. The framework employs a learnable memory buffer to distill global information from current task data and accumulated knowledge from the previous memory buffer. A lightweight distillation module is introduced to generate soft labels for memory buffer data, minimizing computational overhead and overfitting risks. Experimental results demonstrate that the proposed method achieves competitive performance and effectively prevents forgetting across various datasets. The source code for the method will be made publicly available. <br /><br />Summary: <div>
arXiv:2505.20135v2 Announce Type: replace 
Abstract: Replay-based continual learning (CL) methods assume that models trained on a small subset can also effectively minimize the empirical risk of the complete dataset. These methods maintain a memory buffer that stores a sampled subset of data from previous tasks to consolidate past knowledge. However, this assumption is not guaranteed in practice due to the limited capacity of the memory buffer and the heuristic criteria used for buffer data selection. To address this issue, we propose a new dataset distillation framework tailored for CL, which maintains a learnable memory buffer to distill the global information from the current task data and accumulated knowledge preserved in the previous memory buffer. Moreover, to avoid the computational overhead and overfitting risks associated with parameterizing the entire buffer during distillation, we introduce a lightweight distillation module that can achieve global information distillation solely by generating learnable soft labels for the memory buffer data. Extensive experiments show that, our method can achieve competitive results and effectively mitigates forgetting across various datasets. The source code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</title>
<link>https://arxiv.org/abs/2505.19663</link>
<guid>https://arxiv.org/abs/2505.19663</guid>
<content:encoded><![CDATA[
<div> Benchmark, deep learning, audio watermarking, neural compression, audio attacks  
Summary:  
- The article introduces the Robust Audio Watermarking Benchmark (RAW-Bench) for evaluating deep learning-based audio watermarking methods with standardized comparisons.  
- A comprehensive audio attack pipeline is introduced to simulate real-world usage, including various distortions like compression, background noise, and reverberation.  
- Four existing watermarking methods were evaluated on RAW-bench, revealing challenges posed by neural compression techniques and the importance of training with audio attacks for improved robustness.  
- Specific distortions such as polarity inversion, time stretching, or reverb were found to significantly affect certain methods.  
- The evaluation framework is accessible on github.com/SonyResearch/raw_bench.  

<br /><br />Summary: <div>
arXiv:2505.19663v2 Announce Type: replace-cross 
Abstract: We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark for evaluating deep learning-based audio watermarking methods with standardized and systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline with various distortions such as compression, background noise, and reverberation, along with a diverse test dataset including speech, environmental sounds, and music recordings. Evaluating four existing watermarking methods on RAW-bench reveals two main insights: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain methods. The evaluation framework is accessible at github.com/SonyResearch/raw_bench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows</title>
<link>https://arxiv.org/abs/2505.21512</link>
<guid>https://arxiv.org/abs/2505.21512</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graphs, Large language models, KG exploration, User trust, Visualization

Summary:<br /><br />
The study explores the interaction between users and knowledge graphs (KGs) using large language models (LLMs) for exploration. The LinkQ system was developed to convert natural language questions into structured queries with LLMs. Visual mechanisms were designed to help users assess the accuracy of KG queries and LLM responses. Findings from a qualitative evaluation with practitioners revealed that users tended to overtrust the system's outputs, even when incorrect, due to the helpful visualizations. Users showed distinct workflows based on their familiarity with KGs and LLMs, challenging the assumption of one-size-fits-all systems. The study highlights the risks of false trust in LLM-assisted tools and emphasizes the need for further research on visualization as a mitigation strategy. <div>
arXiv:2505.21512v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them effectively remains difficult for even expert users. Large language models (LLMs) are increasingly used to address this gap, yet little is known empirically about how their usage with KGs shapes user trust, exploration strategies, or downstream decision-making - raising key design challenges for LLM-based KG visual analysis systems. To study these effects, we developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM. We collaborated with KG experts to design five visual mechanisms that help users assess the accuracy of both KG queries and LLM responses: an LLM-KG state diagram that illustrates which stage of the exploration pipeline LinkQ is in, a query editor displaying the generated query paired with an LLM explanation, an entity-relation ID table showing extracted KG entities and relations with semantic descriptions, a query structure graph that depicts the path traversed in the KG, and an interactive graph visualization of query results. From a qualitative evaluation with 14 practitioners, we found that users - even KG experts - tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all - despite often being designed as if they are. Our findings highlight the risks of false trust in LLM-assisted data analysis tools and the need for further investigation into the role of visualization as a mitigation technique.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation</title>
<link>https://arxiv.org/abs/2505.21514</link>
<guid>https://arxiv.org/abs/2505.21514</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, coding assistants, benchmark, Java, Python

Summary:<br />
The article introduces SIMCOPILOT, a benchmark that simulates the role of large language models as interactive coding assistants, targeting completion and infill tasks in Java and Python. The benchmark provides a comprehensive framework for evaluating LLM coding capabilities in practical scenarios, addressing critical factors like task-specific performance nuances and contextual understanding. Evaluations across different domains highlight model strengths and challenges in maintaining logical consistency within complex code structures. The study emphasizes the limitations of LLM-driven code generation and the evolving role of LLMs as intelligent software development partners. Overall, the benchmark offers detailed insights into the utility and performance of large language models in coding environments.<br /> <div>
arXiv:2505.21514v1 Announce Type: new 
Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large language models (LLMs) as interactive, "copilot"-style coding assistants. Targeting both completion (finishing incomplete methods or code blocks) and infill tasks (filling missing segments within existing code), SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities. The benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our key contributions include: (a) establishing a realistic, detailed evaluation environment to assess LLM utility in practical coding scenarios, and (b) providing fine-grained analyses that address critical factors frequently overlooked by existing benchmarks, such as task-specific performance nuances, contextual understanding across code segments, and sensitivity to variable scope. Evaluations conducted across domains-including algorithms, databases, computer vision, and neural networks-offer insights into model strengths and highlight persistent challenges in maintaining logical consistency within complex dependency structures. Beyond benchmarking, our study sheds light on the current limitations of LLM-driven code generation and underscores the ongoing transition of LLMs from merely syntax-aware generators toward reliable, intelligent software development partners.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.21525</link>
<guid>https://arxiv.org/abs/2505.21525</guid>
<content:encoded><![CDATA[
<div> Spatial-temporal feature encoder, Multivariate time series, Domain adaptation, Data privacy, Temporal restoration<br />
<br />
Summary:
The paper introduces a new Source-Free Domain Adaptation (SFDA) method called Temporal Restoration and Spatial Rewiring (TERSE) designed specifically for multivariate time series (MTS) data. Traditional SFDA methods struggle with MTS due to a lack of consideration for spatial correlations inherent in the data. TERSE addresses this challenge by utilizing a customized spatial-temporal feature encoder and tasks for temporal restoration and spatial rewiring to capture and transfer spatial-temporal dependencies across domains. By guiding the target encoder to produce consistent features with the source domain, TERSE facilitates implicit feature alignment. It can also be easily integrated into existing SFDA methods as a plug-and-play module. Extensive experiments on real-world time series datasets demonstrate the effectiveness and versatility of TERSE in preserving data privacy and accurately adapting pre-trained models to unlabelled target domains. <div>
arXiv:2505.21525v1 Announce Type: new 
Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</title>
<link>https://arxiv.org/abs/2505.21569</link>
<guid>https://arxiv.org/abs/2505.21569</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Chemistry tasks, Agent-stacking, Prediction errors, AI agent applications

Summary: ChemHAS (Chemical Hierarchical Agent Stacking) is a new method proposed in this paper to enhance chemistry tools by optimizing agent-stacking structures using limited data. By leveraging Large Language Model-based agents, ChemHAS aims to reduce prediction errors of chemistry tools, leading to state-of-the-art performance in fundamental chemistry tasks. The method identifies four distinct agent-stacking behaviors, improving interpretability and opening up new possibilities for AI agent applications in scientific research. The code and dataset for ChemHAS are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.21569v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition</title>
<link>https://arxiv.org/abs/2505.21571</link>
<guid>https://arxiv.org/abs/2505.21571</guid>
<content:encoded><![CDATA[
<div> Keywords: wireless communications, deep learning, Automatic Modulation Recognition, model pruning, channel-level pruning

Summary:<br /><br />
The paper introduces FCOS, a novel Fine-to-Coarse two-Stage pruning framework for Automatic Modulation Recognition (AMR) in wireless communications. FCOS combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression and high performance while maintaining accuracy. In the first stage, hierarchical clustering and parameter fusion are used for channel-level pruning, followed by a Layer Collapse Diagnosis (LaCD) module to identify and remove collapsed layers. Experimental results show that FCOS outperforms existing methods, achieving a 95.51% reduction in FLOPs and 95.31% reduction in parameters while only experiencing a 0.46% drop in accuracy on Sig2019-12 benchmark. The code is available on GitHub for further exploration.  <div>
arXiv:2505.21571v1 Announce Type: new 
Abstract: With the rapid development of wireless communications and the growing complexity of digital modulation schemes, traditional manual modulation recognition methods struggle to extract reliable signal features and meet real-time requirements in modern scenarios. Recently, deep learning based Automatic Modulation Recognition (AMR) approaches have greatly improved classification accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained devices. Model pruning provides a general approach to reduce model complexity, but existing weight, channel, and layer pruning techniques each present a trade-off between compression rate, hardware acceleration, and accuracy preservation. To this end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning framework that combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression, high performance and efficient inference. In the first stage of FCOS, hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning. Then a Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms existing channel and layer pruning methods. Specifically, FCOS achieves 95.51% FLOPs reduction and 95.31% parameter reduction while still maintaining performance close to the original ResNet56, with only a 0.46% drop in accuracy on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</title>
<link>https://arxiv.org/abs/2505.21573</link>
<guid>https://arxiv.org/abs/2505.21573</guid>
<content:encoded><![CDATA[
<div> frequency domain, neural PDE solvers, physics-aware methods, spectral representations, Navier-Stokes equations <br />
Summary: 
The Spectral-inspired Neural Operator (SINO) framework is introduced for learning PDE operators from limited trajectories without known PDE terms. SINO operates in the frequency domain and utilizes a Frequency-to-Vector module for spectral representations. It incorporates a nonlinear operator block with a $\Pi$-Block for modeling nonlinear interactions and introduces operator distillation for efficient inference. SINO outperforms existing methods in PDE benchmarks, showing strong discretization invariance and generalization to out-of-distribution initial conditions. Notably, SINO can accurately simulate globally coupled systems, such as the Navier-Stokes equations, with limited data and no explicit PDE terms present. <div>
arXiv:2505.21573v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $\Pi$-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concentration Distribution Learning from Label Distributions</title>
<link>https://arxiv.org/abs/2505.21576</link>
<guid>https://arxiv.org/abs/2505.21576</guid>
<content:encoded><![CDATA[
<div> Keywords: Label distribution learning, background concentration, concentration distribution learning, probabilistic methods, neural networks

Summary:<br /><br />The article introduces a new concept called background concentration to enhance label distribution learning (LDL) by incorporating the absolute intensity of each label. By considering the total description degree of hidden labels not in the label space, the proposed concentration distribution learning improves the representation of instances and minimizes information loss. A novel model combining probabilistic methods and neural networks is developed to learn both label distributions and background concentrations from existing LDL datasets. Experimental results demonstrate that the approach effectively extracts background concentrations and yields more accurate prediction results compared to existing LDL methods. The code for the proposed method is publicly available on GitHub for further research and implementation. <div>
arXiv:2505.21576v1 Announce Type: new 
Abstract: Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in https://github.com/seutjw/CDL-LD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness in Federated Learning: Fairness for Whom?</title>
<link>https://arxiv.org/abs/2505.21584</link>
<guid>https://arxiv.org/abs/2505.21584</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, federated learning, sociotechnical contexts, harms, stakeholders 

Summary: 
This paper discusses the limitations of current approaches to fairness in federated learning, highlighting five key pitfalls. Firstly, fairness is often defined solely through the server-client architecture perspective. Secondly, there is a mismatch between simulated scenarios and real-world contexts. Thirdly, definitions often prioritize protecting the system over protecting its users. Fourthly, interventions target specific stages of the lifecycle, neglecting broader impacts. Lastly, there is a lack of alignment among multiple stakeholders with different fairness definitions. The paper proposes a harm-centered framework to address these issues, connecting fairness definitions to concrete risks and stakeholder vulnerabilities. Recommendations are made for more holistic, context-aware, and accountable fairness research in federated learning. 

<br /><br />Summary: <div>
arXiv:2505.21584v1 Announce Type: new 
Abstract: Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.21587</link>
<guid>https://arxiv.org/abs/2505.21587</guid>
<content:encoded><![CDATA[
<div> Cellular Complex Contrastive Learning, Adaptive Trimming, self-supervised, topological deep learning, cellular complexes <br />
<br />
Summary: 
Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT) is a novel framework for self-supervised topological deep learning on cellular complexes. It addresses challenges such as extrinsic structural constraints and intrinsic semantic redundancy in cellular representations. CellCLAT utilizes parameter perturbation-based augmentation to preserve cellular topology during contrastive learning while masking task-irrelevant cells through a cellular trimming scheduler. This approach enhances the representation of unlabeled graphs by capturing higher-order interactions in cellular complexes more effectively. The framework demonstrates significant improvements over existing self-supervised graph learning methods, making a valuable contribution to the field of topological deep learning. <div>
arXiv:2505.21587v1 Announce Type: new 
Abstract: Self-supervised topological deep learning (TDL) represents a nascent but underexplored area with significant potential for modeling higher-order interactions in simplicial complexes and cellular complexes to derive representations of unlabeled graphs. Compared to simplicial complexes, cellular complexes exhibit greater expressive power. However, the advancement in self-supervised learning for cellular TDL is largely hindered by two core challenges: \textit{extrinsic structural constraints} inherent to cellular complexes, and intrinsic semantic redundancy in cellular representations. The first challenge highlights that traditional graph augmentation techniques may compromise the integrity of higher-order cellular interactions, while the second underscores that topological redundancy in cellular complexes potentially diminish task-relevant information. To address these issues, we introduce Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), a twofold framework designed to adhere to the combinatorial constraints of cellular complexes while mitigating informational redundancy. Specifically, we propose a parameter perturbation-based augmentation method that injects controlled noise into cellular interactions without altering the underlying cellular structures, thereby preserving cellular topology during contrastive learning. Additionally, a cellular trimming scheduler is employed to mask gradient contributions from task-irrelevant cells through a bi-level meta-learning approach, effectively removing redundant topological elements while maintaining critical higher-order semantics. We provide theoretical justification and empirical validation to demonstrate that CellCLAT achieves substantial improvements over existing self-supervised graph learning methods, marking a significant attempt in this domain.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.21591</link>
<guid>https://arxiv.org/abs/2505.21591</guid>
<content:encoded><![CDATA[
<div> Keywords: model quantization, diffusion models, floating-point quantization, fine-tuning, denoising

Summary:
Model quantization aims to reduce the bit-width of weights and activations in diffusion models for improved memory efficiency and faster inference speed. However, achieving 4-bit quantization has been challenging. Existing methods based on integer quantization and post-training quantization fine-tuning have shown inconsistent performance. This study explores low-bit floating-point (FP) quantization for diffusion models and identifies key challenges such as handling asymmetric activation distributions and considering temporal complexity during the denoising process. To address these challenges, the mixup-sign floating-point quantization (MSFP) framework is proposed, which introduces unsigned FP quantization and incorporates timestep-aware techniques and denoising-factor loss alignment for precise and stable fine-tuning. Experimental results demonstrate superior performance in 4-bit FP quantization for diffusion models compared to existing methods in 4-bit integer quantization. 

<br /><br />Summary: <div>
arXiv:2505.21591v1 Announce Type: new 
Abstract: Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance-driven Input Dropout: an Explanation-guided Regularization Technique</title>
<link>https://arxiv.org/abs/2505.21595</link>
<guid>https://arxiv.org/abs/2505.21595</guid>
<content:encoded><![CDATA[
<div> Keywords: overfitting, machine learning, data augmentation, occlusion, generalization 

Summary: 
The article addresses the issue of overfitting in machine learning models, proposing a novel data augmentation technique called Relevance-driven Input Dropout (RelDrop). Unlike existing methods that randomly mask input features, RelDrop selectively occludes the most relevant regions of the input to encourage the model to utilize other important features during training. Through qualitative and quantitative analyses, the study demonstrates that RelDrop improves model robustness towards occlusion, encourages the use of more features within the region of interest, and enhances generalization performance at inference time. Experimental results on benchmark datasets highlight the effectiveness of the proposed approach in mitigating overfitting and reducing the train-test performance gap. The code implementation for RelDrop is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.21595v1 Announce Type: new 
Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</title>
<link>https://arxiv.org/abs/2505.21605</link>
<guid>https://arxiv.org/abs/2505.21605</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, safety benchmark, hazardous scenarios, SOSBench, model alignment deficiencies

Summary:
The article introduces SOSBench, a new benchmark designed to evaluate the safety and resilience of large language models (LLMs) when handling high-risk scientific scenarios in domains such as chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark consists of 3,000 prompts derived from real-world regulations and laws, including detailed and complex misuse scenarios. The evaluation of state-of-the-art models using SOSBench reveals significant deficiencies in model alignment with safety regulations, with high rates of harmful responses detected across all domains. Advanced models like Deepseek-R1 and GPT-4.1 exhibit alarming levels of policy-violating content, raising urgent concerns about the responsible deployment of powerful LLMs. SOSBench highlights the need for improved safety benchmarks and increased attention to the risks associated with the misuse of advanced language models. 

<br /><br />Summary: <div>
arXiv:2505.21605v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Where to Learn: Training Distribution Selection for Provable OOD Performance</title>
<link>https://arxiv.org/abs/2505.21626</link>
<guid>https://arxiv.org/abs/2505.21626</guid>
<content:encoded><![CDATA[
<div> distribution-aware training, out-of-distribution generalization, machine learning, optimization, probability measures<br />
Summary:<br />
- The study focuses on addressing the challenge of out-of-distribution (OOD) generalization in machine learning, where models often experience performance degradation on shifted or unseen domains. 
- A theoretical analysis establishes generalization bounds that quantify how the choice of training distribution affects OOD error. 
- Two algorithmic strategies are proposed: formulating OOD risk minimization as a bilevel optimization problem over probability measures and minimizing an upper bound on OOD error. 
- Evaluations across various examples show that the proposed methods significantly enhance OOD accuracy compared to standard empirical risk minimization. 
- Distribution-aware training emerges as a promising framework for robust OOD generalization in machine learning. <br /> 

Summary: <div>
arXiv:2505.21626v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in machine learning. Models trained on one data distribution often experience substantial performance degradation when evaluated on shifted or unseen domains. To address this challenge, the present paper studies the design of training data distributions that maximize average-case OOD performance. First, a theoretical analysis establishes a family of generalization bounds that quantify how the choice of training distribution influences OOD error across a predefined family of target distributions. These insights motivate the introduction of two complementary algorithmic strategies: (i) directly formulating OOD risk minimization as a bilevel optimization problem over the space of probability measures and (ii) minimizing a theoretical upper bound on OOD error. Last, the paper evaluates the two approaches across a range of function approximation and operator learning examples. The proposed methods significantly improve OOD accuracy over standard empirical risk minimization with a fixed distribution. These results highlight the potential of distribution-aware training as a principled and practical framework for robust OOD generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apprenticeship learning with prior beliefs using inverse optimization</title>
<link>https://arxiv.org/abs/2505.21639</link>
<guid>https://arxiv.org/abs/2505.21639</guid>
<content:encoded><![CDATA[
<div> framework, inverse reinforcement learning, inverse optimization, Markov decision processes, apprenticeship learning 
<br />
Summary: 
In this paper, the relationship between inverse reinforcement learning (IRL), inverse optimization (IO) for Markov decision processes (MDPs), and apprenticeship learning (AL) is investigated. The authors incorporate prior beliefs on the cost function structure into IRL and AL, showing that AL can be seen as a relaxation of their framework. By formulating AL as a regularized min-max problem, they address the ill-posedness of IRL using a regularizer to guide the search for plausible cost functions. Stochastic mirror descent (SMD) is employed to solve the resulting regularized-convex-concave-min-max problem, with convergence bounds established. Numerical experiments underscore the importance of regularization in learning cost vectors and apprentice policies. <div>
arXiv:2505.21639v1 Announce Type: new 
Abstract: The relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for Markov decision processes (MDPs) has been relatively underexplored in the literature, despite addressing the same problem. In this work, we revisit the relationship between the IO framework for MDPs, IRL, and apprenticeship learning (AL). We incorporate prior beliefs on the structure of the cost function into the IRL and AL problems, and demonstrate that the convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a relaxation of our framework. Notably, the AL formalism is a special case in our framework when the regularization term is absent. Focusing on the suboptimal expert setting, we formulate the AL problem as a regularized min-max problem. The regularizer plays a key role in addressing the ill-posedness of IRL by guiding the search for plausible cost functions. To solve the resulting regularized-convex-concave-min-max problem, we use stochastic mirror descent (SMD) and establish convergence bounds for the proposed method. Numerical experiments highlight the critical role of regularization in learning cost vectors and apprentice policies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diffusion Models for Symmetric Manifolds</title>
<link>https://arxiv.org/abs/2505.21640</link>
<guid>https://arxiv.org/abs/2505.21640</guid>
<content:encoded><![CDATA[
<div> framework, efficient diffusion models, symmetric-space Riemannian manifolds, spatially-varying covariance, training algorithm 

Summary:<br /><br />Researchers introduce a framework for designing efficient diffusion models on symmetric-space Riemannian manifolds like the torus, sphere, special orthogonal group, and unitary group. They develop a new diffusion model with a spatially-varying covariance, allowing them to bypass heat kernel computations by leveraging a projection of Euclidean Brownian motion. The training algorithm minimizes a novel efficient objective derived through Ito's Lemma, running in O(1) gradient evaluations and nearly-linear-in-d (O(d^1.19)) arithmetic operations per step. The model's manifold symmetries ensure an "average-case" Lipschitz condition, facilitating accurate and efficient sample generation. Empirical results demonstrate that their model outperforms previous methods in training speed and enhances sample quality on synthetic datasets across various symmetric manifolds. <div>
arXiv:2505.21640v1 Announce Type: new 
Abstract: We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</title>
<link>https://arxiv.org/abs/2505.21641</link>
<guid>https://arxiv.org/abs/2505.21641</guid>
<content:encoded><![CDATA[
<div> framework, CIs, ATE, treatment effects, differential privacy  
Summary:  
- The article introduces PrivATE, a machine learning framework for computing Confidence Intervals (CIs) for the Average Treatment Effect (ATE) under differential privacy.  
- Privacy is crucial in medical data analysis, and PrivATE allows valid uncertainty quantification while keeping sensitive data private.  
- PrivATE consists of three steps: estimating a differentially private ATE, estimating a differentially private variance, and constructing CIs that account for uncertainty from both steps.  
- The framework is model agnostic, doubly robust, and ensures valid CIs for the ATE.  
- The authors demonstrate the effectiveness of PrivATE using synthetic and real-world medical datasets, showing it is the first general, doubly robust framework for valid CIs of ATE under differential privacy.<br /><br />Summary: <div>
arXiv:2505.21641v1 Announce Type: new 
Abstract: The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2505.21651</link>
<guid>https://arxiv.org/abs/2505.21651</guid>
<content:encoded><![CDATA[
<div> Keywords: learning rate, stochastic gradient descent, AutoSGD, convergence, optimization 

Summary: 
AutoSGD is a novel stochastic gradient descent (SGD) method that automatically adjusts the learning rate at each iteration, eliminating the need for manual tuning. It determines whether to increase or decrease the learning rate based on the current iteration's performance. The method is supported by theoretical convergence guarantees and has a deterministic counterpart for standard gradient descent. Empirical results demonstrate the effectiveness of AutoSGD across various optimization problems and machine learning tasks. This approach addresses the challenge of selecting an optimal learning rate schedule and shows promising performance in practice. <div>
arXiv:2505.21651v1 Announce Type: new 
Abstract: The learning rate is an important tuning parameter for stochastic gradient descent (SGD) and can greatly influence its performance. However, appropriate selection of a learning rate schedule across all iterations typically requires a non-trivial amount of user tuning effort. To address this, we introduce AutoSGD: an SGD method that automatically determines whether to increase or decrease the learning rate at a given iteration and then takes appropriate action. We introduce theory supporting the convergence of AutoSGD, along with its deterministic counterpart for standard gradient descent. Empirical results suggest strong performance of the method on a variety of traditional optimization problems and machine learning tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGenie: An Agentic Framework for High-quality Visual Presentation Generation</title>
<link>https://arxiv.org/abs/2505.21660</link>
<guid>https://arxiv.org/abs/2505.21660</guid>
<content:encoded><![CDATA[
<div> framework, multimodal input, visual presentations, automated creation, deep learning
Summary:
PreGenie is a new framework that aims to improve the automation of creating visual presentations using multimodal large language models (MLLMs). It addresses issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding that have hindered previous attempts at automation. PreGenie operates in two stages: Analysis and Initial Generation, and Review and Re-generation, leveraging multiple MLLMs to collaborate and share information. Through comprehensive experiments, PreGenie has demonstrated superior performance in multimodal understanding, aesthetics, and content consistency compared to existing models. It also aligns closely with human design preferences, making it suitable for use in formal contexts like business and scientific research. <br /><br />Summary: <div>
arXiv:2505.21660v1 Announce Type: new 
Abstract: Visual presentations are vital for effective communication. Early attempts to automate their creation using deep learning often faced issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding, leading to mismatched visuals and text. These limitations restrict their application in formal contexts like business and scientific research. To address these challenges, we propose PreGenie, an agentic and modular framework powered by multimodal large language models (MLLMs) for generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are rendered from Markdown code. It operates in two stages: (1) Analysis and Initial Generation, which summarizes multimodal input and generates initial code, and (2) Review and Re-generation, which iteratively reviews intermediate code and rendered slides to produce final, high-quality presentations. Each stage leverages multiple MLLMs that collaborate and share information. Comprehensive experiments demonstrate that PreGenie excels in multimodal understanding, outperforming existing models in both aesthetics and content consistency, while aligning more closely with human design preferences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Controllable Diffusion via Optimal Classifier Guidance</title>
<link>https://arxiv.org/abs/2505.21666</link>
<guid>https://arxiv.org/abs/2505.21666</guid>
<content:encoded><![CDATA[
<div> diffusion models, controllable generation, reinforcement learning, supervised learning, classification<br />
<br />
Summary: 
The article introduces a method called Supervised Learning based Controllable Diffusion (SLCD) for the controllable generation of diffusion models. SLCD aims to optimize a KL-regularized objective function by iteratively generating online data and training a small classifier. It does not rely on complex concepts from reinforcement learning or control, making it more resource-efficient. The output from SLCD is proven to converge to the optimal solution of the objective under KL divergence. Empirical results show that SLCD can generate high-quality samples with minimal inference time, comparable to the base model, in tasks such as image generation and biological sequence generation. The code for SLCD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.21666v1 Announce Type: new 
Abstract: The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at https://github.com/Owen-Oertell/slcd
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What happens when generative AI models train recursively on each others' generated outputs?</title>
<link>https://arxiv.org/abs/2505.21677</link>
<guid>https://arxiv.org/abs/2505.21677</guid>
<content:encoded><![CDATA[
<div> internet, AI-generated content, genAI models, training data, data-mediated interactions 

Summary:
Data-mediated interactions between generative AI (genAI) models and training data from other models on the internet have potential consequences. This study explores the effects of such interactions, showing that models trained on others' outputs can benefit from exposure to novel concepts but may also result in homogenization of performance on shared tasks. The empirical evidence and theoretical model provided shed light on how these interactions might unfold in practice, highlighting the importance of understanding the downstream effects of data-mediated model interactions in an increasingly genAI-dependent society. <div>
arXiv:2505.21677v1 Announce Type: new 
Abstract: The internet is full of AI-generated content while also serving as a common source of training data for generative AI (genAI) models. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding downstream effects of such data-mediated model interactions is critical. To this end, we provide empirical evidence for how data-mediated interactions might unfold in practice, develop a theoretical model for this interactive training process, and show experimentally possible long-term results of such interactions. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data</title>
<link>https://arxiv.org/abs/2505.21680</link>
<guid>https://arxiv.org/abs/2505.21680</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariateGPT, mixed data, autoregressive sequence decomposition, likelihood estimation, transformer models <br />
<br />
Summary: <br />
The article introduces multivariateGPT, an architecture designed to model sequences of mixed categorical and numeric data. It addresses limitations in existing approaches by incorporating autoregressive sequence decomposition, embedding schemes, and a loss function that enables likelihood estimation of the joint distribution of next token class and value. By extending the next token prediction task, multivariateGPT efficiently generalizes patterns in simple physical systems and complex time series like electrocardiograms and multivariate electronic health record data. This advancement allows transformer models to effectively handle a wider range of data types, showcasing their versatility and potential for enhancing data analysis in real-world applications. <div>
arXiv:2505.21680v1 Announce Type: new 
Abstract: Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Permissionless Distributed Learning of LLMs</title>
<link>https://arxiv.org/abs/2505.21684</link>
<guid>https://arxiv.org/abs/2505.21684</guid>
<content:encoded><![CDATA[
<div> incentive system, distributed deep learning, Gauntlet, bittensor blockchain, pseudo-gradients

Summary:
The article introduces an incentive system called Gauntlet for distributed deep learning, implemented on the bittensor blockchain. It allows for the training of a 1.2B LLM model with permissionless contributions of pseudo-gradients from users with varying hardware. Gauntlet can be applied to synchronous distributed training schemes. The system utilizes a two-stage mechanism to filter peer reliability and synchronization, along with a core component for estimating loss before and after contributions. An OpenSkill rating system tracks the competitiveness of pseudo-gradient scores. A novel mechanism ensures unique computations by peers. The 1.2B run on the system has paid out real-valued tokens to participants based on their contributions, resulting in a competitive model. <div>
arXiv:2505.21684v1 Announce Type: new 
Abstract: We describe an incentive system for distributed deep learning of foundational models where peers are rewarded for contributions. The incentive system, \textit{Gauntlet}, has been deployed on the bittensor blockchain and used to train a 1.2B LLM with completely permissionless contributions of pseudo-gradients: no control over the users that can register or their hardware. \textit{Gauntlet} can be applied to any synchronous distributed training scheme that relies on aggregating updates or pseudo-gradients. We rely on a two-stage mechanism for fast filtering of peer uptime, reliability, and synchronization, combined with the core component that estimates the loss before and after individual pseudo-gradient contributions. We utilized an OpenSkill rating system to track competitiveness of pseudo-gradient scores across time. Finally, we introduce a novel mechanism to ensure peers on the network perform unique computations. Our live 1.2B run, which has paid out real-valued tokens to participants based on the value of their contributions, yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates the utility of our incentive system.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling</title>
<link>https://arxiv.org/abs/2505.21695</link>
<guid>https://arxiv.org/abs/2505.21695</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, communication efficiency, model accuracy, Gradient Difference Approximation, Adaptive Multi-Step Federated Learning framework

Summary: 
The paper introduces a new method called Gradient Difference Approximation (GDA) to address the challenges of balancing communication efficiency and model accuracy in federated learning. GDA utilizes first-order information to estimate local error trends without the need to compute the full Hessian matrix, resulting in a lightweight yet effective solution. This method is a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework, offering a unified error modeling strategy for large-scale multi-step adaptive training environments. By leveraging GDA within the AMSFL framework, the proposed approach aims to improve the efficiency and accuracy of federated learning algorithms. <div>
arXiv:2505.21695v1 Announce Type: new 
Abstract: Federated learning faces critical challenges in balancing communication efficiency and model accuracy. One key issue lies in the approximation of update errors without incurring high computational costs. In this paper, we propose a lightweight yet effective method called Gradient Difference Approximation (GDA), which leverages first-order information to estimate local error trends without computing the full Hessian matrix. The proposed method forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework and provides a unified error modeling strategy for large-scale multi-step adaptive training environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.21717</link>
<guid>https://arxiv.org/abs/2505.21717</guid>
<content:encoded><![CDATA[
<div> LrcSSM, nonlinear recurrent model, long sequences, fast processing, parallel solution, formal gradient-stability guarantee <br />
<br />
Summary: LrcSSM is a nonlinear recurrent model that efficiently processes long sequences with fast parallel computation. By utilizing a diagonal state-transition matrix learned at each step, it achieves $\mathcal{O}(TD)$ time and memory complexity and low $\mathcal{O}(\log T)$ sequential depth. The model offers a gradient-stability guarantee and outperforms other input-varying systems on long-range forecasting tasks. With a compute-optimal scaling law regime similar to Mamba, LrcSSM demonstrates superior performance compared to quadratic-attention Transformers while avoiding memory overhead. The forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, making it computationally efficient, and its parameter count scales optimally with depth. Overall, LrcSSM showcases impressive capabilities in processing long sequences and outperforming established models on forecasting tasks. <br /><br />Summary: <div>
arXiv:2505.21717v1 Announce Type: new 
Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape</title>
<link>https://arxiv.org/abs/2505.21722</link>
<guid>https://arxiv.org/abs/2505.21722</guid>
<content:encoded><![CDATA[
<div> escape directions, ReLU networks, saddle points, low-rank bias, optimization 

Summary: 
The study investigates the behavior of gradient descent (GD) in deep ReLU networks when initialized with small weights. It focuses on the escape directions, analogous to the eigenvectors of the Hessian for strict saddles, and their importance in navigating the parameter space dominated by saddles. The optimal escape direction is characterized by a low-rank bias in deeper layers, with the first singular value of the weight matrix increasing at least as the square root of the layer depth. The research presents results related to these escape directions and suggests that this finding is a crucial step towards understanding Saddle-to-Saddle dynamics in deep ReLU networks, where GD encounters a sequence of saddles with increasing bottleneck rank. <div>
arXiv:2505.21722v1 Announce Type: new 
Abstract: When a deep ReLU network is initialized with small weights, GD is at first dominated by the saddle at the origin in parameter space. We study the so-called escape directions, which play a similar role as the eigenvectors of the Hessian for strict saddles. We show that the optimal escape direction features a low-rank bias in its deeper layers: the first singular value of the $\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any other singular value. We also prove a number of related results about these escape directions. We argue that this result is a first step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of saddles with increasing bottleneck rank.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Agents are not even close to Human Intelligence</title>
<link>https://arxiv.org/abs/2505.21731</link>
<guid>https://arxiv.org/abs/2505.21731</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, HackAtari, task variations, performance drops, generalization testing
<br />
Summary: 
The article introduces HackAtari, a set of task variations in the Arcade Learning Environments, to evaluate the zero-shot adaptation capabilities of deep RL agents. It highlights that RL agents struggle to maintain performance on simpler versions of training tasks, unlike humans. The study reveals a consistent reliance on shortcuts by RL agents, leading to significant performance drops on simplified tasks. The analysis across various algorithms and architectures emphasizes the gap between RL agents and human intelligence. The research underscores the importance of new benchmarks and methodologies for systematic generalization testing beyond standard evaluation protocols. It suggests that training and testing in the same environment are insufficient for developing RL agents with human-like intelligence. 
<br /> <div>
arXiv:2505.21731v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing</title>
<link>https://arxiv.org/abs/2505.21732</link>
<guid>https://arxiv.org/abs/2505.21732</guid>
<content:encoded><![CDATA[
<div> Latent Crossing, Low-rank Factorization, ViTs, LLMs, Pre-training <br />
Summary: 
The research introduces Latent Crossing (LaX), a module enhancing low-rank models' capacity by enabling information flow across subspaces, overcoming limitations of traditional low-rank factorization methods. LaX significantly improves performance on pre-training tasks with ViT-Base/Large and LLaMA-like models, matching or surpassing full-rank baselines while using 2-3 times fewer parameters. When combined with low-rank adapters (LoRA) for fine-tuning larger models like LLaMA-7/13B, LaX consistently enhances performance on arithmetic and common sense reasoning tasks at minimal cost. The research showcases the effectiveness of LaX as a simple yet powerful plug-and-play solution for boosting the performance of low-rank models in various high-computational-cost training scenarios. <br /> <div>
arXiv:2505.21732v1 Announce Type: new 
Abstract: Training foundation models such as ViTs and LLMs requires tremendous computing cost. Low-rank matrix or tensor factorization offers a parameter-efficient alternative, but often downgrades performance due to the restricted parameter space. In this work, we introduce {\textbf{Latent Crossing (LaX)}} -- a simple yet effective plug-and-play module that enhances the capacity of low-rank models by enabling information flow across low-rank subspaces. We extensively validate the benefits of LaX on pre-training tasks with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters. LaX boosts low-rank model performance to match or exceed the full-rank baselines while using 2-3\(\times\) fewer parameters. When equipped with low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently improves performance on arithmetic and common sense reasoning tasks with negligible cost.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen</title>
<link>https://arxiv.org/abs/2505.21743</link>
<guid>https://arxiv.org/abs/2505.21743</guid>
<content:encoded><![CDATA[
<div> learning, counterfactual safety, near-miss events, crash prediction, Vision Zero<br />
Summary:<br />
Traffic safety science faces challenges due to limited crash data. To achieve Vision Zero, a paradigm shift towards counterfactual safety learning is needed. This involves considering potential dangerous scenarios that could have occurred. The proposed agenda combines crash-rate priors, generative scene engines, and diverse driver models to synthesize and explain near-miss events. A crash-focused digital twin testbed connects micro scenes to macro patterns, ensuring statistical realism. This transformation of sparse crash data enables proactive prevention of accidents through crash prediction and stress-testing of vehicles, roads, and policies. By learning from almost-crashes, traffic safety can shift from reactive forensics to proactive prevention, ultimately advancing Vision Zero. <br /><br />Summary: <div>
arXiv:2505.21743v1 Announce Type: new 
Abstract: Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bi-Linear State Transitions in Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2505.21749</link>
<guid>https://arxiv.org/abs/2505.21749</guid>
<content:encoded><![CDATA[
<div> hidden units, recurrent neural networks, bi-linear operations, state tracking tasks, inductive bias<br />
<br />
Summary: In this new research, the role of hidden units in recurrent neural networks is examined from a different perspective. Instead of just being memory stores, hidden units are considered as active participants in network computation. The study focuses on bi-linear operations, which involve interactions between hidden units and input embeddings. The research shows that these operations serve as a natural inductive bias for representing the evolution of hidden states in state tracking tasks. It is demonstrated both theoretically and empirically that bi-linear state updates correspond to a hierarchy of increasing complexity in state tracking tasks, with popular linear recurrent networks like Mamba at the lowest level of complexity. This study sheds new light on the importance of hidden units in network behavior and provides insights into their active role in computations. <br /><br /> <div>
arXiv:2505.21749v1 Announce Type: new 
Abstract: The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals</title>
<link>https://arxiv.org/abs/2505.21750</link>
<guid>https://arxiv.org/abs/2505.21750</guid>
<content:encoded><![CDATA[
<div> Hierarchical reinforcement learning, Diffusion model, Gaussian Process, subgoals, sample efficiency <br />
Summary: <br />
Hierarchical reinforcement learning (HRL) tackles the challenge of making decisions at multiple levels of temporal abstraction. The dynamic nature of low-level policies poses a difficulty for high-level policies to generate effective subgoals. This study introduces a novel approach that combines a conditional diffusion model with a Gaussian Process (GP) prior to generate a diverse set of subgoals while considering uncertainty. By incorporating GP uncertainty quantification, the proposed method surpasses existing HRL techniques in terms of both sample efficiency and performance on continuous control benchmarks. By selecting subgoals from the diffusion policy and GP's predictive mean, this strategy improves decision-making capabilities in complex environments. <div>
arXiv:2505.21750v1 Announce Type: new 
Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSchool: How Reliable are LLMs for Optimization Education?</title>
<link>https://arxiv.org/abs/2505.21775</link>
<guid>https://arxiv.org/abs/2505.21775</guid>
<content:encoded><![CDATA[
<div> conversion, linear program, optimization, LLMs, DualSchool

Summary: 
DualSchool introduces a framework for assessing the performance of large language models (LLMs) in generating duals of linear programs. The framework uses a verification procedure based on Canonical Graph Edit Distance, providing more accurate evaluation compared to existing methods. Experiments show that while LLMs can accurately recite the conversion procedure, they struggle to consistently produce correct duals, even for small instances. This finding has implications for educators, highlighting the limitations of relying solely on LLMs for optimization tasks. It also raises questions about the development of large reasoning systems and the need for further research in the intersection of AI and operations research. <div>
arXiv:2505.21775v1 Announce Type: new 
Abstract: Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization to Generalization: Emergence of Diffusion Models from Associative Memory</title>
<link>https://arxiv.org/abs/2505.21777</link>
<guid>https://arxiv.org/abs/2505.21777</guid>
<content:encoded><![CDATA[
<div> Hopfield networks, associative memory, diffusion models, spurious states, training data<br />
Summary:<br />
This study explores the connection between diffusion models and associative memories (AMs), specifically focusing on the phenomenon of spurious states that occur in Hopfield networks when training data reaches its critical memory load. The training phase of the diffusion model is compared to memory encoding in AMs, while the generation phase represents memory retrieval. In the small data regime, the diffusion model behaves similarly to the Hopfield model below the critical memory load, creating distinct basins of attraction around each sample. In the large data regime, new attractor states are formed corresponding to manifolds of generated samples, with spurious states emerging at the transition boundary. These spurious states exhibit distinct basins of attraction despite not being in the training set. The study offers a new perspective on memorization-generalization in diffusion models, predicts the existence of spurious states, and provides empirical validation of these predictions in commonly-used diffusion models. <br /> <div>
arXiv:2505.21777v1 Announce Type: new 
Abstract: Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\,\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-DROP: Poisson-Based Dropout for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.21783</link>
<guid>https://arxiv.org/abs/2505.21783</guid>
<content:encoded><![CDATA[
<div> Node selection strategy, Poisson processes, Graph Neural Networks, over-smoothing, structure-aware updates

Summary:
The article addresses the issue of over-smoothing in Graph Neural Networks (GNNs) by proposing a novel node selection strategy based on Poisson processes. This strategy introduces stochastic and structure-aware updates for nodes, preventing node representations from converging too quickly and losing discriminative power. By equipping each node with an independent Poisson clock, the approach allows for asynchronous and localized updates that preserve structural diversity. The study explores two applications of the Poisson-based method: as a replacement for traditional dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that the proposed method outperforms traditional approaches like Dropout, DropEdge, and DropNode, especially in later training stages. This showcases the effectiveness and competitiveness of the Poisson-based strategy in improving GNN performance. 

<br /><br />Summary: <div>
arXiv:2505.21783v1 Announce Type: new 
Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs), where repeated message passing causes node representations to converge and lose discriminative power. To address this, we propose a novel node selection strategy based on Poisson processes, introducing stochastic but structure-aware updates. Specifically, we equip each node with an independent Poisson clock, enabling asynchronous and localized updates that preserve structural diversity. We explore two applications of this strategy: as a replacement for dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that our Poisson-based method yields competitive or improved accuracy compared to traditional Dropout, DropEdge, and DropNode approaches, particularly in later training stages.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Born a Transformer -- Always a Transformer?</title>
<link>https://arxiv.org/abs/2505.21785</link>
<guid>https://arxiv.org/abs/2505.21785</guid>
<content:encoded><![CDATA[
<div> limitations, sequence-to-sequence tasks, Transformers, pretraining data, length generalization

Summary:
Pretraining of large-scale Language Models (LLMs) such as Transformers may not fully overcome theoretical limitations in modeling certain sequence-to-sequence tasks. A study of retrieval and copying tasks revealed an induction-versus-anti-induction asymmetry in pretrained models, with better performance in retrieving tokens to the right rather than the left of a query token. This asymmetry can be eliminated through targeted fine-tuning if length generalization is guaranteed. Analysis showed that the strength of induction versus anti-induction circuits within pretrained Transformers contributes to this asymmetry. Practical experiments on real-world tasks confirmed these findings and highlighted the need to consider reliability risks in utilizing pretrained models. Pretraining selectively enhances specific capabilities of Transformers but does not eliminate fundamental length-generalization limits.<br /><br />Summary: <div>
arXiv:2505.21785v1 Announce Type: new 
Abstract: Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining, by studying a family of $\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al. [2024]. We use the recently proposed C-RASP framework for studying length generalization [Huang et al., 2025b] to provide guarantees for each of our settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$ asymmetry, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained Transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain Transformer capabilities, but does not overcome fundamental length-generalization limits.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Rates for Private Adversarial Bandits</title>
<link>https://arxiv.org/abs/2505.21790</link>
<guid>https://arxiv.org/abs/2505.21790</guid>
<content:encoded><![CDATA[
<div> bandits, differential privacy, regret upper bound, algorithms, adversarial 

Summary:<br />
- New differentially private algorithms are proposed for adversarial bandits and bandits with expert advice. 
- A simple and efficient conversion method is introduced for transforming non-private bandit algorithms into private ones in the adversarial bandits problem, improving regret upper bound to $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$. 
- The algorithms allow for sublinear expected regret even with small values of epsilon, marking the first separation between central and local differential privacy for adversarial bandits. 
- The first differentially private algorithms for bandits with expert advice are presented, achieving expected regret rates of $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right)$, $O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$. 
- These rates enable sublinear regret for various combinations of small and large numbers of actions, experts, and epsilon values. <div>
arXiv:2505.21790v1 Announce Type: new 
Abstract: We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms</title>
<link>https://arxiv.org/abs/2505.21792</link>
<guid>https://arxiv.org/abs/2505.21792</guid>
<content:encoded><![CDATA[
<div> paradigms, multimodal data, Federated Learning, challenges, taxonomy
Summary: 
In the paper "Multimodal Federated Learning: A Taxonomy of Challenges and Opportunities," the authors explore the intersection of two key research areas: leveraging multimodal data to enhance inference performance and enabling distributed training for efficiency and privacy preservation. The absence of a comprehensive taxonomy for Multimodal Federated Learning (MFL) is addressed, focusing on Horizontal FL (HFL), Vertical FL (VFL), and Hybrid FL paradigms. The paper examines the problem formulations, algorithms, and challenges associated with each paradigm in the context of multimodal data. Unique challenges such as modality heterogeneity, privacy heterogeneity, and communication inefficiency are discussed. The authors also highlight open challenges and provide insights for future research. This taxonomy sheds light on the novel challenges posed by multimodal data in various FL paradigms, offering a new perspective for advancing MFL research.<br /><br />Summary: <div>
arXiv:2505.21792v1 Announce Type: new 
Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs</title>
<link>https://arxiv.org/abs/2505.21800</link>
<guid>https://arxiv.org/abs/2505.21800</guid>
<content:encoded><![CDATA[
<div> conversational abilities, falsehoods, truthfulness, multi-dimensional cones, concept cones<br />
Summary:<br />
This article explores the truthfulness of large language models (LLMs) and their ability to generate factual statements. By extending the concept cone framework to the domain of truth, the researchers identify multidimensional cones that influence how LLMs behave in response to true/false propositions. Through causal interventions, it is shown that these cones play a crucial role in mediating truth-related behavior across different LLM families. The study demonstrates that these cones can flip model responses to factual statements, generalize across model architectures, and preserve unrelated model behavior. These findings suggest a more complex, multidirectional geometry underlying the truthfulness of simple propositions in LLMs and highlight the potential of concept cones as a valuable tool for investigating abstract behaviors. <br /><br /> <div>
arXiv:2505.21800v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Operational Automated Greenhouse Gas Plume Detection</title>
<link>https://arxiv.org/abs/2505.21806</link>
<guid>https://arxiv.org/abs/2505.21806</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas detection, imaging spectroscopy, deep learning, convolutional neural networks, plume detection

Summary:
In the study, the challenges hindering the operational deployment of a fully automated greenhouse gas plume detection system using imaging spectroscopy were reviewed and addressed. Key obstacles such as data and label quality control, spatiotemporal biases, and modeling objectives alignment were tackled. The study demonstrated that convolutional neural networks (CNNs) can achieve operational detection performance when these obstacles are alleviated. A multitask model combining instance detection and pixelwise segmentation was shown to be effective in leading towards operational deployment. The model's ability to detect plumes across emission source types and regions was evaluated, establishing thresholds for operational readiness. The study also provided data, models, and source code for reproducibility, as well as defined best practices and validation standards for future contributions in the field. <div>
arXiv:2505.21806v1 Announce Type: new 
Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in importance for natural and anthropogenic emissions monitoring. This work reviews and addresses several key obstacles in the field: data and label quality control, prevention of spatiotemporal biases, and correctly aligned modeling objectives. We demonstrate through rigorous experiments using multicampaign data from airborne and spaceborne instruments that convolutional neural networks (CNNs) are able to achieve operational detection performance when these obstacles are alleviated. We demonstrate that a multitask model that learns both instance detection and pixelwise segmentation simultaneously can successfully lead towards an operational pathway. We evaluate the model's plume detectability across emission source types and regions, identifying thresholds for operational deployment. Finally, we provide analysis-ready data, models, and source code for reproducibility, and work to define a set of best practices and validation standards to facilitate future contributions to the field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction</title>
<link>https://arxiv.org/abs/2505.21807</link>
<guid>https://arxiv.org/abs/2505.21807</guid>
<content:encoded><![CDATA[
<div> Interpretable modeling, Gradient boosting machines, Deep learning, Tabular data, Large language models (LLMs)<br />
Summary:<br />
The paper introduces a novel approach that utilizes reasoning-based Large Language Models (LLMs) trained with reinforcement learning to enhance prediction accuracy and interpretability on tabular data. Existing models like gradient boosting machines and deep models excel in performance but lack interpretability. By incorporating custom reward functions, the proposed model not only focuses on accurate predictions but also generates human-understandable explanations. Experimental results on financial benchmark datasets showcase superior performance compared to other LLMs. This approach seeks to bridge the gap between accurate predictions and interpretability, offering a promising solution for real-world applications. <br /> <div>
arXiv:2505.21807v1 Announce Type: new 
Abstract: Predictive modeling on tabular data is the cornerstone of many real-world applications. Although gradient boosting machines and some recent deep models achieve strong performance on tabular data, they often lack interpretability. On the other hand, large language models (LLMs) have demonstrated powerful capabilities to generate human-like reasoning and explanations, but remain under-performed for tabular data prediction. In this paper, we propose a new approach that leverages reasoning-based LLMs, trained using reinforcement learning, to perform more accurate and explainable predictions on tabular data. Our method introduces custom reward functions that guide the model not only toward high prediction accuracy but also toward human-understandable reasons for its predictions. Experimental results show that our model achieves promising performance on financial benchmark datasets, outperforming most existing LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Data Augmentation through Bayesian Model Selection</title>
<link>https://arxiv.org/abs/2505.21813</link>
<guid>https://arxiv.org/abs/2505.21813</guid>
<content:encoded><![CDATA[
<div> Probabilistic View, Data Augmentation, Bayesian Optimization, Marginal Likelihood, Bayesian Principles
<br />
Summary: 
The paper introduces a novel framework for optimizing Data Augmentation (DA) strategies in machine learning. By viewing DA from a probabilistic standpoint, the authors treat augmentation parameters as model (hyper)parameters and optimize the marginal likelihood through a Bayesian model selection approach. A tractable Evidence Lower BOund (ELBO) is derived to enable joint optimization of augmentation parameters and model parameters. Theoretical results on variational approximation quality, generalization guarantees, invariance properties, and links to empirical Bayes are provided. Experimental results on computer vision tasks demonstrate improved calibration and robust performance compared to fixed or no augmentation strategies. The work establishes a rigorous foundation for optimizing DA using Bayesian principles, offering significant potential for enhancing robustness in machine learning. 
<br /> <div>
arXiv:2505.21813v1 Announce Type: new 
Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and generalization of modern machine learning. However, when deciding on DA strategies it is critical to choose parameters carefully, and this can be a daunting task which is traditionally left to trial-and-error or expensive optimization based on validation performance. In this paper, we counter these limitations by proposing a novel framework for optimizing DA. In particular, we take a probabilistic view of DA, which leads to the interpretation of augmentation parameters as model (hyper)-parameters, and the optimization of the marginal likelihood with respect to these parameters as a Bayesian model selection problem. Due to its intractability, we derive a tractable Evidence Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly with model parameters. We provide extensive theoretical results on variational approximation quality, generalization guarantees, invariance properties, and connections to empirical Bayes. Through experiments on computer vision tasks, we show that our approach improves calibration and yields robust performance over fixed or no augmentation. Our work provides a rigorous foundation for optimizing DA through Bayesian principles with significant potential for robust machine learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations</title>
<link>https://arxiv.org/abs/2505.21824</link>
<guid>https://arxiv.org/abs/2505.21824</guid>
<content:encoded><![CDATA[
<div> machine learning, diabetes risk prediction, Non-negative Matrix Factorization, multimorbidity, polypharmacy

Summary:
This article introduces a novel unsupervised framework for predicting the risk of developing type 2 diabetes mellitus (T2DM) by integrating Non-negative Matrix Factorization (NMF) with statistical techniques. T2DM poses significant health and economic challenges, making early detection crucial. Traditional supervised learning approaches are limited by the lack of confirmed negative cases, leading to the development of this new method. By identifying latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients, the framework estimates T2DM risk in undiagnosed individuals. This data-driven approach provides interpretable insights for healthcare providers to implement timely interventions, potentially improving patient outcomes and reducing the future health and economic burden of T2DM. <div>
arXiv:2505.21824v1 Announce Type: new 
Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus (T2DM), is rapidly increasing, posing significant health and economic challenges. T2DM not only disrupts blood glucose regulation but also damages vital organs such as the heart, kidneys, eyes, nerves, and blood vessels, leading to substantial morbidity and mortality. In the US alone, the economic burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of individuals at risk is critical to mitigating these impacts. While machine learning approaches for T2DM prediction are increasingly adopted, many rely on supervised learning, which is often limited by the lack of confirmed negative cases. To address this limitation, we propose a novel unsupervised framework that integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify individuals at risk of developing T2DM. Our method identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate the T2DM risk in undiagnosed individuals. By leveraging data-driven insights from comorbidity and medication usage, our approach provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, ultimately improving patient outcomes and potentially reducing the future health and economic burden of T2DM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones</title>
<link>https://arxiv.org/abs/2505.21825</link>
<guid>https://arxiv.org/abs/2505.21825</guid>
<content:encoded><![CDATA[
<div> graph connectivity, inference-time computation, language models, reasoning strategies, parallel vs sequential scaling

Summary:
In the study, the researchers focused on the optimal allocation of inference-time computation in large language models for reasoning tasks. They explored whether sequential scaling or parallel scaling was more effective in improving performance. Through theoretical analysis and experiments on various language models, including those trained specifically for graph connectivity problems, they discovered scenarios where sequential scaling provided an exponential advantage over parallel scaling. These findings were particularly evident in challenging distributions of graphs. The research sheds light on the importance of considering different strategies for reasoning tasks and highlights the potential benefits of sequential scaling in certain scenarios. <div>
arXiv:2505.21825v1 Announce Type: new 
Abstract: Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Search of Adam's Secret Sauce</title>
<link>https://arxiv.org/abs/2505.21829</link>
<guid>https://arxiv.org/abs/2505.21829</guid>
<content:encoded><![CDATA[
<div> Keywords: Adam, language models, optimization, signed momentum, gradient

Summary:
Adam optimization algorithm has shown remarkable efficacy in training transformer-based language models. Various simplified versions of Adam, such as signed momentum methods, have been proposed but consistently underperform relative to Adam. However, constraining the momentum parameters in Adam to be equal preserves near-optimal performance while offering new insights and formulations. This choice not only ensures robust performance but also allows for a statistical interpretation, showing that Adam can be viewed as an online algorithm for estimating gradient mean and variance from a mean-field Gaussian variational inference perspective. This study conducted over 1,300 language models highlights the significance of Adam in optimization and sheds light on the "secret sauce" behind its success. 

<br /><br />Summary: <div>
arXiv:2505.21829v1 Announce Type: new 
Abstract: Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study - training over 1,300 language models across different data configurations and scales - comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal. Beyond robust performance, this choice affords new theoretical insights, highlights the "secret sauce" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients-one that arises from a mean-field Gaussian variational inference perspective.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TuneComp: Joint Fine-tuning and Compression for Large Foundation Models</title>
<link>https://arxiv.org/abs/2505.21835</link>
<guid>https://arxiv.org/abs/2505.21835</guid>
<content:encoded><![CDATA[
<div> compression, knowledge distillation, low-rank approximation, pruning, model size reduction 

Summary: 
Joint fine-tuning and compression methods are proposed to reduce model size after training, aiming to create a smaller model directly guided by the downstream task. This approach avoids sacrificing performance and eliminates the need for a larger intermediate model. By gradually distilling the model to a pruned low-rank structure, the joint fine-tuning and compression method outperforms sequential compression techniques. Experimental results demonstrate the effectiveness of this approach in significantly reducing model size while maintaining performance in downstream tasks. <div>
arXiv:2505.21835v1 Announce Type: new 
Abstract: To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints</title>
<link>https://arxiv.org/abs/2505.21841</link>
<guid>https://arxiv.org/abs/2505.21841</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, safe learning, online learning, adversarial settings, optimal policy <br />
Summary: The article introduces the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, designed for online safe reinforcement learning in dynamic environments with unknown and adversarial constraints. OMDPD aims to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). The algorithm achieves optimal regret and strong constraint violation bounds without relying on strict assumptions about known safe policies. It is the first to address online CMDPs with adversarial constraints and can work effectively even in uncertain environments. The results demonstrate that OMDPD offers practical guarantees for safe decision-making in adversarial settings. Access to accurate estimates of rewards and transitions can further enhance the algorithm's performance, making it a valuable tool for applications in autonomous driving, robotics, and cybersecurity. <br /><br /> <div>
arXiv:2505.21841v1 Announce Type: new 
Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Provable Approach for End-to-End Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21852</link>
<guid>https://arxiv.org/abs/2505.21852</guid>
<content:encoded><![CDATA[
<div> safe reinforcement learning, Provably Lifetime Safe RL (PLS), offline learning, policy deployment, Gaussian processes

Summary:<br />
- This study introduces Provably Lifetime Safe RL (PLS) to address the challenge of ensuring policy safety from learning to operation.
- PLS combines offline safe RL with safe policy deployment and uses return-conditioned supervised learning.
- The method cautiously optimizes target returns using Gaussian processes, ensuring safety with high probability.
- The mathematical relationship between target and actual returns is analyzed and near-optimal target returns are guaranteed.
- Empirical results show that PLS outperforms baselines in safety and reward performance, achieving high rewards while maintaining policy safety throughout its lifetime.
<br /><br /> <div>
arXiv:2505.21852v1 Announce Type: new 
Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bayesian Model Averaging in the Era of Foundation Models</title>
<link>https://arxiv.org/abs/2505.21857</link>
<guid>https://arxiv.org/abs/2505.21857</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian model averaging, ensemble learning, pre-trained models, classification, optimization

Summary:
In this study, the authors propose a novel approach to enhance classification performance on image and text data by leveraging Bayesian model averaging (BMA) with pre-trained or lightly-finetuned foundation models. They introduce trainable linear classifiers that take features from these models as inputs, allowing for model ensembling based on model posteriors. Additionally, they present a computationally efficient model averaging scheme (OMA) where ensemble weights are optimized to reduce prediction surprise. These methods provide a principled way to combine multiple models and enhance classification tasks, making it possible to incorporate future, improved foundation models for even better performance. <div>
arXiv:2505.21857v1 Announce Type: new 
Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA) paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to enhance the classification performance on image and text data. To make BMA tractable under foundation models, we introduce trainable linear classifiers that take frozen features from the pre-trained foundation models as inputs. The model posteriors over the linear classifiers tell us which linear heads and frozen features are better suited for a given dataset, resulting in a principled model ensembling method. Furthermore, we propose a computationally cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize the model ensemble weights, just like those weights based on model posterior distributions in BMA, by reducing the amount of surprise (expected entropy of the predictions) we get from predictions of ensembled models. With the rapid development of foundation models, these approaches will enable the incorporation of future, possibly significantly better foundation models to enhance the performance of challenging classification tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning</title>
<link>https://arxiv.org/abs/2505.21877</link>
<guid>https://arxiv.org/abs/2505.21877</guid>
<content:encoded><![CDATA[
<div> Keywords: Batch Normalisation, Federated Learning, Statistical Parameters, Hybrid Batch Normalisation, Global Statistics

Summary:
Hybrid Batch Normalisation (HBN) is introduced as a solution for the challenge of applying Batch Normalisation (BN) in federated learning, where data among client nodes is not identically distributed. HBN separates the update of statistical parameters from learnable parameters, allowing for unbiased estimation of global statistical parameters in distributed scenarios. By introducing a learnable hybrid distribution factor, HBN enables each computing node to adaptively mix current batch statistics with global statistics, improving federated learning performance. This approach emphasizes the importance of global statistics in shaping the learning process across varying federated learning settings, particularly beneficial for small batch sizes and heterogeneous data.<br /><br />Summary: <div>
arXiv:2505.21877v1 Announce Type: new 
Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network training to harmonise the input-output distributions for each batch of data. However, federated learning, a distributed learning paradigm, faces the challenge of dealing with non-independent and identically distributed data among the client nodes. Due to the lack of a coherent methodology for updating BN statistical parameters, standard BN degrades the federated learning performance. To this end, it is urgent to explore an alternative normalisation solution for federated learning. In this work, we resolve the dilemma of the BN layer in federated learning by developing a customised normalisation approach, Hybrid Batch Normalisation (HBN). HBN separates the update of statistical parameters (i.e. , means and variances used for evaluation) from that of learnable parameters (i.e. , parameters that require gradient updates), obtaining unbiased estimates of global statistical parameters in distributed scenarios. In contrast with the existing solutions, we emphasise the supportive power of global statistics for federated learning. The HBN layer introduces a learnable hybrid distribution factor, allowing each computing node to adaptively mix the statistical parameters of the current batch with the global statistics. Our HBN can serve as a powerful plugin to advance federated learning performance. It reflects promising merits across a wide range of federated learning settings, especially for small batch sizes and heterogeneous data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis</title>
<link>https://arxiv.org/abs/2505.21882</link>
<guid>https://arxiv.org/abs/2505.21882</guid>
<content:encoded><![CDATA[
<div> Keywords: tennis tournaments, momentum, HydraNet, multi-granularity analysis, state-space duality<br />
Summary:<br />
This study introduces a novel Momentum Score (MS) metric and HydraNet framework to model momentum in tennis tournaments. The framework integrates thirty-two dimensions of athlete performance and employs a Hydra module based on a state-space duality (SSD) framework to capture explicit and implicit momentum. It also includes a Versus Learning method to enhance the adversarial nature of momentum between athletes and a Collaborative-Adversarial Attention Mechanism (CAAM) for dynamic momentum integration. A million-level tennis cross-tournament dataset is used to validate the HydraNet framework's capabilities in modeling momentum at different granularities. Experimental results demonstrate that the MS metric provides valuable insights into the impact of momentum on match outcomes. This research establishes a new foundation for momentum modeling and sports analysis in tennis tournaments. Source code and datasets are available at https://github.com/ReyJerry/HydraNet. <br /> <div>
arXiv:2505.21882v1 Announce Type: new 
Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects the dynamic shifts in performance of athletes that can decisively influence match outcomes. Despite its significance, momentum in terms of effective modeling and multi-granularity analysis across points, games, sets, and matches in tennis tournaments remains underexplored. In this study, we define a novel Momentum Score (MS) metric to quantify a player's momentum level in multi-granularity tennis tournaments, and design HydraNet, a momentum-driven state-space duality-based framework, to model MS by integrating thirty-two heterogeneous dimensions of athletes performance in serve, return, psychology and fatigue. HydraNet integrates a Hydra module, which builds upon a state-space duality (SSD) framework, capturing explicit momentum with a sliding-window mechanism and implicit momentum through cross-game state propagation. It also introduces a novel Versus Learning method to better enhance the adversarial nature of momentum between the two athletes at a macro level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for capturing and integrating intra-player and inter-player dynamic momentum at a micro level. Additionally, we construct a million-level tennis cross-tournament dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate the multi-granularity modeling capability of HydraNet for the MS metric on this dataset. Extensive experimental evaluations demonstrate that the MS metric constructed by the HydraNet framework provides actionable insights into how momentum impacts outcomes at different granularities, establishing a new foundation for momentum modeling and sports analysis. To the best of our knowledge. The source code and datasets are available at https://github.com/ReyJerry/HydraNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training</title>
<link>https://arxiv.org/abs/2505.21893</link>
<guid>https://arxiv.org/abs/2505.21893</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference learning, generative models, diffusion models, optimization, off-policy bias

Summary: 
Preference learning is crucial for aligning generative models with human expectations, with recent advancements extending to diffusion models through methods like Direct Preference Optimization (DPO). However, challenges such as timestep-dependent instability and off-policy bias hinder the performance of existing approaches like Diffusion-DPO. To address these issues, a new practical strategy, DPO-C\&amp;M, is introduced to improve stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Additionally, a principled framework, SDPO, incorporates importance sampling into the objective to fully correct off-policy bias and emphasize informative updates during the diffusion process. Experimental results on various datasets demonstrate that both DPO-C\&amp;M and SDPO outperform standard Diffusion-DPO in terms of VBench scores, human preference alignment, and training robustness. These findings underscore the significance of optimized, distribution-corrected techniques in diffusion-based preference learning.<br /><br />Summary: <div>
arXiv:2505.21893v1 Announce Type: new 
Abstract: Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&amp;M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization</title>
<link>https://arxiv.org/abs/2505.21895</link>
<guid>https://arxiv.org/abs/2505.21895</guid>
<content:encoded><![CDATA[
arXiv:2505.21895v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning, offering substantial reductions in trainable parameters by modeling updates as the product of two low-rank matrices. While effective, the low-rank constraint inherently limits representational capacity, often resulting in reduced performance compared to full-rank fine-tuning. Recent work by Ji et al. (2025) has addressed this limitation by applying a fixed-frequency sinusoidal transformation to low-rank adapters, increasing their stable rank without introducing additional parameters. This raises a crucial question: can the same sine-activated technique be successfully applied within the context of Post-Training Quantization to retain benefits even after model compression? In this paper, we investigate this question by extending the sinusoidal transformation framework to quantized LoRA adapters. We develop a theoretical analysis showing that the stable rank of a quantized adapter is tightly linked to that of its full-precision counterpart, motivating the use of such rank-enhancing functions even under quantization. Our results demonstrate that the expressivity gains from a sinusoidal non-linearity persist after quantization, yielding highly compressed adapters with negligible loss in performance. We validate our approach across a range of fine-tuning tasks for language, vision and text-to-image generation achieving significant memory savings while maintaining competitive accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding</title>
<link>https://arxiv.org/abs/2505.21908</link>
<guid>https://arxiv.org/abs/2505.21908</guid>
<content:encoded><![CDATA[
arXiv:2505.21908v1 Announce Type: new 
Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Transformer Without Using Learning Rate Warmup</title>
<link>https://arxiv.org/abs/2505.21910</link>
<guid>https://arxiv.org/abs/2505.21910</guid>
<content:encoded><![CDATA[
arXiv:2505.21910v1 Announce Type: new 
Abstract: Scaling Transformer to a large scale without using some technical tricks such as learning rate warump and using an obviously lower learning rate is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for the process of training Transformer and reveal the rationale behind the model crash phenomenon in the training process, termed \textit{spectral energy concentration} of ${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse, where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the key in Transformer, respectively. To remedy this problem, motivated by \textit{Weyl's Inequality}, we present a novel optimization strategy, \ie, making the weight updating in successive steps smooth -- if the ratio $\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a threshold, we will automatically bound the learning rate to a weighted multiple of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla \bW_t$ is the updating quantity in step $t$. Such an optimization strategy can prevent spectral energy concentration to only a few directions, and thus can avoid malignant entropy collapse which will trigger the model crash. We conduct extensive experiments using ViT, Swin-Transformer and GPT, showing that our optimization strategy can effectively and stably train these Transformers without using learning rate warmup.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing</title>
<link>https://arxiv.org/abs/2505.21918</link>
<guid>https://arxiv.org/abs/2505.21918</guid>
<content:encoded><![CDATA[
arXiv:2505.21918v1 Announce Type: new 
Abstract: We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
arXiv:2505.21923v1 Announce Type: new 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets</title>
<link>https://arxiv.org/abs/2505.21930</link>
<guid>https://arxiv.org/abs/2505.21930</guid>
<content:encoded><![CDATA[
arXiv:2505.21930v1 Announce Type: new 
Abstract: This paper develops an ensemble method for fine-tuning a language model to multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are efficient when adapting to a single dataset. When training on multiple datasets of different tasks, a common setup in practice, it remains unclear how to design an efficient adaptation for fine-tuning language models. We propose to use an ensemble of multiple smaller adapters instead of a single adapter per task. We design an efficient algorithm that partitions $n$ datasets into $m$ groups, where $m$ is typically much smaller than $n$ in practice, and train one adapter for each group before taking a weighted combination to form the ensemble. The algorithm leverages a first-order approximation property of low-rank adaptation to quickly obtain the fine-tuning performances of dataset combinations since methods like LoRA stay close to the base model. Hence, we use the gradients of the base model to estimate its behavior during fine-tuning. Empirically, this approximation holds with less than $1\%$ error on models with up to $34$ billion parameters, leading to an estimation of true fine-tuning performances under $5\%$ error while speeding up computation compared to base fine-tuning by $105$ times. When applied to fine-tune Llama and GPT models on ten text classification tasks, our approach provides up to $10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title>
<link>https://arxiv.org/abs/2505.21938</link>
<guid>https://arxiv.org/abs/2505.21938</guid>
<content:encoded><![CDATA[
arXiv:2505.21938v1 Announce Type: new 
Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Beyond Experience Rehearsal and Full Model Surrogates</title>
<link>https://arxiv.org/abs/2505.21942</link>
<guid>https://arxiv.org/abs/2505.21942</guid>
<content:encoded><![CDATA[
arXiv:2505.21942v1 Announce Type: new 
Abstract: Continual learning (CL) has remained a significant challenge for deep neural networks as learning new tasks erases previously acquired knowledge, either partially or completely. Existing solutions often rely on experience rehearsal or full model surrogates to mitigate CF. While effective, these approaches introduce substantial memory and computational overhead, limiting their scalability and applicability in real-world scenarios. To address this, we propose SPARC, a scalable CL approach that eliminates the need for experience rehearsal and full-model surrogates. By effectively combining task-specific working memories and task-agnostic semantic memory for cross-task knowledge consolidation, SPARC results in a remarkable parameter efficiency, using only 6% of the parameters required by full-model surrogates. Despite its lightweight design, SPARC achieves superior performance on Seq-TinyImageNet and matches rehearsal-based methods on various CL benchmarks. Additionally, weight re-normalization in the classification layer mitigates task-specific biases, establishing SPARC as a practical and scalable solution for CL under stringent efficiency constraints.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization</title>
<link>https://arxiv.org/abs/2505.21944</link>
<guid>https://arxiv.org/abs/2505.21944</guid>
<content:encoded><![CDATA[
arXiv:2505.21944v1 Announce Type: new 
Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles</title>
<link>https://arxiv.org/abs/2505.21959</link>
<guid>https://arxiv.org/abs/2505.21959</guid>
<content:encoded><![CDATA[
arXiv:2505.21959v1 Announce Type: new 
Abstract: With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4\%, and 3.2\% improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging LLMs on a Simplex</title>
<link>https://arxiv.org/abs/2505.21972</link>
<guid>https://arxiv.org/abs/2505.21972</guid>
<content:encoded><![CDATA[
arXiv:2505.21972v1 Announce Type: new 
Abstract: Automated evaluation of free-form outputs from large language models (LLMs) is challenging because many distinct answers can be equally valid. A common practice is to use LLMs themselves as judges, but the theoretical properties of this approach are not yet well understood. We show that a geometric framework that represents both judges and candidates as points on a probability simplex can provide helpful insight on what is or is not identifiable using LLM judges. Our theoretical analysis uncovers a "phase transition" in ranking identifiability: for binary scoring systems, true rankings are identifiable even with weak judges under mild assumptions, while rankings become non-identifiable for three or more scoring levels even with infinite data, absent additional prior knowledge. This non-identifiability highlights how uncertainty in rankings stems from not only aleatoric uncertainty (i.e., inherent stochasticity in the data) but also epistemic uncertainty regarding which assumptions hold, an aspect that has received limited attention until now. To integrate both types of uncertainty, we use Bayesian inference to encode assumptions as priors and conduct sensitivity analysis of ranking estimates and credible intervals. Empirical evaluations across multiple benchmarks demonstrate that Bayesian inference yields more accurate rankings and substantially improves coverage rates. These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</title>
<link>https://arxiv.org/abs/2505.21974</link>
<guid>https://arxiv.org/abs/2505.21974</guid>
<content:encoded><![CDATA[
arXiv:2505.21974v1 Announce Type: new 
Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the \textit{hypervolume identifiability issue}, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose \textit{BOFormer}, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly outperforms the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems. We have made the source code publicly available to encourage further research in this direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Feature Generation with Transformer and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21978</link>
<guid>https://arxiv.org/abs/2505.21978</guid>
<content:encoded><![CDATA[
arXiv:2505.21978v1 Announce Type: new 
Abstract: Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer's self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning</title>
<link>https://arxiv.org/abs/2505.21987</link>
<guid>https://arxiv.org/abs/2505.21987</guid>
<content:encoded><![CDATA[
arXiv:2505.21987v1 Announce Type: new 
Abstract: With the rapid expansion of large language models (LLMs), the demand for memory and computational resources has grown significantly. Recent advances in LLM pruning aim to reduce the size and computational cost of these models. However, existing methods often suffer from either suboptimal pruning performance or low time efficiency during the pruning process. In this work, we propose an efficient and effective pruning method that simultaneously achieves high pruning performance and fast pruning speed with improved calibration efficiency. Our approach introduces two key innovations: (1) An activation cosine similarity loss-guided pruning metric, which considers the angular deviation of the output activation between the dense and pruned models. (2) An activation variance-guided pruning metric, which helps preserve semantic distinctions in output activations after pruning, enabling effective pruning with shorter input sequences. These two components can be readily combined to enhance LLM pruning in both accuracy and efficiency. Experimental results show that our method achieves up to an 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Compact Spaces with Approximately Normalized Transformers</title>
<link>https://arxiv.org/abs/2505.22014</link>
<guid>https://arxiv.org/abs/2505.22014</guid>
<content:encoded><![CDATA[
arXiv:2505.22014v1 Announce Type: new 
Abstract: In deep learning, regularization and normalization are common solutions for challenges such as overfitting, numerical instabilities, and the increasing variance in the residual stream. An alternative approach is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic but approximate normalization (anTransformer). Our approach constrains the norm of parameters and normalizes all representations via scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. When applied to GPT training, we observe a 40% faster convergence compared to models with QK normalization, with less than 3% additional runtime. Deriving scaling laws for anGPT, we found our method enables training with larger batch sizes and fewer hyperparameters, while matching the favorable scaling characteristics of classic GPT architectures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised Contrastive Learning for Imprecise Class Labels</title>
<link>https://arxiv.org/abs/2505.22028</link>
<guid>https://arxiv.org/abs/2505.22028</guid>
<content:encoded><![CDATA[
arXiv:2505.22028v1 Announce Type: new 
Abstract: Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.22041</link>
<guid>https://arxiv.org/abs/2505.22041</guid>
<content:encoded><![CDATA[
arXiv:2505.22041v1 Announce Type: new 
Abstract: Conformance checking techniques detect undesired process behavior by comparing process executions that are recorded in event logs to desired behavior that is captured in a dedicated process model. If such models are not available, conformance checking techniques are not applicable, but organizations might still be interested in detecting undesired behavior in their processes. To enable this, existing approaches use Large Language Models (LLMs), assuming that they can learn to distinguish desired from undesired behavior through fine-tuning. However, fine-tuning is highly resource-intensive and the fine-tuned LLMs often do not generalize well. To address these limitations, we propose an approach that requires neither a dedicated process model nor resource-intensive fine-tuning to detect undesired process behavior. Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with direct access to a knowledge base that contains both desired and undesired process behavior from other processes, assuming that the LLM can transfer this knowledge to the process at hand. Our evaluation shows that our approach outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that RAG is a viable alternative to resource-intensive fine-tuning, particularly when enriched with relevant context from the event log, such as frequent traces and activities.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Effects of Sample Training Orders for Large Language Models without Retraining</title>
<link>https://arxiv.org/abs/2505.22042</link>
<guid>https://arxiv.org/abs/2505.22042</guid>
<content:encoded><![CDATA[
arXiv:2505.22042v1 Announce Type: new 
Abstract: The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Generalized Sliced Wasserstein Plans</title>
<link>https://arxiv.org/abs/2505.22049</link>
<guid>https://arxiv.org/abs/2505.22049</guid>
<content:encoded><![CDATA[
arXiv:2505.22049v1 Announce Type: new 
Abstract: Optimal Transport (OT) has attracted significant interest in the machine learning community, not only for its ability to define meaningful distances between probability distributions -- such as the Wasserstein distance -- but also for its formulation of OT plans. Its computational complexity remains a bottleneck, though, and slicing techniques have been developed to scale OT to large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a single one-dimensional plan back to the original multidimensional space, finally selecting the slice that yields the lowest Wasserstein distance as an approximation of the full OT plan. Despite its computational and theoretical advantages, min-SWGG inherits typical limitations of slicing methods: (i) the number of required slices grows exponentially with the data dimension, and (ii) it is constrained to linear projections. Here, we reformulate min-SWGG as a bilevel optimization problem and propose a differentiable approximation scheme to efficiently identify the optimal slice, even in high-dimensional settings. We furthermore define its generalized extension for accommodating to data living on manifolds. Finally, we demonstrate the practical value of our approach in various applications, including gradient flows on manifolds and high-dimensional spaces, as well as a novel sliced OT-based conditional flow matching for image generation -- where fast computation of transport plans is essential.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Resurrection of the ReLU</title>
<link>https://arxiv.org/abs/2505.22074</link>
<guid>https://arxiv.org/abs/2505.22074</guid>
<content:encoded><![CDATA[
arXiv:2505.22074v1 Announce Type: new 
Abstract: Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?</title>
<link>https://arxiv.org/abs/2505.22081</link>
<guid>https://arxiv.org/abs/2505.22081</guid>
<content:encoded><![CDATA[
arXiv:2505.22081v1 Announce Type: new 
Abstract: Symbolic regression aims to discover mathematical equations that fit given numerical data. It has been applied in various fields of scientific research, such as producing human-readable expressions that explain physical phenomena. Recently, Neural symbolic regression (NSR) methods that involve Transformers pre-trained on large-scale synthetic datasets have gained attention. While these methods offer advantages such as short inference time, they suffer from low performance, particularly when the number of input variables is large. In this study, we hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. We conducted a quantitative evaluation of this bias in Transformers using a synthetic dataset and found that Transformers rarely generate expressions not present in the training data. Additional theoretical analysis reveals that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. We finally examined if tailoring test-time strategies can lead to reduced memorization bias and better performance. We empirically demonstrate that providing additional information to the model at test time can significantly mitigate memorization bias. On the other hand, we also find that reducing memorization bias does not necessarily correlate with improved performance. These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods. Code is available at https://github.com/Shun-0922/Mem-Bias-NSR .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive, Differentially Private Federated Learning for Clinical Data</title>
<link>https://arxiv.org/abs/2505.22108</link>
<guid>https://arxiv.org/abs/2505.22108</guid>
<content:encoded><![CDATA[
arXiv:2505.22108v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The quest for the GRAph Level autoEncoder (GRALE)</title>
<link>https://arxiv.org/abs/2505.22109</link>
<guid>https://arxiv.org/abs/2505.22109</guid>
<content:encoded><![CDATA[
arXiv:2505.22109v1 Announce Type: new 
Abstract: Although graph-based learning has attracted a lot of attention, graph representation learning is still a challenging task whose resolution may impact key application fields such as chemistry or biology. To this end, we introduce GRALE, a novel graph autoencoder that encodes and decodes graphs of varying sizes into a shared embedding space. GRALE is trained using an Optimal Transport-inspired loss that compares the original and reconstructed graphs and leverages a differentiable node matching module, which is trained jointly with the encoder and decoder. The proposed attention-based architecture relies on Evoformer, the core component of AlphaFold, which we extend to support both graph encoding and decoding. We show, in numerical experiments on simulated and molecular data, that GRALE enables a highly general form of pre-training, applicable to a wide range of downstream tasks, from classification and regression to more complex tasks such as graph interpolation, editing, matching, and prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiMi Sheets: Infosheets for bias mitigation methods</title>
<link>https://arxiv.org/abs/2505.22114</link>
<guid>https://arxiv.org/abs/2505.22114</guid>
<content:encoded><![CDATA[
arXiv:2505.22114v1 Announce Type: new 
Abstract: Over the past 15 years, hundreds of bias mitigation methods have been proposed in the pursuit of fairness in machine learning (ML). However, algorithmic biases are domain-, task-, and model-specific, leading to a `portability trap': bias mitigation solutions in one context may not be appropriate in another. Thus, a myriad of design choices have to be made when creating a bias mitigation method, such as the formalization of fairness it pursues, and where and how it intervenes in the ML pipeline. This creates challenges in benchmarking and comparing the relative merits of different bias mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. This enables researchers and practitioners to quickly learn its main characteristics and to compare with their desiderata. Furthermore, the sheets' structure allow for the creation of a structured database of bias mitigation methods. In order to foster the sheets' adoption, we provide a platform for finding and creating BiMi Sheets at bimisheet.com.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</title>
<link>https://arxiv.org/abs/2505.22151</link>
<guid>https://arxiv.org/abs/2505.22151</guid>
<content:encoded><![CDATA[
arXiv:2505.22151v1 Announce Type: new 
Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory</title>
<link>https://arxiv.org/abs/2505.22152</link>
<guid>https://arxiv.org/abs/2505.22152</guid>
<content:encoded><![CDATA[
arXiv:2505.22152v1 Announce Type: new 
Abstract: While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings. We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can increase with model depth if a node's features are semantically different from its neighbors. Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings. This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily. We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The informativeness of the gradient revisited</title>
<link>https://arxiv.org/abs/2505.22158</link>
<guid>https://arxiv.org/abs/2505.22158</guid>
<content:encoded><![CDATA[
arXiv:2505.22158v1 Announce Type: new 
Abstract: In the past decade gradient-based deep learning has revolutionized several applications. However, this rapid advancement has highlighted the need for a deeper theoretical understanding of its limitations. Research has shown that, in many practical learning tasks, the information contained in the gradient is so minimal that gradient-based methods require an exceedingly large number of iterations to achieve success. The informativeness of the gradient is typically measured by its variance with respect to the random selection of a target function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a parameter related to the pairwise independence of the target function class and the collision entropy of the input distribution. Our bound scales as $ \tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $ \tilde{\mathcal{O}} $ hides factors related to the regularity of the learning model and the loss function, $ \varepsilon $ measures the pairwise independence of the target function class and $\mathcal{E}_c$ is the collision entropy of the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class of Learning with Errors (LWE) mappings and high-frequency functions. In addition to the theoretical analysis, we present experiments to understand better the nature of recent deep learning-based attacks on LWE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Augmentation-Aware Theory for Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.22196</link>
<guid>https://arxiv.org/abs/2505.22196</guid>
<content:encoded><![CDATA[
arXiv:2505.22196v1 Announce Type: new 
Abstract: Self-supervised contrastive learning has emerged as a powerful tool in machine learning and computer vision to learn meaningful representations from unlabeled data. Meanwhile, its empirical success has encouraged many theoretical studies to reveal the learning mechanisms. However, in the existing theoretical research, the role of data augmentation is still under-exploited, especially the effects of specific augmentation types. To fill in the blank, we for the first time propose an augmentation-aware error bound for self-supervised contrastive learning, showing that the supervised risk is bounded not only by the unsupervised risk, but also explicitly by a trade-off induced by data augmentation. Then, under a novel semantic label assumption, we discuss how certain augmentation methods affect the error bound. Lastly, we conduct both pixel- and representation-level experiments to verify our proposed theoretical results.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer</title>
<link>https://arxiv.org/abs/2505.22199</link>
<guid>https://arxiv.org/abs/2505.22199</guid>
<content:encoded><![CDATA[
arXiv:2505.22199v1 Announce Type: new 
Abstract: Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
arXiv:2505.22203v1 Announce Type: new 
Abstract: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models</title>
<link>https://arxiv.org/abs/2505.22208</link>
<guid>https://arxiv.org/abs/2505.22208</guid>
<content:encoded><![CDATA[
arXiv:2505.22208v1 Announce Type: new 
Abstract: Neural network potentials (NNPs) are crucial for accelerating computational materials science by surrogating density functional theory (DFT) calculations. Improving their accuracy is possible through pre-training and fine-tuning, where an NNP model is first pre-trained on a large-scale dataset and then fine-tuned on a smaller target dataset. However, this approach is computationally expensive, mainly due to the cost of DFT-based dataset labeling and load imbalances during large-scale pre-training. To address this, we propose LaMM, a semi-supervised pre-training method incorporating improved denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training. We demonstrate that our approach effectively leverages a large-scale dataset of $\sim$300 million semi-labeled samples to train a single NNP model, resulting in improved fine-tuning performance in terms of both speed and accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solver-Free Decision-Focused Learning for Linear Optimization Problems</title>
<link>https://arxiv.org/abs/2505.22224</link>
<guid>https://arxiv.org/abs/2505.22224</guid>
<content:encoded><![CDATA[
arXiv:2505.22224v1 Announce Type: new 
Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal kernel regression bounds under energy-bounded noise</title>
<link>https://arxiv.org/abs/2505.22235</link>
<guid>https://arxiv.org/abs/2505.22235</guid>
<content:encoded><![CDATA[
arXiv:2505.22235v1 Announce Type: new 
Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation algorithm's accuracy and in view of downstream tasks, such as its deployment in safety-critical contexts. In this paper, we derive a tight, non-asymptotic uncertainty bound for kernel-based estimation, which can also handle correlated noise sequences. Its computation relies on a mild norm-boundedness assumption on the unknown function and the noise, returning the worst-case function realization within the hypothesis class at an arbitrary query input location. The value of this function is shown to be given in terms of the posterior mean and covariance of a Gaussian process for an optimal choice of the measurement noise covariance. By rigorously analyzing the proposed approach and comparing it with other results in the literature, we show its effectiveness in returning tight and easy-to-compute bounds for kernel-based estimates.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
arXiv:2505.22252v1 Announce Type: new 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Online-Offline Framework for Co-Branding Campaign Recommendations</title>
<link>https://arxiv.org/abs/2505.22254</link>
<guid>https://arxiv.org/abs/2505.22254</guid>
<content:encoded><![CDATA[
arXiv:2505.22254v1 Announce Type: new 
Abstract: Co-branding has become a vital strategy for businesses aiming to expand market reach within recommendation systems. However, identifying effective cross-industry partnerships remains challenging due to resource imbalances, uncertain brand willingness, and ever-changing market conditions. In this paper, we provide the first systematic study of this problem and propose a unified online-offline framework to enable co-branding recommendations. Our approach begins by constructing a bipartite graph linking ``initiating'' and ``target'' brands to quantify co-branding probabilities and assess market benefits. During the online learning phase, we dynamically update the graph in response to market feedback, while striking a balance between exploring new collaborations for long-term gains and exploiting established partnerships for immediate benefits. To address the high initial co-branding costs, our framework mitigates redundant exploration, thereby enhancing short-term performance while ensuring sustainable strategic growth. In the offline optimization phase, our framework consolidates the interests of multiple sub-brands under the same parent brand to maximize overall returns, avoid excessive investment in single sub-brands, and reduce unnecessary costs associated with over-prioritizing a single sub-brand. We present a theoretical analysis of our approach, establishing a highly nontrivial sublinear regret bound for online learning in the complex co-branding problem, and enhancing the approximation guarantee for the NP-hard offline budget allocation optimization. Experiments on both synthetic and real-world co-branding datasets demonstrate the practical effectiveness of our framework, with at least 12\% improvement.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
arXiv:2505.22255v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training</title>
<link>https://arxiv.org/abs/2505.22257</link>
<guid>https://arxiv.org/abs/2505.22257</guid>
<content:encoded><![CDATA[
arXiv:2505.22257v1 Announce Type: new 
Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full Domain Analysis in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2505.22275</link>
<guid>https://arxiv.org/abs/2505.22275</guid>
<content:encoded><![CDATA[
arXiv:2505.22275v1 Announce Type: new 
Abstract: Novel techniques in evolutionary optimization, simulation and machine learning allow for a broad analysis of domains like fluid dynamics, in which computation is expensive and flow behavior is complex. Under the term of full domain analysis we understand the ability to efficiently determine the full space of solutions in a problem domain, and analyze the behavior of those solutions in an accessible and interactive manner. The goal of full domain analysis is to deepen our understanding of domains by generating many examples of flow, their diversification, optimization and analysis. We define a formal model for full domain analysis, its current state of the art, and requirements of subcomponents. Finally, an example is given to show what we can learn by using full domain analysis. Full domain analysis, rooted in optimization and machine learning, can be a helpful tool in understanding complex systems in computational physics and beyond.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.22306</link>
<guid>https://arxiv.org/abs/2505.22306</guid>
<content:encoded><![CDATA[
arXiv:2505.22306v1 Announce Type: new 
Abstract: Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2505.22308</link>
<guid>https://arxiv.org/abs/2505.22308</guid>
<content:encoded><![CDATA[
arXiv:2505.22308v1 Announce Type: new 
Abstract: Pretraining on large, semantically rich datasets is key for developing language models. Surprisingly, recent studies have shown that even synthetic data, generated procedurally through simple semantic-free algorithms, can yield some of the same benefits as natural language pretraining. It is unclear what specific capabilities such simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights. In this short paper, we identify several beneficial forms of procedural data, together with specific algorithmic reasoning skills that improve in small transformers. Our core finding is that different procedural rules instil distinct but complementary inductive structures in the model. With extensive ablations and partial-transfer experiments, we discover that these structures reside in different parts of the model. Attention layers often carry the most transferable information, but some pretraining rules impart useful structure to MLP blocks instead. Most interestingly, the structures induced by multiple rules can be composed to jointly reinforce multiple capabilities. These results suggest an exciting possibility of disentangling the acquisition of knowledge from reasoning in language models, with the goal of improving their robustness and data efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title>
<link>https://arxiv.org/abs/2505.22310</link>
<guid>https://arxiv.org/abs/2505.22310</guid>
<content:encoded><![CDATA[
arXiv:2505.22310v1 Announce Type: new 
Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork Open Reasoner 1 Technical Report</title>
<link>https://arxiv.org/abs/2505.22312</link>
<guid>https://arxiv.org/abs/2505.22312</guid>
<content:encoded><![CDATA[
arXiv:2505.22312v1 Announce Type: new 
Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking BPS: A Utility-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2505.22316</link>
<guid>https://arxiv.org/abs/2505.22316</guid>
<content:encoded><![CDATA[
arXiv:2505.22316v1 Announce Type: new 
Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing organizational workflows, supporting decision-making by estimating the impact of process changes. The reliability of such estimates depends on the ability of a BPS model to accurately mimic the process under analysis, making rigorous accuracy evaluation essential. However, the state-of-the-art approach to evaluating BPS models has two key limitations. First, it treats simulation as a forecasting problem, testing whether models can predict unseen future events. This fails to assess how well a model captures the as-is process, particularly when process behavior changes from train to test period. Thus, it becomes difficult to determine whether poor results stem from an inaccurate model or the inherent complexity of the data, such as unpredictable drift. Second, the evaluation approach strongly relies on Earth Mover's Distance-based metrics, which can obscure temporal patterns and thus yield misleading conclusions about simulation quality. To address these issues, we propose a novel framework that evaluates simulation quality based on its ability to generate representative process behavior. Instead of comparing simulated logs to future real-world executions, we evaluate whether predictive process monitoring models trained on simulated data perform comparably to those trained on real data for downstream analysis tasks. Empirical results show that our framework not only helps identify sources of discrepancies but also distinguishes between model accuracy and data complexity, offering a more meaningful way to assess BPS quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective</title>
<link>https://arxiv.org/abs/2505.22322</link>
<guid>https://arxiv.org/abs/2505.22322</guid>
<content:encoded><![CDATA[
arXiv:2505.22322v1 Announce Type: new 
Abstract: Diffusion models have shown strong performance in generating high-quality tabular data, but they carry privacy risks by reproducing exact training samples. While prior work focuses on dataset-level augmentation to reduce memorization, little is known about which individual samples contribute most. We present the first data-centric study of memorization dynamics in tabular diffusion models. We quantify memorization for each real sample based on how many generated samples are flagged as replicas, using a relative distance ratio. Our empirical analysis reveals a heavy-tailed distribution of memorization counts: a small subset of samples contributes disproportionately to leakage, confirmed via sample-removal experiments. To understand this, we divide real samples into top- and non-top-memorized groups and analyze their training-time behaviors. We track when each sample is first memorized and monitor per-epoch memorization intensity (AUC). Memorized samples are memorized slightly earlier and show stronger signals in early training. Based on these insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method: (a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and (c) retrain on the filtered dataset. Across multiple tabular datasets and models, DynamicCut reduces memorization with minimal impact on data diversity and downstream performance. It also complements augmentation-based defenses. Furthermore, DynamicCut enables cross-model transferability: high-ranked samples identified from one model (e.g., a diffusion model) are also effective for reducing memorization when removed from others, such as GANs and VAEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.22355</link>
<guid>https://arxiv.org/abs/2505.22355</guid>
<content:encoded><![CDATA[
arXiv:2505.22355v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable to Full Fine-Tuning (FFT) while requiring significantly fewer computing resources, making it the go-to choice for researchers. We find that although PEFT can achieve competitive results on some benchmarks, its performance falls short of FFT in complex tasks, such as reasoning and instruction-based fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in terms of representational capacity and robustness based on optimization theory. We theoretically demonstrate that PEFT is a strict subset of FFT. By providing theoretical upper bounds for PEFT, we show that the limited parameter space constrains the model's representational ability, making it more susceptible to perturbations. Experiments on 15 datasets encompassing classification, generation, reasoning, instruction fine-tuning tasks and 11 adversarial test sets validate our theories. We hope that these results spark further research beyond the realms of well established PEFT. The source code is in the anonymous Github repository\footnote{https://github.com/misonsky/PEFTEval}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings</title>
<link>https://arxiv.org/abs/2505.22356</link>
<guid>https://arxiv.org/abs/2505.22356</guid>
<content:encoded><![CDATA[
arXiv:2505.22356v1 Announce Type: new 
Abstract: Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs</title>
<link>https://arxiv.org/abs/2505.22358</link>
<guid>https://arxiv.org/abs/2505.22358</guid>
<content:encoded><![CDATA[
arXiv:2505.22358v1 Announce Type: new 
Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in a single end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment. To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency, achieving higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification</title>
<link>https://arxiv.org/abs/2505.22359</link>
<guid>https://arxiv.org/abs/2505.22359</guid>
<content:encoded><![CDATA[
arXiv:2505.22359v1 Announce Type: new 
Abstract: We study the generalization performance of unregularized gradient methods for separable linear classification. While previous work mostly deal with the binary case, we focus on the multiclass setting with $k$ classes and establish novel population risk bounds for Gradient Descent for loss functions that decay to zero. In this setting, we show risk bounds that reveal that convergence rates are crucially influenced by the geometry of the loss template, as formalized by Wang and Scott (2024), rather than of the loss function itself. Particularly, we establish risk upper bounds that holds for any decay rate of the loss whose template is smooth with respect to the $p$-norm. In the case of exponentially decaying losses, our results indicates a contrast between the $p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and $p=2$ where the risk scales linearly with $k$. To establish this separation formally, we also prove a lower bound in the latter scenario, demonstrating that the polynomial dependence on $k$ is unavoidable. Central to our analysis is a novel bound on the Rademacher complexity of low-noise vector-valued linear predictors with a loss template smooth w.r.t.~general $p$-norms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles</title>
<link>https://arxiv.org/abs/2505.22361</link>
<guid>https://arxiv.org/abs/2505.22361</guid>
<content:encoded><![CDATA[
arXiv:2505.22361v1 Announce Type: new 
Abstract: This paper studies a bandit optimization problem where the goal is to maximize a function $f(x)$ over $T$ periods for some unknown strongly concave function $f$. We consider a new pairwise comparison oracle, where the decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of periods and then obtains an estimate of $f(x)-f(x')$. We show that such a pairwise comparison oracle finds important applications to joint pricing and inventory replenishment problems and network revenue management. The challenge in this bandit optimization is twofold. First, the decision-maker not only needs to determine a pair of actions $(x, x')$ but also a stopping time $n$ (i.e., the number of queries based on $(x, x')$). Second, motivated by our inventory application, the estimate of the difference $f(x)-f(x')$ is biased, which is different from existing oracles in stochastic optimization literature. To address these challenges, we first introduce a discretization technique and local polynomial approximation to relate this problem to linear bandits. Then we developed a tournament successive elimination technique to localize the discretized cell and run an interactive batched version of LinUCB algorithm on cells. We establish regret bounds that are optimal up to poly-logarithmic factors. Furthermore, we apply our proposed algorithm and analytical framework to the two operations management problems and obtain results that improve state-of-the-art results in the existing literature.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Homophily-Aware Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.22362</link>
<guid>https://arxiv.org/abs/2505.22362</guid>
<content:encoded><![CDATA[
arXiv:2505.22362v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal performance on directed graphs with asymmetric structures. In this work, we propose Directed Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses these limitations by incorporating homophily-aware and direction-sensitive components. DHGNN employs a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions. Extensive experiments on both homophilic and heterophilic directed graph datasets demonstrate that DHGNN outperforms state-of-the-art methods in node classification and link prediction. In particular, DHGNN improves over the best baseline by up to 15.07% in link prediction. Our analysis further shows that the gating mechanism captures directional homophily gaps and fluctuating homophily across layers, providing deeper insights into message-passing behavior on complex graph structures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting</title>
<link>https://arxiv.org/abs/2505.22370</link>
<guid>https://arxiv.org/abs/2505.22370</guid>
<content:encoded><![CDATA[
arXiv:2505.22370v1 Announce Type: new 
Abstract: Continual Learning requires a model to learn multiple tasks in sequence while maintaining both stability:preserving knowledge from previously learned tasks, and plasticity:effectively learning new tasks. Gradient projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing Gradient Projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation, which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation</title>
<link>https://arxiv.org/abs/2505.22381</link>
<guid>https://arxiv.org/abs/2505.22381</guid>
<content:encoded><![CDATA[
arXiv:2505.22381v1 Announce Type: new 
Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and improving organizational processes by estimating the impact of process changes. A key component of BPS is the case-arrival model, which determines the pattern of new case entries into a process. Although accurate case-arrival modeling is essential for reliable simulations, as it influences waiting and overall cycle times, existing approaches often rely on oversimplified static distributions of inter-arrival times. These approaches fail to capture the dynamic and temporal complexities inherent in organizational environments, leading to less accurate and reliable outcomes. To address this limitation, we propose Auto Time Kernel Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival times of processes by incorporating global dynamics, day-of-week variations, and intraday distributional changes, ensuring both precision and scalability. Experiments conducted across 20 diverse processes demonstrate that AT-KDE is far more accurate and robust than existing approaches while maintaining sensible execution time efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2505.22389</link>
<guid>https://arxiv.org/abs/2505.22389</guid>
<content:encoded><![CDATA[
arXiv:2505.22389v1 Announce Type: new 
Abstract: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \textbf{Perturb-and-Merge (P\&amp;M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\&amp;M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, we minimize the total loss increase across all tasks and derive an analytical solution for the optimal merging coefficient. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\&amp;M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
arXiv:2505.22391v1 Announce Type: new 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v1 Announce Type: new 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals</title>
<link>https://arxiv.org/abs/2505.22422</link>
<guid>https://arxiv.org/abs/2505.22422</guid>
<content:encoded><![CDATA[
arXiv:2505.22422v1 Announce Type: new 
Abstract: The construction of confidence intervals for the mean of a bounded random variable is a classical problem in statistics with numerous applications in machine learning and virtually all scientific fields. In particular, obtaining the tightest possible confidence intervals is vital every time the sampling of the random variables is expensive. The current state-of-the-art method to construct confidence intervals is by using betting algorithms. This is a very successful approach for deriving optimal confidence sequences, even matching the rate of law of iterated logarithms. However, in the fixed horizon setting, these approaches are either sub-optimal or based on heuristic solutions with strong empirical performance but without a finite-time guarantee. Hence, no betting-based algorithm guaranteeing the optimal $\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the confidence intervals are known. This work bridges this gap. We propose a betting-based algorithm to compute confidence intervals that empirically outperforms the competitors. Our betting strategy uses the optimal strategy in every step (in a certain sense), whereas the standard betting methods choose a constant strategy in advance. Leveraging this fact results in strict improvements even for classical concentration inequalities, such as the ones of Hoeffding or Bernstein. Moreover, we also prove that the width of our confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$. The code is available on~https://github.com/vvoracek/STaR-bets-confidence-interval.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning without Attention</title>
<link>https://arxiv.org/abs/2505.22425</link>
<guid>https://arxiv.org/abs/2505.22425</guid>
<content:encoded><![CDATA[
arXiv:2505.22425v1 Announce Type: new 
Abstract: Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.22440</link>
<guid>https://arxiv.org/abs/2505.22440</guid>
<content:encoded><![CDATA[
arXiv:2505.22440v1 Announce Type: new 
Abstract: The rapid evolution of wireless technologies necessitates automated design frameworks to address antenna miniaturization and performance optimization within constrained development cycles. This study demonstrates a machine learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design. The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds, achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest, XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds using 936 simulation datasets, with stacked models showing superior training accuracy (R2=0.9825) and SVM demonstrating optimal validation performance (R2=0.7197). The complete design cycle, encompassing optimization, prediction, and ANSYS validation, required 12.42 minutes on standard desktop hardware (Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of PSADEA-based approaches. This 240 times of acceleration eliminates traditional trial-and-error methods that often extend beyond seven expert-led days. The system enables precise specifications of performance targets with automated generation of fabrication-ready parameters, particularly benefiting compact consumer devices requiring rapid frequency tuning. By bridging AI-driven optimization with CAD validation, this framework reduces engineering workloads while ensuring production-ready designs, establishing a scalable paradigm for next-generation RF systems in 6G and IoT applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22442</link>
<guid>https://arxiv.org/abs/2505.22442</guid>
<content:encoded><![CDATA[
arXiv:2505.22442v1 Announce Type: new 
Abstract: Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: https://github.com/CWibault/sorel\_torel.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: All Current Generative Fidelity and Diversity Metrics are Flawed</title>
<link>https://arxiv.org/abs/2505.22450</link>
<guid>https://arxiv.org/abs/2505.22450</guid>
<content:encoded><![CDATA[
arXiv:2505.22450v1 Announce Type: new 
Abstract: Any method's development and practical application is limited by our ability to measure its reliability. The popularity of generative modeling emphasizes the importance of good synthetic data metrics. Unfortunately, previous works have found many failure cases in current metrics, for example lack of outlier robustness and unclear lower and upper bounds. We propose a list of desiderata for synthetic data metrics, and a suite of sanity checks: carefully chosen simple experiments that aim to detect specific and known generative modeling failure modes. Based on these desiderata and the results of our checks, we arrive at our position: all current generative fidelity and diversity metrics are flawed. This significantly hinders practical use of synthetic data. Our aim is to convince the research community to spend more effort in developing metrics, instead of models. Additionally, through analyzing how current metrics fail, we provide practitioners with guidelines on how these metrics should (not) be used.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pure Exploration with Infinite Answers</title>
<link>https://arxiv.org/abs/2505.22473</link>
<guid>https://arxiv.org/abs/2505.22473</guid>
<content:encoded><![CDATA[
arXiv:2505.22473v1 Announce Type: new 
Abstract: We study pure exploration problems where the set of correct answers is possibly infinite, e.g., the regression of any continuous function of the means of the bandit. We derive an instance-dependent lower bound for these problems. By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop) for finite answer problems fail at being asymptotically optimal in this more general setting. Finally, we present a framework, Sticky-Sequence Track-and-Stop, which generalizes both Track-and-Stop and Sticky Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality, our analysis also highlights special cases where existing methods enjoy optimality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis</title>
<link>https://arxiv.org/abs/2505.22474</link>
<guid>https://arxiv.org/abs/2505.22474</guid>
<content:encoded><![CDATA[
arXiv:2505.22474v1 Announce Type: new 
Abstract: The forecasting of multivariate urban data presents a complex challenge due to the intricate dependencies between various urban metrics such as weather, air pollution, carbon intensity, and energy demand. This paper introduces a novel multivariate time-series forecasting model that utilizes advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. The proposed model incorporates a decomposition-based preprocessing step, isolating trend, seasonal, and residual components to enhance the accuracy and interpretability of forecasts. By leveraging the dynamic capabilities of GNNs, the model effectively captures interdependencies and improves the forecasting performance. Extensive experiments on real-world datasets, including electricity usage, weather metrics, carbon intensity, and air pollution data, demonstrate the effectiveness of the proposed approach across various forecasting scenarios. The results highlight the potential of the model to optimize smart infrastructure systems, contributing to energy-efficient urban development and enhanced public well-being.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Asymptotic Analysis of (Sticky) Track-and-Stop</title>
<link>https://arxiv.org/abs/2505.22475</link>
<guid>https://arxiv.org/abs/2505.22475</guid>
<content:encoded><![CDATA[
arXiv:2505.22475v1 Announce Type: new 
Abstract: In pure exploration problems, a statistician sequentially collects information to answer a question about some stochastic and unknown environment. The probability of returning a wrong answer should not exceed a maximum risk parameter $\delta$ and good algorithms make as few queries to the environment as possible. The Track-and-Stop algorithm is a pioneering method to solve these problems. Specifically, it is well-known that it enjoys asymptotic optimality sample complexity guarantees for $\delta\to 0$ whenever the map from the environment to its correct answers is single-valued (e.g., best-arm identification with a unique optimal arm). The Sticky Track-and-Stop algorithm extends these results to settings where, for each environment, there might exist multiple correct answers (e.g., $\epsilon$-optimal arm identification). Although both methods are optimal in the asymptotic regime, their non-asymptotic guarantees remain unknown. In this work, we fill this gap and provide non-asymptotic guarantees for both algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Multimodal Representation Collapse</title>
<link>https://arxiv.org/abs/2505.22483</link>
<guid>https://arxiv.org/abs/2505.22483</guid>
<content:encoded><![CDATA[
arXiv:2505.22483v1 Announce Type: new 
Abstract: We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Adversarial Training with Energy-based Models</title>
<link>https://arxiv.org/abs/2505.22486</link>
<guid>https://arxiv.org/abs/2505.22486</guid>
<content:encoded><![CDATA[
arXiv:2505.22486v1 Announce Type: new 
Abstract: We aim at using Energy-based Model (EBM) framework to better understand adversarial training (AT) in classifiers, and additionally to analyze the intrinsic generative capabilities of robust classifiers. By viewing standard classifiers through an energy lens, we begin by analyzing how the energies of adversarial examples, generated by various attacks, differ from those of the natural samples. The central focus of our work is to understand the critical phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT from an energy perspective. We analyze the impact of existing AT approaches on the energy of samples during training and observe that the behavior of the ``delta energy' -- change in energy between original sample and its adversarial counterpart -- diverges significantly when CO or RO occurs. After a thorough analysis of these energy dynamics and their relationship with overfitting, we propose a novel regularizer, the Delta Energy Regularizer (DER), designed to smoothen the energy landscape during training. We demonstrate that DER is effective in mitigating both CO and RO across multiple benchmarks. We further show that robust classifiers, when being used as generative models, have limits in handling trade-off between image quality and variability. We propose an improved technique based on a local class-wise principal component analysis (PCA) and energy-based guidance for better class-specific initialization and adaptive stopping, enhancing sample diversity and generation quality. Considering that we do not explicitly train for generative modeling, we achieve a competitive Inception Score (IS) and Fr\'echet inception distance (FID) compared to hybrid discriminative-generative models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling</title>
<link>https://arxiv.org/abs/2505.22491</link>
<guid>https://arxiv.org/abs/2505.22491</guid>
<content:encoded><![CDATA[
arXiv:2505.22491v1 Announce Type: new 
Abstract: The dominant paradigm for training large-scale vision and language models is He initialization and a single global learning rate (\textit{standard parameterization}, SP). Despite its practical success, standard parametrization remains poorly understood from a theoretical perspective: Existing infinite-width theory would predict instability under large learning rates and vanishing feature learning under stable learning rates. However, empirically optimal learning rates consistently decay much slower than theoretically predicted. By carefully studying neural network training dynamics, we demonstrate that this discrepancy is not fully explained by finite-width phenomena such as catapult effects or a lack of alignment between weights and incoming activations. We instead show that the apparent contradiction can be fundamentally resolved by taking the loss function into account: In contrast to Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an intermediate \textit{controlled divergence} regime emerges, where logits diverge but loss, gradients, and activations remain stable. Stable training under large learning rates enables persistent feature evolution at scale in all hidden layers, which is crucial for the practical success of SP. In experiments across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities (vision, language), we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation</title>
<link>https://arxiv.org/abs/2505.22492</link>
<guid>https://arxiv.org/abs/2505.22492</guid>
<content:encoded><![CDATA[
arXiv:2505.22492v1 Announce Type: new 
Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods</title>
<link>https://arxiv.org/abs/2505.22494</link>
<guid>https://arxiv.org/abs/2505.22494</guid>
<content:encoded><![CDATA[
arXiv:2505.22494v1 Announce Type: new 
Abstract: Designing protein sequences of both high fitness and novelty is a challenging task in data-efficient protein engineering. Exploration beyond wild-type neighborhoods often leads to biologically implausible sequences or relies on surrogate models that lose fidelity in novel regions. Here, we propose ProSpero, an active learning framework in which a frozen pre-trained generative model is guided by a surrogate updated from oracle feedback. By integrating fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling, our approach enables exploration beyond wild-type neighborhoods while preserving biological plausibility. We show that our framework remains effective even when the surrogate is misspecified. ProSpero consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric GNNs for Charged Particle Tracking at GlueX</title>
<link>https://arxiv.org/abs/2505.22504</link>
<guid>https://arxiv.org/abs/2505.22504</guid>
<content:encoded><![CDATA[
arXiv:2505.22504v1 Announce Type: new 
Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building blocks of matter. The experiments involve high-energy collisions that produce complex events with many particle trajectories. Tracking charged particles resulting from collisions in the presence of a strong magnetic field is critical to enable the reconstruction of particle trajectories and precise determination of interactions. It is traditionally achieved through combinatorial approaches that scale worse than linearly as the number of hits grows. Since particle hit data naturally form a 3-dimensional point cloud and can be structured as graphs, Graph Neural Networks (GNNs) emerge as an intuitive and effective choice for this task. In this study, we evaluate the GNN model for track finding on the data from the GlueX experiment at Jefferson Lab. We use simulation data to train the model and test on both simulation and real GlueX measurements. We demonstrate that GNN-based track finding outperforms the currently used traditional method at GlueX in terms of segment-based efficiency at a fixed purity while providing faster inferences. We show that the GNN model can achieve significant speedup by processing multiple events in batches, which exploits the parallel computation capability of Graphical Processing Units (GPUs). Finally, we compare the GNN implementation on GPU and FPGA and describe the trade-off.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsification and Reconstruction from the Perspective of Representation Geometry</title>
<link>https://arxiv.org/abs/2505.22506</link>
<guid>https://arxiv.org/abs/2505.22506</guid>
<content:encoded><![CDATA[
arXiv:2505.22506v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic interpretability, aiming to identify interpretable monosemantic features. However, how does sparse encoding organize the representations of activation vector from language models? What is the relationship between this organizational paradigm and feature disentanglement as well as reconstruction performance? To address these questions, we propose the SAEMA, which validates the stratified structure of the representation by observing the variability of the rank of the symmetric semipositive definite (SSPD) matrix corresponding to the modal tensor unfolded along the latent tensor with the level of noise added to the residual stream. To systematically investigate how sparse encoding alters representational structures, we define local and global representations, demonstrating that they amplify inter-feature distinctions by merging similar semantic features and introducing additional dimensionality. Furthermore, we intervene the global representation from an optimization perspective, proving a significant causal relationship between their separability and the reconstruction performance. This study explains the principles of sparsity from the perspective of representational geometry and demonstrates the impact of changes in representational structure on reconstruction performance. Particularly emphasizes the necessity of understanding representations and incorporating representational constraints, providing empirical references for developing new interpretable tools and improving SAEs. The code is available at \hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Optimization via Differentiable Stopping Time</title>
<link>https://arxiv.org/abs/2505.22509</link>
<guid>https://arxiv.org/abs/2505.22509</guid>
<content:encoded><![CDATA[
arXiv:2505.22509v1 Announce Type: new 
Abstract: Optimization is an important module of modern machine learning applications. Tremendous efforts have been made to accelerate optimization algorithms. A common formulation is achieving a lower loss at a given time. This enables a differentiable framework with respect to the algorithm hyperparameters. In contrast, its dual, minimizing the time to reach a target loss, is believed to be non-differentiable, as the time is not differentiable. As a result, it usually serves as a conceptual framework or is optimized using zeroth-order methods. To address this limitation, we propose a differentiable stopping time and theoretically justify it based on differential equations. An efficient algorithm is designed to backpropagate through it. As a result, the proposed differentiable stopping time enables a new differentiable formulation for accelerating algorithms. We further discuss its applications, such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data</title>
<link>https://arxiv.org/abs/2505.22521</link>
<guid>https://arxiv.org/abs/2505.22521</guid>
<content:encoded><![CDATA[
arXiv:2505.22521v1 Announce Type: new 
Abstract: Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2505.22524</link>
<guid>https://arxiv.org/abs/2505.22524</guid>
<content:encoded><![CDATA[
arXiv:2505.22524v1 Announce Type: new 
Abstract: Discrete diffusion models have become highly effective across various domains. However, real-world applications often require the generative process to adhere to certain constraints but without task-specific fine-tuning. To this end, we propose a training-free method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution at the test time. Our approach leverages twisted SMC with an approximate locally optimal proposal, obtained via a first-order Taylor expansion of the reward function. To address the challenge of ill-defined gradients in discrete spaces, we incorporate a Gumbel-Softmax relaxation, enabling efficient gradient-based approximation within the discrete generative framework. Empirical results on both synthetic datasets and image modelling validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training RL Agents for Multi-Objective Network Defense Tasks</title>
<link>https://arxiv.org/abs/2505.22531</link>
<guid>https://arxiv.org/abs/2505.22531</guid>
<content:encoded><![CDATA[
arXiv:2505.22531v1 Announce Type: new 
Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabularQGAN: A Quantum Generative Model for Tabular Data</title>
<link>https://arxiv.org/abs/2505.22533</link>
<guid>https://arxiv.org/abs/2505.22533</guid>
<content:encoded><![CDATA[
arXiv:2505.22533v1 Announce Type: new 
Abstract: In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.22538</link>
<guid>https://arxiv.org/abs/2505.22538</guid>
<content:encoded><![CDATA[
arXiv:2505.22538v1 Announce Type: new 
Abstract: We address the problem of uncertainty quantification and propose measures of total, aleatoric, and epistemic uncertainty based on a known decomposition of (strictly) proper scoring rules, a specific type of loss function, into a divergence and an entropy component. This leads to a flexible framework for uncertainty quantification that can be instantiated with different losses (scoring rules), which makes it possible to tailor uncertainty quantification to the use case at hand. We show that this flexibility is indeed advantageous. In particular, we analyze the task of selective prediction and show that the scoring rule should ideally match the task loss. In addition, we perform experiments on two other common tasks. For out-of-distribution detection, our results confirm that a widely used measure of epistemic uncertainty, mutual information, performs best. Moreover, in the setting of active learning, our measure of epistemic uncertainty based on the zero-one-loss consistently outperforms other uncertainty measures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human-Centric Approach to Explainable AI for Personalized Education</title>
<link>https://arxiv.org/abs/2505.22541</link>
<guid>https://arxiv.org/abs/2505.22541</guid>
<content:encoded><![CDATA[
arXiv:2505.22541v1 Announce Type: new 
Abstract: Deep neural networks form the backbone of artificial intelligence research, with potential to transform the human experience in areas ranging from autonomous driving to personal assistants, healthcare to education. However, their integration into the daily routines of real-world classrooms remains limited. It is not yet common for a teacher to assign students individualized homework targeting their specific weaknesses, provide students with instant feedback, or simulate student responses to a new exam question. While these models excel in predictive performance, this lack of adoption can be attributed to a significant weakness: the lack of explainability of model decisions, leading to a lack of trust from students, parents, and teachers. This thesis aims to bring human needs to the forefront of eXplainable AI (XAI) research, grounded in the concrete use case of personalized learning and teaching. We frame the contributions along two verticals: technical advances in XAI and their aligned human studies. We investigate explainability in AI for education, revealing systematic disagreements between post-hoc explainers and identifying a need for inherently interpretable model architectures. We propose four novel technical contributions in interpretability with a multimodal modular architecture (MultiModN), an interpretable mixture-of-experts model (InterpretCC), adversarial training for explainer stability, and a theory-driven LLM-XAI framework to present explanations to students (iLLuMinaTE), which we evaluate in diverse settings with professors, teachers, learning scientists, and university students. By combining empirical evaluations of existing explainers with novel architectural designs and human studies, our work lays a foundation for human-centric AI systems that balance state-of-the-art performance with built-in transparency and trust.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models</title>
<link>https://arxiv.org/abs/2505.22549</link>
<guid>https://arxiv.org/abs/2505.22549</guid>
<content:encoded><![CDATA[
arXiv:2505.22549v1 Announce Type: new 
Abstract: Scaling foundation model training with Distributed Data Parallel (DDP) methods is bandwidth-limited. Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs. We propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of optimizers assigning independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence. Through extensive experiments on language models of up to 1.7B, we show that DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches, DES-LOC is suited for practical training scenarios prone to system failures. DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Hyena Networks for Large-scale Equivariant Learning</title>
<link>https://arxiv.org/abs/2505.22560</link>
<guid>https://arxiv.org/abs/2505.22560</guid>
<content:encoded><![CDATA[
arXiv:2505.22560v1 Announce Type: new 
Abstract: Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.22573</link>
<guid>https://arxiv.org/abs/2505.22573</guid>
<content:encoded><![CDATA[
arXiv:2505.22573v1 Announce Type: new 
Abstract: Simulation-based inference (SBI) is an established approach for performing Bayesian inference on scientific simulators. SBI so far works best on low-dimensional parametric models. However, it is difficult to infer function-valued parameters, which frequently occur in disciplines that model spatiotemporal processes such as the climate and earth sciences. Here, we introduce an approach for efficient posterior estimation, using a Fourier Neural Operator (FNO) architecture with a flow matching objective. We show that our approach, FNOPE, can perform inference of function-valued parameters at a fraction of the simulation budget of state of the art methods. In addition, FNOPE supports posterior evaluation at arbitrary discretizations of the domain, as well as simultaneous estimation of vector-valued parameters. We demonstrate the effectiveness of our approach on several benchmark tasks and a challenging spatial inference task from glaciology. FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benignity of loss landscape with weight decay requires both large overparametrization and initialization</title>
<link>https://arxiv.org/abs/2505.22578</link>
<guid>https://arxiv.org/abs/2505.22578</guid>
<content:encoded><![CDATA[
arXiv:2505.22578v1 Announce Type: new 
Abstract: The optimization of neural networks under weight decay remains poorly understood from a theoretical standpoint. While weight decay is standard practice in modern training procedures, most theoretical analyses focus on unregularized settings. In this work, we investigate the loss landscape of the $\ell_2$-regularized training loss for two-layer ReLU networks. We show that the landscape becomes benign -- i.e., free of spurious local minima -- under large overparametrization, specifically when the network width $m$ satisfies $m \gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the input dimension. More precisely in this regime, almost all constant activation regions contain a global minimum and no spurious local minima. We further show that this level of overparametrization is not only sufficient but also necessary via the example of orthogonal data. Finally, we demonstrate that such loss landscape results primarily hold relevance in the large initialization regime. In contrast, for small initializations -- corresponding to the feature learning regime -- optimization can still converge to spurious local minima, despite the global benignity of the landscape.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Unlearning under Overparameterization</title>
<link>https://arxiv.org/abs/2505.22601</link>
<guid>https://arxiv.org/abs/2505.22601</guid>
<content:encoded><![CDATA[
arXiv:2505.22601v1 Announce Type: new 
Abstract: Machine unlearning algorithms aim to remove the influence of specific training samples, ideally recovering the model that would have resulted from training on the remaining data alone. We study unlearning in the overparameterized setting, where many models interpolate the data, and defining the unlearning solution as any loss minimizer over the retained set$\unicode{x2013}$as in prior work in the underparameterized setting$\unicode{x2013}$is inadequate, since the original model may already interpolate the retained data and satisfy this condition. In this regime, loss gradients vanish, rendering prior methods based on gradient perturbations ineffective, motivating both new unlearning definitions and algorithms. For this setting, we define the unlearning solution as the minimum-complexity interpolator over the retained data and propose a new algorithmic framework that only requires access to model gradients on the retained set at the original solution. We minimize a regularized objective over perturbations constrained to be orthogonal to these model gradients, a first-order relaxation of the interpolation condition. For different model classes, we provide exact and approximate unlearning guarantees, and we demonstrate that an implementation of our framework outperforms existing baselines across various unlearning experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Rank at a Time: Cascading Error Dynamics in Sequential Learning</title>
<link>https://arxiv.org/abs/2505.22602</link>
<guid>https://arxiv.org/abs/2505.22602</guid>
<content:encoded><![CDATA[
arXiv:2505.22602v1 Announce Type: new 
Abstract: Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2505.22617</link>
<guid>https://arxiv.org/abs/2505.22617</guid>
<content:encoded><![CDATA[
arXiv:2505.22617v1 Announce Type: new 
Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding (Un)Reliability of Steering Vectors in Language Models</title>
<link>https://arxiv.org/abs/2505.22637</link>
<guid>https://arxiv.org/abs/2505.22637</guid>
<content:encoded><![CDATA[
arXiv:2505.22637v1 Announce Type: new 
Abstract: Steering vectors are a lightweight method to control language model behavior by adding a learned bias to the activations at inference time. Although steering demonstrates promising performance, recent work shows that it can be unreliable or even counterproductive in some cases. This paper studies the influence of prompt types and the geometry of activation differences on steering reliability. First, we find that all seven prompt types used in our experiments produce a net positive steering effect, but exhibit high variance across samples, and often give an effect opposite of the desired one. No prompt type clearly outperforms the others, and yet the steering vectors resulting from the different prompt types often differ directionally (as measured by cosine similarity). Second, we show that higher cosine similarity between training set activation differences predicts more effective steering. Finally, we observe that datasets where positive and negative activations are better separated are more steerable. Our results suggest that vector steering is unreliable when the target behavior is not represented by a coherent direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Survival Analysis</title>
<link>https://arxiv.org/abs/2505.22641</link>
<guid>https://arxiv.org/abs/2505.22641</guid>
<content:encoded><![CDATA[
arXiv:2505.22641v1 Announce Type: new 
Abstract: Survival analysis is widely deployed in a diverse set of fields, including healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model is a semi-parametric model often encountered in the literature. Despite its popularity, wide deployment, and numerous variants, scaling CoxPH to large datasets and deep architectures poses a challenge, especially in the high-dimensional regime. We identify a fundamental connection between rank regression and the CoxPH model: this allows us to adapt and extend the so-called spectral method for rank regression to survival analysis. Our approach is versatile, naturally generalizing to several CoxPH variants, including deep models. We empirically verify our method's scalability on multiple real-world high-dimensional datasets; our method outperforms legacy methods w.r.t. predictive performance and efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Learning Verifiers for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.22650</link>
<guid>https://arxiv.org/abs/2505.22650</guid>
<content:encoded><![CDATA[
arXiv:2505.22650v1 Announce Type: new 
Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving complex mathematical and logical problems. However, it can often veer off track through incorrect or unsubstantiated inferences. Formal mathematical reasoning, which can be checked with a formal verifier, is one approach to addressing this issue. However, currently LLMs are simply not good enough to solve complex problems in a formal way, and even just formalizing an informal problem statement can be challenging. Motivated by this fact, in this work we consider the problem of learning reliable verifiers for natural language Chain-of-Thought reasoning. That is, given a problem statement and step-by-step solution in natural language, the aim of the verifier is to output [Yes] if the reasoning steps in the solution are all valid, and [No] otherwise. In this work we give a formal PAC-learning framework for studying this problem. We propose and analyze several natural verification goals, at different levels of strength, in this framework. We provide sample complexity upper-bounds for learning verifiers satisfying these goals, as well as lower-bound and impossibility results for learning other natural verification objectives without additional assumptions.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents</title>
<link>https://arxiv.org/abs/2505.22655</link>
<guid>https://arxiv.org/abs/2505.22655</guid>
<content:encoded><![CDATA[
arXiv:2505.22655v1 Announce Type: new 
Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Confidence Alone Improves Reasoning</title>
<link>https://arxiv.org/abs/2505.22660</link>
<guid>https://arxiv.org/abs/2505.22660</guid>
<content:encoded><![CDATA[
arXiv:2505.22660v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offset Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.11045</link>
<guid>https://arxiv.org/abs/2404.11045</guid>
<content:encoded><![CDATA[
arXiv:2404.11045v2 Announce Type: cross 
Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose {\delta}-Unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. {\delta}-Unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise</title>
<link>https://arxiv.org/abs/2505.18478</link>
<guid>https://arxiv.org/abs/2505.18478</guid>
<content:encoded><![CDATA[
arXiv:2505.18478v1 Announce Type: cross 
Abstract: Advancements in quantum computing have spurred significant interest in harnessing its potential for speedups over classical systems. However, noise remains a major obstacle to achieving reliable quantum algorithms. In this work, we present a provably noise-resilient training theory and algorithm to enhance the robustness of parameterized quantum circuit classifiers. Our method, with a natural connection to Evolutionary Strategies, guarantees resilience to parameter noise with minimal adjustments to commonly used optimization algorithms. Our approach is function-agnostic and adaptable to various quantum circuits, successfully demonstrated in quantum phase classification tasks. By developing provably guaranteed optimization theory with quantum circuits, our work opens new avenues for practical, robust applications of near-term quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability-Based Scaling Laws for LLM Red-Teaming</title>
<link>https://arxiv.org/abs/2505.20162</link>
<guid>https://arxiv.org/abs/2505.20162</guid>
<content:encoded><![CDATA[
arXiv:2505.20162v1 Announce Type: cross 
Abstract: As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI</title>
<link>https://arxiv.org/abs/2505.20344</link>
<guid>https://arxiv.org/abs/2505.20344</guid>
<content:encoded><![CDATA[
arXiv:2505.20344v1 Announce Type: cross 
Abstract: Brain aging trajectories differ between males and females, yet the genetic factors underlying these differences remain underexplored. Using structural MRI and genotyping data from 40,940 UK Biobank participants (aged 45-83), we computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and ventricular volumes. We conducted sex-stratified genome-wide association studies (GWAS) and Post-GWAS analyses to identify genetic variants associated with accelerated brain aging. Distinct gene sets emerged by sex: in females, neurotransmitter transport and mitochondrial stress response genes were implicated; in males, immune and inflammation-related genes dominated. Shared genes, including GMNC and OSTN, were consistently linked to brain volumes across sexes, suggesting core roles in neurostructural maintenance. Tissue expression analyses revealed sex-specific enrichment in pathways tied to neurodegeneration. These findings highlight the importance of sex-stratified approaches in aging research and suggest genetic targets for personalized interventions against age-related cognitive decline.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models</title>
<link>https://arxiv.org/abs/2505.21507</link>
<guid>https://arxiv.org/abs/2505.21507</guid>
<content:encoded><![CDATA[
arXiv:2505.21507v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis of numerous neurological disorders. Due to the large volume of EEGs requiring interpretation and the specific expertise involved, artificial intelligence-based tools are being developed to assist in their visual analysis. In this paper, we compare two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1, a recently proposed foundation model, in the task of classifying entire EEG recordings as normal or abnormal. The three models were trained or finetuned on 2,500 EEG recordings and their performances were evaluated on two private and one public datasets: a large multicenter dataset annotated by a single specialist (dataset A composed of n = 4,480 recordings), a small multicenter dataset annotated by three specialists (dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus evaluation dataset (n = 276). On dataset A, the three models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced accuracy. The models were then validated on TUAB evaluation dataset, whose corresponding training set was not used during training, where they achieved at least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results highlight the usefulness of leveraging pre-trained models for automatic EEG classification: enabling robust and efficient interpretation of EEG data with fewer resources and broader applicability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vision Transformer Explainability Using Artificial Astrocytes</title>
<link>https://arxiv.org/abs/2505.21513</link>
<guid>https://arxiv.org/abs/2505.21513</guid>
<content:encoded><![CDATA[
arXiv:2505.21513v1 Announce Type: cross 
Abstract: Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures</title>
<link>https://arxiv.org/abs/2505.21522</link>
<guid>https://arxiv.org/abs/2505.21522</guid>
<content:encoded><![CDATA[
arXiv:2505.21522v1 Announce Type: cross 
Abstract: While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Shared Representations from Unpaired Data</title>
<link>https://arxiv.org/abs/2505.21524</link>
<guid>https://arxiv.org/abs/2505.21524</guid>
<content:encoded><![CDATA[
arXiv:2505.21524v1 Announce Type: cross 
Abstract: Learning shared representations is a primary area of multimodal representation learning. The current approaches to achieve a shared embedding space rely heavily on paired samples from each modality, which are significantly harder to obtain than unpaired ones. In this work, we demonstrate that shared representations can be learned almost exclusively from unpaired data. Our arguments are grounded in the spectral embeddings of the random walk matrices constructed independently from each unimodal representation. Empirical results in computer vision and natural language processing domains support its potential, revealing the effectiveness of unpaired data in capturing meaningful cross-modal relations, demonstrating high capabilities in retrieval tasks, generation, arithmetics, zero-shot, and cross-domain classification. This work, to the best of our knowledge, is the first to demonstrate these capabilities almost exclusively from unpaired samples, giving rise to a cross-modal embedding that could be viewed as universal, i.e., independent of the specific modalities of the data. Our code IS publicly available at https://github.com/shaham-lab/SUE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDB++: Fast Sampling of Unified Diffusion Bridge</title>
<link>https://arxiv.org/abs/2505.21528</link>
<guid>https://arxiv.org/abs/2505.21528</guid>
<content:encoded><![CDATA[
arXiv:2505.21528v1 Announce Type: cross 
Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's reliance on iterative Euler sampling methods results in slow, computationally expensive inference, while existing acceleration techniques for diffusion or diffusion bridge models fail to address its unique challenges: missing terminal mean constraints and SOC-specific penalty coefficients in its SDEs. We present UniDB++, a training-free sampling algorithm that significantly improves upon these limitations. The method's key advancement comes from deriving exact closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the error accumulation inherent in Euler approximations and enabling high-quality generation with up to 20$\times$ fewer sampling steps. This method is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes (5-10 steps). Additionally, we demonstrate that UniDB++ aligns with existing diffusion bridge acceleration methods by evaluating their update rules, and UniDB++ can recover DBIMs as special cases under some theoretical conditions. Experiments demonstrate UniDB++'s state-of-the-art performance in image restoration tasks, outperforming Euler-based methods in fidelity and speed while reducing inference time significantly. This work bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models. Our code is available at https://github.com/2769433owo/UniDB-plusplus.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media</title>
<link>https://arxiv.org/abs/2505.21532</link>
<guid>https://arxiv.org/abs/2505.21532</guid>
<content:encoded><![CDATA[
arXiv:2505.21532v1 Announce Type: cross 
Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Visual Prototypes for Non-Parametric Representation Learning</title>
<link>https://arxiv.org/abs/2505.21533</link>
<guid>https://arxiv.org/abs/2505.21533</guid>
<content:encoded><![CDATA[
arXiv:2505.21533v1 Announce Type: cross 
Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning. Unlike existing prototypical self-supervised learning (SSL) methods that rely on a single prototype to encode all relevant features of a hidden cluster in the data, we propose the SOP strategy. In this strategy, a prototype is represented by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features that together better characterize their region in space and maximize training performance. We reaffirm the feasibility of non-parametric SSL by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. Notably, we introduce the SOP Masked Image Modeling (SOP-MIM) task, where masked representations are reconstructed from the perspective of multiple non-parametric local SEs. We comprehensively evaluate the representations learned using the SOP strategy on a range of benchmarks, including retrieval, linear evaluation, fine-tuning, and object detection. Our pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement</title>
<link>https://arxiv.org/abs/2505.21535</link>
<guid>https://arxiv.org/abs/2505.21535</guid>
<content:encoded><![CDATA[
arXiv:2505.21535v1 Announce Type: cross 
Abstract: While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.21545</link>
<guid>https://arxiv.org/abs/2505.21545</guid>
<content:encoded><![CDATA[
arXiv:2505.21545v1 Announce Type: cross 
Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the first corruption-aware training framework for LVDMs that improves robustness through structured, data-aligned noise injection. Our method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency. BCNI is especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness, showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101. Ablation studies confirm the benefit of low-rank, data-aligned noise. Our theoretical analysis further explains how such perturbations tighten entropy, Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise. Code and models: https://github.com/chikap421/catlvdm
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper</title>
<link>https://arxiv.org/abs/2505.21551</link>
<guid>https://arxiv.org/abs/2505.21551</guid>
<content:encoded><![CDATA[
arXiv:2505.21551v1 Announce Type: cross 
Abstract: Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the learned look-ahead behavior of chess neural networks</title>
<link>https://arxiv.org/abs/2505.21552</link>
<guid>https://arxiv.org/abs/2505.21552</guid>
<content:encoded><![CDATA[
arXiv:2505.21552v1 Announce Type: cross 
Abstract: We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.21553</link>
<guid>https://arxiv.org/abs/2505.21553</guid>
<content:encoded><![CDATA[
arXiv:2505.21553v1 Announce Type: cross 
Abstract: Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification</title>
<link>https://arxiv.org/abs/2505.21558</link>
<guid>https://arxiv.org/abs/2505.21558</guid>
<content:encoded><![CDATA[
arXiv:2505.21558v1 Announce Type: cross 
Abstract: Agricultural research has accelerated in recent years, yet farmers often lack the time and resources for on-farm research due to the demands of crop production and farm operations. Seed classification offers valuable insights into quality control, production efficiency, and impurity detection. Early identification of seed types is critical to reducing the cost and risk associated with field emergence, which can lead to yield losses or disruptions in downstream processes like harvesting. Seed sampling supports growers in monitoring and managing seed quality, improving precision in determining seed purity levels, guiding management adjustments, and enhancing yield estimations. This study proposes a novel convolutional neural network (CNN)-based framework for the efficient classification of ten common Brassica seed types. The approach addresses the inherent challenge of texture similarity in seed images using a custom-designed CNN architecture. The model's performance was evaluated against several pre-trained state-of-the-art architectures, with adjustments to layer configurations for optimized classification. Experimental results using our collected Brassica seed dataset demonstrate that the proposed model achieved a high accuracy rate of 93 percent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment</title>
<link>https://arxiv.org/abs/2505.21561</link>
<guid>https://arxiv.org/abs/2505.21561</guid>
<content:encoded><![CDATA[
arXiv:2505.21561v1 Announce Type: cross 
Abstract: We introduce a novel deep learning framework for the automated staging of spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in both orthodontics and forensic anthropology. Our approach leverages a dual-model architecture wherein a teacher model, trained on manually cropped images, transfers its precise spatial understanding to a student model that operates on full, uncropped images. This knowledge distillation is facilitated by a newly formulated loss function that aligns spatial logits as well as incorporates gradient-based attention spatial mapping, ensuring that the student model internalizes the anatomically relevant features without relying on external cropping or YOLO-based segmentation. By leveraging expert-curated data and feedback at each step, our framework attains robust diagnostic accuracy, culminating in a clinically viable end-to-end pipeline. This streamlined approach obviates the need for additional pre-processing tools and accelerates deployment, thereby enhancing both the efficiency and consistency of skeletal maturation assessment in diverse clinical settings.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model</title>
<link>https://arxiv.org/abs/2505.21564</link>
<guid>https://arxiv.org/abs/2505.21564</guid>
<content:encoded><![CDATA[
arXiv:2505.21564v1 Announce Type: cross 
Abstract: In deep multi-instance learning, the number of applicable instances depends on the data set. In histopathology images, deep learning multi-instance learners usually assume there are hundreds to thousands instances in a bag. However, when the number of instances in a bag increases to 256 in brain hematoma CT, learning becomes extremely difficult. In this paper, we address this drawback. To overcome this problem, we propose using a pre-trained model with self-supervised learning for the multi-instance learner as a downstream task. With this method, even when the original target task suffers from the spurious correlation problem, we show improvements of 5% to 13% in accuracy and 40% to 55% in the F1 measure for the hypodensity marker classification of brain hematoma CT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model-based Activity Completion for AI Motion Capture from Videos</title>
<link>https://arxiv.org/abs/2505.21566</link>
<guid>https://arxiv.org/abs/2505.21566</guid>
<content:encoded><![CDATA[
arXiv:2505.21566v1 Announce Type: cross 
Abstract: AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.21567</link>
<guid>https://arxiv.org/abs/2505.21567</guid>
<content:encoded><![CDATA[
arXiv:2505.21567v1 Announce Type: cross 
Abstract: With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>