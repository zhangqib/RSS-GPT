<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</title>
<link>https://arxiv.org/abs/2505.03793</link>
<guid>https://arxiv.org/abs/2505.03793</guid>
<content:encoded><![CDATA[
<div> derive, LENSLLM, Neural Tangent Kernel, PAC-Bayesian Generalization Bound, computational efficiency

Summary:
The article addresses the challenge of efficient model selection for Large Language Models (LLMs) within diverse downstream tasks. It introduces a theoretical framework that assesses the generalization capabilities of LLMs during fine-tuning. By deriving a PAC-Bayesian Generalization Bound, the fine-tuning dynamics of LLMs are unveiled, leading to the development of the LENSLLM model. This model, based on Neural Tangent Kernel (NTK), enables accurate performance predictions across various tasks while maintaining computational efficiency. Empirical results on large-scale benchmarks demonstrate the effectiveness of LENSLLM, achieving up to 91.1% accuracy and reducing computational costs by up to 88.5% compared to state-of-the-art methods. The open-sourced LENSLLM model and results are available at LensLLM.io. 

<br /><br />Summary: <div>
arXiv:2505.03793v3 Announce Type: replace 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title>
<link>https://arxiv.org/abs/2505.21938</link>
<guid>https://arxiv.org/abs/2505.21938</guid>
<content:encoded><![CDATA[
<div> fake data injection, stochastic bandits, adversarial attacks, bounded perturbations, attack strategies

Summary:<br />
The article introduces a new practical threat model called Fake Data Injection for adversarial attacks on stochastic bandits. This model allows attackers to inject a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. The proposed attack strategies efficiently mislead Upper Confidence Bound (UCB) and Thompson Sampling algorithms by addressing both magnitude and temporal constraints. The theoretical analysis demonstrates that these attacks can manipulate the algorithms into selecting a target arm in nearly all rounds with minimal attack cost. Experiments on synthetic and real-world datasets confirm the effectiveness of the strategies and highlight vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios. <div>
arXiv:2505.21938v2 Announce Type: replace 
Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Approach to Improving Fairness in K-means Clustering</title>
<link>https://arxiv.org/abs/2505.22984</link>
<guid>https://arxiv.org/abs/2505.22984</guid>
<content:encoded><![CDATA[
<div> fairness, K-means clustering, bias, sensitive variable, optimization

Summary:
This study addresses the fairness issue in K-means clustering algorithms, where clusters may exhibit bias based on sensitive variables such as gender or race. The proposed approach involves a two-stage optimization process that first performs clustering and then adjusts the cluster membership of selected data points to improve fairness. Two efficient algorithms are introduced to identify problematic data points that may impact fairness. One algorithm focuses on nearest data points outside of a cluster, while the other targets highly 'mixed' data points. Experimental results on benchmark datasets demonstrate significant improvements in fairness while maintaining clustering quality. The proposed algorithms can be easily adapted to various clustering methods and fairness metrics, offering a practical solution to address biases in clustering outcomes. 

<br /><br />Summary: <div>
arXiv:2505.22984v2 Announce Type: replace 
Abstract: The popular K-means clustering algorithm potentially suffers from a major weakness for further analysis or interpretation. Some cluster may have disproportionately more (or fewer) points from one of the subpopulations in terms of some sensitive variable, e.g., gender or race. Such a fairness issue may cause bias and unexpected social consequences. This work attempts to improve the fairness of K-means clustering with a two-stage optimization formulation--clustering first and then adjust cluster membership of a small subset of selected data points. Two computationally efficient algorithms are proposed in identifying those data points that are expensive for fairness, with one focusing on nearest data points outside of a cluster and the other on highly 'mixed' data points. Experiments on benchmark datasets show substantial improvement on fairness with a minimal impact to clustering quality. The proposed algorithms can be easily extended to a broad class of clustering algorithms or fairness metrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Model Learning for Improved Elastic Student Models</title>
<link>https://arxiv.org/abs/2505.23337</link>
<guid>https://arxiv.org/abs/2505.23337</guid>
<content:encoded><![CDATA[
<div> Teacher-TA-Student recipe, MatTA framework, multiple accurate student models, serving cost, A/B tests<br />
<br />
Summary: 
The paper introduces the MatTA framework, which is designed to efficiently train multiple accurate student models using a novel Teacher-TA-Student recipe. By leveraging TA models, which are larger versions of student models, MatTA enables student models to better relate to the teacher model and incorporate domain-specific expertise. This approach allows for multiple servable options with varying levels of accuracy without the need for additional training runs, thereby providing a cost-effective solution. The practical efficacy of MatTA is demonstrated through live A/B tests within a production ML system, showcasing a 20% improvement on a key metric. Additionally, the method is applied to the GPT-2 Medium model, achieving significant relative improvements on both the SAT Math and LAMBADA benchmark tasks. Overall, the MatTA framework offers a versatile and efficient approach to model training that can enhance performance and reduce serving costs. 

<br /><br /> <div>
arXiv:2505.23337v2 Announce Type: replace 
Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models</title>
<link>https://arxiv.org/abs/2505.23378</link>
<guid>https://arxiv.org/abs/2505.23378</guid>
<content:encoded><![CDATA[
<div> speaker-dependent modelling, speech-based health monitoring, meta-learning, ensemble-based distance models, transformer-based sequence models

Summary:
- Speaker-dependent modelling improves performance in speech-based health monitoring.
- Mixed-effect models for speaker adaptation are computationally expensive for new observations.
- The task is reformulated as a meta-learning problem.
- Three approaches are explored: ensemble-based distance models, prototypical networks, and transformer-based sequence models.
- Pre-trained speech embeddings are used on a dataset of shift workers to predict time since sleep from speech as a function of fatigue.
- Meta-learning approaches outperformed conventional models, with transformer-based method showing the strongest performance. 

<br /><br />Summary: <div>
arXiv:2505.23378v2 Announce Type: replace 
Abstract: Speaker-dependent modelling can substantially improve performance in speech-based health monitoring applications. While mixed-effect models are commonly used for such speaker adaptation, they require computationally expensive retraining for each new observation, making them impractical in a production environment. We reformulate this task as a meta-learning problem and explore three approaches of increasing complexity: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Using pre-trained speech embeddings, we evaluate these methods on a large longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting time since sleep from speech as a function of fatigue, a symptom commonly associated with ill-health. Our results demonstrate that all meta-learning approaches tested outperformed both cross-sectional and conventional mixed-effects models, with a transformer-based method achieving the strongest performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalizing Flows are Capable Models for RL</title>
<link>https://arxiv.org/abs/2505.23527</link>
<guid>https://arxiv.org/abs/2505.23527</guid>
<content:encoded><![CDATA[
<div> transformers, energy-based models, diffusion/flow-based models, reinforcement learning, normalizing flows

Summary:
- The article discusses the use of powerful probabilistic models in modern reinforcement learning algorithms, such as transformers, energy-based models, and diffusion/flow-based models.
- Researchers often face challenges integrating these models into their algorithms due to computational intensity or discrete representations requirements.
- Normalizing flows (NFs) offer a promising alternative for likelihoods and sampling without the drawbacks of other models.
- The study introduces a single NF architecture that seamlessly integrates into RL algorithms as a policy, Q-function, and occupancy measure.
- This approach simplifies algorithms and improves performance in imitation learning, offline, goal conditioned RL, and unsupervised RL. 

<br /><br />Summary: <div>
arXiv:2505.23527v2 Announce Type: replace 
Abstract: Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
<div> framework, anomaly detection, deep learning, clustering, MADCluster
Summary:
- MADCluster is a novel model-agnostic anomaly detection framework utilizing self-supervised clustering.
- The framework addresses the 'hypersphere collapse' problem in deep learning-based anomaly detection methods by clustering normal pattern data into a single cluster.
- MADCluster introduces a new 'One-directed Adaptive loss' to improve expressiveness and enable effective single clustering, with its optimization mathematically proven.
- The framework consists of Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates.
- MADCluster's model-agnostic characteristics allow for compatibility with various architectures and demonstrate improved overall performance on time series benchmark datasets. 

Summary:<br />
framework, anomaly detection, deep learning, clustering, MADCluster <div>
arXiv:2505.16223v4 Announce Type: replace-cross 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
<div> survival bandit framework, trade-offs, AI agents, resource constraints, utility functions <br />
Summary: <br />
The article discusses the impact of resource constraints on AI agents with agentic capabilities, highlighting how such constraints can lead to shifts in their rational behavior. It focuses on a survival bandit framework to formalize the setting where agents face trade-offs due to limited resources. The study explores the emergence of misalignment between human objectives and agent incentives in such scenarios, offering theoretical and empirical results to quantify the effects. The research identifies conditions under which misalignment occurs and proposes strategies to mitigate risk-seeking or risk-averse behaviors in AI agents. By enhancing understanding of how agents operate under survival pressure, the work aims to provide guidelines for safely deploying AI systems in critical resource-limited environments. <div>
arXiv:2505.23436v2 Announce Type: replace-cross 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation</title>
<link>https://arxiv.org/abs/2505.23657</link>
<guid>https://arxiv.org/abs/2505.23657</guid>
<content:encoded><![CDATA[
<div> Keywords: decoding methods, factuality, large language models, hallucinations, generation

Summary: 
Active Layer-Contrastive Decoding (ActLCD) is a novel decoding strategy aimed at improving the factuality of large language models (LLMs) by actively deciding when to apply contrasting layers during generation. By using a reinforcement learning policy guided by a reward-aware classifier, ActLCD optimizes factuality beyond the token level. This innovative approach overcomes the limitations of existing decoding methods by effectively mitigating hallucinations in diverse generation scenarios. Experimental results demonstrate that ActLCD outperforms current state-of-the-art methods across five benchmarks, showcasing its efficacy in enhancing factuality and reducing the occurrence of erroneous outputs. <div>
arXiv:2505.23657v2 Announce Type: replace-cross 
Abstract: Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, software development, performance tests, optimization tasks

Summary:
The article introduces GSO, a benchmark for evaluating language models in developing high-performance software. An automated pipeline is used to analyze repository commit histories and identify 102 optimization tasks across diverse codebases. Agents are tasked with improving runtime efficiency based on expert developer optimization, but leading SWE-Agents struggle, achieving less than a 5% success rate. Challenges include difficulties with low-level languages, lazy optimization strategies, and accurately localizing bottlenecks. The release of the benchmark's code and artifacts, along with agent trajectories, aims to support future research in this area. <div>
arXiv:2505.23671v2 Announce Type: replace-cross 
Abstract: Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement</title>
<link>https://arxiv.org/abs/2506.00030</link>
<guid>https://arxiv.org/abs/2506.00030</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal fusion, Shapley-guided alternating training, modality imbalance, memory module, equilibrium deviation metric (EDM) <br />
Summary: 
The article proposes a Shapley-guided alternating training framework to address modality imbalance in multimodal fusion, prioritizing minor modalities to enhance fusion balance. The method uses Shapley Value-based scheduling for adaptive training sequencing and introduces a memory module for refining modality-specific representations. By adopting conventional and LLM-based backbones in the encoder module, the approach achieves state-of-the-art results across four multimodal benchmark datasets. The proposed equilibrium deviation metric (EDM) evaluates performance in balance and accuracy, demonstrating robustness under missing modalities. The findings highlight the potential of modality prioritization in balancing multimodal learning dynamics, offering a new paradigm for optimizing multimodal training. <br /><br />Summary: <div>
arXiv:2506.00030v1 Announce Type: new 
Abstract: Multimodal fusion is susceptible to modality imbalance, where dominant modalities overshadow weak ones, easily leading to biased learning and suboptimal fusion, especially for incomplete modality conditions. To address this problem, we propose a Shapley-guided alternating training framework that adaptively prioritizes minor modalities to balance and thus enhance the fusion. Our method leverages Shapley Value-based scheduling to improve the training sequence adaptively, ensuring that under-optimized modalities receive sufficient learning. Additionally, we introduce the memory module to refine and inherit modality-specific representations with a cross-modal mapping mechanism to align features at both the feature and sample levels. To further validate the adaptability of the proposed approach, the encoder module empirically adopts both conventional and LLM-based backbones. With building up a novel multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we evaluate the performance in both balance and accuracy across four multimodal benchmark datasets, where our method achieves state-of-the-art (SOTA) results. Meanwhile, robustness analysis under missing modalities highlights its strong generalization capabilities. Accordingly, our findings reveal the untapped potential of alternating training, demonstrating that strategic modality prioritization fundamentally balances and promotes multimodal learning, offering a new paradigm for optimizing multimodal training dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing</title>
<link>https://arxiv.org/abs/2506.00039</link>
<guid>https://arxiv.org/abs/2506.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, fNIRS, auditory processing, AbsoluteNet, classification

Summary: 
AbsoluteNet, a novel deep learning architecture, has been developed for classifying auditory event-related responses captured by fNIRS. It utilizes spatio-temporal convolution and custom activation functions to achieve superior performance in comparison to existing models such as fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. AbsoluteNet demonstrated an accuracy of 87.0%, sensitivity of 84.8%, and specificity of 89.2% in binary classification tasks, outperforming fNIRSNET by 3.8% in accuracy. The success of AbsoluteNet highlights the significance of spatio-temporal feature aggregation and customized activation functions in decoding hemodynamic responses linked to auditory processing using fNIRS technology. This research showcases the potential of deep learning approaches in BCI applications and sheds light on optimizing neural network architectures for analyzing fNIRS data. 

Summary:<br /><br />Keywords: deep learning, fNIRS, auditory processing, AbsoluteNet, classification<br /><br />AbsoluteNet, a novel deep learning architecture, outperforms existing models in classifying auditory event-related responses recorded with fNIRS. It achieves 87.0% accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification, surpassing fNIRSNET by 3.8% in accuracy. The effectiveness of AbsoluteNet emphasizes the importance of spatio-temporal feature aggregation and customized activation functions in decoding hemodynamic responses related to auditory processing with fNIRS. This research showcases the potential of deep learning models for BCI applications and underscores the need for tailored neural network architectures for analyzing fNIRS data. <div>
arXiv:2506.00039v1 Announce Type: new 
Abstract: In recent years, deep learning (DL) approaches have demonstrated promising results in decoding hemodynamic responses captured by functional near-infrared spectroscopy (fNIRS), particularly in the context of brain-computer interface (BCI) applications. This work introduces AbsoluteNet, a novel deep learning architecture designed to classify auditory event-related responses recorded using fNIRS. The proposed network is built upon principles of spatio-temporal convolution and customized activation functions. Our model was compared against several models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The results showed that AbsoluteNet outperforms existing models, reaching 87.0% accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification, surpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings underscore the effectiveness of our proposed deep learning model in decoding hemodynamic responses related to auditory processing and highlight the importance of spatio-temporal feature aggregation and customized activation functions to better fit fNIRS dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Offline Reinforcement Learning with Online Delays</title>
<link>https://arxiv.org/abs/2506.00131</link>
<guid>https://arxiv.org/abs/2506.00131</guid>
<content:encoded><![CDATA[
<div> Offline-to-online deployment, reinforcement learning (RL), sim-to-real gap, interaction gap, delay-robust actions <br />
<br />
Summary:
Offline-to-online deployment of RL agents faces challenges in bridging the sim-to-real and interaction gaps. DT-CORL, a framework introduced in this study, addresses these challenges by producing delay-robust actions using a transformer-based belief predictor and improving sample efficiency compared to history-augmentation baselines. The framework demonstrates superior performance on D4RL benchmarks with varying delay settings, outperforming both history-augmentation and vanilla belief-based methods. By narrowing the sim-to-real latency gap and maintaining data efficiency, DT-CORL shows promise in addressing the challenges associated with deploying RL agents in real-world environments with delayed dynamics. <div>
arXiv:2506.00131v1 Announce Type: new 
Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than na\"ive history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning</title>
<link>https://arxiv.org/abs/2506.00135</link>
<guid>https://arxiv.org/abs/2506.00135</guid>
<content:encoded><![CDATA[
<div> Online learning, transductive learning, Empirical Risk Minimization (ERM), weak consistency oracles, concept class<br />
<br />Summary: <br />This study examines online and transductive online learning with interaction through ERM or weak consistency oracles on subsets of a concept class. Tight lower bounds are proven in standard online settings with ERM access. Existing online learning results with ERM access are shown to carry over to the weak consistency setting with an additional cost in oracle calls. For the transductive online model, optimal mistake bounds can be achieved using weak consistency oracle calls for general Littlestone classes. Limiting the learner to a certain number of queries is necessary for efficient transductive online learning. Randomized algorithms are employed to reduce oracle calls while maintaining mistake bounds for specific concept classes such as Thresholds and $k$-Intervals. In particular, a logarithmic number of ERM queries are sufficient for Thresholds, while a polynomial number of weak consistency queries are required for $k$-Intervals. <div>
arXiv:2506.00135v1 Announce Type: new 
Abstract: We study online and transductive online learning when the learner interacts with the concept class only via Empirical Risk Minimization (ERM) or weak consistency oracles on arbitrary instance subsets. This contrasts with standard online models, where the learner knows the entire class. The ERM oracle returns a hypothesis minimizing loss on a given subset, while the weak consistency oracle returns a binary signal indicating whether the subset is realizable by some concept. The learner is evaluated by the number of mistakes and oracle calls. In the standard online setting with ERM access, we prove tight lower bounds in both realizable and agnostic cases: $\Omega(2^{d_{VC}})$ mistakes and $\Omega(\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and $d_{LD}$ is the Littlestone dimension. We further show that existing online learning results with ERM access carry over to the weak consistency setting, incurring an additional $O(T)$ in oracle calls. We then consider the transductive online model, where the instance sequence is known but labels are revealed sequentially. For general Littlestone classes, we show that optimal realizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$ weak consistency oracle calls. On the negative side, we show that limiting the learner to $\Omega(T)$ weak consistency queries is necessary for transductive online learnability, and that restricting the learner to $\Omega(T)$ ERM queries is necessary to avoid exponential dependence on the Littlestone dimension. Finally, for certain concept classes, we reduce oracle calls via randomized algorithms while maintaining similar mistake bounds. In particular, for Thresholds on an unknown ordering, $O(\log T)$ ERM queries suffice; for $k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning</title>
<link>https://arxiv.org/abs/2506.00136</link>
<guid>https://arxiv.org/abs/2506.00136</guid>
<content:encoded><![CDATA[
<div> Diffusion autoencoders, latent variable, generative models, downstream tasks, domain transfer
<br />
Diffusion autoencoders (DAs) are models that use an input-dependent latent variable to capture representations in addition to the diffusion process. Another class of diffusion models focuses on learning the forward process. By combining design decisions from both classes, a new model termed DMZ is proposed. The DMZ model achieves effective representations for downstream tasks and domain transfer while also improving generative performance by reducing denoising steps compared to standard models. This innovative approach enhances the efficiency of generative modelling while maintaining the quality of learned representations.
<br /><br />Summary: <div>
arXiv:2506.00136v1 Announce Type: new 
Abstract: Diffusion autoencoders (DAs) are variants of diffusion generative models that use an input-dependent latent variable to capture representations alongside the diffusion process. These representations, to varying extents, can be used for tasks such as downstream classification, controllable generation, and interpolation. However, the generative performance of DAs relies heavily on how well the latent variables can be modelled and subsequently sampled from. Better generative modelling is also the primary goal of another class of diffusion models -- those that learn their forward (noising) process. While effective at adjusting the noise process in an input-dependent manner, they must satisfy additional constraints derived from the terminal conditions of the diffusion process. Here, we draw a connection between these two classes of models and show that certain design decisions (latent variable choice, conditioning method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us to obtain the best of both worlds: effective representations as evaluated on downstream tasks, including domain transfer, as well as more efficient modelling and generation with fewer denoising steps compared to standard DMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective</title>
<link>https://arxiv.org/abs/2506.00152</link>
<guid>https://arxiv.org/abs/2506.00152</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, observational data, confounders, DeconfoundLM

Summary: 

Large language models (LLMs) are commonly used in various industries for content generation to improve key performance metrics. Fine-tuning these models with good-quality labeled data is crucial for aligning with human preferences and business objectives. However, conducting controlled experiments like A/B tests for gathering such data can be expensive and challenging. This study explores the potential of using observational data, which is often underutilized, for fine-tuning LLMs. While observational outcomes can serve as useful guidance, directly fine-tuning models on such data may lead to learning spurious correlations. The DeconfoundLM method proposed in this work aims to address this issue by removing the impact of known confounding variables from reward signals. Empirical evidence from real-world datasets showcases the importance of causal corrections in utilizing observational data effectively for LLM alignment. By mitigating the risks associated with confounding variables, DeconfoundLM enhances the recovery of causal relationships and improves the performance of fine-tuned models. <div>
arXiv:2506.00152v1 Announce Type: new 
Abstract: Large language models are being widely used across industries to generate content that contributes directly to key performance metrics, such as conversion rates. Pretrained models, however, often fall short when it comes to aligning with human preferences or optimizing for business objectives. As a result, fine-tuning with good-quality labeled data is essential to guide models to generate content that achieves better results. Controlled experiments, like A/B tests, can provide such data, but they are often expensive and come with significant engineering and logistical challenges. Meanwhile, companies have access to a vast amount of historical (observational) data that remains underutilized. In this work, we study the challenges and opportunities of fine-tuning LLMs using observational data. We show that while observational outcomes can provide valuable supervision, directly fine-tuning models on such data can lead them to learn spurious correlations. We present empirical evidence of this issue using various real-world datasets and propose DeconfoundLM, a method that explicitly removes the effect of known confounders from reward signals. Using simulation experiments, we demonstrate that DeconfoundLM improves the recovery of causal relationships and mitigates failure modes found in fine-tuning methods that ignore or naively incorporate confounding variables. Our findings highlight that while observational data presents risks, with the right causal corrections, it can be a powerful source of signal for LLM alignment. Please refer to the project page for code and related resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States</title>
<link>https://arxiv.org/abs/2506.00158</link>
<guid>https://arxiv.org/abs/2506.00158</guid>
<content:encoded><![CDATA[
<div> emerging, zeroth-order optimization, differential privacy, memory constraints, smooth loss functions
<br />
Zeroth-order optimization has become a popular method for fine-tuning large language models with domain-specific data while ensuring differential privacy and working within memory constraints. This approach is promising but lacks comprehensive analysis and design for privacy protection. The question of hidden-state differential privacy analysis in zeroth-order optimization has been unanswered, hindering further development in this area. This work addresses this gap by proving a convergent differential privacy bound for zeroth-order optimization, adapting the privacy amplification-by-iteration framework to smooth loss functions. This novel analysis leads to improved algorithmic designs for zeroth-order optimization that were previously unknown in the literature.
<br /><br />Summary: <div>
arXiv:2506.00158v1 Announce Type: new 
Abstract: Zeroth-order optimization has emerged as a promising approach for fine-tuning large language models on domain-specific data, particularly under differential privacy (DP) and memory constraints. While first-order methods have been extensively studied from a privacy perspective, the privacy analysis and algorithmic design for zeroth-order methods remain significantly underexplored. A critical open question concerns hidden-state DP analysis: although convergent privacy bounds are known for first-order methods, it has remained unclear whether similar guarantees can be established for zeroth-order methods. In this work, we provide an affirmative answer by proving a convergent DP bound for zeroth-order optimization. Our analysis generalizes the celebrated privacy amplification-by-iteration framework to the setting of smooth loss functions in zeroth-order optimization. Furthermore, it induces better DP zeroth-order algorithmic designs that are previously unknown to the literature.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2506.00166</link>
<guid>https://arxiv.org/abs/2506.00166</guid>
<content:encoded><![CDATA[
<div> guardrail models, alignment training, Disentangled Safety Adapters, efficiency, flexibility

Summary:
Disentangled Safety Adapters (DSA) is introduced as a novel framework addressing challenges in AI safety. DSA decouples safety-specific computations from a task-optimized base model, utilizing lightweight adapters to enable diverse safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails outperform standalone models in hallucination detection, hate speech classification, and identifying unsafe inputs and responses. DSA-based safety alignment allows dynamic adjustment of alignment strength at inference time, offering a fine-grained trade-off between instruction following performance and model safety. Combining DSA safety guardrail with DSA safety alignment enhances context-dependent alignment strength, significantly boosting safety on StrongReject while maintaining high performance on MTBench. Overall, DSA presents a promising approach towards modular, efficient, and adaptable AI safety and alignment.<br /><br />Summary: <div>
arXiv:2506.00166v1 Announce Type: new 
Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents</title>
<link>https://arxiv.org/abs/2506.00172</link>
<guid>https://arxiv.org/abs/2506.00172</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, benchmarking methodology, code-repair tasks, task difficulty, automatic generation

Summary:
The article introduces Breakpoint, a benchmarking methodology designed to assess the capabilities of large language models (LLMs) in solving complex code-repair tasks in real-world software repositories. Breakpoint automatically generates tasks by corrupting functions in these repositories, controlling task difficulty based on local and system-level reasoning dimensions. Through more than 900 generated tasks, Breakpoint demonstrates scalable difficulty levels, with success rates of state-of-the-art models ranging from 55% on easier tasks to 0% on the most challenging ones. This novel approach allows for the evaluation of LLMs in tasks that require swift comprehension and manipulation of intricate structures, crucial for fields like software engineering and scientific research. <div>
arXiv:2506.00172v1 Announce Type: new 
Abstract: Benchmarks for large language models (LLMs) have predominantly assessed short-horizon, localized reasoning. Existing long-horizon suites (e.g. SWE-bench) rely on manually curated issues, so expanding or tuning difficulty demands expensive human effort and evaluations quickly saturate. However, many real-world tasks, such as software engineering or scientific research, require agents to rapidly comprehend and manipulate novel, complex structures dynamically; evaluating these capabilities requires the ability to construct large and varied sets of problems for agents to solve. We introduce Breakpoint, a benchmarking methodology that automatically generates code-repair tasks by adversarially corrupting functions within real-world software repositories. Breakpoint systematically controls task difficulty along two clear dimensions: local reasoning (characterized by code complexity metrics such as cyclomatic complexity) and system-level reasoning (characterized by call-graph centrality and the number of simultaneously corrupted interdependent functions). In experiments across more than 900 generated tasks we demonstrate that our methodology can scale to arbitrary difficulty, with state-of-the-art models' success rates ranging from 55% on the easiest tasks down to 0% on the hardest.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountability Attribution: Tracing Model Behavior to Training Processes</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
<div> accountability, AI development pipelines, model behavior, training stages, estimators
Summary: 
The article addresses the issue of accountability attribution in modern AI development pipelines, where multiple stages such as pretraining and fine-tuning rounds play a role in the model's success or failure. The proposed framework aims to trace model behavior back to specific training stages by answering counterfactual questions about the effects of each stage. Estimators based on first-order approximations are introduced to efficiently quantify the effects of training stages without the need for retraining. These estimators consider elements such as learning rate schedules, momentum, and weight decay in optimization dynamics. Empirical demonstrations show that the approach can identify the training stages responsible for specific model behaviors, providing a practical tool for model analysis and promoting more accountable AI development. 
<br /><br />Summary: <div>
arXiv:2506.00175v1 Announce Type: new 
Abstract: Modern AI development pipelines often involve multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment-with numerous model update steps within each stage. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the problem of accountability attribution, which aims to trace model behavior back to specific stages of the training process. To address this, we propose a general framework that answers counterfactual questions about stage effects: how would the model behavior have changed if the updates from a training stage had not been executed?. Within this framework, we introduce estimators based on first-order approximations that efficiently quantify the stage effects without retraining. Our estimators account for both the training data and key aspects of optimization dynamics, including learning rate schedules, momentum, and weight decay. Empirically, we demonstrate that our approach identifies training stages accountable for specific behaviors, offering a practical tool for model analysis and a step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach</title>
<link>https://arxiv.org/abs/2506.00181</link>
<guid>https://arxiv.org/abs/2506.00181</guid>
<content:encoded><![CDATA[
<div> stochastic differential equation, Distributed SGD, Distributed Compressed SGD, Distributed SignSGD, batch noise <br />
Summary: <br />
This article investigates the dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed SignSGD using stochastic differential equation approximations. The study considers smoothness and noise assumptions and explores the interactions between batch noise, stochastic gradient compression, and adaptivity. The analysis reveals that adaptive methods like Distributed SignSGD can converge successfully with standard learning rate scheduler assumptions, even with heavy-tailed noise. Conversely, Distributed (Compressed) SGD with pre-scheduled decaying learning rate struggles to converge unless the schedule incorporates an inverse dependency on the gradient norm, essentially behaving like an adaptive method in practice. <div>
arXiv:2506.00181v1 Announce Type: new 
Abstract: Using stochastic differential equation (SDE) approximations, we study the dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed SignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our analysis provides insights -- which we validate through simulation -- into the intricate interactions between batch noise, stochastic gradient compression, and adaptivity in this modern theoretical setup. For instance, we show that \textit{adaptive} methods such as Distributed SignSGD can successfully converge under standard assumptions on the learning rate scheduler, even under heavy-tailed noise. On the contrary, Distributed (Compressed) SGD with pre-scheduled decaying learning rate fails to achieve convergence, unless such a schedule also accounts for an inverse dependency on the gradient norm -- de facto falling back into an adaptive method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series</title>
<link>https://arxiv.org/abs/2506.00188</link>
<guid>https://arxiv.org/abs/2506.00188</guid>
<content:encoded><![CDATA[
<div> MLP-based mixer models, anomaly detection, multivariate time series, causal mechanism, cluster-aware, temporal dependencies<br />
<br />
Summary:<br />
Anomaly detection in time series data is crucial, and the lack of causality in MLP-based mixer models can hinder accurate detection. To address this, a novel cluster-aware causal mixer model is proposed, grouping channels into clusters based on correlations. Each cluster has a dedicated embedding layer, enhancing the model's ability to capture complex inter-channel relationships. The introduction of a causal mixer ensures that temporal dependencies are preserved while mixing information. Furthermore, an anomaly detection framework accumulates evidence over time to prevent false positives. The model operates online, making it suitable for real-time tasks. Experimental results on benchmark datasets show the model consistently achieves superior F1 scores, highlighting its effectiveness in multivariate time series anomaly detection tasks. <br /><br /> <div>
arXiv:2506.00188v1 Announce Type: new 
Abstract: Early and accurate detection of anomalies in time series data is critical, given the significant risks associated with false or missed detections. While MLP-based mixer models have shown promise in time series analysis, they lack a causality mechanism to preserve temporal dependencies inherent in the system. Moreover, real-world multivariate time series often contain numerous channels with diverse inter-channel correlations. A single embedding mechanism for all channels does not effectively capture these complex relationships. To address these challenges, we propose a novel cluster-aware causal mixer to effectively detect anomalies in multivariate time series. Our model groups channels into clusters based on their correlations, with each cluster processed through a dedicated embedding layer. In addition, we introduce a causal mixer in our model, which mixes the information while maintaining causality. Furthermore, we present an anomaly detection framework that accumulates the anomaly evidence over time to prevent false positives due to nominal outliers. Our proposed model operates in an online fashion, making it suitable for real-time time-series anomaly detection tasks. Experimental evaluations across six public benchmark datasets demonstrate that our model consistently achieves superior F1 scores.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models</title>
<link>https://arxiv.org/abs/2506.00198</link>
<guid>https://arxiv.org/abs/2506.00198</guid>
<content:encoded><![CDATA[
<div> MOFs, transformer-based framework, reinforcement learning, de novo design, generative modeling<br />
<br />
Summary: 
The article introduces a novel approach for the design of Metal-Organic Frameworks (MOFs) using a reinforcement learning-enhanced, transformer-based framework. The method involves a generative model trained on MOFid sequences, a transformer-based property predictor, and a reinforcement learning module to optimize MOF candidates based on desired properties. By integrating property feedback into the sequence generation process, the approach aims to accelerate the inverse design of MOFs with specific functional attributes. This innovative use of large language models coupled with reinforcement learning demonstrates the potential for advancing computational MOF discovery and expanding possibilities in materials chemistry. <div>
arXiv:2506.00198v1 Announce Type: new 
Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2506.00205</link>
<guid>https://arxiv.org/abs/2506.00205</guid>
<content:encoded><![CDATA[
<div> Keywords: rehearsal-based methods, continual learning, forgetting, generalization error, overparameterized linear models

Summary: 
Rehearsal-based methods have been effective in combating catastrophic forgetting in continual learning. This study investigates if sequential rehearsal, like human learning processes, can offer advantages over standard concurrent rehearsal. By analyzing overparameterized linear models, the researchers compare the two strategies and find that sequential rehearsal performs better in scenarios where tasks are less similar. Motivated by these findings, a Hybrid Rehearsal method is proposed, combining concurrent training for similar tasks and sequential revisiting for dissimilar tasks. The theoretical analysis demonstrates the benefits of this approach in mitigating forgetting and improving generalization performance. Experimental results with deep neural networks further validate the superiority of the hybrid method over standard concurrent rehearsal. This study contributes a comprehensive theoretical analysis of rehearsal-based continual learning strategies. 

<br /><br />Summary: <div>
arXiv:2506.00205v1 Announce Type: new 
Abstract: Rehearsal-based methods have shown superior performance in addressing catastrophic forgetting in continual learning (CL) by storing and training on a subset of past data alongside new data in current task. While such a concurrent rehearsal strategy is widely used, it remains unclear if this approach is always optimal. Inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting, we explore whether sequential rehearsal can offer greater benefits for CL compared to standard concurrent rehearsal. To address this question, we conduct a theoretical analysis of rehearsal-based CL in overparameterized linear models, comparing two strategies: 1) Concurrent Rehearsal, where past and new data are trained together, and 2) Sequential Rehearsal, where new data is trained first, followed by revisiting past data sequentially. By explicitly characterizing forgetting and generalization error, we show that sequential rehearsal performs better when tasks are less similar. These insights further motivate a novel Hybrid Rehearsal method, which trains similar tasks concurrently and revisits dissimilar tasks sequentially. We characterize its forgetting and generalization performance, and our experiments with deep neural networks further confirm that the hybrid approach outperforms standard concurrent rehearsal. This work provides the first comprehensive theoretical analysis of rehearsal-based CL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models</title>
<link>https://arxiv.org/abs/2506.00209</link>
<guid>https://arxiv.org/abs/2506.00209</guid>
<content:encoded><![CDATA[
<div> Keywords: Cancer screening, CATCH-FM, healthcare records, risk prediction, EHR foundation models <br />
<br />
Summary: 
The article introduces CATCH-FM, a cancer pre-screening methodology that uses historical medical records to identify high-risk patients for further screening. By pretraining and fine-tuning foundation models on electronic healthcare records, CATCH-FM achieves strong efficacy with low risk, outperforming other models in cancer risk prediction. The methodology demonstrates robustness across different patient distributions, excels in capturing non-trivial cancer risk factors, and achieves state-of-the-art results in pancreatic cancer risk prediction. By operating in the ICD code space, CATCH-FM proves its ability to leverage healthcare data efficiently. The open-sourcing of the code will further contribute to advancing cancer screening methods and potentially saving lives. <div>
arXiv:2506.00209v1 Announce Type: new 
Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately, existing screening techniques require expensive and intrusive medical procedures, not globally available, resulting in too many lost would-be-saved lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation Models, a cancer pre-screening methodology that identifies high-risk patients for further screening solely based on their historical medical records. With millions of electronic healthcare records (EHR), we establish the scaling law of EHR foundation models pretrained on medical code sequences, pretrain compute-optimal foundation models of up to 2.4 billion parameters, and finetune them on clinician-curated cancer risk prediction cohorts. In our retrospective evaluation comprising of thirty thousand patients, CATCH-FM achieved strong efficacy (60% sensitivity) with low risk (99% specificity and Negative Predictive Value), outperforming feature-based tree models as well as general and medical large language models by large margins. Despite significant demographic, healthcare system, and EHR coding differences, CATCH-FM achieves state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot leaderboard, outperforming EHR foundation models pretrained using on-site patient data. Our analysis demonstrates the robustness of CATCH-FM in various patient distributions, the benefits of operating in the ICD code space, and its ability to capture non-trivial cancer risk factors. Our code will be open-sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00236</link>
<guid>https://arxiv.org/abs/2506.00236</guid>
<content:encoded><![CDATA[
<div> Parameter-efficient fine-tuning, LoRA, low-rank updates, pretrained weights, Localized LoRA <br />
<br />Summary:
Parameter-efficient fine-tuning methods like LoRA offer compact alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. Existing approaches often overlook spatial patterns by relying on global low-rank structures. This study introduces Localized LoRA, a framework modeling weight updates as low-rank matrices applied to structured blocks of the weight matrix. This enables dense, localized updates without increasing trainable parameters. Formal comparisons between global, diagonal-local, and fully localized low-rank approximations show that Localized LoRA consistently achieves lower approximation errors under matched parameter budgets. Experiments in synthetic and practical settings demonstrate that Localized LoRA provides a more expressive and adaptable alternative, facilitating efficient fine-tuning with improved performance. <div>
arXiv:2506.00236v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact and effective alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. However, most existing approaches rely on global low-rank structures, which can overlook spatial patterns spread across the parameter space. In this work, we propose Localized LoRA, a generalized framework that models weight updates as a composition of low-rank matrices applied to structured blocks of the weight matrix. This formulation enables dense, localized updates throughout the parameter space-without increasing the total number of trainable parameters. We provide a formal comparison between global, diagonal-local, and fully localized low-rank approximations, and show that our method consistently achieves lower approximation error under matched parameter budgets. Experiments on both synthetic and practical settings demonstrate that Localized LoRA offers a more expressive and adaptable alternative to existing methods, enabling efficient fine-tuning with improved performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeGLIF for Label Noise Robust Node Classification using GNNs</title>
<link>https://arxiv.org/abs/2506.00244</link>
<guid>https://arxiv.org/abs/2506.00244</guid>
<content:encoded><![CDATA[
<div> Keywords: denoising, graph data, label noise, node-level prediction, influence function

Summary:
DeGLIF is a denoising technique that uses clean data and the leave-one-out influence function to improve node-level prediction on graph data. It approximates the change in model parameters when a training point is removed and extends this to estimate the change in validation loss for GNNs. DeGLIF does not require knowledge of the noise model or level in the dataset, and two variants are proposed to identify noisy nodes. Computational experiments demonstrate DeGLIF's effectiveness, outperforming baseline algorithms in accuracy. One variant is proven to detect noisy points that increase risk. Overall, DeGLIF offers a robust method for denoising graph data and improving prediction accuracy. 

<br /><br />Summary: <div>
arXiv:2506.00244v1 Announce Type: new 
Abstract: Noisy labelled datasets are generally inexpensive compared to clean labelled datasets, and the same is true for graph data. In this paper, we propose a denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence Function. DeGLIF uses a small set of clean data and the leave-one- out influence function to make label noise robust node-level prediction on graph data. Leave-one-out influence function approximates the change in the model parameters if a training point is removed from the training dataset. Recent advances propose a way to calculate the leave-one-out influence function for Graph Neural Networks (GNNs). We extend that recent work to estimate the change in validation loss, if a training node is removed from the training dataset. We use this estimate and a new theoretically motivated relabelling function to denoise the training dataset. We propose two DeGLIF variants to identify noisy nodes. Both these variants do not require any information about the noise model or the noise level in the dataset; DeGLIF also does not estimate these quantities. For one of these variants, we prove that the noisy points detected can indeed increase risk. We carry out detailed computational experiments on different datasets to show the effectiveness of DeGLIF. It achieves better accuracy than other baseline algorithms
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity</title>
<link>https://arxiv.org/abs/2506.00245</link>
<guid>https://arxiv.org/abs/2506.00245</guid>
<content:encoded><![CDATA[
<div> semantic entropy, uncertainty quantification, nearest neighbor, text generation tasks, language models

Summary:
Nearest neighbor-inspired black-box method is proposed for detecting hallucination in large language models by quantifying uncertainty effectively. Intra-cluster and inter-cluster similarities are taken into account, enhancing traditional entropy estimation. The method, extendable to white-box settings, outperforms semantic entropy in text generation tasks like question answering, text summarization, and machine translation across Phi3 and Llama3 models. Theoretical results support the method's generalization of semantic entropy. The code repository for this approach is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2506.00245v1 Announce Type: new 
Abstract: Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at https://github.com/BigML-CS-UCLA/SNNE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming</title>
<link>https://arxiv.org/abs/2506.00247</link>
<guid>https://arxiv.org/abs/2506.00247</guid>
<content:encoded><![CDATA[
<div> Keywords: Convolutional Neural Networks, Quantum Computing, Optimization, Stochastic Gradient Descent, MNIST dataset

Summary: 
Convolutional Neural Networks (CNNs) are widely used in computer vision and Big Data analytics but require significant computational resources for training. Traditional training methods using back-propagation may be sub-optimal and time-consuming. Quantum computing offers a promising alternative to efficiently search complex optimization landscapes. This study proposes a hybrid optimization method combining Unconstrained Binary Quadratic Programming and Stochastic Gradient Descent to accelerate CNN training. Tested on the MNIST dataset, the approach achieves a 10-15% accuracy improvement over standard methods while maintaining similar execution times. These results demonstrate the potential of hybrid quantum-classical techniques in High-Performance Computing environments for Big Data and Deep Learning. However, successfully harnessing these benefits requires careful alignment of algorithmic structures with underlying quantum mechanisms. 

<br /><br />Summary: <div>
arXiv:2506.00247v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big Data analytics but demand significant computational resources when trained on large-scale datasets. Conventional training via back-propagation (BP) with losses like Mean Squared Error or Cross-Entropy often requires extensive iterations and may converge sub-optimally. Quantum computing offers a promising alternative by leveraging superposition, tunneling, and entanglement to search complex optimization landscapes more efficiently. In this work, we propose a hybrid optimization method that combines an Unconstrained Binary Quadratic Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to accelerate CNN training. Evaluated on the MNIST dataset, our approach achieves a 10--15\% accuracy improvement over a standard BP-CNN baseline while maintaining similar execution times. These results illustrate the potential of hybrid quantum-classical techniques in High-Performance Computing (HPC) environments for Big Data and Deep Learning. Fully realizing these benefits, however, requires a careful alignment of algorithmic structures with underlying quantum mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction</title>
<link>https://arxiv.org/abs/2506.00259</link>
<guid>https://arxiv.org/abs/2506.00259</guid>
<content:encoded><![CDATA[
<div> Keywords: Remaining Useful Life Prediction, Convolutional Neural Networks, Vision Transformer, Permutation-based Approach, PHM Applications

Summary:
The article introduces the PerFormer, a permutation-based vision transformer approach designed to improve the accuracy of Remaining Useful Life (RUL) prediction for degradation systems. This approach addresses the challenge of applying Vision Transformer (ViT) to multivariate sensor data for RUL prediction by permuting time series data to mimic spatial characteristics of image data. A novel permutation loss function is utilized to guide the convergence of matrices towards a permutation matrix. Experimental results on NASA's C-MAPSS dataset demonstrate the superior performance of the PerFormer in RUL prediction compared to state-of-the-art methods using CNNs, RNNs, and other Transformer models. The study highlights the effectiveness and potential of the PerFormer in Prognostic and Health Management (PHM) applications. 

Summary: <br />
1. Introduction of PerFormer, a permutation-based vision transformer for RUL prediction. <br />
2. Addressing challenges in applying ViT to multivariate sensor data. <br />
3. Novel permutation loss function to guide matrix convergence. <br />
4. Superior performance of PerFormer in RUL prediction compared to CNNs, RNNs, and other Transformer models. <br />
5. Demonstration of PerFormer's effectiveness and potential in PHM applications. <div>
arXiv:2506.00259v1 Announce Type: new 
Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</title>
<link>https://arxiv.org/abs/2506.00286</link>
<guid>https://arxiv.org/abs/2506.00286</guid>
<content:encoded><![CDATA[
<div> risk-preferences, Markov decision process, model-based approach, PAC-bounds, policy-learning

Summary:<br />
The paper investigates the sample complexities of learning the optimal state-action value function and an optimal policy in a discounted Markov decision process with recursive entropic risk-preferences. The model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) approach is introduced to provide PAC-bounds on the difference between the estimated value functions and the optimal ones. The bounds exhibit exponential dependence on the effective horizon and the learner's risk-sensitivity. Lower bounds are derived to show the tightness of the PAC-bounds in both $\varepsilon$ and $\delta$, as well as in terms of the number of actions involved. The analysis highlights the complexity of learning in risk-sensitive settings and reveals the trade-offs between risk-preferences and the efficiency of learning optimal policies. <div>
arXiv:2506.00286v1 Announce Type: new 
Abstract: In this paper we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and $\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the strength of this dependence grows with the learners risk-sensitivity $|\beta|$. We also provide two lower bounds which shows that exponential dependence on $|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and that the PAC-bound on policy-learning is nearly tight in $A$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Protein Sequence Design through Designability Preference Optimization</title>
<link>https://arxiv.org/abs/2506.00297</link>
<guid>https://arxiv.org/abs/2506.00297</guid>
<content:encoded><![CDATA[
<div> Direct Preference Optimization, AlphaFold, Residue-level Designability Preference Optimization, LigandMPNN, EnhancedMPNN

Summary:
Direct Preference Optimization and Residue-level Designability Preference Optimization were used to improve protein sequence design methods. By integrating AlphaFold pLDDT scores and applying residue-level structural rewards, the training objective was redefined to increase designability. This approach significantly increased the success rate of in silico protein design, particularly in challenging enzyme design benchmarks. The fine-tuned LigandMPNN with ResiDPO, called EnhancedMPNN, demonstrated a nearly 3-fold improvement in design success rate, from 6.56% to 17.57%. This method focuses on generating sequences with high designability, ensuring that the designed sequences have a higher likelihood of folding into the desired structure. By optimizing at a residue-level granularity and decoupling optimization across residues, the ResiDPO technique allows for direct improvement in designability while preserving well-performing regions. Overall, this approach enhances the effectiveness and reliability of protein sequence design for de novo protein design projects. 

<br /><br />Summary: <div>
arXiv:2506.00297v1 Announce Type: new 
Abstract: Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms</title>
<link>https://arxiv.org/abs/2506.00299</link>
<guid>https://arxiv.org/abs/2506.00299</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, alignment, evolutionary algorithms, black-box optimization, generative models

Summary: 
Evolutionary algorithms are introduced as an inference-time alignment framework for diffusion models, treating them as black-boxes to maximize alignment objectives in their latent space. The method allows for efficient alignment for differentiable and non-differentiable objectives, outperforming gradient-based methods on benchmark tests like DrawBench and Open Image Preferences. It also shows significant reductions in memory consumption, requiring 55% to 76% less GPU memory, and faster running times, being 72% to 80% quicker than gradient-based approaches. The evolutionary algorithms achieve higher alignment scores on the Open Image Preferences benchmark over 50 optimization steps compared to both gradient-based and gradient-free methods. <div>
arXiv:2506.00299v1 Announce Type: new 
Abstract: Diffusion models are state-of-the-art generative models in various domains, yet their samples often fail to satisfy downstream objectives such as safety constraints or domain-specific validity. Existing techniques for alignment require gradients, internal model access, or large computational budgets. We introduce an inference-time alignment framework based on evolutionary algorithms. We treat diffusion models as black-boxes and search their latent space to maximize alignment objectives. Our method enables efficient inference-time alignment for both differentiable and non-differentiable alignment objectives across a range of diffusion models. On the DrawBench and Open Image Preferences benchmark, our EA methods outperform state-of-the-art gradient-based and gradient-free inference-time methods. In terms of memory consumption, we require 55% to 76% lower GPU memory than gradient-based methods. In terms of running-time, we are 72% to 80% faster than gradient-based methods. We achieve higher alignment scores over 50 optimization steps on Open Image Preferences than gradient-based and gradient-free methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework</title>
<link>https://arxiv.org/abs/2506.00302</link>
<guid>https://arxiv.org/abs/2506.00302</guid>
<content:encoded><![CDATA[
<div> Dataset, Materials science, Multimodal learning, Machine learning, Annotation quality

Summary:
The article introduces MultiCrystalSpectrumSet (MCS-Set), a framework that enhances materials science datasets by incorporating atomic structures, 2D projections, and structured textual annotations. This expansion allows for multimodal property and summary prediction and constrained crystal generation with partial cluster supervision. The MCS-Set utilizes a human-in-the-loop pipeline to combine domain expertise with standardized descriptors for high-quality annotation. Evaluations using advanced language and vision-language models demonstrate significant performance differences between modalities, underscoring the importance of annotation quality for generalization. The MCS-Set serves as a basis for benchmarking multimodal models, improving annotation practices, and creating accessible, versatile materials science datasets. The dataset and implementations are accessible on GitHub at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet. 

<br /><br />Summary: <div>
arXiv:2506.00302v1 Announce Type: new 
Abstract: Most materials science datasets are limited to atomic geometries (e.g., XYZ files), restricting their utility for multimodal learning and comprehensive data-centric analysis. These constraints have historically impeded the adoption of advanced machine learning techniques in the field. This work introduces MultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials datasets by integrating atomic structures with 2D projections and structured textual annotations, including lattice parameters and coordination metrics. MCS-Set enables two key tasks: (1) multimodal property and summary prediction, and (2) constrained crystal generation with partial cluster supervision. Leveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with standardized descriptors for high-quality annotation. Evaluations using state-of-the-art language and vision-language models reveal substantial modality-specific performance gaps and highlight the importance of annotation quality for generalization. MCS-Set offers a foundation for benchmarking multimodal models, advancing annotation practices, and promoting accessible, versatile materials science datasets. The dataset and implementations are available at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning via Regression Beyond Realizability</title>
<link>https://arxiv.org/abs/2506.00316</link>
<guid>https://arxiv.org/abs/2506.00316</guid>
<content:encoded><![CDATA[
<div> active learning, multiclass classification, surrogate risk minimization, realizability assumption, convex models

Summary:
In the context of active learning for multiclass classification, a new framework based on surrogate risk minimization is introduced. Unlike existing methods that rely on the realizability assumption, this framework operates beyond this constraint, allowing for applicability in practical, misspecified settings. The algorithm proposed in this work demonstrates that label and sample complexity can be achieved with weaker assumptions than realizability, as long as the model class is convex. The novel epoch-based approach involves fitting a model from the entire class to the queried data in each epoch and producing an improper classifier by aggregating these models. This departure from previous methods ensures comparable rates of performance, even in non-realizable settings where conventional approaches may fail to yield accurate results. <div>
arXiv:2506.00316v1 Announce Type: new 
Abstract: We present a new active learning framework for multiclass classification based on surrogate risk minimization that operates beyond the standard realizability assumption. Existing surrogate-based active learning algorithms crucially rely on realizability$\unicode{x2014}$the assumption that the optimal surrogate predictor lies within the model class$\unicode{x2014}$limiting their applicability in practical, misspecified settings. In this work we show that under conditions significantly weaker than realizability, as long as the class of models considered is convex, one can still obtain a label and sample complexity comparable to prior work. Despite achieving similar rates, the algorithmic approaches from prior works can be shown to fail in non-realizable settings where our assumption is satisfied. Our epoch-based active learning algorithm departs from prior methods by fitting a model from the full class to the queried data in each epoch and returning an improper classifier obtained by aggregating these models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2506.00329</link>
<guid>https://arxiv.org/abs/2506.00329</guid>
<content:encoded><![CDATA[
<div> transformers, video generation, adaptive layer-reuse, computational efficiency, Foresight

Summary:
Foresight introduces an adaptive layer-reuse technique to enhance the computational efficiency of Diffusion Transformers (DiTs) in video generation tasks. The proposed method dynamically identifies and reuses DiT block outputs across denoising steps, optimizing efficiency based on generation parameters like resolution and denoising schedules. By applying Foresight to OpenSora, Latte, and CogVideoX models, up to a 1.63x speedup in end-to-end processing is achieved while maintaining video quality. This approach addresses the computational cost challenges of spatial-temporal attention in DiTs by reducing redundancy without compromising performance. Foresight's source code is publicly available for further exploration and implementation. <div>
arXiv:2506.00329v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \texttt{https://github.com/STAR-Laboratory/foresight}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification</title>
<link>https://arxiv.org/abs/2506.00337</link>
<guid>https://arxiv.org/abs/2506.00337</guid>
<content:encoded><![CDATA[
<div> Method, Channel Imposed Fusion, Classification, EEG, ECG

Summary:
Channel Imposed Fusion (CIF) is introduced for transparent medical time series signal classification, enhancing signal-to-noise ratio and reducing redundancy. The method integrates with Temporal Convolutional Network (TCN) for efficiency and explicitness. Experimental results on EEG and ECG datasets show CIF outperforms existing state-of-the-art approaches in classification metrics, improving transparency in the process. This novel approach offers a new perspective for medical time series classification.<br /><br />Summary: <div>
arXiv:2506.00337v1 Announce Type: new 
Abstract: The automatic classification of medical time series signals, such as electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in clinical decision support and early detection of diseases. Although Transformer based models have achieved notable performance by implicitly modeling temporal dependencies through self-attention mechanisms, their inherently complex architectures and opaque reasoning processes undermine their trustworthiness in high stakes clinical settings. In response to these limitations, this study shifts focus toward a modeling paradigm that emphasizes structural transparency, aligning more closely with the intrinsic characteristics of medical data. We propose a novel method, Channel Imposed Fusion (CIF), which enhances the signal-to-noise ratio through cross-channel information fusion, effectively reduces redundancy, and improves classification performance. Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN), known for its structural simplicity and controllable receptive field, to construct an efficient and explicit classification framework. Experimental results on multiple publicly available EEG and ECG datasets demonstrate that the proposed method not only outperforms existing state-of-the-art (SOTA) approaches in terms of various classification metrics, but also significantly enhances the transparency of the classification process, offering a novel perspective for medical time series classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Performance of Perforated Backpropagation through Further Experiments</title>
<link>https://arxiv.org/abs/2506.00356</link>
<guid>https://arxiv.org/abs/2506.00356</guid>
<content:encoded><![CDATA[
<div> Perforated Backpropagation, neural network optimization, dendrites, biological neurons, hackathon
Summary:<br /><br />Perforated Backpropagation is a neural network optimization technique inspired by the computational significance of dendrites in biological neurons. This study presents additional experiments conducted during a hackathon at Carnegie Mellon Swartz Center in February 2025. Participants explored the algorithm's impact on their projects and found that it could achieve up to 90% model compression without compromising accuracy. Additionally, some projects experienced a 16% increase in accuracy compared to their original models. The collaboration between students and ML practitioners in Pittsburgh showcased the potential of Perforated Backpropagation in enhancing machine learning projects. <div>
arXiv:2506.00356v1 Announce Type: new 
Abstract: Perforated Backpropagation is a neural network optimization technique based on modern understanding of the computational importance of dendrites within biological neurons. This paper explores further experiments from the original publication, generated from a hackathon held at the Carnegie Mellon Swartz Center in February 2025. Students and local Pittsburgh ML practitioners were brought together to experiment with the Perforated Backpropagation algorithm on the datasets and models which they were using for their projects. Results showed that the system could enhance their projects, with up to 90% model compression without negative impact on accuracy, or up to 16% increased accuracy of their original models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees</title>
<link>https://arxiv.org/abs/2506.00362</link>
<guid>https://arxiv.org/abs/2506.00362</guid>
<content:encoded><![CDATA[
<div> Machine learning, constrained optimization, feasibility-seeking, neural network, real-time<br />
Summary:<br />
The study proposes FSNet, a novel neural network aimed at efficiently solving constrained optimization problems while ensuring feasibility. By integrating a feasibility-seeking step into the solution procedure, FSNet minimizes constraint violations in a differentiable manner, allowing for end-to-end training. Experimental results across various optimization problems show that FSNet can provide feasible solutions at faster speeds with comparable or superior solution quality to traditional solvers. The model proves effective in handling smooth/nonsmooth, convex/nonconvex problems, demonstrating its versatility and efficiency in real-world applications. <div>
arXiv:2506.00362v1 Announce Type: new 
Abstract: Efficiently solving constrained optimization problems is crucial for numerous real-world applications, yet traditional solvers are often computationally prohibitive for real-time use. Machine learning-based approaches have emerged as a promising alternative to provide approximate solutions at faster speeds, but they struggle to strictly enforce constraints, leading to infeasible solutions in practice. To address this, we propose the Feasibility-Seeking-Integrated Neural Network (FSNet), which integrates a feasibility-seeking step directly into its solution procedure to ensure constraint satisfaction. This feasibility-seeking step solves an unconstrained optimization problem that minimizes constraint violations in a differentiable manner, enabling end-to-end training and providing guarantees on feasibility and convergence. Our experiments across a range of different optimization problems, including both smooth/nonsmooth and convex/nonconvex problems, demonstrate that FSNet can provide feasible solutions with solution quality comparable to (or in some cases better than) traditional solvers, at significantly faster speeds.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Insights into Data-Oblivious Critical Layers in Large Language Models</title>
<link>https://arxiv.org/abs/2506.00382</link>
<guid>https://arxiv.org/abs/2506.00382</guid>
<content:encoded><![CDATA[
<div> CKA, layer analysis, pre-fine-tuned LLMs, spectral analysis, domain adaptation <br />
Summary: 
The study investigates the evolution of feature representations across layers in large language models (LLMs) using a data-oblivious approach. Critical layers in pre-fine-tuned LLMs are identified by analyzing representation dynamics through Centered Kernel Alignment (CKA). It is observed that layers experiencing significant shifts in representation space are most affected during fine-tuning, consistently across tasks for a given model. Spectral analysis reveals that these shifts are influenced by changes in the top principal components, representing semantic transitions. In practical applications, fine-tuning critical layers proves more effective for efficient domain adaptation, leading to greater loss reduction compared to non-critical layers. Additionally, freezing critical layers assists in backdoor defense, reducing attack success rates by up to 40%. <div>
arXiv:2506.00382v1 Announce Type: new 
Abstract: Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a data-oblivious approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment(CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning--a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions. We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-Learning-Driven Prefetching for Far Memory</title>
<link>https://arxiv.org/abs/2506.00384</link>
<guid>https://arxiv.org/abs/2506.00384</guid>
<content:encoded><![CDATA[
<div> Keywords: far-memory, deep learning, data prefetching, runtime performance, software optimization <br />
<br />
Summary: <br />
The article introduces FarSight, a far-memory system that utilizes deep learning for efficient data prefetching in modern software systems facing performance demands. FarSight separates application semantics from memory layout, enabling offline-trained deep learning models to predict access patterns using a compact vocabulary. Through asynchronous inference, lookahead prediction, and cache-resident models, FarSight achieves high prediction accuracy with low runtime overhead. Evaluation on four data-intensive workloads shows FarSight outperforms existing far-memory systems by up to 3.6 times. This highlights the potential of applying modern machine learning techniques to address complex, performance-critical software runtime challenges. <div>
arXiv:2506.00384v1 Announce Type: new 
Abstract: Modern software systems face increasing runtime performance demands, particularly in emerging architectures like far memory, where local-memory misses incur significant latency. While machine learning (ML) has proven effective in offline systems optimization, its application to high-frequency, runtime-level problems remains limited due to strict performance, generalization, and integration constraints. We present FarSight, a Linux-based far-memory system that leverages deep learning (DL) to efficiently perform accurate data prefetching. FarSight separates application semantics from runtime memory layout, allowing offline-trained DL models to predict access patterns using a compact vocabulary of ordinal possibilities, resolved at runtime through lightweight mapping structures. By combining asynchronous inference, lookahead prediction, and a cache-resident DL model, FarSight achieves high prediction accuracy with low runtime overhead. Our evaluation of FarSight on four data-intensive workloads shows that it outperforms the state-of-the-art far-memory system by up to 3.6 times. Overall, this work demonstrates the feasibility and advantages of applying modern ML techniques to complex, performance-critical software runtime problems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries</title>
<link>https://arxiv.org/abs/2506.00388</link>
<guid>https://arxiv.org/abs/2506.00388</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference-based reinforcement learning, human preferences, trajectory embedding space, unambiguous queries, CLARIFY<br />
Summary:<br />
Preference-based reinforcement learning (PbRL) aims to infer reward functions from human preference comparisons, eliminating the need for explicit reward engineering. However, labeling clear preferences between similar segments is challenging for humans, leading to reduced label efficiency. In response to this, the offline PbRL method CLARIFY is introduced, which focuses on resolving ambiguous feedback by learning a trajectory embedding space that incorporates preference information. By ensuring clearly distinguished segments are spaced apart, CLARIFY facilitates the selection of more unambiguous queries. Experimental results show that CLARIFY outperforms baseline methods in scenarios with non-ideal teachers and real human feedback. This approach not only improves query selection but also results in meaningful trajectory embeddings. Overall, CLARIFY addresses the challenge of ambiguous feedback in PbRL, enhancing its practical applicability in real-world settings. <br /><br />Summary: <div>
arXiv:2506.00388v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL's real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias as a Virtue: Rethinking Generalization under Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.00407</link>
<guid>https://arxiv.org/abs/2506.00407</guid>
<content:encoded><![CDATA[
<div> Framework, machine learning, generalization, bias, validation
<br />
Summary: 
The article introduces the Adaptive Distribution Bridge (ADB) framework, which challenges traditional validation paradigms in machine learning. It demonstrates that higher in-distribution (ID) bias can actually lead to better out-of-distribution (OOD) generalization. By introducing controlled statistical diversity during training, ADB allows models to develop bias profiles that improve generalization across distributions. Empirical evaluations on various datasets show that ADB significantly enhances OOD generalization, with mean error reductions of up to 26.8% compared to traditional cross-validation. The framework also consistently identifies high-performing training strategies, with percentile ranks often exceeding 74.4%. This work provides a practical approach for improving generalization in machine learning models and offers a theoretical framework for reevaluating the role of bias in achieving robustness. 
<br /> <div>
arXiv:2506.00407v1 Announce Type: new 
Abstract: Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering</title>
<link>https://arxiv.org/abs/2506.00410</link>
<guid>https://arxiv.org/abs/2506.00410</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, clustering, contrastive learning, shrinkage estimator, hierarchical Bayesian estimation

Summary:
JojoSCL is introduced as a self-supervised contrastive learning framework for clustering single-cell RNA sequencing data. It incorporates a shrinkage estimator based on hierarchical Bayesian estimation to adjust gene expression estimates and reduce intra-cluster dispersion. The framework is optimized using Stein's Unbiased Risk Estimate (SURE) to refine both instance-level and cluster-level contrastive learning. Experimental results on ten scRNA-seq datasets demonstrate that JojoSCL outperforms existing clustering methods consistently. The practicality of JojoSCL is validated through robustness analysis and ablation studies. The code for JojoSCL is open-source and available on GitHub. <div>
arXiv:2506.00410v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular processes by enabling gene expression analysis at the individual cell level. Clustering allows for the identification of cell types and the further discovery of intrinsic patterns in single-cell data. However, the high dimensionality and sparsity of scRNA-seq data continue to challenge existing clustering models. In this paper, we introduce JojoSCL, a novel self-supervised contrastive learning framework for scRNA-seq clustering. By incorporating a shrinkage estimator based on hierarchical Bayesian estimation, which adjusts gene expression estimates towards more reliable cluster centroids to reduce intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate (SURE), JojoSCL refines both instance-level and cluster-level contrastive learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL consistently outperforms prevalent clustering methods, with further validation of its practicality through robustness analysis and ablation studies. JojoSCL's code is available at: https://github.com/ziwenwang28/JojoSCL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title>
<link>https://arxiv.org/abs/2506.00416</link>
<guid>https://arxiv.org/abs/2506.00416</guid>
<content:encoded><![CDATA[
<div> Optimized Second-Order FL, Personalized Healthcare Systems, Federated Edge Learning, Blockchain, Privacy and Security <br />
Summary: <br />
The study introduces a new framework called BFEL (blockchain-enhanced federated edge learning) for personalized healthcare systems. It utilizes an optimized second-order FL approach, FedCurv, which maintains stability and consistency in non-iid datasets while improving personalized model training. The framework minimizes communication rounds required for convergence and manages personalized training on heterogeneous data. Incorporating Ethereum-based model aggregation ensures trust, verifiability, and auditability, while public key encryption enhances privacy and security. Experimental results using federated CNNs and MLPs on various datasets illustrate the high efficiency and scalability of the proposed framework. <div>
arXiv:2506.00416v1 Announce Type: new 
Abstract: Federated learning (FL) has attracted increasing attention to mitigate security and privacy challenges in traditional cloud-centric machine learning models specifically in healthcare ecosystems. FL methodologies enable the training of global models through localized policies, allowing independent operations at the edge clients' level. Conventional first-order FL approaches face several challenges in personalized model training due to heterogeneous non-independent and identically distributed (non-iid) data of each edge client. Recently, second-order FL approaches maintain the stability and consistency of non-iid datasets while improving personalized model training. This study proposes and develops a verifiable and auditable optimized second-order FL framework BFEL (blockchain-enhanced federated edge learning) based on optimized FedCurv for personalized healthcare systems. FedCurv incorporates information about the importance of each parameter to each client's task (through Fisher Information Matrix) which helps to preserve client-specific knowledge and reduce model drift during aggregation. Moreover, it minimizes communication rounds required to achieve a target precision convergence for each edge client while effectively managing personalized training on non-iid and heterogeneous data. The incorporation of Ethereum-based model aggregation ensures trust, verifiability, and auditability while public key encryption enhances privacy and security. Experimental results of federated CNNs and MLPs utilizing Mnist, Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2506.00420</link>
<guid>https://arxiv.org/abs/2506.00420</guid>
<content:encoded><![CDATA[
<div> model architecture, spatiotemporal correlation, two-stage training, anomaly detection, WSNs

Summary:
The proposed MTAD-RD anomaly detection model addresses challenges in WSN anomaly detection by integrating spatiotemporal correlation features and implementing a two-stage training strategy. The model architecture includes a RetNet enhanced by a CR module, multigranular feature fusion, and a graph attention network module to extract internode correlation information for improved anomaly detection. A two-stage training approach involves a contrastive learning proxy task for unlabeled data and a sample sampler for addressing sample imbalance. In experiments on real datasets, the MTAD-RD method achieved a high F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods. The serialized inference characteristic of the model also helps reduce inference overhead, making it a reliable and efficient solution for detecting anomalies in WSN data. 

<br /><br />Summary: <div>
arXiv:2506.00420v1 Announce Type: new 
Abstract: Detecting anomalies in the data collected by WSNs can provide crucial evidence for assessing the reliability and stability of WSNs. Existing methods for WSN anomaly detection often face challenges such as the limited extraction of spatiotemporal correlation features, the absence of sample labels, few anomaly samples, and an imbalanced sample distribution. To address these issues, a spatiotemporal correlation detection model (MTAD-RD) considering both model architecture and a two-stage training strategy perspective is proposed. In terms of model structure design, the proposed MTAD-RD backbone network includes a retentive network (RetNet) enhanced by a cross-retention (CR) module, a multigranular feature fusion module, and a graph attention network module to extract internode correlation information. This proposed model can integrate the intermodal correlation features and spatial features of WSN neighbor nodes while extracting global information from time series data. Moreover, its serialized inference characteristic can remarkably reduce inference overhead. For model training, a two-stage training approach was designed. First, a contrastive learning proxy task was designed for time series data with graph structure information in WSNs, enabling the backbone network to learn transferable features from unlabeled data using unsupervised contrastive learning methods, thereby addressing the issue of missing sample labels in the dataset. Then, a caching-based sample sampler was designed to divide samples into few-shot and contrastive learning data. A specific joint loss function was developed to jointly train the dual-graph discriminator network to address the problem of sample imbalance effectively. In experiments carried out on real public datasets, the designed MTAD-RD anomaly detection method achieved an F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</title>
<link>https://arxiv.org/abs/2506.00424</link>
<guid>https://arxiv.org/abs/2506.00424</guid>
<content:encoded><![CDATA[
<div> Sparse tensor programs, deep learning, graph analytics, specialized hardware accelerators, COGNATE.

Summary:
COGNATE is a framework designed to optimize sparse tensor programs for specialized hardware accelerators. It addresses challenges such as sensitivity to input variations and reliance on expensive simulators. The framework leverages data samples from general-purpose hardware for cost model training, followed by fine-tuning on emerging hardware. By exploiting input feature homogeneity and mitigating heterogeneity, COGNATE achieves comparable performance to accelerator-specific models with just 5% of the data samples. Extensive experiments demonstrate that COGNATE outperforms existing techniques, with average speedups of 1.47x for SpMM and 1.39x for SDDMM. <div>
arXiv:2506.00424v1 Announce Type: new 
Abstract: Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer</title>
<link>https://arxiv.org/abs/2506.00431</link>
<guid>https://arxiv.org/abs/2506.00431</guid>
<content:encoded><![CDATA[
<div> Proposed method, SAM interpretability, Temporal dynamics, Interactive dynamics, Efficiency <br />
<br />
Summary: TIDFormer is introduced as a dynamic graph TransFormer that efficiently captures temporal and interactive dynamics. The self-attention mechanism (SAM) used is interpretable and effectively models changes in historical interaction patterns. Time partitioning information and interaction embeddings are utilized for bipartite and non-bipartite graphs, extracted from first-order neighbors. TIDFormer excels in performance compared to existing models on various dynamic graph datasets, demonstrating its effectiveness. Additionally, TIDFormer exhibits efficiency advantages over previous Transformer-based methods, making it a promising approach for dynamic graph modeling. <div>
arXiv:2506.00431v1 Announce Type: new 
Abstract: Due to the proficiency of self-attention mechanisms (SAMs) in capturing dependencies in sequence modeling, several existing dynamic graph neural networks (DGNNs) utilize Transformer architectures with various encoding designs to capture sequential evolutions of dynamic graphs. However, the effectiveness and efficiency of these Transformer-based DGNNs vary significantly, highlighting the importance of properly defining the SAM on dynamic graphs and comprehensively encoding temporal and interactive dynamics without extra complex modules. In this work, we propose TIDFormer, a dynamic graph TransFormer that fully exploits Temporal and Interactive Dynamics in an efficient manner. We clarify and verify the interpretability of our proposed SAM, addressing the open problem of its uninterpretable definitions on dynamic graphs in previous works. To model the temporal and interactive dynamics, respectively, we utilize the calendar-based time partitioning information and extract informative interaction embeddings for both bipartite and non-bipartite graphs using merely the sampled first-order neighbors. In addition, we jointly model temporal and interactive features by capturing potential changes in historical interaction patterns through a simple decomposition. We conduct extensive experiments on several dynamic graph datasets to verify the effectiveness and efficiency of TIDFormer. The experimental results demonstrate that TIDFormer excels, outperforming state-of-the-art models across most datasets and experimental settings. Furthermore, TIDFormer exhibits significant efficiency advantages compared to previous Transformer-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel Normalization for Time Series Channel Identification</title>
<link>https://arxiv.org/abs/2506.00432</link>
<guid>https://arxiv.org/abs/2506.00432</guid>
<content:encoded><![CDATA[
<div> Channel identifiability (CID), Channel Normalization (CN), Adaptive CN (ACN), Prototypical CN (PCN), time series modeling <br />
Summary: 
Channel identifiability (CID) in time series (TS) modeling is crucial to distinguish between individual channels. A new normalization strategy called Channel Normalization (CN) enhances CID by assigning distinct parameters to each channel. Adaptive CN (ACN) adjusts parameters based on input TS, improving adaptability. Prototypical CN (PCN) uses learnable prototypes instead of per-channel parameters, making it applicable to datasets with unknown or varying channels. Applying CN and its variants to various TS models results in significant performance gains for both non-CID and CID models. The success of the approach is analyzed from an information theory perspective. To implement this approach, the code is available at https://github.com/seunghan96/CN. <br /> <div>
arXiv:2506.00432v1 Announce Type: new 
Abstract: Channel identifiability (CID) refers to the ability to distinguish between individual channels in time series (TS) modeling. The absence of CID often results in producing identical outputs for identical inputs, disregarding channel-specific characteristics. In this paper, we highlight the importance of CID and propose Channel Normalization (CN), a simple yet effective normalization strategy that enhances CID by assigning distinct affine transformation parameters to each channel. We further extend CN in two ways: 1) Adaptive CN (ACN) dynamically adjusts parameters based on the input TS, improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a set of learnable prototypes instead of per-channel parameters, enabling applicability to datasets with unknown or varying number of channels and facilitating use in TS foundation models. We demonstrate the effectiveness of CN and its variants by applying them to various TS models, achieving significant performance gains for both non-CID and CID models. In addition, we analyze the success of our approach from an information theory perspective. Code is available at https://github.com/seunghan96/CN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
<link>https://arxiv.org/abs/2506.00436</link>
<guid>https://arxiv.org/abs/2506.00436</guid>
<content:encoded><![CDATA[
<div> method, identifying, potential customers, targeted marketing, learning

Summary:
- The study proposes a method for targeted marketing using learning from positive and unlabeled data (PU learning).
- It addresses a scenario where a company can only observe customers who have made purchases and aims to market effectively to customers who lack loyalty.
- By focusing on individuals interested in a product but lacking loyalty, marketing efforts can be more efficient.
- The proposed algorithm, called double PU learning, combines two losses derived from standard PU learning settings to identify potential customers without loyalty.
- Numerical experiments confirm the effectiveness of the algorithm for the targeted marketing problem. 

<br /><br />Summary: <div>
arXiv:2506.00436v1 Announce Type: new 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00437</link>
<guid>https://arxiv.org/abs/2506.00437</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, explanation methods, reliability, confidence scoring, GIB-CC <br />
Summary: 
Explaining Graph Neural Networks (GNNs) is crucial for interpretability. Existing post-hoc explanation methods for GNNs lack reliability, especially when dealing with out-of-distribution data. To address this, a new framework called ConfExplainer is introduced, incorporating a confidence scoring module based on the theoretical concept of Generalized Graph Information Bottleneck with Confidence Constraint (GIB-CC). This framework enhances the trustworthiness and robustness of GNN explanations by quantifying the reliability of generated explanations. Experimental results demonstrate the effectiveness of the confidence score in improving the reliability of explanations provided by GNNs. ConfExplainer offers a promising solution to the challenge of interpreting GNN predictions, particularly in scenarios involving unknown test datasets.<br /><br />Summary: <div>
arXiv:2506.00437v1 Announce Type: new 
Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention due to the need for interpretability, enabling users to understand the behavior of these black-box models better and extract valuable insights from their predictions. While numerous post-hoc instance-level explanation methods have been proposed to interpret GNN predictions, the reliability of these explanations remains uncertain, particularly in the out-of-distribution or unknown test datasets. In this paper, we address this challenge by introducing an explainer framework with the confidence scoring module ( ConfExplainer), grounded in theoretical principle, which is generalized graph information bottleneck with confidence constraint (GIB-CC), that quantifies the reliability of generated explanations. Experimental results demonstrate the superiority of our approach, highlighting the effectiveness of the confidence score in enhancing the trustworthiness and robustness of GNN explanations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge</title>
<link>https://arxiv.org/abs/2506.00438</link>
<guid>https://arxiv.org/abs/2506.00438</guid>
<content:encoded><![CDATA[
<div> PointODE, ResNet-like architecture, point cloud feature extraction, Neural ODE, lightweight version <br />
Summary: <br />
PointODE introduces a parameter-efficient ResNet-like architecture for point cloud feature extraction, utilizing Neural ODE to compress the model and handle non-uniform point distributions. The lightweight PointODE-Elite version with 0.58M parameters includes a dedicated FPGA accelerator that speeds up inference by 3.7x compared to a CPU, with 3.5x better energy efficiency. Despite its simplicity, PointODE-Elite achieves competitive accuracy on classification datasets, improving the trade-off between accuracy and inference cost. <div>
arXiv:2506.00438v1 Announce Type: new 
Abstract: Embedded edge devices are often used as a computing platform to run real-world point cloud applications, but recent deep learning-based methods may not fit on such devices due to limited resources. In this paper, we aim to fill this gap by introducing PointODE, a parameter-efficient ResNet-like architecture for point cloud feature extraction based on a stack of MLP blocks with residual connections. We leverage Neural ODE (Ordinary Differential Equation), a continuous-depth version of ResNet originally developed for modeling the dynamics of continuous-time systems, to compress PointODE by reusing the same parameters across MLP blocks. The point-wise normalization is proposed for PointODE to handle the non-uniform distribution of feature points. We introduce PointODE-Elite as a lightweight version with 0.58M trainable parameters and design its dedicated accelerator for embedded FPGAs. The accelerator consists of a four-stage pipeline to parallelize the feature extraction for multiple points and stores the entire parameters on-chip to eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53 CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature extraction by 4.9x, leading to 3.7x faster inference and 3.5x better energy-efficiency. Despite the simple architecture, PointODE-Elite shows competitive accuracy to the state-of-the-art models on both synthetic and real-world classification datasets, greatly improving the trade-off between accuracy and inference cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</title>
<link>https://arxiv.org/abs/2506.00439</link>
<guid>https://arxiv.org/abs/2506.00439</guid>
<content:encoded><![CDATA[
<div> Ensembling, Large Language Models, Reinforcement Learning, Markov Decision Process, RLAE Ensemble<br />
Summary:<br />
Ensembling large language models (LLMs) with fixed weighting strategies is limiting. This work proposes RLAE, a framework that uses Reinforcement Learning to dynamically adjust ensemble weights based on input context and intermediate generation states. RLAE outperforms conventional methods by up to 3.3% accuracy points on various tasks. The RL agent in RLAE is trained using rewards tied to output quality. Implementing RLAE with single-agent and multi-agent reinforcement learning algorithms shows significant improvements. RLAE showcases better generalization abilities across tasks, without needing retraining. It also achieves lower time latency, making it an effective framework for LLM ensembling.<br /><br />Summary: <div>
arXiv:2506.00439v1 Announce Type: new 
Abstract: Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2506.00440</link>
<guid>https://arxiv.org/abs/2506.00440</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Personalized Federated Learning, Population Stability Index, Data heterogeneity, Non-IIDness <br />
<br />
Summary: <br />
The article introduces PSI-PFL, a client selection framework for Personalized Federated Learning that uses the Population Stability Index (PSI) to address data heterogeneity challenges in non-independent and identically distributed (non-IID) data scenarios. The approach aims to mitigate label skew, a key factor affecting Federated Learning (FL) performance. Experimental results across various data types show that PSI-PFL significantly improves global model accuracy, surpassing existing baselines by up to 10% in non-IID settings. By selecting more homogeneous clients based on PSI, the framework enhances FL performance while ensuring fairer local model performance. Overall, PSI-PFL offers practical benefits in scenarios where data privacy and heterogeneity are crucial considerations. <br /> <div>
arXiv:2506.00440v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized machine learning (ML) model training while preserving data privacy by keeping data localized across clients. However, non-independent and identically distributed (non-IID) data across clients poses a significant challenge, leading to skewed model updates and performance degradation. Addressing this, we propose PSI-PFL, a novel client selection framework for Personalized Federated Learning (PFL) that leverages the Population Stability Index (PSI) to quantify and mitigate data heterogeneity (so-called non-IIDness). Our approach selects more homogeneous clients based on PSI, reducing the impact of label skew, one of the most detrimental factors in FL performance. Experimental results over multiple data modalities (tabular, image, text) demonstrate that PSI-PFL significantly improves global model accuracy, outperforming state-of-the-art baselines by up to 10\% under non-IID scenarios while ensuring fairer local performance. PSI-PFL enhances FL performance and offers practical benefits in applications where data privacy and heterogeneity are critical.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction</title>
<link>https://arxiv.org/abs/2506.00453</link>
<guid>https://arxiv.org/abs/2506.00453</guid>
<content:encoded><![CDATA[
<div> Dynamic graphs, Dynamic graph neural network models, Dowker Zigzag Persistence (DZP), TMetaNet, Meta-learning<br />
<br />
Dynamic graphs pose challenges for traditional graph learning methods due to their changing structures. To address this, the authors propose a new method called Dowker Zigzag Persistence (DZP) that captures high-order features of dynamic graphs. Based on this, they introduce TMetaNet, a meta-learning parameter update model that incorporates dynamic topological features to enable more effective adaptation across snapshots. Experimental results on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, showcasing its potential for meta-learning and dynamic graph analysis.<br /><br />Summary: <div>
arXiv:2506.00453v1 Announce Type: new 
Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional graph learning due to their changing structures and temporal dependencies. Recent advancements have shown potential in addressing these challenges by developing suitable meta-learning-based dynamic graph neural network models. However, most meta-learning approaches for dynamic graphs rely on fixed weight update parameters, neglecting the essential intrinsic complex high-order topological information of dynamically evolving graphs. We have designed Dowker Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent homology representation method based on Dowker complex and zigzag persistence, to capture the high-order features of dynamic graphs. Armed with the DZP ideas, we propose TMetaNet, a new meta-learning parameter update model based on dynamic topological features. By utilizing the distances between high-order topological features, TMetaNet enables more effective adaptation across snapshots. Experiments on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, illustrating its high potential for meta-learning and dynamic graph analysis. Our code is available at https://github.com/Lihaogx/TMetaNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models</title>
<link>https://arxiv.org/abs/2506.00457</link>
<guid>https://arxiv.org/abs/2506.00457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, zero-shot forecasting, state-of-the-art models, sensitivity to noise, fine-tuning<br />
Summary:<br />
Large Language Models (LLMs) have shown great performance in various tasks but struggle as zero-shot forecasters due to sensitivity to noise. Recent studies indicate LLMs may not be as effective in forecasting compared to domain-specific models. The study evaluates LLMs as zero-shot forecasters and finds they often underperform even simple domain-specific models. Solutions to reduce sensitivity to noise in LLMs have been explored but remain challenging. It is suggested that instead of focusing on zero-shot forecasting, fine-tuning LLMs for better processing of numerical sequences shows more promise. The experimental code is available for further exploration. The findings highlight the need for rigorous validation when assessing the effectiveness of LLMs in forecasting tasks. <div>
arXiv:2506.00457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting through prompting alone, recent studies suggest that LLMs lack inherent effectiveness in forecasting. Given these conflicting findings, a rigorous validation is essential for drawing reliable conclusions. In this paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared to state-of-the-art domain-specific models. Our experiments show that LLM-based zero-shot forecasters often struggle to achieve high accuracy due to their sensitivity to noise, underperforming even simple domain-specific models. We have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot setting, but improving their robustness remains a significant challenge. Our findings suggest that rather than emphasizing zero-shot forecasting, a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences. Our experimental code is available at https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Hanabi</title>
<link>https://arxiv.org/abs/2506.00458</link>
<guid>https://arxiv.org/abs/2506.00458</guid>
<content:encoded><![CDATA[
<div> Keywords: Hanabi, reinforcement learning, tabular agents, deep reinforcement learning, TD algorithms

Summary:
Hanabi, a cooperative card game challenging for reinforcement learning (RL) agents due to incomplete knowledge, was studied using tabular and deep RL algorithms. Different agents excelled against specific opponents or through adaptation. Conditions for each algorithm's advantage were identified, with interactions between agents of different types noted. Temporal difference (TD) algorithms, particularly tabular Expected SARSA and deep Q-Learning, exhibited superior overall performance and play type balancing. <div>
arXiv:2506.00458v1 Announce Type: new 
Abstract: Hanabi has become a popular game for research when it comes to reinforcement learning (RL) as it is one of the few cooperative card games where you have incomplete knowledge of the entire environment, thus presenting a challenge for a RL agent. We explored different tabular and deep reinforcement learning algorithms to see which had the best performance both against an agent of the same type and also against other types of agents. We establish that certain agents played their highest scoring games against specific agents while others exhibited higher scores on average by adapting to the opposing agent's behavior. We attempted to quantify the conditions under which each algorithm provides the best advantage and identified the most interesting interactions between agents of different types. In the end, we found that temporal difference (TD) algorithms had better overall performance and balancing of play types compared to tabular agents. Specifically, tabular Expected SARSA and deep Q-Learning agents showed the best performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control</title>
<link>https://arxiv.org/abs/2506.00459</link>
<guid>https://arxiv.org/abs/2506.00459</guid>
<content:encoded><![CDATA[
<div> Keywords: energy storage management, traditional methods, reinforcement learning, micro-grid model, optimization challenges

Summary: 
This study compares traditional methods and reinforcement learning (RL) approaches for energy storage management in a simplified micro-grid model. Three use cases are examined: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. The study provides detailed formulations of each use case and optimization challenges. Comparisons are made between the performance of traditional methods and RL methods, discussing the scenarios in which each method is beneficial. The aim is to promote the use of RL-based methods in challenging domains like energy storage management and suggest future research directions. <div>
arXiv:2506.00459v1 Announce Type: new 
Abstract: We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2506.00467</link>
<guid>https://arxiv.org/abs/2506.00467</guid>
<content:encoded><![CDATA[
<div> Self-training, Self-adaptive Thresholding, Semi-supervised learning, Neural networks, SSL <br />
Summary: <br />
The paper introduces a novel framework called Self-training with Self-adaptive Thresholding (SST) for semi-supervised learning (SSL) utilizing a Self-Adaptive Thresholding (SAT) mechanism to adjust class-specific thresholds based on the model's learning progress. This innovative approach aims to select high-quality pseudo-labeled data efficiently and mitigate risks of inaccurate labels and confirmation bias. Experimental results show that SST outperforms existing methods, achieving state-of-the-art performance with high efficiency, generalization, and scalability across various architectures and datasets. In particular, Semi-SST-ViT-Huge achieves impressive results on ImageNet-1K SSL benchmarks, achieving 80.7% / 84.9% Top-1 accuracy with only 1% / 10% labeled data, surpassing the performance of fully-supervised methods with significantly less labeled data. The proposed framework showcases superior performance and demonstrates the potential of adaptive thresholding techniques in SSL. <br /> <div>
arXiv:2506.00467v1 Announce Type: new 
Abstract: Neural networks have demonstrated exceptional performance in supervised learning, benefiting from abundant high-quality annotated data. However, obtaining such data in real-world scenarios is costly and labor-intensive. Semi-supervised learning (SSL) offers a solution to this problem. Recent studies, such as Semi-ViT and Noisy Student, which employ consistency regularization or pseudo-labeling, have demonstrated significant achievements. However, they still face challenges, particularly in accurately selecting sufficient high-quality pseudo-labels due to their reliance on fixed thresholds. Recent methods such as FlexMatch and FreeMatch have introduced flexible or self-adaptive thresholding techniques, greatly advancing SSL research. Nonetheless, their process of updating thresholds at each iteration is deemed time-consuming, computationally intensive, and potentially unnecessary. To address these issues, we propose Self-training with Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL framework. SST introduces an innovative Self-Adaptive Thresholding (SAT) mechanism that adaptively adjusts class-specific thresholds based on the model's learning progress. SAT ensures the selection of high-quality pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and confirmation bias. Extensive experiments demonstrate that SST achieves state-of-the-art performance with remarkable efficiency, generalization, and scalability across various architectures and datasets. Semi-SST-ViT-Huge achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7% / 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using 100% labeled data, our method demonstrates superior performance using only 10% labeled data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet - A ResNet-based Model Classification Dataset</title>
<link>https://arxiv.org/abs/2506.00476</link>
<guid>https://arxiv.org/abs/2506.00476</guid>
<content:encoded><![CDATA[
<div> Dataset, Federated Learning, Privacy, Image Classification, ModelNet

Summary: 
The paper introduces ModelNet, a novel image classification dataset created from embeddings of a pre-trained ResNet50 model. The dataset includes three client-specific variants with different domain heterogeneities (homogeneous, heterogeneous, random) to simulate realistic Federated Learning (FL) settings. The paper proposes a new FL algorithm that accesses anonymized model parameters to enhance local privacy protection. This dataset aims to address the lack of domain heterogeneity and client-specific segregation in FL benchmarks. ModelNet-S, ModelNet-D, and ModelNet-R cater to homogeneous, heterogeneous, and random data settings, respectively, making it the first cross-environment client-specific FL dataset. The paper conducts extensive experiments on domain shifts and aggregation strategies, showcasing the effectiveness of the dataset for both classical and graph-based FL research. The dataset and code are openly available for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2506.00476v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training machine learning models across distributed data sources while preserving data locality. However, the privacy of local data is always a pivotal concern and has received a lot of attention in recent research on the FL regime. Moreover, the lack of domain heterogeneity and client-specific segregation in the benchmarks remains a critical bottleneck for rigorous evaluation. In this paper, we introduce ModelNet, a novel image classification dataset constructed from the embeddings extracted from a pre-trained ResNet50 model. First, we modify the CIFAR100 dataset into three client-specific variants, considering three domain heterogeneities (homogeneous, heterogeneous, and random). Subsequently, we train each client-specific subset of all three variants on the pre-trained ResNet50 model to save model parameters. In addition to multi-domain image data, we propose a new hypothesis to define the FL algorithm that can access the anonymized model parameters to preserve the local privacy in a more effective manner compared to existing ones. ModelNet is designed to simulate realistic FL settings by incorporating non-IID data distributions and client diversity design principles in the mainframe for both conventional and futuristic graph-driven FL algorithms. The three variants are ModelNet-S, ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and random data settings, respectively. To the best of our knowledge, we are the first to propose a cross-environment client-specific FL dataset along with the graph-based variant. Extensive experiments based on domain shifts and aggregation strategies show the effectiveness of the above variants, making it a practical benchmark for classical and graph-based FL research. The dataset and related code are available online.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flashbacks to Harmonize Stability and Plasticity in Continual Learning</title>
<link>https://arxiv.org/abs/2506.00477</link>
<guid>https://arxiv.org/abs/2506.00477</guid>
<content:encoded><![CDATA[
arXiv:2506.00477v1 Announce Type: new 
Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize the stability and plasticity of models in Continual Learning (CL). Unlike prior approaches that primarily focus on regularizing model updates to preserve old information while learning new concepts, FL explicitly balances this trade-off through a bidirectional form of regularization. This approach effectively guides the model to swiftly incorporate new knowledge while actively retaining its old knowledge. FL operates through a two-phase training process and can be seamlessly integrated into various CL methods, including replay, parameter regularization, distillation, and dynamic architecture techniques. In designing FL, we use two distinct knowledge bases: one to enhance plasticity and another to improve stability. FL ensures a more balanced model by utilizing both knowledge bases to regularize model updates. Theoretically, we analyze how the FL mechanism enhances the stability-plasticity balance. Empirically, FL demonstrates tangible improvements over baseline methods within the same training budget. By integrating FL into at least one representative baseline from each CL category, we observed an average accuracy improvement of up to 4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard image classification benchmarks. Additionally, measurements of the stability-to-plasticity ratio confirm that FL effectively enhances this balance. FL also outperforms state-of-the-art CL methods on more challenging datasets like ImageNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF</title>
<link>https://arxiv.org/abs/2506.00478</link>
<guid>https://arxiv.org/abs/2506.00478</guid>
<content:encoded><![CDATA[
arXiv:2506.00478v1 Announce Type: new 
Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator power outputs by utilizing the non-linear relationships between voltage magnitudes and phase angles in a power system. However, current AC-OPF solvers struggle to effectively represent the complex relationship between variable distributions in the constraint space and their corresponding optimal solutions. This limitation in constraint modeling restricts the system's ability to develop diverse knowledge representations. Additionally, modeling the power grid solely based on spatial topology further limits the integration of additional prior knowledge, such as temporal information. To overcome these challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven Physics-Informed Graph Convolutional Network), a new method designed to address constraint-related issues and build a graph-based learning framework that incorporates spatiotemporal features. DDA-PIGCN improves consistency optimization for features with varying long-range dependencies by applying multi-layer, hard physics-informed constraints. It also uses a dynamic domain adaptation learning mechanism that iteratively updates and refines key state variables under predefined constraints, enabling precise constraint verification. Moreover, it captures spatiotemporal dependencies between generators and loads by leveraging the physical structure of the power grid, allowing for deep integration of topological information across time and space. Extensive comparative and ablation studies show that DDA-PIGCN delivers strong performance across several IEEE standard test cases (such as case9, case30, and case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and constraint satisfaction rates between 99.6% and 100%, establishing it as a reliable and efficient AC-OPF solver.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.00482</link>
<guid>https://arxiv.org/abs/2506.00482</guid>
<content:encoded><![CDATA[
arXiv:2506.00482v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs</title>
<link>https://arxiv.org/abs/2506.00486</link>
<guid>https://arxiv.org/abs/2506.00486</guid>
<content:encoded><![CDATA[
arXiv:2506.00486v1 Announce Type: new 
Abstract: Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts</title>
<link>https://arxiv.org/abs/2506.00495</link>
<guid>https://arxiv.org/abs/2506.00495</guid>
<content:encoded><![CDATA[
arXiv:2506.00495v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely adopted strategy for adapting pre-trained Large Language Models (LLMs) to downstream tasks, significantly reducing memory and computational costs. However, most existing PEFT techniques uniformly deploy LoRA adapters across all layers, disregarding the intrinsic heterogeneity of layer contributions and task-specific rank requirements. This uniform paradigm leads to redundant parameter allocation and suboptimal adaptation efficiency. To address these limitations, we propose FLoE, a novel PEFT framework that introduces two key innovations: (i) a Fisher information-guided importance scoring mechanism to dynamically identify task-critical transformer layers for MoE-based low-rank adaptation, enabling sparse adapter deployment; and (ii) a Bayesian optimization-driven rank allocator that automatically determines optimal LoRA ranks on specific datasets without exhaustive grid search. Extensive experiments across diverse LLMs and benchmarks reveal that FLoE achieves impressive efficiency-accuracy trade-offs, making FLoE particularly advantageous in resource-constrained environments that necessitate rapid adaptation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study</title>
<link>https://arxiv.org/abs/2506.00499</link>
<guid>https://arxiv.org/abs/2506.00499</guid>
<content:encoded><![CDATA[
arXiv:2506.00499v1 Announce Type: new 
Abstract: Complex systems such as aircraft engines are continuously monitored by sensors. In predictive aircraft maintenance, the collected sensor measurements are used to estimate the health condition and the Remaining Useful Life (RUL) of such systems. However, a major challenge when developing prognostics is the limited number of run-to-failure data samples. This challenge could be overcome if multiple airlines would share their run-to-failure data samples such that sufficient learning can be achieved. Due to privacy concerns, however, airlines are reluctant to share their data in a centralized setting. In this paper, a collaborative federated learning framework is therefore developed instead. Here, several airlines cooperate to train a collective RUL prognostic machine learning model, without the need to centrally share their data. For this, a decentralized validation procedure is proposed to validate the prognostics model without sharing any data. Moreover, sensor data is often noisy and of low quality. This paper therefore proposes four novel methods to aggregate the parameters of the global prognostic model. These methods enhance the robustness of the FL framework against noisy data. The proposed framework is illustrated for training a collaborative RUL prognostic model for aircraft engines, using the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in the FL framework to train a collective RUL prognostic model for their aircraft's engines. When comparing the proposed FL framework with the case where each airline independently develops their own prognostic model, the results show that FL leads to more accurate RUL prognostics for five out of the six airlines. Moreover, the novel robust aggregation methods render the FL framework robust to noisy data samples.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending</title>
<link>https://arxiv.org/abs/2506.00505</link>
<guid>https://arxiv.org/abs/2506.00505</guid>
<content:encoded><![CDATA[
arXiv:2506.00505v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) lending enables permissionless borrowing via smart contracts. However, it faces challenges in optimizing interest rates, mitigating bad debt, and improving capital efficiency. Rule-based interest-rate models struggle to adapt to dynamic market conditions, leading to inefficiencies. This work applies Offline Reinforcement Learning (RL) to optimize interest rate adjustments in DeFi lending protocols. Using historical data from Aave protocol, we evaluate three RL approaches: Conservative Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning (TD3-BC). TD3-BC demonstrates superior performance in balancing utilization, capital stability, and risk, outperforming existing models. It adapts effectively to historical stress events like the May 2021 crash and the March 2023 USDC depeg, showcasing potential for automated, real-time governance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings</title>
<link>https://arxiv.org/abs/2506.00528</link>
<guid>https://arxiv.org/abs/2506.00528</guid>
<content:encoded><![CDATA[
arXiv:2506.00528v1 Announce Type: new 
Abstract: Many modern search domains comprise high-dimensional vectors of floating point numbers derived from neural networks, in the form of embeddings. Typical embeddings range in size from hundreds to thousands of dimensions, making the size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values with a smaller representation, for example a short integer. This gives an approximation of the embedding space in return for a smaller data representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of arbitrary-precision floating point values can be replaced by vectors whose elements are drawn from the set {-1,0,1}. This yields very significant savings in space and metric evaluation cost, while maintaining a strong correlation for similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the high-dimensional space. In this article we give an outline description of these objects, and show how they can be used for the basis of such radical quantisation while maintaining a surprising degree of accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model</title>
<link>https://arxiv.org/abs/2506.00531</link>
<guid>https://arxiv.org/abs/2506.00531</guid>
<content:encoded><![CDATA[
arXiv:2506.00531v1 Announce Type: new 
Abstract: The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems</title>
<link>https://arxiv.org/abs/2506.00533</link>
<guid>https://arxiv.org/abs/2506.00533</guid>
<content:encoded><![CDATA[
arXiv:2506.00533v1 Announce Type: new 
Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges: poor generalization for scalable TSPs and high training costs. To address these challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN). Focusing on the scale-dependent features (i.e., features varied with problem scales) related to nodes and edges that influence the sensitivity of GCNs to the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization capability by (1) rescaling adjacent nodes to construct a subgraph with a uniform number of adjacent nodes for each node across various scales of TSPs, which stabilizes the graph message aggregation; (2) rescaling subgraph edges to adjust the lengths of subgraph edges to the same magnitude, which maintains numerical consistency. In addition, an efficient training strategy with a mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit the heatmaps generated by RsGCN, we design an efficient post-search algorithm termed Re2Opt, in which a reconstruction process based on adaptive weight is incorporated to help avoid local optima. Based on a combined architecture of RsGCN and Re2Opt, our solver achieves remarkable generalization and low training cost: with only 3 epochs of training on the mixed-scale dataset containing instances with up to 100 nodes, it can be generalized successfully to 10K-node instances without any fine-tuning. Extensive experiments demonstrate our state-of-the-art performance across uniform distribution instances of 9 different scales from 20 to 10K nodes and 78 real-world instances from TSPLIB, while requiring the fewest learnable parameters and training epochs among neural competitors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach</title>
<link>https://arxiv.org/abs/2506.00545</link>
<guid>https://arxiv.org/abs/2506.00545</guid>
<content:encoded><![CDATA[
arXiv:2506.00545v1 Announce Type: new 
Abstract: Missing data is a relevant issue in time series, especially in biomedical sequences such as those corresponding to smooth pursuit eye movements, which often contain gaps due to eye blinks and track losses, complicating the analysis and extraction of meaningful biomarkers. In this paper, a novel imputation framework is proposed using Self-Attention-based Imputation networks for time series, which leverages the power of deep learning and self-attention mechanisms to impute missing data. We further refine the imputed data using a custom made autoencoder, tailored to represent smooth pursuit eye movement sequences. The proposed approach was implemented using 5,504 sequences from 172 Parkinsonian patients and healthy controls. Results show a significant improvement in the accuracy of reconstructed eye movement sequences with respect to other state of the art techniques, substantially reducing the values for common time domain error metrics such as the mean absolute error, mean relative error, and root mean square error, while also preserving the signal's frequency domain characteristics. Moreover, it demonstrates robustness when large intervals of data are missing. This method offers an alternative solution for robustly handling missing data in time series, enhancing the reliability of smooth pursuit analysis for the screening and monitoring of neurodegenerative disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[
arXiv:2506.00555v1 Announce Type: new 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 18.4% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2506.00563</link>
<guid>https://arxiv.org/abs/2506.00563</guid>
<content:encoded><![CDATA[
arXiv:2506.00563v1 Announce Type: new 
Abstract: A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs</title>
<link>https://arxiv.org/abs/2506.00569</link>
<guid>https://arxiv.org/abs/2506.00569</guid>
<content:encoded><![CDATA[
arXiv:2506.00569v1 Announce Type: new 
Abstract: When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach -- which simply optimizes the total loss across all tasks -- and also outperforms model merging methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Estimation for Scaling Entropic Multimarginal Optimal Transport</title>
<link>https://arxiv.org/abs/2506.00573</link>
<guid>https://arxiv.org/abs/2506.00573</guid>
<content:encoded><![CDATA[
arXiv:2506.00573v1 Announce Type: new 
Abstract: Multimarginal optimal transport (MOT) is a powerful framework for modeling interactions between multiple distributions, yet its applicability is bottlenecked by a high computational overhead. Entropic regularization provides computational speedups via the multimarginal Sinkhorn algorithm, whose time complexity, for a dataset size $n$ and $k$ marginals, generally scales as $O(n^k)$. However, this dependence on the dataset size $n$ is computationally prohibitive for many machine learning problems. In this work, we propose a new computational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT), that enjoys significantly improved scalability. NEMOT employs neural networks trained using mini-batches, which transfers the computational complexity from the dataset size to the size of the mini-batch, leading to substantial gains. We provide formal guarantees on the accuracy of NEMOT via non-asymptotic error bounds. We supplement these with numerical results that demonstrate the performance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to neural computation of multimarginal entropic Gromov-Wasserstein alignment. In particular, orders-of-magnitude speedups are observed relative to the state-of-the-art, with a notable increase in the feasible number of samples and marginals. NEMOT seamlessly integrates as a module in large-scale machine learning pipelines, and can serve to expand the practical applicability of entropic MOT for tasks involving multimarginal data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00574</link>
<guid>https://arxiv.org/abs/2506.00574</guid>
<content:encoded><![CDATA[
arXiv:2506.00574v1 Announce Type: new 
Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00576</link>
<guid>https://arxiv.org/abs/2506.00576</guid>
<content:encoded><![CDATA[
arXiv:2506.00576v1 Announce Type: new 
Abstract: Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow Feature Analysis as Variational Inference Objective</title>
<link>https://arxiv.org/abs/2506.00580</link>
<guid>https://arxiv.org/abs/2506.00580</guid>
<content:encoded><![CDATA[
arXiv:2506.00580v1 Announce Type: new 
Abstract: This work presents a novel probabilistic interpretation of Slow Feature Analysis (SFA) through the lens of variational inference. Unlike prior formulations that recover linear SFA from Gaussian state-space models with linear emissions, this approach relaxes the key constraint of linearity. While it does not lead to full equivalence to non-linear SFA, it recasts the classical slowness objective in a variational framework. Specifically, it allows the slowness objective to be interpreted as a regularizer to a reconstruction loss. Furthermore, we provide arguments, why -- from the perspective of slowness optimization -- the reconstruction loss takes on the role of the constraints that ensure informativeness in SFA. We conclude with a discussion of potential new research directions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding the Stressed Brain with Geometric Machine Learning</title>
<link>https://arxiv.org/abs/2506.00587</link>
<guid>https://arxiv.org/abs/2506.00587</guid>
<content:encoded><![CDATA[
arXiv:2506.00587v1 Announce Type: new 
Abstract: Stress significantly contributes to both mental and physical disorders, yet traditional self-reported questionnaires are inherently subjective. In this study, we introduce a novel framework that employs geometric machine learning to detect stress from raw EEG recordings. Our approach constructs graphs by integrating structural connectivity (derived from electrode spatial arrangement) with functional connectivity from pairwise signal correlations. A spatio-temporal graph convolutional network (ST-GCN) processes these graphs to capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show that the ST-GCN outperforms standard machine learning models on all key classification metrics and enhances interpretability, explored through ablation analyses of key channels and brain regions. These results pave the way for more objective and accurate stress detection methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Chunking Enhances Recognition of Implicit Sequential Patterns</title>
<link>https://arxiv.org/abs/2506.00588</link>
<guid>https://arxiv.org/abs/2506.00588</guid>
<content:encoded><![CDATA[
arXiv:2506.00588v1 Announce Type: new 
Abstract: In this pilot study, we propose a neuro-inspired approach that compresses temporal sequences into context-tagged chunks, where each tag represents a recurring structural unit or``community'' in the sequence. These tags are generated during an offline sleep phase and serve as compact references to past experience, allowing the learner to incorporate information beyond its immediate input range. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. Our results, while preliminary, suggest that temporal chunking can significantly enhance learning efficiency under resource constrained settings. A small-scale human pilot study using a Serial Reaction Time Task further motivates the idea of structural abstraction. Although limited to synthetic tasks, this work serves as an early proof-of-concept, with initial evidence that learned context tags can transfer across related task, offering potential for future applications in transfer learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn</title>
<link>https://arxiv.org/abs/2506.00592</link>
<guid>https://arxiv.org/abs/2506.00592</guid>
<content:encoded><![CDATA[
arXiv:2506.00592v1 Announce Type: new 
Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Evidential Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.00594</link>
<guid>https://arxiv.org/abs/2506.00594</guid>
<content:encoded><![CDATA[
arXiv:2506.00594v1 Announce Type: new 
Abstract: Graph anomaly detection faces significant challenges due to the scarcity of reliable anomaly-labeled datasets, driving the development of unsupervised methods. Graph autoencoders (GAEs) have emerged as a dominant approach by reconstructing graph structures and node features while deriving anomaly scores from reconstruction errors. However, relying solely on reconstruction error for anomaly detection has limitations, as it increases the sensitivity to noise and overfitting. To address these issues, we propose Graph Evidential Learning (GEL), a probabilistic framework that redefines the reconstruction process through evidential learning. By modeling node features and graph topology using evidential distributions, GEL quantifies two types of uncertainty: graph uncertainty and reconstruction uncertainty, incorporating them into the anomaly scoring mechanism. Extensive experiments demonstrate that GEL achieves state-of-the-art performance while maintaining high robustness against noise and structural perturbations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data</title>
<link>https://arxiv.org/abs/2506.00614</link>
<guid>https://arxiv.org/abs/2506.00614</guid>
<content:encoded><![CDATA[
arXiv:2506.00614v1 Announce Type: new 
Abstract: Real-world multichannel time series prediction faces growing demands for efficiency across edge and cloud environments, making channel compression a timely and essential problem. Motivated by success of Multiple-Input Multiple-Output (MIMO) methods, we propose a predictability-aware compression-decompression framework to reduce runtime, lower communication cost, and maintain prediction accuracy across diverse predictors. The core idea involves using a circular periodicity key matrix with orthogonality to capture underlying time series predictability during compression and to mitigate reconstruction errors during decompression by relaxing oversimplified data assumptions. Theoretical and empirical analyses show that the proposed framework is both time-efficient and scalable under a large number of channels. Extensive experiments on six datasets across various predictors demonstrate that the proposed method achieves superior overall performance by jointly considering prediction accuracy and runtime, while maintaining strong compatibility with diverse predictors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Reprogramming Demystified: A Neural Tangent Kernel Perspective</title>
<link>https://arxiv.org/abs/2506.00620</link>
<guid>https://arxiv.org/abs/2506.00620</guid>
<content:encoded><![CDATA[
arXiv:2506.00620v1 Announce Type: new 
Abstract: Model Reprogramming (MR) is a resource-efficient framework that adapts large pre-trained models to new tasks with minimal additional parameters and data, offering a promising solution to the challenges of training large models for diverse tasks. Despite its empirical success across various domains such as computer vision and time-series forecasting, the theoretical foundations of MR remain underexplored. In this paper, we present a comprehensive theoretical analysis of MR through the lens of the Neural Tangent Kernel (NTK) framework. We demonstrate that the success of MR is governed by the eigenvalue spectrum of the NTK matrix on the target dataset and establish the critical role of the source model's effectiveness in determining reprogramming outcomes. Our contributions include a novel theoretical framework for MR, insights into the relationship between source and target models, and extensive experiments validating our findings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models</title>
<link>https://arxiv.org/abs/2506.00630</link>
<guid>https://arxiv.org/abs/2506.00630</guid>
<content:encoded><![CDATA[
arXiv:2506.00630v1 Announce Type: new 
Abstract: Decision-making in building energy systems critically depends on the predictive accuracy of relevant time-series models. In scenarios lacking extensive data from a target building, foundation models (FMs) represent a promising technology that can leverage prior knowledge from vast and diverse pre-training datasets to construct accurate probabilistic predictors for use in decision-making tools. This paper investigates the applicability and fine-tuning strategies of time-series foundation models (TSFMs) in building energy forecasting. We analyze both full fine-tuning and parameter-efficient fine-tuning approaches, particularly low-rank adaptation (LoRA), by using real-world data from a commercial net-zero energy building to capture signals such as room occupancy, carbon emissions, plug loads, and HVAC energy consumption. Our analysis reveals that the zero-shot predictive performance of TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that employing either full fine-tuning or parameter-efficient fine-tuning significantly enhances forecasting accuracy, even with limited historical data. Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs consistently outperform state-of-the-art deep forecasting models (e.g., temporal fusion transformers) in accuracy, robustness, and generalization across varying building zones and seasonal conditions. These results underline the efficacy of TSFMs for practical, data-constrained building energy management systems, enabling improved decision-making in pursuit of energy efficiency and sustainability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2506.00635</link>
<guid>https://arxiv.org/abs/2506.00635</guid>
<content:encoded><![CDATA[
arXiv:2506.00635v1 Announce Type: new 
Abstract: Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Neural-based Matrix Inversion: Why can't, and Where can</title>
<link>https://arxiv.org/abs/2506.00642</link>
<guid>https://arxiv.org/abs/2506.00642</guid>
<content:encoded><![CDATA[
arXiv:2506.00642v1 Announce Type: new 
Abstract: Deep neural networks have achieved substantial success across various scientific computing tasks. A pivotal challenge within this domain is the rapid and parallel approximation of matrix inverses, critical for numerous applications. Despite significant progress, there currently exists no universal neural-based method for approximating matrix inversion. This paper presents a theoretical analysis demonstrating the fundamental limitations of neural networks in developing a general matrix inversion model. We expand the class of Lipschitz functions to encompass a wider array of neural network models, thereby refining our theoretical approach. Moreover, we delineate specific conditions under which neural networks can effectively approximate matrix inverses. Our theoretical results are supported by experimental results from diverse matrix datasets, exploring the efficacy of neural networks in addressing the matrix inversion challenge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</title>
<link>https://arxiv.org/abs/2506.00653</link>
<guid>https://arxiv.org/abs/2506.00653</guid>
<content:encoded><![CDATA[
arXiv:2506.00653v1 Announce Type: new 
Abstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings</title>
<link>https://arxiv.org/abs/2506.00656</link>
<guid>https://arxiv.org/abs/2506.00656</guid>
<content:encoded><![CDATA[
arXiv:2506.00656v1 Announce Type: new 
Abstract: We propose a permutation-invariant neural architecture for indoor localization using RSSI scans from Wi-Fi access points. Each scan is modeled as an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned embeddings and concatenated with signal strength. These are processed by a Set Transformer, enabling the model to handle variable-length, sparse inputs while learning attention- based representations over access point relationships. We evaluate the model on a dataset collected across a campus environment consisting of six buildings. Results show that the model accurately recovers fine-grained spatial structure and maintains performance across physically distinct domains. In our experiments, a simple LSTM consistently outperformed all other models, achieving the lowest mean localization error across three tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer performed competitively, ranking second in every experiment and outperforming the MLP, RNN, and basic attention models, particularly in scenarios involving multiple buildings (E2) and multiple floors (E3). Performance degraded most in E2, where signal conditions varied substantially across buildings, highlighting the importance of architectural robustness to domain diversity. This work demonstrates that set-based neural models are a natural fit for signal-based localization, offering a principled approach to handling sparse, unordered inputs in real-world positioning tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy for Deep Learning in Medicine</title>
<link>https://arxiv.org/abs/2506.00660</link>
<guid>https://arxiv.org/abs/2506.00660</guid>
<content:encoded><![CDATA[
arXiv:2506.00660v1 Announce Type: new 
Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00676</link>
<guid>https://arxiv.org/abs/2506.00676</guid>
<content:encoded><![CDATA[
arXiv:2506.00676v1 Announce Type: new 
Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Existing Large Language Model Unlearning Evaluations Are Inconclusive</title>
<link>https://arxiv.org/abs/2506.00688</link>
<guid>https://arxiv.org/abs/2506.00688</guid>
<content:encoded><![CDATA[
arXiv:2506.00688v1 Announce Type: new 
Abstract: Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00691</link>
<guid>https://arxiv.org/abs/2506.00691</guid>
<content:encoded><![CDATA[
arXiv:2506.00691v1 Announce Type: new 
Abstract: Training reinforcement learning (RL) agents often requires significant computational resources and extended training times. To address this, we build upon the foundation laid by Google Brain's Sensory Neuron, which introduced a novel neural architecture for reinforcement learning tasks that maintained permutation in-variance in the sensory neuron system. While the baseline model demonstrated significant performance improvements over traditional approaches, we identified opportunities to enhance the efficiency of the learning process further. We propose a modified attention mechanism incorporating a non-linear transformation of the key vectors (K) using a mapping function, resulting in a new set of key vectors (K'). This non-linear mapping enhances the representational capacity of the attention mechanism, allowing the model to encode more complex feature interactions and accelerating convergence without compromising performance. Our enhanced model demonstrates significant improvements in learning efficiency, showcasing the potential for non-linear attention mechanisms in advancing reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Path Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2506.00700</link>
<guid>https://arxiv.org/abs/2506.00700</guid>
<content:encoded><![CDATA[
arXiv:2506.00700v1 Announce Type: new 
Abstract: In constrained Markov decision processes, enforcing constraints during training is often thought of as decreasing the final return. Recently, it was shown that constraints can be incorporated directly in the policy geometry, yielding an optimization trajectory close to the central path of a barrier method, which does not compromise final return. Building on this idea, we introduce Central Path Proximal Policy Optimization (C3PO), a simple modification of PPO that produces policy iterates, which stay close to the central path of the constrained optimization problem. Compared to existing on-policy methods, C3PO delivers improved performance with tighter constraint enforcement, suggesting that central path-guided updates offer a promising direction for constrained policy optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Inference of Training Dataset Membership</title>
<link>https://arxiv.org/abs/2506.00701</link>
<guid>https://arxiv.org/abs/2506.00701</guid>
<content:encoded><![CDATA[
arXiv:2506.00701v1 Announce Type: new 
Abstract: Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, a challenge often addressed through membership inference attacks (MIAs). Traditional MIAs typically require access to model internals or rely on computationally intensive shadow models. This paper proposes an efficient, interpretable and principled Bayesian inference method for membership inference. By analyzing post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model, our approach computes posterior probabilities of membership without requiring extensive model training. Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. Beyond membership inference, this method can also detect distribution shifts, offering a practical and interpretable alternative to existing approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00710</link>
<guid>https://arxiv.org/abs/2506.00710</guid>
<content:encoded><![CDATA[
arXiv:2506.00710v1 Announce Type: new 
Abstract: Real-world databases are predominantly relational, comprising multiple interlinked tables that contain complex structural and statistical dependencies. Learning generative models on relational data has shown great promise in generating synthetic data and imputing missing values. However, existing methods often struggle to capture this complexity, typically reducing relational data to conditionally generated flat tables and imposing limiting structural assumptions. To address these limitations, we introduce RelDiff, a novel diffusion generative model that synthesizes complete relational databases by explicitly modeling their foreign key graph structure. RelDiff combines a joint graph-conditioned diffusion process across all tables for attribute synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model for structure generation. The decomposition of graph structure and relational attributes ensures both high fidelity and referential integrity, both of which are crucial aspects of synthetic relational database generation. Experiments on 11 benchmark datasets demonstrate that RelDiff consistently outperforms prior methods in producing realistic and coherent synthetic relational databases. Code is available at https://github.com/ValterH/RelDiff.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</title>
<link>https://arxiv.org/abs/2506.00711</link>
<guid>https://arxiv.org/abs/2506.00711</guid>
<content:encoded><![CDATA[
arXiv:2506.00711v1 Announce Type: new 
Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls in Evaluating Language Model Forecasters</title>
<link>https://arxiv.org/abs/2506.00723</link>
<guid>https://arxiv.org/abs/2506.00723</guid>
<content:encoded><![CDATA[
arXiv:2506.00723v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A condensing approach to multiple shooting neural ordinary differential equation</title>
<link>https://arxiv.org/abs/2506.00724</link>
<guid>https://arxiv.org/abs/2506.00724</guid>
<content:encoded><![CDATA[
arXiv:2506.00724v1 Announce Type: new 
Abstract: Multiple-shooting is a parameter estimation approach for ordinary differential equations. In this approach, the trajectory is broken into small intervals, each of which can be integrated independently. Equality constraints are then applied to eliminate the shooting gap between the end of the previous trajectory and the start of the next trajectory. Unlike single-shooting, multiple-shooting is more stable, especially for highly oscillatory and long trajectories. In the context of neural ordinary differential equations, multiple-shooting is not widely used due to the challenge of incorporating general equality constraints. In this work, we propose a condensing-based approach to incorporate these shooting equality constraints while training a multiple-shooting neural ordinary differential equation (MS-NODE) using first-order optimization methods such as Adam.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00727</link>
<guid>https://arxiv.org/abs/2506.00727</guid>
<content:encoded><![CDATA[
arXiv:2506.00727v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in plane reformatting tasks. In these methods, an agent sequentially adjusts the position and orientation of an initial plane towards an objective location. This process allows accurate plane reformatting, without the need for detailed landmarks, which makes it suitable for images with limited contrast and resolution, such as 4D flow MRI. However, current DRL methods require the test dataset to be in the same position and orientation as the training dataset. In this paper, we present a novel technique that utilizes a flexible coordinate system based on the current state, enabling navigation in volumes at any position or orientation. We adopted the Asynchronous Advantage Actor Critic (A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN). Experimental results in 4D flow MRI demonstrate improved accuracy in plane reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75 mm), as well as statistically equivalent flow measurements determined by a plane reformatting process done by an expert (p=0.21). The method's flexibility and adaptability make it a promising candidate for other medical imaging applications beyond 4D flow MRI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[
arXiv:2506.00731v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms</title>
<link>https://arxiv.org/abs/2506.00732</link>
<guid>https://arxiv.org/abs/2506.00732</guid>
<content:encoded><![CDATA[
arXiv:2506.00732v1 Announce Type: new 
Abstract: We propose a novel discriminative model for sequence labeling called Bregman conditional random fields (BCRF). Contrary to standard linear-chain conditional random fields, BCRF allows fast parallelizable inference algorithms based on iterative Bregman projections. We show how such models can be learned using Fenchel-Young losses, including extension for learning from partial labels. Experimentally, our approach delivers comparable results to CRF while being faster, and achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers</title>
<link>https://arxiv.org/abs/2506.00744</link>
<guid>https://arxiv.org/abs/2506.00744</guid>
<content:encoded><![CDATA[
arXiv:2506.00744v1 Announce Type: new 
Abstract: We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with dynamic synaptic memory through fast-weight programming (FW-memory) -- the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift</title>
<link>https://arxiv.org/abs/2506.00756</link>
<guid>https://arxiv.org/abs/2506.00756</guid>
<content:encoded><![CDATA[
arXiv:2506.00756v1 Announce Type: new 
Abstract: Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing targeted corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how average performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a Subgroup-scanning Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first asks "Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we explain this using more detailed variable(subset)-specific shifts?" (How?). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Juntas under Markov Random Fields</title>
<link>https://arxiv.org/abs/2506.00764</link>
<guid>https://arxiv.org/abs/2506.00764</guid>
<content:encoded><![CDATA[
arXiv:2506.00764v1 Announce Type: new 
Abstract: We give an algorithm for learning $O(\log n)$ juntas in polynomial-time with respect to Markov Random Fields (MRFs) in a smoothed analysis framework where only the external field has been randomly perturbed. This is a broad generalization of the work of Kalai and Teng, who gave an algorithm that succeeded with respect to smoothed product distributions (i.e., MRFs whose dependency graph has no edges). Our algorithm has two phases: (1) an unsupervised structure learning phase and (2) a greedy supervised learning algorithm. This is the first example where algorithms for learning the structure of an undirected graphical model lead to provably efficient algorithms for supervised learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies</title>
<link>https://arxiv.org/abs/2506.00770</link>
<guid>https://arxiv.org/abs/2506.00770</guid>
<content:encoded><![CDATA[
arXiv:2506.00770v1 Announce Type: new 
Abstract: Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space</title>
<link>https://arxiv.org/abs/2506.00771</link>
<guid>https://arxiv.org/abs/2506.00771</guid>
<content:encoded><![CDATA[
arXiv:2506.00771v1 Announce Type: new 
Abstract: Medicinal chemists often optimize drugs considering their 3D structures and designing structurally distinct molecules that retain key features, such as shapes, pharmacophores, or chemical properties. Previous deep learning approaches address this through supervised tasks like molecule inpainting or property-guided optimization. In this work, we propose a flexible zero-shot molecule manipulation method by navigating in a shared latent space of 3D molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space independent of atom counts. MolFLAE encodes 3D molecules using an SE(3)-equivariant neural network into fixed number of latent nodes, distinguished by learned embeddings. The latent space is regularized, and molecular structures are reconstructed via a Bayesian Flow Network (BFN) conditioned on the encoder's latent output. MolFLAE achieves competitive performance on standard unconditional 3D molecule generation benchmarks. Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation, including atom number editing, structure reconstruction, and coordinated latent interpolation for both structure and properties. We further demonstrate our approach on a drug optimization task for the human glucocorticoid receptor, generating molecules with improved hydrophilicity while preserving key interactions, under computational evaluations. These results highlight the flexibility, robustness, and real-world utility of our method, opening new avenues for molecule editing and optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00772</link>
<guid>https://arxiv.org/abs/2506.00772</guid>
<content:encoded><![CDATA[
arXiv:2506.00772v1 Announce Type: new 
Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization</title>
<link>https://arxiv.org/abs/2506.00795</link>
<guid>https://arxiv.org/abs/2506.00795</guid>
<content:encoded><![CDATA[
arXiv:2506.00795v1 Announce Type: new 
Abstract: Recently, supervised learning (SL) methodology has emerged as an effective approach for offline reinforcement learning (RL) due to their simplicity, stability, and efficiency. However, recent studies show that SL methods lack the trajectory stitching capability, typically associated with temporal difference (TD)-based approaches. A question naturally surfaces: How can we endow SL methods with stitching capability and bridge its performance gap with TD learning? To answer this question, we introduce $Q$-conditioned maximization supervised learning for offline goal-conditioned RL, which enhances SL with the stitching capability through $Q$-conditioned policy and $Q$-conditioned maximization. Concretely, we propose Goal-Conditioned Reinforced Supervised Learning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE from the offline dataset and (2) finding the maximum $Q$-value within the data support by integrating $Q$-function maximization with Expectile Regression. In inference time, our policy chooses optimal actions based on such a maximum $Q$-value. Experimental results from stitching evaluations on offline RL datasets demonstrate that our method outperforms prior SL approaches with stitching capabilities and goal data augmentation techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00797</link>
<guid>https://arxiv.org/abs/2506.00797</guid>
<content:encoded><![CDATA[
arXiv:2506.00797v1 Announce Type: new 
Abstract: Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.00798</link>
<guid>https://arxiv.org/abs/2506.00798</guid>
<content:encoded><![CDATA[
arXiv:2506.00798v1 Announce Type: new 
Abstract: Spatio-temporal time series (STTS) have been widely used in many applications. However, accurately forecasting STTS is challenging due to complex dynamic correlations in both time and space dimensions. Existing graph neural networks struggle to balance effectiveness and efficiency in modeling dynamic spatio-temporal relations. To address this problem, we propose the Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently process STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix in SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded as a filtered graph spectral convolution. We also propose the Linear Dynamic Graph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn the SGFT matrix from the dynamic graph and significantly reduce the computational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that efficiently captures complex spatio-temporal correlations. Extensive experiments on seven spatio-temporal datasets show that DST-SGNN outperforms state-of-the-art methods while maintaining relatively low computational costs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-LoRA: One Vector is All You Need</title>
<link>https://arxiv.org/abs/2506.00799</link>
<guid>https://arxiv.org/abs/2506.00799</guid>
<content:encoded><![CDATA[
arXiv:2506.00799v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space $R^D$, can be reconstructed through a projection from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM - making Uni-LoRA both a unified framework and a "one-vector-only" solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Inversion Attacks for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00808</link>
<guid>https://arxiv.org/abs/2506.00808</guid>
<content:encoded><![CDATA[
arXiv:2506.00808v1 Announce Type: new 
Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery</title>
<link>https://arxiv.org/abs/2506.00844</link>
<guid>https://arxiv.org/abs/2506.00844</guid>
<content:encoded><![CDATA[
arXiv:2506.00844v1 Announce Type: new 
Abstract: This paper critically re-evaluates LLMs' role in causal discovery and argues against their direct involvement in determining causal relationships. We demonstrate that LLMs' autoregressive, correlation-driven modeling inherently lacks the theoretical grounding for causal reasoning and introduces unreliability when used as priors in causal discovery algorithms. Through empirical studies, we expose the limitations of existing LLM-based methods and reveal that deliberate prompt engineering (e.g., injecting ground-truth knowledge) could overstate their performance, helping to explain the consistently favorable results reported in much of the current literature. Based on these findings, we strictly confined LLMs' role to a non-decisional auxiliary capacity: LLMs should not participate in determining the existence or directionality of causal relationships, but can assist the search process for causal graphs (e.g., LLM-based heuristic search). Experiments across various settings confirm that, by strictly isolating LLMs from causal decision-making, LLM-guided heuristic search can accelerate the convergence and outperform both traditional and LLM-based methods in causal structure learning. We conclude with a call for the community to shift focus from naively applying LLMs to developing specialized models and training method that respect the core principles of causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00845</link>
<guid>https://arxiv.org/abs/2506.00845</guid>
<content:encoded><![CDATA[
arXiv:2506.00845v1 Announce Type: new 
Abstract: Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shortest path: we need generalization from synthetic graph data to real-world tasks with implicit graph structures. In this work, we propose to unlock generalizable learning of graph synthetic data with reinforcement learning. We first design solution-based and process-based rewards for synthetic graph problems: instead of rigid memorizing response patterns in direct fine-tuning, we posit that RL would help LLMs grasp the essentials underlying graph reasoning and alleviate overfitting. We employ RL algorithms such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph data. We then compare them against existing settings on both in-domain synthetic tasks and out-of-domain real-world tasks with implicit graph structures such as multi-hop QA, structured planning, and more. Extensive experiments demonstrate that our RL recipe leads to statistically significant improvement on 5 datasets, with an average gain of 12.9\% over baseline settings. Further analysis reveals that process-based rewards consistently outperform solution-based rewards, mixing synthetic and real-world task data yields potential gains, while compositionality and explainable intermediate steps remains a critical challenge even after RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs</title>
<link>https://arxiv.org/abs/2506.00846</link>
<guid>https://arxiv.org/abs/2506.00846</guid>
<content:encoded><![CDATA[
arXiv:2506.00846v1 Announce Type: new 
Abstract: In modern theoretical analyses of neural networks, the infinite-width limit is often invoked to justify Gaussian approximations of neuron preactivations (e.g., via neural network Gaussian processes or Tensor Programs). However, these Gaussian-based asymptotic theories have so far been unable to capture the behavior of attention layers, except under special regimes such as infinitely many heads or tailored scaling schemes. In this paper, leveraging the Tensor Programs framework, we rigorously identify the infinite-width limit distribution of variables within a single attention layer under realistic architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$ dimensionality. We derive the exact form of this limit law without resorting to infinite-head approximations or tailored scalings, demonstrating that it departs fundamentally from Gaussianity. This limiting distribution exhibits non-Gaussianity from a hierarchical structure, being Gaussian conditional on the random similarity scores. Numerical experiments validate our theoretical predictions, confirming the effectiveness of our theory at finite width and accurate description of finite-head attentions. Beyond characterizing a standalone attention layer, our findings lay the groundwork for developing a unified theory of deep Transformer architectures in the infinite-width regime.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Unlearning</title>
<link>https://arxiv.org/abs/2506.00848</link>
<guid>https://arxiv.org/abs/2506.00848</guid>
<content:encoded><![CDATA[
arXiv:2506.00848v1 Announce Type: new 
Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2506.00849</link>
<guid>https://arxiv.org/abs/2506.00849</guid>
<content:encoded><![CDATA[
arXiv:2506.00849v1 Announce Type: new 
Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that provides guarantees for the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing computable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling</title>
<link>https://arxiv.org/abs/2506.00862</link>
<guid>https://arxiv.org/abs/2506.00862</guid>
<content:encoded><![CDATA[
arXiv:2506.00862v1 Announce Type: new 
Abstract: Modeling complex fluid systems, especially turbulence governed by partial differential equations (PDEs), remains a fundamental challenge in science and engineering. Recently, diffusion-based generative models have gained attention as a powerful approach for these tasks, owing to their capacity to capture long-range dependencies and recover hierarchical structures. However, we present both empirical and theoretical evidence showing that generative models struggle with significant spectral bias and common-mode noise when generating high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative turbulence modeling framework that enhances the frequency-aware learning by both implicitly and explicitly mitigating spectral bias and common-mode noise. FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch backbone architecture, consisting of a salient flow attention branch with local-global awareness to focus on sensitive turbulence areas. Secondly, we introduce a frequency-guided Fourier mixing branch, which is integrated via an adaptive fusion strategy to explicitly mitigate spectral bias in the generative model. Thirdly, we leverage the high-frequency modeling capabilities of the masked auto-encoder pre-training and implicitly align the features of the generative model toward high-frequency components. We validate the effectiveness of FourierFlow on three canonical turbulent flow scenarios, demonstrating superior performance compared to state-of-the-art methods. Furthermore, we show that our model exhibits strong generalization capabilities in challenging settings such as out-of-distribution domains, long-term temporal extrapolation, and robustness to noisy inputs. The code can be found at https://github.com/AI4Science-WestlakeU/FourierFlow.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning</title>
<link>https://arxiv.org/abs/2506.00867</link>
<guid>https://arxiv.org/abs/2506.00867</guid>
<content:encoded><![CDATA[
arXiv:2506.00867v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose Local Manifold Approximation and Projection (LoMAP), a training-free method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2506.00880</link>
<guid>https://arxiv.org/abs/2506.00880</guid>
<content:encoded><![CDATA[
arXiv:2506.00880v1 Announce Type: new 
Abstract: Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-Covering Trajectory Stitching for Diffusion Planners</title>
<link>https://arxiv.org/abs/2506.00895</link>
<guid>https://arxiv.org/abs/2506.00895</guid>
<content:encoded><![CDATA[
arXiv:2506.00895v1 Announce Type: new 
Abstract: Diffusion-based generative models are emerging as powerful tools for long-horizon planning in reinforcement learning (RL), particularly with offline datasets. However, their performance is fundamentally limited by the quality and diversity of training data. This often restricts their generalization to tasks outside their training distribution or longer planning horizons. To overcome this challenge, we propose State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory augmentation method that incrementally stitches together short trajectory segments, systematically generating diverse and extended trajectories. SCoTS first learns a temporal distance-preserving latent representation that captures the underlying temporal structure of the environment, then iteratively stitches trajectory segments guided by directional exploration and novelty to effectively cover and expand this latent space. We demonstrate that SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Furthermore, augmented trajectories generated by SCoTS significantly improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00910</link>
<guid>https://arxiv.org/abs/2506.00910</guid>
<content:encoded><![CDATA[
arXiv:2506.00910v1 Announce Type: new 
Abstract: Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs--i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-learning with Posterior Sampling</title>
<link>https://arxiv.org/abs/2506.00917</link>
<guid>https://arxiv.org/abs/2506.00917</guid>
<content:encoded><![CDATA[
arXiv:2506.00917v1 Announce Type: new 
Abstract: Bayesian posterior sampling techniques have demonstrated superior empirical performance in many exploration-exploitation settings. However, their theoretical analysis remains a challenge, especially in complex settings like reinforcement learning. In this paper, we introduce Q-Learning with Posterior Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian posteriors on Q-values for exploration, akin to the popular Thompson Sampling algorithm in the multi-armed bandit setting. We show that in the tabular episodic MDP setting, PSQL achieves a regret bound of $\tilde O(H^2\sqrt{SAT})$, closely matching the known lower bound of $\Omega(H\sqrt{SAT})$. Here, S, A denote the number of states and actions in the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the number of episodes and $H$ being the planning horizon. Our work provides several new technical insights into the core challenges in combining posterior sampling with dynamic programming and TD-learning-based RL algorithms, along with novel ideas for resolving those difficulties. We hope this will form a starting point for analyzing this efficient and important algorithmic technique in even more complex RL settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks</title>
<link>https://arxiv.org/abs/2506.00918</link>
<guid>https://arxiv.org/abs/2506.00918</guid>
<content:encoded><![CDATA[
arXiv:2506.00918v1 Announce Type: new 
Abstract: Uncertainty quantification is critical in safety-sensitive applications but is often omitted from off-the-shelf neural networks due to adverse effects on predictive performance. Retrofitting uncertainty estimates post-hoc typically requires access to model parameters or gradients, limiting feasibility in practice. We propose a theoretically grounded framework for post-hoc uncertainty estimation in regression tasks by fitting an auxiliary model to both original inputs and frozen model outputs. Drawing from principles of maximum likelihood estimation and sequential parameter fitting, we formalize an exact post-hoc optimization objective that recovers the canonical MLE of Gaussian parameters, without requiring sampling or approximation at inference. While prior work has used model outputs to estimate uncertainty, we explicitly characterize the conditions under which this is valid and demonstrate the extent to which structured outputs can support quasi-epistemic inference. We find that using diverse auxiliary data, such as augmented subsets of the original training data, significantly enhances OOD detection and metric performance. Our hypothesis that frozen model outputs contain generalizable latent information about model error and predictive uncertainty is tested and confirmed. Finally, we ensure that our method maintains proper estimation of input-dependent uncertainty without relying exclusively on base model forecasts. These findings are demonstrated in toy problems and adapted to both UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation</title>
<link>https://arxiv.org/abs/2506.00920</link>
<guid>https://arxiv.org/abs/2506.00920</guid>
<content:encoded><![CDATA[
arXiv:2506.00920v1 Announce Type: new 
Abstract: Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity</title>
<link>https://arxiv.org/abs/2506.00932</link>
<guid>https://arxiv.org/abs/2506.00932</guid>
<content:encoded><![CDATA[
arXiv:2506.00932v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy, leveraging aggregated updates to build robust global models. However, this training paradigm faces significant challenges due to data heterogeneity and limited local datasets, which often impede effective collaboration. In such scenarios, we identify the Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model undergo minimal updates after early communication rounds, ultimately limiting the effectiveness of global aggregation. We demonstrate the presence of this phenomenon across a wide range of federated settings, spanning diverse datasets and architectures. To address this issue, we propose LIPS (Layer-wise Inertia Phenomenon with Sparsity), a simple yet effective method that periodically introduces transient sparsity to stimulate meaningful updates and empower global aggregation. Experiments demonstrate that LIPS effectively mitigates layer-wise inertia, enhances aggregation effectiveness, and improves overall performance in various FL scenarios. This work not only deepens the understanding of layer-wise learning dynamics in FL but also paves the way for more effective collaboration strategies in resource-constrained environments. Our code is publicly available at: https://github.com/QiaoXiao7282/LIPS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.00936</link>
<guid>https://arxiv.org/abs/2506.00936</guid>
<content:encoded><![CDATA[
arXiv:2506.00936v1 Announce Type: new 
Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for drug research and development but remains challenging due to the complex interplay of molecular interactions. Despite recent advances in graph neural networks (GNNs) for MS prediction, current approaches face two critical limitations: (1) incomplete molecular modeling due to atom-centric message-passing mechanisms that disregard bond-level topological features, and (2) prediction frameworks that lack reliable uncertainty quantification. To address these challenges, we propose TrustworthyMS, a novel contrastive learning framework designed for uncertainty-aware metabolic stability prediction. First, a molecular graph topology remapping mechanism synchronizes atom-bond interactions through edge-induced feature propagation, capturing both localized electronic effects and global conformational constraints. Second, contrastive topology-bond alignment enforces consistency between molecular topology views and bond patterns via feature alignment, enhancing representation robustness. Third, uncertainty modeling through Beta-Binomial uncertainty quantification enables simultaneous prediction and confidence calibration under epistemic uncertainty. Through extensive experiments, our results demonstrate that TrustworthyMS outperforms current state-of-the-art methods in terms of predictive performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation</title>
<link>https://arxiv.org/abs/2506.00959</link>
<guid>https://arxiv.org/abs/2506.00959</guid>
<content:encoded><![CDATA[
arXiv:2506.00959v1 Announce Type: new 
Abstract: Marketing optimization, commonly formulated as an online budget allocation problem, has emerged as a pivotal factor in driving user growth. Most existing research addresses this problem by following the principle of 'first predict then optimize' for each individual, which presents challenges related to large-scale counterfactual prediction and solving complexity trade-offs. Note that the practical data quality is uncontrollable, and the solving scale tends to be tens of millions. Therefore, the existing approaches make the robust budget allocation non-trivial, especially in industrial scenarios with considerable data noise. To this end, this paper proposes a novel approach that solves the problem from the cluster perspective. Specifically, we propose a multi-task representation network to learn the inherent attributes of individuals and project the original features into high-dimension hidden representations through the first two layers of the trained network. Then, we divide these hidden representations into $K$ groups through partitioning-based clustering, thus reformulating the problem as an integer stochastic programming problem under different total budgets. Finally, we distill the representation module and clustering model into a multi-category model to facilitate online deployment. Offline experiments validate the effectiveness and superiority of our approach compared to six state-of-the-art marketing optimization algorithms. Online A/B tests on the Meituan platform indicate that the approach outperforms the online algorithm by 0.53% and 0.65%, considering order volume (OV) and gross merchandise volume (GMV), respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Parallelism in Decentralized Stochastic Convex Optimization</title>
<link>https://arxiv.org/abs/2506.00961</link>
<guid>https://arxiv.org/abs/2506.00961</guid>
<content:encoded><![CDATA[
arXiv:2506.00961v1 Announce Type: new 
Abstract: Decentralized learning has emerged as a powerful approach for handling large datasets across multiple machines in a communication-efficient manner. However, such methods often face scalability limitations, as increasing the number of machines beyond a certain point negatively impacts convergence rates. In this work, we propose Decentralized Anytime SGD, a novel decentralized learning algorithm that significantly extends the critical parallelism threshold, enabling the effective use of more machines without compromising performance. Within the stochastic convex optimization (SCO) framework, we establish a theoretical upper bound on parallelism that surpasses the current state-of-the-art, allowing larger networks to achieve favorable statistical guarantees and closing the gap with centralized learning in highly connected topologies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Random Time Horizons</title>
<link>https://arxiv.org/abs/2506.00962</link>
<guid>https://arxiv.org/abs/2506.00962</guid>
<content:encoded><![CDATA[
arXiv:2506.00962v1 Announce Type: new 
Abstract: We extend the standard reinforcement learning framework to random time horizons. While the classical setting typically assumes finite and deterministic or infinite runtimes of trajectories, we argue that multiple real-world applications naturally exhibit random (potentially trajectory-dependent) stopping times. Since those stopping times typically depend on the policy, their randomness has an effect on policy gradient formulas, which we (mostly for the first time) derive rigorously in this work both for stochastic and deterministic policies. We present two complementary perspectives, trajectory or state-space based, and establish connections to optimal control theory. Our numerical experiments demonstrate that using the proposed formulas can significantly improve optimization convergence compared to traditional approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO</title>
<link>https://arxiv.org/abs/2506.00967</link>
<guid>https://arxiv.org/abs/2506.00967</guid>
<content:encoded><![CDATA[
arXiv:2506.00967v1 Announce Type: new 
Abstract: Optimization-based power control algorithms are predominantly iterative with high computational complexity, making them impractical for real-time applications in cell-free massive multiple-input multiple-output (CFmMIMO) systems. Learning-based methods have emerged as a promising alternative, and among them, graph neural networks (GNNs) have demonstrated their excellent performance in solving power control problems. However, all existing GNN-based approaches assume ideal orthogonality among pilot sequences for user equipments (UEs), which is unrealistic given that the number of UEs exceeds the available orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based methods assume a fixed number of UEs, whereas the number of active UEs varies over time in practice. Additionally, supervised training necessitates costly computational resources for computing the target power control solutions for a large volume of training samples. To address these issues, we propose a graph attention network for downlink power control in CFmMIMO systems that operates in a self-supervised manner while effectively handling pilot contamination and adapting to a dynamic number of UEs. Experimental results show its effectiveness, even in comparison to the optimal accelerated projected gradient method as a baseline.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Heterogeneity Modeling for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2506.00969</link>
<guid>https://arxiv.org/abs/2506.00969</guid>
<content:encoded><![CDATA[
arXiv:2506.00969v1 Announce Type: new 
Abstract: Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization-based Bounds on the Wasserstein Metric</title>
<link>https://arxiv.org/abs/2506.00976</link>
<guid>https://arxiv.org/abs/2506.00976</guid>
<content:encoded><![CDATA[
arXiv:2506.00976v1 Announce Type: new 
Abstract: The Wasserstein metric has become increasingly important in many machine learning applications such as generative modeling, image retrieval and domain adaptation. Despite its appeal, it is often too costly to compute. This has motivated approximation methods like entropy-regularized optimal transport, downsampling, and subsampling, which trade accuracy for computational efficiency. In this paper, we consider the challenge of computing efficient approximations to the Wasserstein metric that also serve as strict upper or lower bounds. Focusing on discrete measures on regular grids, our approach involves formulating and exactly solving a Kantorovich problem on a coarse grid using a quantized measure and specially designed cost matrix, followed by an upscaling and correction stage. This is done either in the primal or dual space to obtain valid upper and lower bounds on the Wasserstein metric of the full-resolution inputs. We evaluate our methods on the DOTmark optimal transport images benchmark, demonstrating a 10x-100x speedup compared to entropy-regularized OT while keeping the approximation error below 2%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers</title>
<link>https://arxiv.org/abs/2506.00998</link>
<guid>https://arxiv.org/abs/2506.00998</guid>
<content:encoded><![CDATA[
arXiv:2506.00998v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) improves performance on domain-specific tasks but can lead to overfitting, making them unreliable on out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD detection monitors to the LoRA layer using boxed abstraction to filter questions beyond the model's competence. Feature vectors from the fine-tuning data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a question is flagged as OoD if its feature vector falls outside all boxes. To improve interpretability and robustness, we introduce a regularization loss during fine-tuning that encourages paraphrased questions to stay close in the feature space, and the enlargement of the decision boundary is based on the feature variance within a cluster. Our method complements existing defenses by providing lightweight and interpretable OoD detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts</title>
<link>https://arxiv.org/abs/2506.01000</link>
<guid>https://arxiv.org/abs/2506.01000</guid>
<content:encoded><![CDATA[
arXiv:2506.01000v1 Announce Type: new 
Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying only the input and output spaces. Visual reprogramming (VR) is one instance for vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to input images to facilitate downstream classification. The existing VR approaches for CLIP train a single visual prompt using all descriptions of different downstream classes. However, the limited learning capacity may result in (1) a failure to capture diverse aspects of the descriptions (e.g., shape, color, and texture), and (2) a possible bias toward less informative attributes that do not help distinguish between classes. In this paper, we introduce a decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are optimized using descriptions grouped by explicit causes (DVP-cse) or unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual prompts with a probabilistic reweighting matrix (PRM) that measures their contributions to each downstream class. Theoretically, DVP lowers the empirical risk bound. Experimentally, DVP outperforms baselines on average across 11 downstream datasets. Notably, the DVP-PRM integration enables insights into how individual visual prompts influence classification decisions, providing a probabilistic framework for understanding reprogramming. Our code is available at https://github.com/tmlr-group/DecoupledVP.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic critics can empower small actors</title>
<link>https://arxiv.org/abs/2506.01016</link>
<guid>https://arxiv.org/abs/2506.01016</guid>
<content:encoded><![CDATA[
arXiv:2506.01016v1 Announce Type: new 
Abstract: Actor-critic methods have been central to many of the recent advances in deep reinforcement learning. The most common approach is to use symmetric architectures, whereby both actor and critic have the same network topology and number of parameters. However, recent works have argued for the advantages of asymmetric setups, specifically with the use of smaller actors. We perform broad empirical investigations and analyses to better understand the implications of this and find that, in general, smaller actors result in performance degradation and overfit critics. Our analyses suggest poor data collection, due to value underestimation, as one of the main causes for this behavior, and further highlight the crucial role the critic can play in alleviating this pathology. We explore techniques to mitigate the observed value underestimation, which enables further research in asymmetric actor-critic methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming LLMs by Scaling Learning Rates with Gradient Grouping</title>
<link>https://arxiv.org/abs/2506.01049</link>
<guid>https://arxiv.org/abs/2506.01049</guid>
<content:encoded><![CDATA[
arXiv:2506.01049v1 Announce Type: new 
Abstract: Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity</title>
<link>https://arxiv.org/abs/2506.01052</link>
<guid>https://arxiv.org/abs/2506.01052</guid>
<content:encoded><![CDATA[
arXiv:2506.01052v1 Announce Type: new 
Abstract: We investigate the finite-time convergence properties of Temporal Difference (TD) learning with linear function approximation, a cornerstone algorithm in reinforcement learning. While prior work has established convergence guarantees, these results typically rely on the assumption that each iterate is projected onto a bounded set or that the learning rate is set according to the unknown strong convexity constant -- conditions that are both artificial and do not match the current practice.
  In this paper, we challenge the necessity of such assumptions and present a refined analysis of TD learning. We show that the simple projection-free variant converges with a rate of $\tilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence of Markovian noise. Our analysis reveals a novel self-bounding property of the TD updates and exploits it to guarantee bounded iterates.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</title>
<link>https://arxiv.org/abs/2506.01054</link>
<guid>https://arxiv.org/abs/2506.01054</guid>
<content:encoded><![CDATA[
arXiv:2506.01054v1 Announce Type: new 
Abstract: The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI-Units: Benchmarking Explainability Methods with Unit Tests</title>
<link>https://arxiv.org/abs/2506.01059</link>
<guid>https://arxiv.org/abs/2506.01059</guid>
<content:encoded><![CDATA[
arXiv:2506.01059v1 Announce Type: new 
Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering LLM Uncertainty Estimation Methods in the Wild</title>
<link>https://arxiv.org/abs/2506.01114</link>
<guid>https://arxiv.org/abs/2506.01114</guid>
<content:encoded><![CDATA[
arXiv:2506.01114v1 Announce Type: new 
Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer</title>
<link>https://arxiv.org/abs/2506.01115</link>
<guid>https://arxiv.org/abs/2506.01115</guid>
<content:encoded><![CDATA[
arXiv:2506.01115v1 Announce Type: new 
Abstract: The Transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of algorithmic tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based training on next-token prediction. While the core component of a Transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard Transformers to variants in which either the multi-layer perceptron (MLP) layers or the attention projectors (queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce MixiT -- the Mixing Transformer -- a simplified, principled model in which the attention coefficients are entirely random and fixed at initialization, eliminating any input-dependent computation or learning in attention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on various algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization. For retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently beneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits such as induction heads -- a specific circuit known to be crucial for learning and exploiting repeating patterns in input sequences. Even more interestingly, we find that attention with frozen key and query projectors is not only able to form induction heads, but can also perform competitively on language modeling. Our results underscore the importance of architectural heterogeneity, where distinct components contribute complementary inductive biases crucial for solving different classes of tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</title>
<link>https://arxiv.org/abs/2506.01121</link>
<guid>https://arxiv.org/abs/2506.01121</guid>
<content:encoded><![CDATA[
arXiv:2506.01121v1 Announce Type: new 
Abstract: Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow Feature Analysis on Markov Chains from Goal-Directed Behavior</title>
<link>https://arxiv.org/abs/2506.01145</link>
<guid>https://arxiv.org/abs/2506.01145</guid>
<content:encoded><![CDATA[
arXiv:2506.01145v1 Announce Type: new 
Abstract: Slow Feature Analysis is a unsupervised representation learning method that extracts slowly varying features from temporal data and can be used as a basis for subsequent reinforcement learning. Often, the behavior that generates the data on which the representation is learned is assumed to be a uniform random walk. Less research has focused on using samples generated by goal-directed behavior, as commonly the case in a reinforcement learning setting, to learn a representation. In a spatial setting, goal-directed behavior typically leads to significant differences in state occupancy between states that are close to a reward location and far from a reward location.
  Through the perspective of optimal slow features on ergodic Markov chains, this work investigates the effects of these differences on value-function approximation in an idealized setting. Furthermore, three correction routes, which can potentially alleviate detrimental scaling effects, are evaluated and discussed. In addition, the special case of goal-averse behavior is considered.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earley-Driven Dynamic Pruning for Efficient Structured Decoding</title>
<link>https://arxiv.org/abs/2506.01151</link>
<guid>https://arxiv.org/abs/2506.01151</guid>
<content:encoded><![CDATA[
arXiv:2506.01151v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight-Space Linear Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2506.01153</link>
<guid>https://arxiv.org/abs/2506.01153</guid>
<content:encoded><![CDATA[
arXiv:2506.01153v1 Announce Type: new 
Abstract: We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet powerful framework that unifies weight-space learning with linear recurrence to redefine sequence modeling. Unlike conventional recurrent neural networks (RNNs) which collapse temporal dynamics into fixed-dimensional hidden states, WARP explicitly parametrizes the hidden state as the weights of a distinct root neural network. This formulation promotes higher-resolution memory, gradient-free adaptation at test-time, and seamless integration of domain-specific physical priors. Empirical validation shows that WARP matches or surpasses state-of-the-art baselines on diverse classification tasks, spanning synthetic benchmarks to real-world datasets. Furthermore, extensive experiments across sequential image completion, dynamical system reconstruction, and multivariate time series forecasting demonstrate its expressiveness and generalization capabilities. Critically, WARP's weight trajectories offer valuable insights into the model's inner workings. Ablation studies confirm the architectural necessity of key components, solidifying weight-space linear RNNs as a transformative paradigm for adaptive machine intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FORT: Forward-Only Regression Training of Normalizing Flows</title>
<link>https://arxiv.org/abs/2506.01158</link>
<guid>https://arxiv.org/abs/2506.01158</guid>
<content:encoded><![CDATA[
arXiv:2506.01158v1 Announce Type: new 
Abstract: Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to neural dynamical systems that encompass modern large-scale diffusion and flow matching models. Despite the scalability of training, the generation of high-quality samples and their corresponding likelihood under the model requires expensive numerical simulation -- inhibiting adoption in numerous scientific applications such as equilibrium sampling of molecular systems. In this paper, we revisit classical normalizing flows as one-step generative models with exact likelihoods and propose a novel, scalable training objective that does not require computing the expensive change of variable formula used in conventional maximum likelihood training. We propose Forward-Only Regression Training (FORT), a simple $\ell_2$-regression objective that maps prior samples under our flow to specifically chosen targets. We demonstrate that FORT supports a wide class of targets, such as optimal transport targets and targets from pre-trained continuous-time normalizing flows (CNF). We further demonstrate that by using CNF targets, our one-step flows allow for larger-scale training that exceeds the performance and stability of maximum likelihood training, while unlocking a broader class of architectures that were previously challenging to train. Empirically, we elucidate that our trained flows can perform equilibrium conformation sampling in Cartesian coordinates of alanine dipeptide, alanine tripeptide, and alanine tetrapeptide.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Learning with Linear Temporal Logic using Differentiable Simulation</title>
<link>https://arxiv.org/abs/2506.01167</link>
<guid>https://arxiv.org/abs/2506.01167</guid>
<content:encoded><![CDATA[
arXiv:2506.01167v1 Announce Type: new 
Abstract: To ensure learned controllers comply with safety and reliability requirements for reinforcement learning in real-world settings remains challenging. Traditional safety assurance approaches, such as state avoidance and constrained Markov decision processes, often inadequately capture trajectory requirements or may result in overly conservative behaviors. To address these limitations, recent studies advocate the use of formal specification languages such as linear temporal logic (LTL), enabling the derivation of correct-by-construction learning objectives from the specified requirements. However, the sparse rewards associated with LTL specifications make learning extremely difficult, whereas dense heuristic-based rewards risk compromising correctness. In this work, we propose the first method, to our knowledge, that integrates LTL with differentiable simulators, facilitating efficient gradient-based learning directly from LTL specifications by coupling with differentiable paradigms. Our approach introduces soft labeling to achieve differentiable rewards and states, effectively mitigating the sparse-reward issue intrinsic to LTL without compromising objective correctness. We validate the efficacy of our method through experiments, demonstrating significant improvements in both reward attainment and training time compared to the discrete methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation</title>
<link>https://arxiv.org/abs/2506.01177</link>
<guid>https://arxiv.org/abs/2506.01177</guid>
<content:encoded><![CDATA[
arXiv:2506.01177v1 Announce Type: new 
Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture for generative adversarial networks (GANs) in molecular discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, using over 60% fewer parameters. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01183</link>
<guid>https://arxiv.org/abs/2506.01183</guid>
<content:encoded><![CDATA[
arXiv:2506.01183v1 Announce Type: new 
Abstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA</title>
<link>https://arxiv.org/abs/2506.01194</link>
<guid>https://arxiv.org/abs/2506.01194</guid>
<content:encoded><![CDATA[
arXiv:2506.01194v1 Announce Type: new 
Abstract: LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiresolution Analysis and Statistical Thresholding on Dynamic Networks</title>
<link>https://arxiv.org/abs/2506.01208</link>
<guid>https://arxiv.org/abs/2506.01208</guid>
<content:encoded><![CDATA[
arXiv:2506.01208v1 Announce Type: new 
Abstract: Detecting structural change in dynamic network data has wide-ranging applications. Existing approaches typically divide the data into time bins, extract network features within each bin, and then compare these features over time. This introduces an inherent tradeoff between temporal resolution and the statistical stability of the extracted features. Despite this tradeoff, reminiscent of time-frequency tradeoffs in signal processing, most methods rely on a fixed temporal resolution. Choosing an appropriate resolution parameter is typically difficult and can be especially problematic in domains like cybersecurity, where anomalous behavior may emerge at multiple time scales. We address this challenge by proposing ANIE (Adaptive Network Intensity Estimation), a multi-resolution framework designed to automatically identify the time scales at which network structure evolves, enabling the joint detection of both rapid and gradual changes. Modeling interactions as Poisson processes, our method proceeds in two steps: (1) estimating a low-dimensional subspace of node behavior, and (2) deriving a set of novel empirical affinity coefficients that quantify change in interaction intensity between latent factors and support statistical testing for structural change across time scales. We provide theoretical guarantees for subspace estimation and the asymptotic behavior of the affinity coefficients, enabling model-based change detection. Experiments on synthetic networks show that ANIE adapts to the appropriate time resolution and is able to capture sharp structural changes while remaining robust to noise. Furthermore, applications to real-world data showcase the practical benefits of ANIE's multiresolution approach to detecting structural change over fixed resolution methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Modes as Time Representation for Spatiotemporal Forecasting</title>
<link>https://arxiv.org/abs/2506.01212</link>
<guid>https://arxiv.org/abs/2506.01212</guid>
<content:encoded><![CDATA[
arXiv:2506.01212v1 Announce Type: new 
Abstract: This paper introduces a data-driven time embedding method for modeling long-range seasonal dependencies in spatiotemporal forecasting tasks. The proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal modes directly from observed data, eliminating the need for explicit timestamps or hand-crafted time features. These temporal modes serve as time representations that can be seamlessly integrated into deep spatiotemporal forecasting models. Unlike conventional embeddings such as time-of-day indicators or sinusoidal functions, our method captures complex multi-scale periodicity through spectral analysis of spatiotemporal data. Extensive experiments on urban mobility, highway traffic, and climate datasets demonstrate that the DMD-based embedding consistently improves long-horizon forecasting accuracy, reduces residual correlation, and enhances temporal generalization. The method is lightweight, model-agnostic, and compatible with any architecture that incorporates time covariates.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective</title>
<link>https://arxiv.org/abs/2506.01213</link>
<guid>https://arxiv.org/abs/2506.01213</guid>
<content:encoded><![CDATA[
arXiv:2506.01213v1 Announce Type: new 
Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Refining Training for Amortized Density Functional Theory</title>
<link>https://arxiv.org/abs/2506.01225</link>
<guid>https://arxiv.org/abs/2506.01225</guid>
<content:encoded><![CDATA[
arXiv:2506.01225v1 Announce Type: new 
Abstract: Density Functional Theory (DFT) allows for predicting all the chemical and physical properties of molecular systems from first principles by finding an approximate solution to the many-body Schr\"odinger equation. However, the cost of these predictions becomes infeasible when increasing the scale of the energy evaluations, e.g., when calculating the ground-state energy for simulating molecular dynamics. Recent works have demonstrated that, for substantially large datasets of molecular conformations, Deep Learning-based models can predict the outputs of the classical DFT solvers by amortizing the corresponding optimization problems. In this paper, we propose a novel method that reduces the dependency of amortized DFT solvers on large pre-collected datasets by introducing a self-refining training strategy. Namely, we propose an efficient method that simultaneously trains a deep-learning model to predict the DFT outputs and samples molecular conformations that are used as training data for the model. We derive our method as a minimization of the variational upper bound on the KL-divergence measuring the discrepancy between the generated samples and the target Boltzmann distribution defined by the ground state energy. To demonstrate the utility of the proposed scheme, we perform an extensive empirical study comparing it with the models trained on the pre-collected datasets. Finally, we open-source our implementation of the proposed algorithm, optimized with asynchronous training and sampling stages, which enables simultaneous sampling and training. Code is available at https://github.com/majhas/self-refining-dft.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing ML Pipelines with Adversarial Data Corruption</title>
<link>https://arxiv.org/abs/2506.01230</link>
<guid>https://arxiv.org/abs/2506.01230</guid>
<content:encoded><![CDATA[
arXiv:2506.01230v1 Announce Type: new 
Abstract: Structured data-quality issues, such as missing values correlated with demographics, culturally biased labels, or systemic selection biases, routinely degrade the reliability of machine-learning pipelines. Regulators now increasingly demand evidence that high-stakes systems can withstand these realistic, interdependent errors, yet current robustness evaluations typically use random or overly simplistic corruptions, leaving worst-case scenarios unexplored. We introduce SAVAGE, a causally inspired framework that (i) formally models realistic data-quality issues through dependency graphs and flexible corruption templates, and (ii) systematically discovers corruption patterns that maximally degrade a target performance metric. SAVAGE employs a bi-level optimization approach to efficiently identify vulnerable data subpopulations and fine-tune corruption severity, treating the full ML pipeline, including preprocessing and potentially non-differentiable models, as a black box. Extensive experiments across multiple datasets and ML tasks (data cleaning, fairness-aware learning, uncertainty quantification) demonstrate that even a small fraction (around 5 %) of structured corruptions identified by SAVAGE severely impacts model performance, far exceeding random or manually crafted errors, and invalidating core assumptions of existing techniques. Thus, SAVAGE provides a practical tool for rigorous pipeline stress-testing, a benchmark for evaluating robustness methods, and actionable guidance for designing more resilient data workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution</title>
<link>https://arxiv.org/abs/2506.01231</link>
<guid>https://arxiv.org/abs/2506.01231</guid>
<content:encoded><![CDATA[
arXiv:2506.01231v1 Announce Type: new 
Abstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration</title>
<link>https://arxiv.org/abs/2506.01250</link>
<guid>https://arxiv.org/abs/2506.01250</guid>
<content:encoded><![CDATA[
arXiv:2506.01250v1 Announce Type: new 
Abstract: In this paper, we address the contextual dueling bandit problem by proposing variance-aware algorithms that leverage neural networks to approximate nonlinear utility functions. Our approach employs a \textit{variance-aware exploration strategy}, which adaptively accounts for uncertainty in pairwise comparisons while relying only on the gradients with respect to the learnable parameters of the last layer. This design effectively balances the exploration--exploitation tradeoff under both the Upper Confidence Bound (UCB) and Thompson Sampling (TS) frameworks. As a result, under standard assumptions, we establish theoretical guarantees showing that our algorithms achieve sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T \sigma_t^2} + \sqrt{dT}\rt),$ for sufficiently wide neural networks, where $ d $ is the contextual dimension, $ \sigma_t^2 $ the variance of comparisons at round $ t $, and $ T $ the total number of rounds. We also empirically validate that our approach offers reasonable computational efficiency and achieves sublinear regret on both synthetic tasks with nonlinear utilities and real-world tasks, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism</title>
<link>https://arxiv.org/abs/2506.01260</link>
<guid>https://arxiv.org/abs/2506.01260</guid>
<content:encoded><![CDATA[
arXiv:2506.01260v1 Announce Type: new 
Abstract: Scaling models has led to significant advancements in deep learning, but training these models in decentralized settings remains challenging due to communication bottlenecks. While existing compression techniques are effective in data-parallel, they do not extend to model parallelism. Unlike data-parallel training, where weight gradients are exchanged, model-parallel requires compressing activations and activation gradients as they propagate through layers, accumulating compression errors. We propose a novel compression algorithm that compresses both forward and backward passes, enabling up to 99% compression with no convergence degradation with negligible memory/compute overhead. By leveraging a recursive structure in transformer networks, we predefine a low-dimensional subspace to confine the activations and gradients, allowing full reconstruction in subsequent layers. Our method achieves up to 100x improvement in communication efficiency and enables training billion-parameter-scale models over low-end GPUs connected via consumer-grade internet speeds as low as 80Mbps, matching the convergence of centralized datacenter systems with 100Gbps connections with model parallel.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01261</link>
<guid>https://arxiv.org/abs/2506.01261</guid>
<content:encoded><![CDATA[
arXiv:2506.01261v1 Announce Type: new 
Abstract: In the context of Federated Reinforcement Learning (FRL), applying Proximal Policy Optimization (PPO) faces challenges related to the update order of its actor and critic due to the aggregation step occurring between successive iterations. In particular, when local actors are updated based on local critic estimations, the algorithm becomes vulnerable to data heterogeneity. As a result, the conventional update order in PPO (critic first, then actor) may cause heterogeneous gradient directions among clients, hindering convergence to a globally optimal policy. To address this issue, we propose FedRAC, which reverses the update order (actor first, then critic) to eliminate the divergence of critics from different clients. Theoretical analysis shows that the convergence bound of FedRAC is immune to data heterogeneity under mild conditions, i.e., bounded level of heterogeneity and accurate policy evaluation. Empirical results indicate that the proposed algorithm obtains higher cumulative rewards and converges more rapidly in five experiments, including three classical RL environments and a highly heterogeneous autonomous driving scenario using the SUMO traffic simulator.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment</title>
<link>https://arxiv.org/abs/2506.01290</link>
<guid>https://arxiv.org/abs/2506.01290</guid>
<content:encoded><![CDATA[
arXiv:2506.01290v1 Announce Type: new 
Abstract: High-quality time series (TS) data are essential for ensuring TS model performance, rendering research on rating TS data quality indispensable. Existing methods have shown promising rating accuracy within individual domains, primarily by extending data quality rating techniques such as influence functions and Shapley values to account for temporal characteristics. However, they neglect the fact that real-world TS data can span vastly different domains and exhibit distinct properties, hampering the accurate and efficient rating of diverse TS data. In this paper, we propose TSRating, a novel and unified framework for rating the quality of time series data crawled from diverse domains. TSRating is built on the assumption that LLMs inherit ample knowledge, acquired during their extensive pretraining, enabling them to comprehend and discern quality differences in diverse TS data. We verify this assumption by devising a series of prompts to elicit quality comparisons from LLMs for pairs of TS samples. We then fit a dedicated rating model, termed TSRater, to convert the LLMs' judgments into efficient quality predictions via TSRater's inference on future TS samples. To ensure cross-domain adaptability, we develop a meta-learning scheme to train TSRater on quality comparisons collected from nine distinct domains. To improve training efficiency, we employ signSGD for inner-loop updates, thus circumventing the demanding computation of hypergradients. Extensive experimental results on eleven benchmark datasets across three time series tasks, each using both conventional TS models and TS foundation models, demonstrate that TSRating outperforms baselines in terms of estimation accuracy, efficiency, and domain adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Developments in GNNs for Drug Discovery</title>
<link>https://arxiv.org/abs/2506.01302</link>
<guid>https://arxiv.org/abs/2506.01302</guid>
<content:encoded><![CDATA[
arXiv:2506.01302v1 Announce Type: new 
Abstract: In this paper, we review recent developments and the role of Graph Neural Networks (GNNs) in computational drug discovery, including molecule generation, molecular property prediction, and drug-drug interaction prediction. By summarizing the most recent developments in this area, we underscore the capabilities of GNNs to comprehend intricate molecular patterns, while exploring both their current and prospective applications. We initiate our discussion by examining various molecular representations, followed by detailed discussions and categorization of existing GNN models based on their input types and downstream application tasks. We also collect a list of commonly used benchmark datasets for a variety of applications. We conclude the paper with brief discussions and summarize common trends in this important research area.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Structured Hopfield Network for Semantic Association and Retrieval</title>
<link>https://arxiv.org/abs/2506.01303</link>
<guid>https://arxiv.org/abs/2506.01303</guid>
<content:encoded><![CDATA[
arXiv:2506.01303v1 Announce Type: new 
Abstract: Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored. Inspired by hippocampal CA3 dynamics and its role in associative memory, we propose the Latent Structured Hopfield Network (LSHN), a biologically inspired framework that integrates continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input. Unlike traditional Hopfield networks, our model is trained end-to-end with gradient descent, achieving scalable and robust memory retrieval. Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models. Our work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Considerations for Large Pretrained Neural Networks</title>
<link>https://arxiv.org/abs/2506.01311</link>
<guid>https://arxiv.org/abs/2506.01311</guid>
<content:encoded><![CDATA[
arXiv:2506.01311v1 Announce Type: new 
Abstract: Increasingly complex neural network architectures have achieved phenomenal performance. However, these complex models require massive computational resources that consume substantial amounts of electricity, which highlights the potential environmental impact of such models. Previous studies have demonstrated that substantial redundancies exist in large pre-trained models. However, previous work has primarily focused on compressing models while retaining comparable model performance, and the direct impact on electricity consumption appears to have received relatively little attention. By quantifying the energy usage associated with both uncompressed and compressed models, we investigate compression as a means of reducing electricity consumption. We consider nine different pre-trained models, ranging in size from 8M parameters to 138M parameters. To establish a baseline, we first train each model without compression and record the electricity usage and time required during training, along with other relevant statistics. We then apply three compression techniques: Steganographic capacity reduction, pruning, and low-rank factorization. In each of the resulting cases, we again measure the electricity usage, training time, model accuracy, and so on. We find that pruning and low-rank factorization offer no significant improvements with respect to energy usage or other related statistics, while steganographic capacity reduction provides major benefits in almost every case. We discuss the significance of these findings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2506.01317</link>
<guid>https://arxiv.org/abs/2506.01317</guid>
<content:encoded><![CDATA[
arXiv:2506.01317v1 Announce Type: new 
Abstract: Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples using 40 minutes on a single GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack</title>
<link>https://arxiv.org/abs/2506.01318</link>
<guid>https://arxiv.org/abs/2506.01318</guid>
<content:encoded><![CDATA[
arXiv:2506.01318v1 Announce Type: new 
Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: "over-unlearning" that deteriorates retained data near the forget set, and post-hoc "relearning" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@{\epsilon}, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Psi$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title>
<link>https://arxiv.org/abs/2506.01320</link>
<guid>https://arxiv.org/abs/2506.01320</guid>
<content:encoded><![CDATA[
arXiv:2506.01320v1 Announce Type: new 
Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation</title>
<link>https://arxiv.org/abs/2506.01327</link>
<guid>https://arxiv.org/abs/2506.01327</guid>
<content:encoded><![CDATA[
arXiv:2506.01327v1 Announce Type: new 
Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models</title>
<link>https://arxiv.org/abs/2506.01337</link>
<guid>https://arxiv.org/abs/2506.01337</guid>
<content:encoded><![CDATA[
arXiv:2506.01337v1 Announce Type: new 
Abstract: Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks. In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization. Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes and Reinforcement Learning. Our code will be available at https://github.com/HKUST-SAIL/NoiseAR/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01339</link>
<guid>https://arxiv.org/abs/2506.01339</guid>
<content:encoded><![CDATA[
arXiv:2506.01339v1 Announce Type: new 
Abstract: Machine unlearning offers a promising solution to privacy and safety concerns in large language models (LLMs) by selectively removing targeted knowledge while preserving utility. However, current methods are highly sensitive to downstream fine-tuning, which can quickly recover forgotten information-even from unrelated tasks. To address this, we introduce invariance into unlearning for the first time, inspired by invariant risk minimization (IRM). Building on this principle, we propose invariant LLM unlearning (ILU), a regularization-based framework that enhances robustness. Notably, ILU generalizes well to diverse fine-tuning tasks, even when trained using a single dataset. A task vector analysis is also provided to further elucidate the rationale behind ILU's effectiveness. Extensive experiments on the WMDP and MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art unlearning methods, including negative preference optimization (NPO) and representation misdirection for unlearning (RMU). Notably, ILU achieves superior unlearning robustness across diverse downstream fine-tuning scenarios (e.g., math, paraphrase detection, and sentiment analysis) while preserving the fine-tuning performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Learning in Survival Analysis</title>
<link>https://arxiv.org/abs/2506.01348</link>
<guid>https://arxiv.org/abs/2506.01348</guid>
<content:encoded><![CDATA[
arXiv:2506.01348v1 Announce Type: new 
Abstract: We introduce an innovative approach that incorporates a Distributionally Robust Learning (DRL) approach into Cox regression to enhance the robustness and accuracy of survival predictions. By formulating a DRL framework with a Wasserstein distance-based ambiguity set, we develop a variant Cox model that is less sensitive to assumptions about the underlying data distribution and more resilient to model misspecification and data perturbations. By leveraging Wasserstein duality, we reformulate the original min-max DRL problem into a tractable regularized empirical risk minimization problem, which can be computed by exponential conic programming. We provide guarantees on the finite sample behavior of our DRL-Cox model. Moreover, through extensive simulations and real world case studies, we demonstrate that our regression model achieves superior performance in terms of prediction accuracy and robustness compared with traditional methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2506.01350</link>
<guid>https://arxiv.org/abs/2506.01350</guid>
<content:encoded><![CDATA[
arXiv:2506.01350v1 Announce Type: new 
Abstract: This paper proposes a novel stable learning theory for recurrent neural networks (RNNs), so-called variational adaptive noise and dropout (VAND). As stabilizing factors for RNNs, noise and dropout on the internal state of RNNs have been separately confirmed in previous studies. We reinterpret the optimization problem of RNNs as variational inference, showing that noise and dropout can be derived simultaneously by transforming the explicit regularization term arising in the optimization problem into implicit regularization. Their scale and ratio can also be adjusted appropriately to optimize the main objective of RNNs, respectively. In an imitation learning scenario with a mobile manipulator, only VAND is able to imitate sequential and periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network</title>
<link>https://arxiv.org/abs/2506.01352</link>
<guid>https://arxiv.org/abs/2506.01352</guid>
<content:encoded><![CDATA[
arXiv:2506.01352v1 Announce Type: new 
Abstract: Decentralized training of large language models offers the opportunity to pool computational resources across geographically distributed participants but faces significant network communication bottlenecks, particularly in pipeline-parallel settings. While pipeline parallelism partitions model layers across devices to handle large-scale models, it necessitates frequent communication of intermediate activations, creating challenges when network bandwidth is limited. Existing activation compression methods, such as AQ-SGD, mitigate quantization-induced errors through error compensation but impose prohibitive memory overhead by requiring storage of previous activations. To address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard Quantization), a novel activation quantization framework designed specifically for pipeline parallelism. Our approach integrates fine-grained tile-wise quantization for precise control, entropy-guided token-level adaptive bit allocation for optimal bit usage, and a Hadamard-based transform with pivot element swapping to effectively suppress quantization outliers. We further provide a theoretical analysis, proving that pipeline parallel training equipped with TAH-Quant maintains a convergence rate of $\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant achieves aggressive activation quantization (3-4 bits) ratio, which provides up to 4.3$\times$ end-to-end speedup without compromising training convergence, matches state-of-the-art methods, incurs no extra memory overhead, and generalizes well across different training scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion</title>
<link>https://arxiv.org/abs/2506.01356</link>
<guid>https://arxiv.org/abs/2506.01356</guid>
<content:encoded><![CDATA[
arXiv:2506.01356v1 Announce Type: new 
Abstract: Learning-based neural network (NN) control policies have shown impressive empirical performance. However, obtaining stability guarantees and estimations of the region of attraction of these learned neural controllers is challenging due to the lack of stable and scalable training and verification algorithms. Although previous works in this area have achieved great success, much conservatism remains in their framework. In this work, we propose a novel two-stage training framework to jointly synthesize the controller and Lyapunov function for continuous-time systems. By leveraging a Zubov-inspired region of attraction characterization to directly estimate stability boundaries, we propose a novel training data sampling strategy and a domain updating mechanism that significantly reduces the conservatism in training. Moreover, unlike existing works on continuous-time systems that rely on an SMT solver to formally verify the Lyapunov condition, we extend state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN with the capability of performing automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme that avoids expensive bisection. To demonstrate the effectiveness of our approach, we conduct numerical experiments by synthesizing and verifying controllers on several challenging nonlinear systems across multiple dimensions. We show that our training can yield region of attractions with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and our verification on continuous systems can be up to $40-10000$ times faster compared to the traditional SMT solver dReal. Our code is available at https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases</title>
<link>https://arxiv.org/abs/2506.01360</link>
<guid>https://arxiv.org/abs/2506.01360</guid>
<content:encoded><![CDATA[
arXiv:2506.01360v1 Announce Type: new 
Abstract: Relational databases (RDBs) are composed of interconnected tables, where relationships between them are defined through foreign keys. Recent research on applying machine learning to RDBs has explored graph-based representations of RDBs, where rows of tables are modeled as nodes, and foreign key relationships are modeled as edges. RDB-to-graph modeling helps capture cross-table dependencies, ultimately leading to enhanced performance across diverse tasks. However, there are numerous ways to model RDBs as graphs, and performance varies significantly depending on the chosen graph model. In our analysis, applying a common heuristic rule for graph modeling leads to up to a 10% drop in performance compared to the best-performing graph model, which remains non-trivial to identify. To foster research on intelligent RDB-to-graph modeling, we introduce RDB2G-Bench, the first benchmark framework for evaluating such methods. We construct extensive datasets covering 5 real-world RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs for efficient and reproducible evaluations. Thanks to our precomputed datasets, we were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12 tasks over 600x faster than on-the-fly evaluation, which requires repeated model training. Our analysis of the datasets and benchmark results reveals key structural patterns affecting graph model effectiveness, along with practical implications for effective graph modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery</title>
<link>https://arxiv.org/abs/2506.01361</link>
<guid>https://arxiv.org/abs/2506.01361</guid>
<content:encoded><![CDATA[
arXiv:2506.01361v1 Announce Type: new 
Abstract: Robust causal discovery in time series datasets depends on reliable benchmark datasets with known ground-truth causal relationships. However, such datasets remain scarce, and existing synthetic alternatives often overlook critical temporal properties inherent in real-world data, including nonstationarity driven by trends and seasonality, irregular sampling intervals, and the presence of unobserved confounders. To address these challenges, we introduce TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets that systematically incorporates both linear and nonlinear dependencies while modeling key temporal characteristics such as trends, seasonal effects, and heterogeneous noise patterns. Each dataset is accompanied by a fully specified causal graph featuring varying densities and diverse noise distributions and is provided in two versions: one including unobserved confounders and one without, thereby offering extensive coverage of real-world complexity while preserving methodological neutrality. We further demonstrate the utility of TimeGraph through systematic evaluations of state-of-the-art causal discovery algorithms including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and metrics. Our experiments reveal significant variations in algorithmic performance under realistic temporal conditions, underscoring the need for robust synthetic benchmarks in the fair and transparent assessment of causal discovery methods. The complete TimeGraph suite, including dataset generation scripts, evaluation metrics, and recommended experimental protocols, is freely available to facilitate reproducible research and foster community-driven advancements in time-series causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.01364</link>
<guid>https://arxiv.org/abs/2506.01364</guid>
<content:encoded><![CDATA[
arXiv:2506.01364v1 Announce Type: new 
Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such data to support tasks like prediction. However, previous deep learning models designed for specific tasks typically require separate training for each use case, leading to increased computational and storage costs. To address this issue, spatio-temporal foundation models have emerged, offering a unified framework capable of solving multiple spatio-temporal tasks. These foundation models achieve remarkable success by learning general knowledge with spatio-temporal data or transferring the general capabilities of pre-trained language models. While previous surveys have explored spatio-temporal data and methodologies separately, they have ignored a comprehensive examination of how foundation models are designed, selected, pre-trained, and adapted. As a result, the overall pipeline for spatio-temporal foundation models remains unclear. To bridge this gap, we innovatively provide an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline begins with an introduction to different types of spatio-temporal data, followed by details of data preprocessing and embedding techniques. The pipeline then presents a novel data property taxonomy to divide existing methods according to data sources and dependencies, providing efficient and effective model design and selection for researchers. On this basis, we further illustrate the training objectives of primitive models, as well as the adaptation techniques of transferred models. Overall, our survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly. Additionally, we introduce emerging opportunities such as multi-objective training in the field of spatio-temporal foundation models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing LLMs to Self-Verify Their Answers</title>
<link>https://arxiv.org/abs/2506.01369</link>
<guid>https://arxiv.org/abs/2506.01369</guid>
<content:encoded><![CDATA[
arXiv:2506.01369v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance during inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compiler Optimization via LLM Reasoning for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v1 Announce Type: new 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training</title>
<link>https://arxiv.org/abs/2506.01376</link>
<guid>https://arxiv.org/abs/2506.01376</guid>
<content:encoded><![CDATA[
arXiv:2506.01376v1 Announce Type: new 
Abstract: Understanding the various properties of glycans with machine learning has shown some preliminary promise. However, previous methods mainly focused on modeling the backbone structure of glycans as graphs of monosaccharides (i.e., sugar units), while they neglected the atomic structures underlying each monosaccharide, which are actually important indicators of glycan properties. We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide nodes representing its global backbone structure and atom nodes representing its local atomic-level structures. Based on such a graph, GlycanAA performs hierarchical message passing to capture from local atomic-level interactions to global monosaccharide-level interactions. To further enhance model capability, we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow the model about different levels of dependencies in a glycan. Extensive benchmark results show the superiority of GlycanAA over existing glycan encoders and verify the further improvements achieved by PreGlycanAA. We maintain all resources at https://github.com/kasawa1234/GlycanAA
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.01386</link>
<guid>https://arxiv.org/abs/2506.01386</guid>
<content:encoded><![CDATA[
arXiv:2506.01386v1 Announce Type: new 
Abstract: Model editing has become an important tool for addressing privacy, bias, and misinformation in large language models (LLMs) by enabling updates to knowledge without the need for retraining from scratch. However, existing editing techniques often target isolated facts, ignoring ripple effects on related knowledge, allowing edited facts to remain deducible and compromising broader contextual integrity. For example, changing Harry Potter's school from Hogwarts to Ilvermorny requires reassigning his house from Gryffindor to a suitable alternative while preserving Gryffindor's relationship with Hogwarts. In this work, we present a new model-editing setting, deep editing, to show: (1) how editing techniques fail to handle connected facts, evaluating how original knowledge sneaks through unchanged causal links, and (2) their impact on broader contextual knowledge. We introduce ThinkEval, a framework to systematically evaluate model- editing techniques by building model-specific knowledge graphs to analyze pre- and post-edit effects on fact persistence and catastrophic forgetting. We present KnowGIC, a benchmark created with ThinkEval, consisting of sequentially linked queries to measure these effects. We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE across multiple LLMs. We find that these techniques struggle to balance indirect fact suppression with the preservation of related knowledge. Our dataset is available at: https://anonymous.4open.science/r/KnowGIC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Part Deployment of Neural Network</title>
<link>https://arxiv.org/abs/2506.01387</link>
<guid>https://arxiv.org/abs/2506.01387</guid>
<content:encoded><![CDATA[
arXiv:2506.01387v1 Announce Type: new 
Abstract: The increasing scale of modern neural networks, exemplified by architectures from IBM (530 billion neurons) and Google (500 billion parameters), presents significant challenges in terms of computational cost and infrastructure requirements. As deep neural networks continue to grow, traditional training paradigms relying on monolithic GPU clusters become increasingly unsustainable. This paper proposes a distributed system architecture that partitions a neural network across multiple servers, each responsible for a subset of neurons. Neurons are classified as local or remote, with inter-server connections managed via a metadata-driven lookup mechanism. A Multi-Part Neural Network Execution Engine facilitates seamless execution and training across distributed partitions by dynamically resolving and invoking remote neurons using stored metadata. All servers share a unified model through a network file system (NFS), ensuring consistency during parallel updates. A Neuron Distributor module enables flexible partitioning strategies based on neuron count, percentage, identifiers, or network layers. This architecture enables cost-effective, scalable deployment of deep learning models on cloud infrastructure, reducing dependency on high-performance centralized compute resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization</title>
<link>https://arxiv.org/abs/2506.01393</link>
<guid>https://arxiv.org/abs/2506.01393</guid>
<content:encoded><![CDATA[
arXiv:2506.01393v1 Announce Type: new 
Abstract: This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\sqrt{T \ln^4 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound for GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling a more refined analysis of the GP's information gain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping</title>
<link>https://arxiv.org/abs/2506.01396</link>
<guid>https://arxiv.org/abs/2506.01396</guid>
<content:encoded><![CDATA[
arXiv:2506.01396v1 Announce Type: new 
Abstract: Differential privacy (DP) has become an essential framework for privacy-preserving machine learning. Existing DP learning methods, however, often have disparate impacts on model predictions, e.g., for minority groups. Gradient clipping, which is often used in DP learning, can suppress larger gradients from challenging samples. We show that this problem is amplified by adaptive clipping, which will often shrink the clipping bound to tiny values to match a well-fitting majority, while significantly reducing the accuracy for others. We propose bounded adaptive clipping, which introduces a tunable lower bound to prevent excessive gradient suppression. Our method improves the accuracy of the worst-performing class on average over 10 percentage points on skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and over 5 percentage points over constant clipping.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs</title>
<link>https://arxiv.org/abs/2506.01404</link>
<guid>https://arxiv.org/abs/2506.01404</guid>
<content:encoded><![CDATA[
arXiv:2506.01404v1 Announce Type: new 
Abstract: This paper introduces an innovative error feedback framework designed to mitigate quantization noise in distributed graph filtering, where communications are constrained to quantized messages. It comes from error spectrum shaping techniques from state-space digital filters, and therefore establishes connections between quantized filtering processes over different domains. In contrast to existing error compensation methods, our framework quantitatively feeds back the quantization noise for exact compensation. We examine the framework under three key scenarios: (i) deterministic graph filtering, (ii) graph filtering over random graphs, and (iii) graph filtering with random node-asynchronous updates. Rigorous theoretical analysis demonstrates that the proposed framework significantly reduces the effect of quantization noise, and we provide closed-form solutions for the optimal error feedback coefficients. Moreover, this quantitative error feedback mechanism can be seamlessly integrated into communication-efficient decentralized optimization frameworks, enabling lower error floors. Numerical experiments validate the theoretical results, consistently showing that our method outperforms conventional quantization strategies in terms of both accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification</title>
<link>https://arxiv.org/abs/2506.01405</link>
<guid>https://arxiv.org/abs/2506.01405</guid>
<content:encoded><![CDATA[
arXiv:2506.01405v1 Announce Type: new 
Abstract: The identification of drug-target interactions (DTI) is crucial for drug discovery and repositioning, as it reveals potential uses of existing drugs, aiding in the acceleration of the drug development process and reducing associated costs. Despite the similarity information in DTI is important, most models are limited to mining direct similarity information within homogeneous graphs, overlooking the potential yet rich similarity information in heterogeneous graphs. Inspired by real-world social interaction behaviors, we propose SOC-DGL, which comprises two specialized modules: the Affinity-Driven Graph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL) module. The ADGL module adopts a comprehensive social interaction strategy, leveraging an affinity-enhanced global drug-target graph to learn both global DTI and the individual similarity information of drugs and targets. In contrast, the EDGL module employs a higher-order social interaction strategy, amplifying the influence of even-hop neighbors through an even-polynomial graph filter grounded in balance theory, enabling the indirect mining of higher-order homogeneous information. This dual approach enables SOC-DGL to effectively and comprehensively capture similarity information across diverse interaction scales within the affinity matrices and drug-target association matrices, significantly enhancing the model's generalization capability and predictive accuracy in DTI tasks. To address the issue of imbalance in drug-target interaction datasets, this paper proposes an adjustable imbalance loss function that mitigates the impact of sample imbalance by adjusting the weight of negative samples and a parameter. Extensive experiments on four benchmark datasets demonstrate significant accuracy improvements achieved by SOC-DGL, particularly in scenarios involving data imbalance and unseen drugs or targets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Latent Space Optimization with Nebula Variational Coding</title>
<link>https://arxiv.org/abs/2506.01414</link>
<guid>https://arxiv.org/abs/2506.01414</guid>
<content:encoded><![CDATA[
arXiv:2506.01414v1 Announce Type: new 
Abstract: Deep learning approaches process data in a layer-by-layer way with intermediate (or latent) features. We aim at designing a general solution to optimize the latent manifolds to improve the performance on classification, segmentation, completion and/or reconstruction through probabilistic models. This paper proposes a variational inference model which leads to a clustered embedding. We introduce additional variables in the latent space, called \textbf{nebula anchors}, that guide the latent variables to form clusters during training. To prevent the anchors from clustering among themselves, we employ the variational constraint that enforces the latent features within an anchor to form a Gaussian distribution, resulting in a generative model we refer as Nebula Variational Coding (NVC). Since each latent feature can be labeled with the closest anchor, we also propose to apply metric learning in a self-supervised way to make the separation between clusters more explicit. As a consequence, the latent variables of our variational coder form clusters which adapt to the generated semantic of the training data, \textit{e.g.} the categorical labels of each sample. We demonstrate experimentally that it can be used within different architectures designed to solve different problems including text sequence, images, 3D point clouds and volumetric data, validating the advantage of our proposed method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variance-Based Defense Against Blended Backdoor Attacks</title>
<link>https://arxiv.org/abs/2506.01444</link>
<guid>https://arxiv.org/abs/2506.01444</guid>
<content:encoded><![CDATA[
arXiv:2506.01444v1 Announce Type: new 
Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things</title>
<link>https://arxiv.org/abs/2506.01450</link>
<guid>https://arxiv.org/abs/2506.01450</guid>
<content:encoded><![CDATA[
arXiv:2506.01450v1 Announce Type: new 
Abstract: Industrial Internet of Things environments increasingly rely on advanced Anomaly Detection and explanation techniques to rapidly detect and mitigate cyberincidents, thereby ensuring operational safety. The sequential nature of data collected from these environments has enabled improvements in Anomaly Detection using Machine Learning and Deep Learning models by processing time windows rather than treating the data as tabular. However, conventional explanation methods often neglect this temporal structure, leading to imprecise or less actionable explanations. This work presents ShaTS (Shapley values for Time Series models), which is a model-agnostic explainable Artificial Intelligence method designed to enhance the precision of Shapley value explanations for time series models. ShaTS addresses the shortcomings of traditional approaches by incorporating an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS accurately identifies critical time instants, precisely pinpoints the sensors, actuators, and processes affected by anomalies, and outperforms SHAP in terms of both explainability and resource efficiency, fulfilling the real-time requirements of industrial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-aware Hypergraph Generation via Next-Scale Prediction</title>
<link>https://arxiv.org/abs/2506.01467</link>
<guid>https://arxiv.org/abs/2506.01467</guid>
<content:encoded><![CDATA[
arXiv:2506.01467v1 Announce Type: new 
Abstract: Hypergraphs generalize traditional graphs by allowing hyperedges to connect multiple nodes, making them well-suited for modeling complex structures with higher-order relationships, such as 3D meshes, molecular systems, and electronic circuits. While topology is central to hypergraph structure, many real-world applications also require node and hyperedge features. Existing hypergraph generation methods focus solely on topology, often overlooking feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph generation via next-scale prediction), a hierarchical approach that jointly generates hypergraph topology and features. FAHNES builds a multi-scale representation through node coarsening, then learns to reconstruct finer levels via localized expansion and refinement, guided by a new node budget mechanism that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs, 3D meshes, and molecular datasets. FAHNES achieves competitive results in reconstructing topology and features, establishing a foundation for future research in featured hypergraph generative modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions</title>
<link>https://arxiv.org/abs/2506.01478</link>
<guid>https://arxiv.org/abs/2506.01478</guid>
<content:encoded><![CDATA[
arXiv:2506.01478v1 Announce Type: new 
Abstract: Understanding the interaction between different drugs (drug-drug interaction or DDI) is critical for ensuring patient safety and optimizing therapeutic outcomes. Existing DDI datasets primarily focus on textual information, overlooking multimodal data that reflect complex drug mechanisms. In this paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark learning methods to study it. In brief, MUDI provides a comprehensive multimodal representation of drugs by combining pharmacological text, chemical formulas, molecular structure graphs, and images across 310,532 annotated drug pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to effectively evaluate machine-learning based generalization, MUDI consists of unseen drug pairs in the test set. We evaluate benchmark models using both late fusion voting and intermediate fusion strategies. All data, annotations, evaluation scripts, and baselines are released under an open research license.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?</title>
<link>https://arxiv.org/abs/2506.01482</link>
<guid>https://arxiv.org/abs/2506.01482</guid>
<content:encoded><![CDATA[
arXiv:2506.01482v1 Announce Type: new 
Abstract: Stage lighting plays an essential role in live music performances, influencing the engaging experience of both musicians and audiences. Given the high costs associated with hiring or training professional lighting engineers, Automatic Stage Lighting Control (ASLC) has gained increasing attention. However, most existing approaches only classify music into limited categories and map them to predefined light patterns, resulting in formulaic and monotonous outcomes that lack rationality. To address this issue, this paper presents an end-to-end solution that directly learns from experienced lighting engineers -- Skip-BART. To the best of our knowledge, this is the first work to conceptualize ASLC as a generative task rather than merely a classification problem. Our method modifies the BART model to take audio music as input and produce light hue and value (intensity) as output, incorporating a novel skip connection mechanism to enhance the relationship between music and light within the frame grid.We validate our method through both quantitative analysis and an human evaluation, demonstrating that Skip-BART outperforms conventional rule-based methods across all evaluation metrics and shows only a limited gap compared to real lighting engineers.Specifically, our method yields a p-value of 0.72 in a statistical comparison based on human evaluations with human lighting engineers, suggesting that the proposed approach closely matches human lighting engineering performance. To support further research, we have made our self-collected dataset, code, and trained model parameters available at https://github.com/RS2002/Skip-BART .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-agnostic Mitigation Strategies of Data Imbalance for Regression</title>
<link>https://arxiv.org/abs/2506.01486</link>
<guid>https://arxiv.org/abs/2506.01486</guid>
<content:encoded><![CDATA[
arXiv:2506.01486v1 Announce Type: new 
Abstract: Data imbalance persists as a pervasive challenge in regression tasks, introducing bias in model performance and undermining predictive reliability. This is particularly detrimental in applications aimed at predicting rare events that fall outside the domain of the bulk of the training data. In this study, we review the current state-of-the-art regarding sampling-based methods and cost-sensitive learning. Additionally, we propose novel approaches to mitigate model bias. To better asses the importance of data, we introduce the density-distance and density-ratio relevance functions, which effectively integrate empirical frequency of data with domain-specific preferences, offering enhanced interpretability for end-users. Furthermore, we present advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and improve existing sampling methods. In a comprehensive quantitative evaluation, we benchmark state-of-the-art methods on 10 synthetic and 42 real-world datasets, using neural networks, XGBoosting trees and Random Forest models. Our analysis reveals that while most strategies improve performance on rare samples, they often degrade it on frequent ones. We demonstrate that constructing an ensemble of models -- one trained with imbalance mitigation and another without -- can significantly reduce these negative effects. The key findings underscore the superior performance of our novel crbSMOGN sampling technique with the density-ratio relevance function for neural networks, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities</title>
<link>https://arxiv.org/abs/2506.01490</link>
<guid>https://arxiv.org/abs/2506.01490</guid>
<content:encoded><![CDATA[
arXiv:2506.01490v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. In real-world scenarios, practical factors often lead to uncertain modality missingness. Existing methods for handling modality missingness are based on data reconstruction or common subspace projections. However, these methods neglect the confidence in multimodal combinations and impose constraints on intra-class representation, hindering the capture of modality-specific information and resulting in suboptimal performance. To address these challenges, we propose a Confidence-Aware Self-Distillation (CASD) strategy that effectively incorporates multimodal probabilistic embeddings via a mixture of Student's $t$-distributions, enhancing its robustness by incorporating confidence and accommodating heavy-tailed properties. This strategy estimates joint distributions with uncertainty scores and reduces uncertainty in the student network by consistency distillation. Furthermore, we introduce a reparameterization representation module that facilitates CASD in robust multimodal learning by sampling embeddings from the joint distribution for the prediction module to calculate the task loss. As a result, the directional constraint from the loss minimization is alleviated by the sampled representation. Experimental results on three benchmark datasets demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme</title>
<link>https://arxiv.org/abs/2506.01502</link>
<guid>https://arxiv.org/abs/2506.01502</guid>
<content:encoded><![CDATA[
arXiv:2506.01502v1 Announce Type: new 
Abstract: Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Importance of Blank for CTC-Based Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.01503</link>
<guid>https://arxiv.org/abs/2506.01503</guid>
<content:encoded><![CDATA[
arXiv:2506.01503v1 Announce Type: new 
Abstract: With the rise of large pre-trained foundation models for automatic speech recognition new challenges appear. While the performance of these models is good, runtime and cost of inference increases. One approach to make use of their strength while retaining efficiency is to distill their knowledge to smaller models during training. In this work, we explore different CTC-based distillation variants, focusing on blank token handling. We show that common approaches like blank elimination do not always work off the shelf. We explore new blank selection patterns as a potential sweet spot between standard knowledge distillation and blank elimination mechanisms. Through the introduction of a symmetric selection method, we are able to remove the CTC loss during knowledge distillation with minimal to no performance degradation. With this, we make the training independent from target labels, potentially allowing for distillation on untranscribed audio data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows</title>
<link>https://arxiv.org/abs/2506.01522</link>
<guid>https://arxiv.org/abs/2506.01522</guid>
<content:encoded><![CDATA[
arXiv:2506.01522v1 Announce Type: new 
Abstract: Variational Autoencoders (VAEs) are powerful generative models widely used for learning interpretable latent spaces, quantifying uncertainty, and compressing data for downstream generative tasks. VAEs typically rely on diagonal Gaussian posteriors due to computational constraints. Using arguments grounded in differential geometry, we demonstrate inherent limitations in the representational capacity of diagonal covariance VAEs, as illustrated by explicit low-dimensional examples. In response, we show that a regularized variant of the recently introduced Free-form Injective Flow (FIF) can be interpreted as a VAE featuring a highly flexible, implicitly defined posterior. Crucially, this regularization yields a posterior equivalent to a full Gaussian covariance distribution, yet maintains computational costs comparable to standard diagonal covariance VAEs. Experiments on image datasets validate our approach, demonstrating that incorporating full covariance substantially improves model likelihood.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</title>
<link>https://arxiv.org/abs/2506.01523</link>
<guid>https://arxiv.org/abs/2506.01523</guid>
<content:encoded><![CDATA[
arXiv:2506.01523v1 Announce Type: new 
Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, when viewed as `loss + regularization,' the standard RLHF objective lacks theoretical justification and incentivizes degenerate, deterministic solutions, an issue that variants such as Direct Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by framing it as \emph{distribution learning} from pairwise preference feedback by explicitly modeling how information about the target language model bleeds through the preference data. This explicit modeling leads us to propose three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We theoretically show that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Finally, we empirically demonstrate that our distribution learning framework, especially preference distillation, consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Abstract World Models with a Group-Structured Latent Space</title>
<link>https://arxiv.org/abs/2506.01529</link>
<guid>https://arxiv.org/abs/2506.01529</guid>
<content:encoded><![CDATA[
arXiv:2506.01529v1 Announce Type: new 
Abstract: Learning meaningful abstract models of Markov Decision Processes (MDPs) is crucial for improving generalization from limited data. In this work, we show how geometric priors can be imposed on the low-dimensional representation manifold of a learned transition model. We incorporate known symmetric structures via appropriate choices of the latent space and the associated group actions, which encode prior knowledge about invariances in the environment. In addition, our framework allows the embedding of additional unstructured information alongside these symmetries. We show experimentally that this leads to better predictions of the latent transition model than fully unstructured approaches, as well as better learning on downstream RL tasks, in environments with rotational and translational features, including in first-person views of 3D environments. Additionally, our experiments show that this leads to simpler and more disentangled representations. The full code is available on GitHub to ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments</title>
<link>https://arxiv.org/abs/2506.01533</link>
<guid>https://arxiv.org/abs/2506.01533</guid>
<content:encoded><![CDATA[
arXiv:2506.01533v1 Announce Type: new 
Abstract: In medicine, treatments often influence multiple, interdependent outcomes, such as primary endpoints, complications, adverse events, or other secondary endpoints. Hence, to make optimal treatment decisions, clinicians are interested in learning the distribution of multi-dimensional treatment outcomes. However, the vast majority of machine learning methods for predicting treatment effects focus on single-outcome settings, despite the fact that medical data often include multiple, interdependent outcomes. To address this limitation, we propose a novel diffusion-based method called DIME to learn the joint distribution of multiple outcomes of medical treatments. We addresses three challenges relevant in medical practice: (i)it is tailored to learn the joint interventional distribution of multiple medical outcomes, which enables reliable decision-making with uncertainty quantification rather than relying solely on point estimates; (ii)it explicitly captures the dependence structure between outcomes; (iii)it can handle outcomes of mixed type, including binary, categorical, and continuous variables. In DIME, we take into account the fundamental problem of causal inference through causal masking. For training, our method decomposes the joint distribution into a series of conditional distributions with a customized conditional masking to account for the dependence structure across outcomes. For inference, our method auto-regressively generates predictions. This allows our method to move beyond point estimates of causal quantities and thus learn the joint interventional distribution. To the best of our knowledge, DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments. Across various experiments, we demonstrate that our method effectively learns the joint distribution and captures shared information among multiple outcomes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Destruction Processes for Diffusion Samplers</title>
<link>https://arxiv.org/abs/2506.01541</link>
<guid>https://arxiv.org/abs/2506.01541</guid>
<content:encoded><![CDATA[
arXiv:2506.01541v1 Announce Type: new 
Abstract: This paper explores the challenges and benefits of a trainable destruction process in diffusion samplers -- diffusion-based generative models trained to sample an unnormalised density without access to data samples. Contrary to the majority of work that views diffusion samplers as approximations to an underlying continuous-time model, we view diffusion models as discrete-time policies trained to produce samples in very few generation steps. We propose to trade some of the elegance of the underlying theory for flexibility in the definition of the generative and destruction policies. In particular, we decouple the generation and destruction variances, enabling both transition kernels to be learned as unconstrained Gaussian densities. We show that, when the number of steps is limited, training both generation and destruction processes results in faster convergence and improved sampling quality on various benchmarks. Through a robust ablation study, we investigate the design choices necessary to facilitate stable training. Finally, we show the scalability of our approach through experiments on GAN latent space sampling for conditional image generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Variational Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2506.01544</link>
<guid>https://arxiv.org/abs/2506.01544</guid>
<content:encoded><![CDATA[
arXiv:2506.01544v1 Announce Type: new 
Abstract: We introduce Temporal Variational Implicit Neural Representations (TV-INRs), a probabilistic framework for modeling irregular multivariate time series that enables efficient individualized imputation and forecasting. By integrating implicit neural representations with latent variable models, TV-INRs learn distributions over time-continuous generator functions conditioned on signal-specific covariates. Unlike existing approaches that require extensive training, fine-tuning or meta-learning, our method achieves accurate individualized predictions through a single forward pass. Our experiments demonstrate that with a single TV-INRs instance, we can accurately solve diverse imputation and forecasting tasks, offering a computationally efficient and scalable solution for real-world applications. TV-INRs excel especially in low-data regimes, where it outperforms existing methods by an order of magnitude in mean squared error for imputation task.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Incremental Learning for Algorithm Selection</title>
<link>https://arxiv.org/abs/2506.01545</link>
<guid>https://arxiv.org/abs/2506.01545</guid>
<content:encoded><![CDATA[
arXiv:2506.01545v1 Announce Type: new 
Abstract: Algorithm selection is commonly used to predict the best solver from a portfolio per per-instance. In many real scenarios, instances arrive in a stream: new instances become available over time, while the number of class labels can also grow as new data distributions arrive downstream. As a result, the classification model needs to be periodically updated to reflect additional solvers without catastrophic forgetting of past data. In machine-learning (ML), this is referred to as Class Incremental Learning (CIL). While commonly addressed in ML settings, its relevance to algorithm-selection in optimisation has not been previously studied. Using a bin-packing dataset, we benchmark 8 continual learning methods with respect to their ability to withstand catastrophic forgetting. We find that rehearsal-based methods significantly outperform other CIL methods. While there is evidence of forgetting, the loss is small at around 7%. Hence, these methods appear to be a viable approach to continual learning in streaming optimisation scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers</title>
<link>https://arxiv.org/abs/2506.01552</link>
<guid>https://arxiv.org/abs/2506.01552</guid>
<content:encoded><![CDATA[
arXiv:2506.01552v1 Announce Type: new 
Abstract: Hierarchical classification offers an approach to incorporate the concept of mistake severity by leveraging a structured, labeled hierarchy. However, decoding in such settings frequently relies on heuristic decision rules, which may not align with task-specific evaluation metrics. In this work, we propose a framework for the optimal decoding of an output probability distribution with respect to a target metric. We derive optimal decision rules for increasingly complex prediction settings, providing universal algorithms when candidates are limited to the set of nodes. In the most general case of predicting a subset of nodes, we focus on rules dedicated to the hierarchical $hF_{\beta}$ scores, tailored to hierarchical settings. To demonstrate the practical utility of our approach, we conduct extensive empirical evaluations, showcasing the superiority of our proposed optimal strategies, particularly in underdetermined scenarios. These results highlight the potential of our methods to enhance the performance and reliability of hierarchical classifiers in real-world applications. The code is available at https://github.com/RomanPlaud/hierarchical_decision_rules
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization</title>
<link>https://arxiv.org/abs/2506.01562</link>
<guid>https://arxiv.org/abs/2506.01562</guid>
<content:encoded><![CDATA[
arXiv:2506.01562v1 Announce Type: new 
Abstract: The softmax function is a fundamental building block of deep neural networks, commonly used to define output distributions in classification tasks or attention weights in transformer architectures. Despite its widespread use and proven effectiveness, its influence on learning dynamics and learned representations remains poorly understood, limiting our ability to optimize model behavior. In this paper, we study the pivotal role of the softmax function in shaping the model's representation. We introduce the concept of rank deficit bias - a phenomenon in which softmax-based deep networks find solutions of rank much lower than the number of classes. This bias depends on the softmax function's logits norm, which is implicitly influenced by hyperparameters or directly modified by softmax temperature. Furthermore, we demonstrate how to exploit the softmax dynamics to learn compressed representations or to enhance their performance on out-of-distribution data. We validate our findings across diverse architectures and real-world datasets, highlighting the broad applicability of temperature tuning in improving model performance. Our work provides new insights into the mechanisms of softmax, enabling better control over representation learning in deep neural networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory First: A Curriculum for Discovering Diverse Policies</title>
<link>https://arxiv.org/abs/2506.01568</link>
<guid>https://arxiv.org/abs/2506.01568</guid>
<content:encoded><![CDATA[
arXiv:2506.01568v1 Announce Type: new 
Abstract: Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima. In this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel. However, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity. To improve diversity optimization in RL, we therefore propose a curriculum that first explores at the trajectory level before learning step-based policies. In our empirical evaluation, we provide novel insights into the shortcoming of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Topology Evolution in Multilayer Perceptrons</title>
<link>https://arxiv.org/abs/2506.01569</link>
<guid>https://arxiv.org/abs/2506.01569</guid>
<content:encoded><![CDATA[
arXiv:2506.01569v1 Announce Type: new 
Abstract: This paper introduces a topological framework for interpreting the internal representations of Multilayer Perceptrons (MLPs). We construct a simplicial tower, a sequence of simplicial complexes connected by simplicial maps, that captures how data topology evolves across network layers. Our approach enables bi-persistence analysis: layer persistence tracks topological features within each layer across scales, while MLP persistence reveals how these features transform through the network. We prove stability theorems for our topological descriptors and establish that linear separability in latent spaces is related to disconnected components in the nerve complexes. To make our framework practical, we develop a combinatorial algorithm for computing MLP persistence and introduce trajectory-based visualisations that track data flow through the network. Experiments on synthetic and real-world medical data demonstrate our method's ability to identify redundant layers, reveal critical topological transitions, and provide interpretable insights into how MLPs progressively organise data for classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayes optimal learning of attention-indexed models</title>
<link>https://arxiv.org/abs/2506.01582</link>
<guid>https://arxiv.org/abs/2506.01582</guid>
<content:encoded><![CDATA[
arXiv:2506.01582v1 Announce Type: new 
Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for analyzing learning in deep attention layers. Inspired by multi-index models, AIM captures how token-level outputs emerge from layered bilinear interactions over high-dimensional embeddings. Unlike prior tractable attention models, AIM allows full-width key and query matrices, aligning more closely with practical transformers. Using tools from statistical mechanics and random matrix theory, we derive closed-form predictions for Bayes-optimal generalization error and identify sharp phase transitions as a function of sample complexity, model width, and sequence length. We propose a matching approximate message passing algorithm and show that gradient descent can reach optimal performance. AIM offers a solvable playground for understanding learning in modern attention architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VirnyFlow: A Design Space for Responsible Model Development</title>
<link>https://arxiv.org/abs/2506.01584</link>
<guid>https://arxiv.org/abs/2506.01584</guid>
<content:encoded><![CDATA[
arXiv:2506.01584v1 Announce Type: new 
Abstract: Developing machine learning (ML) models requires a deep understanding of real-world problems, which are inherently multi-objective. In this paper, we present VirnyFlow, the first design space for responsible model development, designed to assist data scientists in building ML pipelines that are tailored to the specific context of their problem. Unlike conventional AutoML frameworks, VirnyFlow enables users to define customized optimization criteria, perform comprehensive experimentation across pipeline stages, and iteratively refine models in alignment with real-world constraints. Our system integrates evaluation protocol definition, multi-objective Bayesian optimization, cost-aware multi-armed bandits, query optimization, and distributed parallelism into a unified architecture. We show that VirnyFlow significantly outperforms state-of-the-art AutoML systems in both optimization quality and scalability across five real-world benchmarks, offering a flexible, efficient, and responsible alternative to black-box automation in ML development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</title>
<link>https://arxiv.org/abs/2506.01594</link>
<guid>https://arxiv.org/abs/2506.01594</guid>
<content:encoded><![CDATA[
arXiv:2506.01594v1 Announce Type: new 
Abstract: As machine learning models are increasingly embedded into society through high-stakes decision-making, selecting the right algorithm for a given task, audience, and sector presents a critical challenge, particularly in the context of fairness. Traditional assessments of model fairness have often framed fairness as an objective mathematical property, treating model selection as an optimization problem under idealized informational conditions. This overlooks model multiplicity as a consideration--that multiple models can deliver similar performance while exhibiting different fairness characteristics. Legal scholars have engaged this challenge through the concept of Less Discriminatory Algorithms (LDAs), which frames model selection as a civil rights obligation. In real-world deployment, this normative challenge is bounded by constraints on fairness experimentation, e.g., regulatory standards, institutional priorities, and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s relational fairness approach using updated 2021 Home Mortgage Disclosure Act (HMDA) data, and proposes an expansion of the scope of the LDA search process. We argue that extending the LDA search horizontally, considering fairness across model families themselves, provides a lightweight complement, or alternative, to within-model hyperparameter optimization, when operationalizing fairness in non-experimental, resource constrained settings. Fairness metrics alone offer useful, but insufficient signals to accurately evaluate candidate LDAs. Rather, by using a horizontal LDA search approach with the relational trade-off framework, we demonstrate a responsible minimum viable LDA search on real-world lending outcomes. Organizations can modify this approach to systematically compare, evaluate, and select LDAs that optimize fairness and accuracy in a sector-based contextualized manner.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Improving Laplacian Positional Encodings For Temporal GNNs</title>
<link>https://arxiv.org/abs/2506.01596</link>
<guid>https://arxiv.org/abs/2506.01596</guid>
<content:encoded><![CDATA[
arXiv:2506.01596v1 Announce Type: new 
Abstract: Temporal graph learning has applications in recommendation systems, traffic forecasting, and social network analysis. Although multiple architectures have been introduced, progress in positional encoding for temporal graphs remains limited. Extending static Laplacian eigenvector approaches to temporal graphs through the supra-Laplacian has shown promise, but also poses key challenges: high eigendecomposition costs, limited theoretical understanding, and ambiguity about when and how to apply these encodings. In this paper, we address these issues by (1) offering a theoretical framework that connects supra-Laplacian encodings to per-time-slice encodings, highlighting the benefits of leveraging additional temporal connectivity, (2) introducing novel methods to reduce the computational overhead, achieving up to 56x faster runtimes while scaling to graphs with 50,000 active nodes, and (3) conducting an extensive experimental study to identify which models, tasks, and datasets benefit most from these encodings. Our findings reveal that while positional encodings can significantly boost performance in certain scenarios, their effectiveness varies across different models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Newton Algorithm in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2506.01597</link>
<guid>https://arxiv.org/abs/2506.01597</guid>
<content:encoded><![CDATA[
arXiv:2506.01597v1 Announce Type: new 
Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel Hilbert Spaces (RKHS) offer powerful representational capabilities. While second-order optimization methods like Newton's method demonstrate faster convergence than first-order approaches, current RKHS-based policy optimization remains constrained to first-order techniques. This limitation stems primarily from the intractability of explicitly computing and inverting the infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in RKHS, the first second-order optimization framework specifically designed for RL policies represented in RKHS. Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function. Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume. We establish theoretical guarantees proving convergence to a local optimum with a local quadratic convergence rate. Empirical evaluations on a toy financial asset allocation problem validate these theoretical properties, while experiments on standard RL benchmarks demonstrate that Policy Newton in RKHS achieves superior convergence speed and higher episodic rewards compared to established first-order RKHS approaches and parametric second-order methods. Our work bridges a critical gap between non-parametric policy representations and second-order optimization methods in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations</title>
<link>https://arxiv.org/abs/2506.01598</link>
<guid>https://arxiv.org/abs/2506.01598</guid>
<content:encoded><![CDATA[
arXiv:2506.01598v1 Announce Type: new 
Abstract: Neural operators, which aim to approximate mappings between infinite-dimensional function spaces, have been widely applied in the simulation and prediction of physical systems. However, the limited representational capacity of network architectures, combined with their heavy reliance on large-scale data, often hinder effective training and result in poor extrapolation performance. In this paper, inspired by traditional numerical methods, we propose a novel physics guided multi-step neural operator (PMNO) architecture to address these challenges in long-horizon prediction of complex physical systems. Distinct from general operator learning methods, the PMNO framework replaces the single-step input with multi-step historical data in the forward pass and introduces an implicit time-stepping scheme based on the Backward Differentiation Formula (BDF) during backpropagation. This design not only strengthens the model's extrapolation capacity but also facilitates more efficient and stable training with fewer data samples, especially for long-term predictions. Meanwhile, a causal training strategy is employed to circumvent the need for multi-stage training and to ensure efficient end-to-end optimization. The neural operator architecture possesses resolution-invariant properties, enabling the trained model to perform fast extrapolation on arbitrary spatial resolutions. We demonstrate the superior predictive performance of PMNO predictor across a diverse range of physical systems, including 2D linear system, modeling over irregular domain, complex-valued wave dynamics, and reaction-diffusion processes. Depending on the specific problem setting, various neural operator architectures, including FNO, DeepONet, and their variants, can be seamlessly integrated into the PMNO framework.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting Neural Models Latent Geometries with Relative Geodesic Representations</title>
<link>https://arxiv.org/abs/2506.01599</link>
<guid>https://arxiv.org/abs/2506.01599</guid>
<content:encoded><![CDATA[
arXiv:2506.01599v1 Announce Type: new 
Abstract: Neural models learn representations of high-dimensional data on low-dimensional manifolds. Multiple factors, including stochasticities in the training process, model architectures, and additional inductive biases, may induce different representations, even when learning the same task on the same data. However, it has recently been shown that when a latent structure is shared between distinct latent spaces, relative distances between representations can be preserved, up to distortions. Building on this idea, we demonstrate that exploiting the differential-geometric structure of latent spaces of neural models, it is possible to capture precisely the transformations between representational spaces trained on similar data distributions. Specifically, we assume that distinct neural models parametrize approximately the same underlying manifold, and introduce a representation based on the pullback metric that captures the intrinsic structure of the latent space, while scaling efficiently to large models. We validate experimentally our method on model stitching and retrieval tasks, covering autoencoders and vision foundation discriminative models, across diverse architectures, datasets, and pretraining schemes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains</title>
<link>https://arxiv.org/abs/2506.01614</link>
<guid>https://arxiv.org/abs/2506.01614</guid>
<content:encoded><![CDATA[
arXiv:2506.01614v1 Announce Type: new 
Abstract: This paper introduces a Machine Learning (ML) approach for scalability of UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding struggle with distributing UTXOs effectively across validators, creating substantial communication overhead due to child-parent transaction dependencies. This overhead, which arises from the need to locate parent UTXOs, significantly hampers transaction processing speeds. Our solution uses ML to optimize not only UTXO set sharding but also the routing of incoming transactions, ensuring that transactions are directed to shards containing their parent UTXOs. At the heart of our approach is a framework that combines contrastive and unsupervised learning to create an embedding space for transaction outputs. This embedding allows the model to group transaction outputs based on spending relationships, making it possible to route transactions efficiently to the correct validation microservices. Trained on historical transaction data with triplet loss and online semi-hard negative mining, the model embeds parent-child spending patterns directly into its parameters, thus eliminating the need for costly, real-time parent transaction lookups. This significantly reduces cross-shard communication overhead, boosting throughput and scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[
arXiv:2506.01625v1 Announce Type: new 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</title>
<link>https://arxiv.org/abs/2506.01631</link>
<guid>https://arxiv.org/abs/2506.01631</guid>
<content:encoded><![CDATA[
arXiv:2506.01631v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01639</link>
<guid>https://arxiv.org/abs/2506.01639</guid>
<content:encoded><![CDATA[
arXiv:2506.01639v1 Announce Type: new 
Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum entropy reinforcement learning, traditionally relies on minimizing reverse Kullback-Leibler (KL) divergence for policy updates. However, this approach leads to an intractable optimal projection policy, necessitating gradient-based approximations that can suffer from instability and poor sample efficiency. This paper investigates the alternative use of forward KL divergence within SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an explicit optimal projection policy -- corresponding to the mean and variance of the target Boltzmann distribution's action marginals. Building on the distinct advantages of both KL directions, we propose Bidirectional SAC, an algorithm that first initializes the policy using the explicit forward KL projection and then refines it by optimizing the reverse KL divergence. Comprehensive experiments on continuous control benchmarks show that Bidirectional SAC significantly outperforms standard SAC and other baselines, achieving up to a $30\%$ increase in episodic rewards, alongside enhanced sample efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning</title>
<link>https://arxiv.org/abs/2506.01656</link>
<guid>https://arxiv.org/abs/2506.01656</guid>
<content:encoded><![CDATA[
arXiv:2506.01656v1 Announce Type: new 
Abstract: Mixture of Experts (MoE), an ensemble of specialized models equipped with a router that dynamically distributes each input to appropriate experts, has achieved successful results in the field of machine learning. However, theoretical understanding of this architecture is falling behind due to its inherent complexity. In this paper, we theoretically study the sample and runtime complexity of MoE following the stochastic gradient descent (SGD) when learning a regression task with an underlying cluster structure of single index models. On the one hand, we prove that a vanilla neural network fails in detecting such a latent organization as it can only process the problem as a whole. This is intrinsically related to the concept of information exponent which is low for each cluster, but increases when we consider the entire task. On the other hand, we show that a MoE succeeds in dividing this problem into easier subproblems by leveraging the ability of each expert to weakly recover the simpler function corresponding to an individual cluster. To the best of our knowledge, this work is among the first to explore the benefits of the MoE framework by examining its SGD dynamics in the context of nonlinear regression.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Safe Reinforcement Learning from Analytic Gradients</title>
<link>https://arxiv.org/abs/2506.01665</link>
<guid>https://arxiv.org/abs/2506.01665</guid>
<content:encoded><![CDATA[
arXiv:2506.01665v1 Announce Type: new 
Abstract: Deploying autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research which aims to provide such guarantees using safeguards. These safeguards should be integrated during training to prevent a large sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance and sample efficiency. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them with a state-of-the-art learning algorithm and a differentiable simulation. We evaluate how different safeguards affect policy optimisation using numerical experiments on two classical control tasks. The results demonstrate safeguarded training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Impact ControlNet: Advancing Multi-ControlNet Integration</title>
<link>https://arxiv.org/abs/2506.01672</link>
<guid>https://arxiv.org/abs/2506.01672</guid>
<content:encoded><![CDATA[
arXiv:2506.01672v1 Announce Type: new 
Abstract: With the advancement of diffusion models, there is a growing demand for high-quality, controllable image generation, particularly through methods that utilize one or multiple control signals based on ControlNet. However, in current ControlNet training, each control is designed to influence all areas of an image, which can lead to conflicts when different control signals are expected to manage different parts of the image in practical applications. This issue is especially pronounced with edge-type control conditions, where regions lacking boundary information often represent low-frequency signals, referred to as silent control signals. When combining multiple ControlNets, these silent control signals can suppress the generation of textures in related areas, resulting in suboptimal outcomes. To address this problem, we propose Minimal Impact ControlNet. Our approach mitigates conflicts through three key strategies: constructing a balanced dataset, combining and injecting feature signals in a balanced manner, and addressing the asymmetry in the score function's Jacobian matrix induced by ControlNet. These improvements enhance the compatibility of control signals, allowing for freer and more harmonious generation in areas with silent control signals.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses</title>
<link>https://arxiv.org/abs/2506.01722</link>
<guid>https://arxiv.org/abs/2506.01722</guid>
<content:encoded><![CDATA[
arXiv:2506.01722v1 Announce Type: new 
Abstract: We consider the problem setting of prediction with expert advice with possibly heavy-tailed losses, i.e.\ the only assumption on the losses is an upper bound on their second moments, denoted by $\theta$. We develop adaptive algorithms that do not require any prior knowledge about the range or the second moment of the losses. Existing adaptive algorithms have what is typically considered a lower-order term in their regret guarantees. We show that this lower-order term, which is often the maximum of the losses, can actually dominate the regret bound in our setting. Specifically, we show that even with small constant $\theta$, this lower-order term can scale as $\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We propose adaptive algorithms with improved regret bounds that avoid the dependence on such a lower-order term and guarantee $\mathcal{O}(\sqrt{\theta T\log(K)})$ regret in the worst case, and $\mathcal{O}(\theta \log(KT)/\Delta_{\min})$ regret when the losses are sampled i.i.d.\ from some fixed distribution, where $\Delta_{\min}$ is the difference between the mean losses of the second best expert and the best expert. Additionally, when the loss function is the squared loss, our algorithm also guarantees improved regret bounds over prior results.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled data augmentation for learning to solve quadratic programming problems</title>
<link>https://arxiv.org/abs/2506.01728</link>
<guid>https://arxiv.org/abs/2506.01728</guid>
<content:encoded><![CDATA[
arXiv:2506.01728v1 Announce Type: new 
Abstract: Linear and quadratic optimization are crucial in numerous real-world applications, from training machine learning models to integer-linear optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, requiring solving many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised learning framework based on contrastive learning, thereby pretraining MPNNs for enhanced performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Manifold Learning for Reduced Order Modeling</title>
<link>https://arxiv.org/abs/2506.01741</link>
<guid>https://arxiv.org/abs/2506.01741</guid>
<content:encoded><![CDATA[
arXiv:2506.01741v1 Announce Type: new 
Abstract: The problem of identifying geometric structure in data is a cornerstone of (unsupervised) learning. As a result, Geometric Representation Learning has been widely applied across scientific and engineering domains. In this work, we investigate the use of Geometric Representation Learning for the data-driven discovery of system dynamics from spatial-temporal data. We propose to encode similarity structure in such data in a spatial-temporal proximity graph, to which we apply a range of classical and deep learning-based manifold learning approaches to learn reduced order dynamics. We observe that while manifold learning is generally capable of recovering reduced order dynamics, the quality of the learned representations varies substantially across different algorithms and hyperparameter choices. This is indicative of high sensitivity to the inherent geometric assumptions of the respective approaches and suggests a need for careful hyperparameter tuning, which can be expensive in practise. To overcome these challenges, we propose a framework for Automated Manifold Learning, which selects a manifold learning approach and corresponding hyperparameter choices based on representative subsamples of the input graph. We demonstrate that the proposed framework leads to performance gains both in scalability and in the learned representations' accuracy in capturing local and global geometric features of the underlying system dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems</title>
<link>https://arxiv.org/abs/2506.01777</link>
<guid>https://arxiv.org/abs/2506.01777</guid>
<content:encoded><![CDATA[
arXiv:2506.01777v1 Announce Type: new 
Abstract: Federated Unlearning (FU) enables clients to remove the influence of specific data from a collaboratively trained shared global model, addressing regulatory requirements such as GDPR and CCPA. However, this unlearning process introduces a new privacy risk: A malicious server may exploit unlearning updates to reconstruct the data requested for removal, a form of Data Reconstruction Attack (DRA). While DRAs for machine unlearning have been studied extensively in centralized Machine Learning-as-a-Service (MLaaS) settings, their applicability to FU remains unclear due to the decentralized, client-driven nature of FU. This work presents DRAUN, the first attack framework to reconstruct unlearned data in FU systems. DRAUN targets optimization-based unlearning methods, which are widely adopted for their efficiency. We theoretically demonstrate why existing DRAs targeting machine unlearning in MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate our approach through extensive experiments on four datasets and four model architectures, evaluating its performance against five popular unlearning methods, effectively demonstrating that state-of-the-art FU methods remain vulnerable to DRAs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2506.01780</link>
<guid>https://arxiv.org/abs/2506.01780</guid>
<content:encoded><![CDATA[
arXiv:2506.01780v1 Announce Type: new 
Abstract: This paper introduces FedGenGMM, a novel one-shot federated learning approach for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios. In federated learning (FL), where multiple decentralized clients collaboratively train models without sharing raw data, significant challenges include statistical heterogeneity, high communication costs, and privacy concerns. FedGenGMM addresses these issues by allowing local GMM models, trained independently on client devices, to be aggregated through a single communication round. This approach leverages the generative property of GMMs, enabling the creation of a synthetic dataset on the server side to train a global model efficiently. Evaluation across diverse datasets covering image, tabular, and time series data demonstrates that FedGenGMM consistently achieves performance comparable to non-federated and iterative federated methods, even under significant data heterogeneity. Additionally, FedGenGMM significantly reduces communication overhead, maintains robust performance in anomaly detection tasks, and offers flexibility in local model complexities, making it particularly suitable for edge computing environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.01781</link>
<guid>https://arxiv.org/abs/2506.01781</guid>
<content:encoded><![CDATA[
arXiv:2506.01781v1 Announce Type: new 
Abstract: Customer service chatbots are conversational systems aimed at addressing customer queries, often by directing them to automated workflows. A crucial aspect of this process is the classification of the customer's intent. Presently, most intent classification models for customer care utilise only customer query for intent prediction. This may result in low-accuracy models, which cannot handle ambiguous queries. An ambiguous query like "I didn't receive my package" could indicate a delayed order, or an order that was delivered but the customer failed to receive it. Resolution of each of these scenarios requires the execution of very different sequence of steps. Utilizing additional information, such as the customer's order delivery status, in the right manner can help identify the intent for such ambiguous queries. In this paper, we have introduced a context-aware NLU model that incorporates both, the customer query and contextual information from the customer's order status for predicting customer intent. A novel selective attention module is used to extract relevant context features. We have also proposed a multi-task learning paradigm for the effective utilization of different label types available in our training data. Our suggested method, Multi-Task Learning Contextual NLU with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8% increase in top 2 accuracy score over the baseline model which only uses user queries, and a 3.5% improvement over existing state-of-the-art models that combine query and context. We have deployed our model to production for Walmart's customer care domain. Accurate intent prediction through MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby significantly reducing escalations to human agents, leading to almost a million dollars in yearly savings for the company.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability</title>
<link>https://arxiv.org/abs/2506.01789</link>
<guid>https://arxiv.org/abs/2506.01789</guid>
<content:encoded><![CDATA[
arXiv:2506.01789v1 Announce Type: new 
Abstract: High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs</title>
<link>https://arxiv.org/abs/2506.01790</link>
<guid>https://arxiv.org/abs/2506.01790</guid>
<content:encoded><![CDATA[
arXiv:2506.01790v1 Announce Type: new 
Abstract: We study how training data contributes to the emergence of toxic behaviors in large-language models. Most prior work on reducing model toxicity adopts $reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a $proactive$ approach$-$IF-Guide$-$which leverages influence functions to identify harmful tokens within any training data and suppress their impact during training. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-Guide does not rely on human-preference data, which is typically required by existing alignment methods. In evaluation, we demonstrate that IF-Guide substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$ compared to uncensored models, and up to 3$\times$ compared to baseline alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning scenarios. IF-Guide is computationally efficient: a billion-parameter model is $not$ $necessary$ for computing influence scores; a million-parameter model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy for identifying harmful data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique</title>
<link>https://arxiv.org/abs/2506.01815</link>
<guid>https://arxiv.org/abs/2506.01815</guid>
<content:encoded><![CDATA[
arXiv:2506.01815v1 Announce Type: new 
Abstract: We provide an introduction to the topic of path signatures as means of feature extraction for machine learning from data streams. The article stresses the mathematical theory underlying the signature methodology, highlighting the conceptual character without plunging into the technical details of rigorous proofs. These notes are based on an introductory presentation given to students of the Research Experience for Undergraduates in Industrial Mathematics and Statistics at Worcester Polytechnic Institute in June 2024.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming</title>
<link>https://arxiv.org/abs/2506.01826</link>
<guid>https://arxiv.org/abs/2506.01826</guid>
<content:encoded><![CDATA[
arXiv:2506.01826v1 Announce Type: new 
Abstract: Signed graphs are equipped with both positive and negative edge weights, encoding pairwise correlations as well as anti-correlations in data. A balanced signed graph is a signed graph with no cycles containing an odd number of negative edges. Laplacian of a balanced signed graph has eigenvectors that map via a simple linear transform to ones in a corresponding positive graph Laplacian, thus enabling reuse of spectral filtering tools designed for positive graphs. We propose an efficient method to learn a balanced signed graph Laplacian directly from data. Specifically, extending a previous linear programming (LP) based sparse inverse covariance estimation method called CLIME, we formulate a new LP problem for each Laplacian column $i$, where the linear constraints restrict weight signs of edges stemming from node $i$, so that nodes of same / different polarities are connected by positive / negative edges. Towards optimal model selection, we derive a suitable CLIME parameter $\rho$ based on a combination of the Hannan-Quinn information criterion and a minimum feasibility criterion. We solve the LP problem efficiently by tailoring a sparse LP method based on ADMM. We theoretically prove local solution convergence of our proposed iterative algorithm. Extensive experimental results on synthetic and real-world datasets show that our balanced graph learning method outperforms competing methods and enables reuse of spectral filters, wavelets, and graph convolutional nets (GCN) constructed for positive graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts</title>
<link>https://arxiv.org/abs/2506.01827</link>
<guid>https://arxiv.org/abs/2506.01827</guid>
<content:encoded><![CDATA[
arXiv:2506.01827v1 Announce Type: new 
Abstract: As machine learning algorithms are shown to be an increasingly valuable tool, the demand for their access has grown accordingly. Oftentimes, it is infeasible to run inference with larger models without an accelerator, which may be unavailable in environments that have constraints such as energy consumption, security, or cost. To increase the availability of these models, we aim to im- prove the LLM inference speed on a CPU-only environment by modifying the cache architecture. To determine what improvements could be made, we conducted two experiments using Llama.cpp and the QWEN model: running various cache configurations and evaluating their performance, and outputting a trace of the memory footprint. Using these experiments, we investigate the memory access patterns and performance characteristics to identify potential optimizations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model</title>
<link>https://arxiv.org/abs/2506.01833</link>
<guid>https://arxiv.org/abs/2506.01833</guid>
<content:encoded><![CDATA[
arXiv:2506.01833v1 Announce Type: new 
Abstract: Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our $\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive $\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners. The code is available at https://github.com/ZhuJiwei111/SPACE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics</title>
<link>https://arxiv.org/abs/2506.01844</link>
<guid>https://arxiv.org/abs/2506.01844</guid>
<content:encoded><![CDATA[
arXiv:2506.01844v1 Announce Type: new 
Abstract: Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trojan Horse Hunt in Time Series Forecasting for Space Operations</title>
<link>https://arxiv.org/abs/2506.01849</link>
<guid>https://arxiv.org/abs/2506.01849</guid>
<content:encoded><![CDATA[
arXiv:2506.01849v1 Announce Type: new 
Abstract: This competition hosted on Kaggle (https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first part of a series of follow-up competitions and hackathons related to the "Assurance for Space Domain AI Applications" project funded by the European Space Agency (https://assurance-ai.space-codev.org/). The competition idea is based on one of the real-life AI security threats identified within the project -- the adversarial poisoning of continuously fine-tuned satellite telemetry forecasting models. The task is to develop methods for finding and reconstructing triggers (trojans) in advanced models for satellite telemetry forecasting used in safety-critical space operations. Participants are provided with 1) a large public dataset of real-life multivariate satellite telemetry (without triggers), 2) a reference model trained on the clean data, 3) a set of poisoned neural hierarchical interpolation (N-HiTS) models for time series forecasting trained on the dataset with injected triggers, and 4) Jupyter notebook with the training pipeline and baseline algorithm (the latter will be published in the last month of the competition). The main task of the competition is to reconstruct a set of 45 triggers (i.e., short multivariate time series segments) injected into the training data of the corresponding set of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and duration) of these triggers must be identified by participants. The popular Neural Cleanse method is adopted as a baseline, but it is not designed for time series analysis and new approaches are necessary for the task. The impact of the competition is not limited to the space domain, but also to many other safety-critical applications of advanced time series analysis where model poisoning may lead to serious consequences.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trade-offs in Data Memorization via Strong Data Processing Inequalities</title>
<link>https://arxiv.org/abs/2506.01855</link>
<guid>https://arxiv.org/abs/2506.01855</guid>
<content:encoded><![CDATA[
arXiv:2506.01855v1 Announce Type: new 
Abstract: Recent research demonstrated that training large language models involves memorization of a significant fraction of training data. Such memorization can lead to privacy violations when training on sensitive user data and thus motivates the study of data memorization's role in learning. In this work, we develop a general approach for proving lower bounds on excess data memorization, that relies on a new connection between strong data processing inequalities and data memorization. We then demonstrate that several simple and natural binary classification problems exhibit a trade-off between the number of samples available to a learning algorithm, and the amount of information about the training data that a learning algorithm needs to memorize to be accurate. In particular, $\Omega(d)$ bits of information about the training data need to be memorized when $O(1)$ $d$-dimensional examples are available, which then decays as the number of examples grows at a problem-specific rate. Further, our lower bounds are generally matched (up to logarithmic factors) by simple learning algorithms. We also extend our lower bounds to more general mixture-of-clusters models. Our definitions and results build on the work of Brown et al. (2021) and address several limitations of the lower bounds in their work.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Scaling Laws for Compressed Representations</title>
<link>https://arxiv.org/abs/2506.01863</link>
<guid>https://arxiv.org/abs/2506.01863</guid>
<content:encoded><![CDATA[
arXiv:2506.01863v1 Announce Type: new 
Abstract: Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials</title>
<link>https://arxiv.org/abs/2506.01868</link>
<guid>https://arxiv.org/abs/2506.01868</guid>
<content:encoded><![CDATA[
arXiv:2506.01868v1 Announce Type: new 
Abstract: As a machine-learned potential, the neuroevolution potential (NEP) method features exceptional computational efficiency and has been successfully applied in materials science. Constructing high-quality training datasets is crucial for developing accurate NEP models. However, the preparation and screening of NEP training datasets remain a bottleneck for broader applications due to their time-consuming, labor-intensive, and resource-intensive nature. In this work, we have developed NepTrain and NepTrainKit, which are dedicated to initializing and managing training datasets to generate high-quality training sets while automating NEP model training. NepTrain is an open-source Python package that features a bond length filtering method to effectively identify and remove non-physical structures from molecular dynamics trajectories, thereby ensuring high-quality training datasets. NepTrainKit is a graphical user interface (GUI) software designed specifically for NEP training datasets, providing functionalities for data editing, visualization, and interactive exploration. It integrates key features such as outlier identification, farthest-point sampling, non-physical structure detection, and configuration type selection. The combination of these tools enables users to process datasets more efficiently and conveniently. Using $\rm CsPbI_3$ as a case study, we demonstrate the complete workflow for training NEP models with NepTrain and further validate the models through materials property predictions. We believe this toolkit will greatly benefit researchers working with machine learning interatomic potentials.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.01869</link>
<guid>https://arxiv.org/abs/2506.01869</guid>
<content:encoded><![CDATA[
arXiv:2506.01869v1 Announce Type: new 
Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine Learning (ML) models that are efficient, cost-effective, and mindful of resource constraints. This field aims to achieve acceptable performance while minimizing the use of computational resources, time, energy, and data for both training and inference. FML strategies can be broadly categorized into input frugality, learning process frugality, and model frugality, each focusing on reducing resource consumption at different stages of the ML pipeline. This chapter explores recent advancements, applications, and open challenges in FML, emphasizing its importance for smart environments that incorporate edge computing and IoT devices, which often face strict limitations in bandwidth, energy, or latency. Technological enablers such as model compression, energy-efficient hardware, and data-efficient learning techniques are discussed, along with adaptive methods including parameter regularization, knowledge distillation, and dynamic architecture design that enable incremental model updates without full retraining. Furthermore, it provides a comprehensive taxonomy of frugal methods, discusses case studies across diverse domains, and identifies future research directions to drive innovation in this evolving field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Explore: An In-Context Learning Approach for Pure Exploration</title>
<link>https://arxiv.org/abs/2506.01876</link>
<guid>https://arxiv.org/abs/2506.01876</guid>
<content:encoded><![CDATA[
arXiv:2506.01876v1 Announce Type: new 
Abstract: In this work, we study the active sequential hypothesis testing problem, also known as pure exploration, where the goal is to actively control a data collection process to efficiently identify the correct hypothesis underlying a decision problem. While relevant across multiple domains, devising adaptive exploration strategies remains challenging, particularly due to difficulties in encoding appropriate inductive biases. Existing Reinforcement Learning (RL)-based methods often underperform when relevant information structures are inadequately represented, whereas more complex methods, like Best Arm Identification (BAI) techniques, may be difficult to devise and typically rely on explicit modeling assumptions. To address these limitations, we introduce In-Context Pure Exploration (ICPE), an in-context learning approach that uses Transformers to learn exploration strategies directly from experience. ICPE combines supervised learning and reinforcement learning to identify and exploit latent structure across related tasks, without requiring prior assumptions. Numerical results across diverse synthetic and semi-synthetic benchmarks highlight ICPE's capability to achieve robust performance performance in deterministic, stochastic, and structured settings. These results demonstrate ICPE's ability to match optimal instance-dependent algorithms using only deep learning techniques, making it a practical and general approach to data-efficient exploration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics</title>
<link>https://arxiv.org/abs/2506.01883</link>
<guid>https://arxiv.org/abs/2506.01883</guid>
<content:encoded><![CDATA[
arXiv:2506.01883v1 Announce Type: new 
Abstract: Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$ speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and an 18$\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agnostic Reinforcement Learning: Foundations and Algorithms</title>
<link>https://arxiv.org/abs/2506.01884</link>
<guid>https://arxiv.org/abs/2506.01884</guid>
<content:encoded><![CDATA[
arXiv:2506.01884v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success across numerous challenging domains. However, we lack a strong theoretical understanding of the statistical complexity of RL in environments with large state spaces, where function approximation is required for sample-efficient learning. This thesis addresses this gap by rigorously examining the statistical complexity of RL with function approximation from a learning theoretic perspective. Departing from a long history of prior work, we consider the weakest form of function approximation, called agnostic policy learning, in which the learner seeks to find the best policy in a given class $\Pi$, with no guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes: environment access -- how a learner collects data from the environment; coverage conditions -- intrinsic properties of the underlying MDP measuring the expansiveness of state-occupancy measures for policies in the class $\Pi$, and representational conditions -- structural assumptions on the class $\Pi$ itself. Within this comprehensive framework, we (1) design new learning algorithms with theoretical guarantees and (2) characterize fundamental performance bounds of any algorithm. Our results reveal significant statistical separations that highlight the power and limitations of agnostic policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection</title>
<link>https://arxiv.org/abs/2506.01890</link>
<guid>https://arxiv.org/abs/2506.01890</guid>
<content:encoded><![CDATA[
arXiv:2506.01890v1 Announce Type: new 
Abstract: Early detection of cognitive disorders such as Alzheimer's disease is critical for enabling timely clinical intervention and improving patient outcomes. In this work, we introduce CogniAlign, a multimodal architecture for Alzheimer's detection that integrates audio and textual modalities, two non-intrusive sources of information that offer complementary insights into cognitive health. Unlike prior approaches that fuse modalities at a coarse level, CogniAlign leverages a word-level temporal alignment strategy that synchronizes audio embeddings with corresponding textual tokens based on transcription timestamps. This alignment supports the development of token-level fusion techniques, enabling more precise cross-modal interactions. To fully exploit this alignment, we propose a Gated Cross-Attention Fusion mechanism, where audio features attend over textual representations, guided by the superior unimodal performance of the text modality. In addition, we incorporate prosodic cues, specifically interword pauses, by inserting pause tokens into the text and generating audio embeddings for silent intervals, further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset, where it achieves an accuracy of 90.36%, outperforming existing state-of-the-art methods. A detailed ablation study confirms the advantages of our alignment strategy, attention-based fusion, and prosodic modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLorc: Momentum Low-rank Compression for Large Language Model Adaptation</title>
<link>https://arxiv.org/abs/2506.01897</link>
<guid>https://arxiv.org/abs/2506.01897</guid>
<content:encoded><![CDATA[
arXiv:2506.01897v1 Announce Type: new 
Abstract: With increasing size of large language models (LLMs), full-parameter fine-tuning imposes substantial memory demands. To alleviate this, we propose a novel memory-efficient training paradigm called Momentum Low-rank compression (MLorc). By directly compressing and reconstructing momentum rather than gradients, MLorc avoids imposing a fixed-rank constraint on weight update matrices and better preserves the training dynamics of full-parameter fine-tuning, in contrast to existing low-rank approaches such as LoRA and GaLore. Empirically, MLorc consistently outperforms other memory-efficient training methods, matches or even exceeds the performance of full fine-tuning with a small rank (e.g., $r=4$), and generalizes well across different optimizers -- all while not compromising time or memory efficiency. Furthermore, we provide a theoretical guarantee for its convergence under reasonable assumptions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data</title>
<link>https://arxiv.org/abs/2506.01907</link>
<guid>https://arxiv.org/abs/2506.01907</guid>
<content:encoded><![CDATA[
arXiv:2506.01907v1 Announce Type: new 
Abstract: Privacy-preserving data publication, including synthetic data sharing, often experiences trade-offs between privacy and utility. Synthetic data is generally more effective than data anonymization in balancing this trade-off, however, not without its own challenges. Synthetic data produced by generative models trained on source data may inadvertently reveal information about outliers. Techniques specifically designed for preserving privacy, such as introducing noise to satisfy differential privacy, often incur unpredictable and significant losses in utility. In this work we show that, with the right mechanism of synthetic data generation, we can achieve strong privacy protection without significant utility loss. Synthetic data generators producing contracting data patterns, such as Synthetic Minority Over-sampling Technique (SMOTE), can enhance a differentially private data generator, leveraging the strengths of both. We prove in theory and through empirical demonstration that this SMOTE-DP technique can produce synthetic data that not only ensures robust privacy protection but maintains utility in downstream learning tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Gradient Norm Clipping &amp; Non-Euclidean $(L_0,L_1)$-Smoothness</title>
<link>https://arxiv.org/abs/2506.01913</link>
<guid>https://arxiv.org/abs/2506.01913</guid>
<content:encoded><![CDATA[
arXiv:2506.01913v1 Announce Type: new 
Abstract: This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning and demonstrate their properties on image classification and language modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models</title>
<link>https://arxiv.org/abs/2506.01919</link>
<guid>https://arxiv.org/abs/2506.01919</guid>
<content:encoded><![CDATA[
arXiv:2506.01919v1 Announce Type: new 
Abstract: Transformer based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Despite their empirical success, a comprehensive theoretical understanding of this phenomenon remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model, i.e, Hidden Markov Models, which are fundamental to many language tasks, we observe that: first, lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens; second, on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement. Building on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer's effectiveness and efficiency on sequence learning across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images</title>
<link>https://arxiv.org/abs/2411.19276</link>
<guid>https://arxiv.org/abs/2411.19276</guid>
<content:encoded><![CDATA[
arXiv:2411.19276v2 Announce Type: cross 
Abstract: In this study, we compare the performance of randomized classical and quantum neural networks (NNs) as well as classical and quantum-classical hybrid convolutional neural networks (CNNs) for the task of binary image classification. We use two distinct methodologies: using randomized NNs on dimensionality-reduced data, and applying CNNs to full image data. We evaluate these approaches on three data sets of increasing complexity: an artificial hypercube dataset, MNIST handwritten digits and real-world industrial images. We analyze correlations between classification accuracy and quantum model hyperparameters, including the number of trainable parameters, feature encoding methods, circuit layers, entangling gate type and structure, gate entangling power, and measurement operators. For random quantum NNs, we compare their performance against literature models. Classical and quantum/hybrid models achieved statistically equivalent classification accuracies across most datasets, with no approach demonstrating consistent superiority. We observe that quantum models show lower variance with respect to initial training parameters, suggesting better training stability. Among the hyperparameters analyzed, only the number of trainable parameters showed a positive correlation with the model performance. Around 94% of the best-performing quantum NNs had entangling gates, although for hybrid CNNs, models without entanglement performed equally well but took longer to converge. Cross-dataset performance analysis revealed limited transferability of quantum models between different classification tasks. Our study provides an industry perspective on quantum machine learning for practical image classification tasks, highlighting both current limitations and potential avenues for further research in quantum circuit design, entanglement utilization, and model transferability across varied applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization</title>
<link>https://arxiv.org/abs/2506.00002</link>
<guid>https://arxiv.org/abs/2506.00002</guid>
<content:encoded><![CDATA[
arXiv:2506.00002v1 Announce Type: cross 
Abstract: Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classical digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hardware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamental challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$ times speedup, depending on the difficulty of the generation tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[
arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies</title>
<link>https://arxiv.org/abs/2506.00008</link>
<guid>https://arxiv.org/abs/2506.00008</guid>
<content:encoded><![CDATA[
arXiv:2506.00008v1 Announce Type: cross 
Abstract: The rapid growth of large-language models (LLMs) is driving a new wave of specialized hardware for inference. This paper presents the first workload-centric, cross-architectural performance study of commercial AI accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale engines. We compare memory hierarchies, compute fabrics, and on-chip interconnects, and observe up to 3.7x performance variation across architectures as batch size and sequence length change. Four scaling techniques for trillion-parameter models are examined; expert parallelism offers an 8.4x parameter-to-compute advantage but incurs 2.1x higher latency variance than tensor parallelism. These findings provide quantitative guidance for matching workloads to accelerators and reveal architectural gaps that next-generation designs must address.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Physical Reasoning with the PHYSICS Dataset</title>
<link>https://arxiv.org/abs/2506.00022</link>
<guid>https://arxiv.org/abs/2506.00022</guid>
<content:encoded><![CDATA[
arXiv:2506.00022v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00033</link>
<guid>https://arxiv.org/abs/2506.00033</guid>
<content:encoded><![CDATA[
arXiv:2506.00033v1 Announce Type: cross 
Abstract: The large underlying assumption of climate models today relies on the basis of a "confident" initial condition, a reasonably plausible snapshot of the Earth for which all future predictions depend on. However, given the inherently chaotic nature of our system, this assumption is complicated by sensitive dependence, where small uncertainties in initial conditions can lead to exponentially diverging outcomes over time. This challenge is particularly salient at global spatial scales and over centennial timescales, where data gaps are not just common but expected. The source of uncertainty is two-fold: (1) sparse, noisy observations from satellites and ground stations, and (2) internal variability stemming from the simplifying approximations within the models themselves.
  In practice, data assimilation methods are used to reconcile this missing information by conditioning model states on partial observations. Our work builds on this idea but operates at the extreme end of sparsity. We propose a conditional data imputation framework that reconstructs full temperature fields from as little as 1% observational coverage. The method leverages a diffusion model guided by a prekriged mask, effectively inferring the full-state fields from minimal data points. We validate our framework over the Southern Great Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the summer months of 2018-2020. Across varying observational densities--from swath data to isolated in-situ sensors--our model achieves strong reconstruction accuracy, highlighting its potential to fill in critical data gaps in both historical reanalysis and real-time forecasting pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models</title>
<link>https://arxiv.org/abs/2506.00037</link>
<guid>https://arxiv.org/abs/2506.00037</guid>
<content:encoded><![CDATA[
arXiv:2506.00037v1 Announce Type: cross 
Abstract: Text embedding models enable semantic search, powering several NLP applications like Retrieval Augmented Generation by efficient information retrieval (IR). However, text embedding models are commonly studied in scenarios where the training data is static, thus limiting its applications to dynamic scenarios where new training data emerges over time. IR methods generally encode a huge corpus of documents to low-dimensional embeddings and store them in a database index. During retrieval, a semantic search over the corpus is performed and the document whose embedding is most similar to the query embedding is returned. When updating an embedding model with new training data, using the already indexed corpus is suboptimal due to the non-compatibility issue, since the model which was used to obtain the embeddings of the corpus has changed. While re-indexing of old corpus documents using the updated model enables compatibility, it requires much higher computation and time. Thus, it is critical to study how the already indexed corpus can still be effectively used without the need of re-indexing. In this work, we establish a continual learning benchmark with large-scale datasets and continually train dense retrieval embedding models on query-document pairs from new datasets in each task and observe forgetting on old tasks due to significant drift of embeddings. We employ embedding distillation on both query and document embeddings to maintain stability and propose a novel query drift compensation method during retrieval to project new model query embeddings to the old embedding space. This enables compatibility with previously indexed corpus embeddings extracted using the old model and thus reduces the forgetting. We show that the proposed method significantly improves performance without any re-indexing. Code is available at https://github.com/dipamgoswami/QDC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval</title>
<link>https://arxiv.org/abs/2506.00041</link>
<guid>https://arxiv.org/abs/2506.00041</guid>
<content:encoded><![CDATA[
arXiv:2506.00041v1 Announce Type: cross 
Abstract: Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability. In this work, we propose a novel interpretability framework that leverages Sparse Autoencoders (SAEs) to decompose previously uninterpretable dense embeddings from DPR models into distinct, interpretable latent concepts. We generate natural language descriptions for each latent concept, enabling human interpretations of both the dense embeddings and the query-document similarity scores of DPR models. We further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework that directly utilizes the extracted latent concepts as indexing units. CL-SR effectively combines the semantic expressiveness of dense embeddings with the transparency and efficiency of sparse representations. We show that CL-SR achieves high index-space and computational efficiency while maintaining robust performance across vocabulary and semantic mismatches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic intraday electricity price forecasting using generative machine learning</title>
<link>https://arxiv.org/abs/2506.00044</link>
<guid>https://arxiv.org/abs/2506.00044</guid>
<content:encoded><![CDATA[
arXiv:2506.00044v1 Announce Type: cross 
Abstract: The growing importance of intraday electricity trading in Europe calls for improved price forecasting and tailored decision-support tools. In this paper, we propose a novel generative neural network model to generate probabilistic path forecasts for intraday electricity prices and use them to construct effective trading strategies for Germany's continuous-time intraday market. Our method demonstrates competitive performance in terms of statistical evaluation metrics compared to two state-of-the-art statistical benchmark approaches. To further assess its economic value, we consider a realistic fixed-volume trading scenario and propose various strategies for placing market sell orders based on the path forecasts. Among the different trading strategies, the price paths generated by our generative model lead to higher profit gains than the benchmark methods. Our findings highlight the potential of generative machine learning tools in electricity price forecasting and underscore the importance of economic evaluation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL</title>
<link>https://arxiv.org/abs/2506.00048</link>
<guid>https://arxiv.org/abs/2506.00048</guid>
<content:encoded><![CDATA[
arXiv:2506.00048v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are powerful tools for recommendation systems, but they often struggle under data sparsity and noise. To address these issues, we implemented LightGCL, a graph contrastive learning model that uses Singular Value Decomposition (SVD) for robust graph augmentation, preserving semantic integrity without relying on stochastic or heuristic perturbations. LightGCL enables structural refinement and captures global collaborative signals, achieving significant gains over state-of-the-art models across benchmark datasets. Our experiments also demonstrate improved fairness and resilience to popularity bias, making it well-suited for real-world recommender systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving statistical learning methods via features selection without replacement sampling and random projection</title>
<link>https://arxiv.org/abs/2506.00053</link>
<guid>https://arxiv.org/abs/2506.00053</guid>
<content:encoded><![CDATA[
arXiv:2506.00053v1 Announce Type: cross 
Abstract: Cancer is fundamentally a genetic disease characterized by genetic and epigenetic alterations that disrupt normal gene expression, leading to uncontrolled cell growth and metastasis. High-dimensional microarray datasets pose challenges for classification models due to the "small n, large p" problem, resulting in overfitting. This study makes three different key contributions: 1) we propose a machine learning-based approach integrating the Feature Selection Without Re-placement (FSWOR) technique and a projection method to improve classification accuracy. 2) We apply the Kendall statistical test to identify the most significant genes from the brain cancer mi-croarray dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3) we apply machine learning models using k-fold cross validation techniques in which our model incorpo-rates ensemble classifiers with LDA projection and Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by 9.09%. The results demonstrate the effectiveness of our ap-proach in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting. This study contributes to cancer biomarker discovery, offering a robust computational method for analyzing microarray data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education</title>
<link>https://arxiv.org/abs/2506.00057</link>
<guid>https://arxiv.org/abs/2506.00057</guid>
<content:encoded><![CDATA[
arXiv:2506.00057v1 Announce Type: cross 
Abstract: Educators teaching entry-level university engineering modules face the challenge of identifying which topics students find most difficult and how to support diverse student needs effectively. This study demonstrates a rigorous yet interpretable statistical approach -- hierarchical Bayesian modeling -- that leverages detailed student response data to quantify both skill difficulty and individual student abilities. Using a large-scale dataset from an undergraduate Statics course, we identified clear patterns of skill mastery and uncovered distinct student subgroups based on their learning trajectories. Our analysis reveals that certain concepts consistently present challenges, requiring targeted instructional support, while others are readily mastered and may benefit from enrichment activities. Importantly, the hierarchical Bayesian method provides educators with intuitive, reliable metrics without sacrificing predictive accuracy. This approach allows for data-informed decisions, enabling personalized teaching strategies to improve student engagement and success. By combining robust statistical methods with clear interpretability, this study equips educators with actionable insights to better support diverse learner populations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?</title>
<link>https://arxiv.org/abs/2506.00062</link>
<guid>https://arxiv.org/abs/2506.00062</guid>
<content:encoded><![CDATA[
arXiv:2506.00062v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a common practice to adapt general-purpose models to the telecom domain. However, little attention has been paid to how this process may compromise model safety. Recent research has shown that even benign fine-tuning can degrade the safety alignment of LLMs, causing them to respond to harmful or unethical user queries. In this paper, we investigate this issue for telecom-tuned LLMs using three representative datasets featured by the GenAINet initiative. We show that safety degradation persists even for structured and seemingly harmless datasets such as 3GPP standards and tabular records, indicating that telecom-specific data is not immune to safety erosion during fine-tuning. We further extend our analysis to publicly available Telecom LLMs trained via continual pre-training, revealing that safety alignment is often severely lacking, primarily due to the omission of safety-focused instruction tuning. To address these issues in both fine-tuned and pre-trained models, we conduct extensive experiments and evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and SafeMERGE) using established red-teaming benchmarks. The results show that, across all settings, the proposed defenses can effectively restore safety after harmful degradation without compromising downstream task performance, leading to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as a diagnostic study and practical guide for safety realignment in telecom-tuned LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning for real-world deployments of Telecom LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs</title>
<link>https://arxiv.org/abs/2506.00072</link>
<guid>https://arxiv.org/abs/2506.00072</guid>
<content:encoded><![CDATA[
arXiv:2506.00072v1 Announce Type: cross 
Abstract: This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry</title>
<link>https://arxiv.org/abs/2506.00076</link>
<guid>https://arxiv.org/abs/2506.00076</guid>
<content:encoded><![CDATA[
arXiv:2506.00076v1 Announce Type: cross 
Abstract: Television networks face high financial risk when making programming decisions, often relying on limited historical data to forecast episodic viewership. This study introduces a machine learning framework that integrates natural language processing (NLP) features from over 25000 television episodes with traditional viewership data to enhance predictive accuracy. By extracting emotional tone, cognitive complexity, and narrative structure from episode dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost, and feature selection models. While prior viewership remains a strong baseline predictor, NLP features contribute meaningful improvements for some series. We also introduce a similarity scoring method based on Euclidean distance between aggregate dialogue vectors to compare shows by content. Tested across diverse genres, including Better Call Saul and Abbott Elementary, our framework reveals genre-specific performance and offers interpretable metrics for writers, executives, and marketers seeking data-driven insight into audience behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian mixture models as a proxy for interacting language models</title>
<link>https://arxiv.org/abs/2506.00077</link>
<guid>https://arxiv.org/abs/2506.00077</guid>
<content:encoded><![CDATA[
arXiv:2506.00077v1 Announce Type: cross 
Abstract: Large language models (LLMs) are a powerful tool with the ability to match human capabilities and behavior in many settings. Retrieval-augmented generation (RAG) further allows LLMs to generate diverse output depending on the contents of their RAG database. This motivates their use in the social sciences to study human behavior between individuals when large-scale experiments are infeasible. However, LLMs depend on complex, computationally expensive algorithms. In this paper, we introduce interacting Gaussian mixture models (GMMs) as an alternative to similar frameworks using LLMs. We compare a simplified model of GMMs to select experimental simulations of LLMs whose updating and response depend on feedback from other LLMs. We find that interacting GMMs capture important features of the dynamics in interacting LLMs, and we investigate key similarities and differences between interacting LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture models, potential modifications, and future research directions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values</title>
<link>https://arxiv.org/abs/2506.00079</link>
<guid>https://arxiv.org/abs/2506.00079</guid>
<content:encoded><![CDATA[
arXiv:2506.00079v1 Announce Type: cross 
Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2506.00088</link>
<guid>https://arxiv.org/abs/2506.00088</guid>
<content:encoded><![CDATA[
arXiv:2506.00088v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey</title>
<link>https://arxiv.org/abs/2506.00098</link>
<guid>https://arxiv.org/abs/2506.00098</guid>
<content:encoded><![CDATA[
arXiv:2506.00098v1 Announce Type: cross 
Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robot's behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC</title>
<link>https://arxiv.org/abs/2506.00102</link>
<guid>https://arxiv.org/abs/2506.00102</guid>
<content:encoded><![CDATA[
arXiv:2506.00102v1 Announce Type: cross 
Abstract: The pursuit of discovering new phenomena at the Large Hadron Collider (LHC) demands constant innovation in algorithms and technologies. Tensor networks are mathematical models on the intersection of classical and quantum machine learning, which present a promising and efficient alternative for tackling these challenges. In this work, we propose a tensor network-based strategy for anomaly detection at the LHC and demonstrate its superior performance in identifying new phenomena compared to established quantum methods. Our model is a parametrized Matrix Product State with an isometric feature map, processing a latent representation of simulated LHC data generated by an autoencoder. Our results highlight the potential of tensor networks to enhance new-physics discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generator Based Inference (GBI)</title>
<link>https://arxiv.org/abs/2506.00119</link>
<guid>https://arxiv.org/abs/2506.00119</guid>
<content:encoded><![CDATA[
arXiv:2506.00119v1 Announce Type: cross 
Abstract: Statistical inference in physics is often based on samples from a generator (sometimes referred to as a ``forward model") that emulate experimental data and depend on parameters of the underlying theory. Modern machine learning has supercharged this workflow to enable high-dimensional and unbinned analyses to utilize much more information than ever before. We propose a general framework for describing the integration of machine learning with generators called Generator Based Inference (GBI). A well-studied special case of this setup is Simulation Based Inference (SBI) where the generator is a physics-based simulator. In this work, we examine other methods within the GBI toolkit that use data-driven methods to build the generator. In particular, we focus on resonant anomaly detection, where the generator describing the background is learned from sidebands. We show how to perform machine learning-based parameter estimation in this context with data-derived generators. This transforms the statistical outputs of anomaly detection to be directly interpretable and the performance on the LHCO community benchmark dataset establishes a new state-of-the-art for anomaly detection sensitivity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models</title>
<link>https://arxiv.org/abs/2506.00128</link>
<guid>https://arxiv.org/abs/2506.00128</guid>
<content:encoded><![CDATA[
arXiv:2506.00128v1 Announce Type: cross 
Abstract: Effective prioritization of issue reports in software engineering helps to optimize resource allocation and information recovery. However, manual issue classification is laborious and lacks scalability. As an alternative, many open source software (OSS) projects employ automated processes for this task, yet this method often relies on large datasets for adequate training. Traditionally, machine learning techniques have been used for issue classification. More recently, large language models (LLMs) have emerged as powerful tools for addressing a range of software engineering challenges, including code and test generation, mapping new requirements to legacy software endpoints, and conducting code reviews. The following research investigates an automated approach to issue classification based on LLMs. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports, mitigating the necessity for extensive training data while also maintaining reliability in classification. In our research, we developed an LLM-based approach for accurately labeling issues by selecting two of the most prominent large language models. We then compared their performance across multiple datasets. Our findings show that GPT-4o achieved the best results in classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o outperformed DeepSeek R1, achieving an F1 score 20% higher when both models were trained on the same dataset from the NLBSE 2023 competition, which was ten times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved 59.33%. Increasing the dataset size did not improve the F1 score, reducing the dependence on massive datasets for building an efficient solution to issue classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation</title>
<link>https://arxiv.org/abs/2506.00129</link>
<guid>https://arxiv.org/abs/2506.00129</guid>
<content:encoded><![CDATA[
arXiv:2506.00129v1 Announce Type: cross 
Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on improving the representational capacity of large language models to incorporate Sign Language features. This work explores an alternative direction: enhancing the geometric properties of skeletal representations themselves. We propose Geo-Sign, a method that leverages the properties of hyperbolic geometry to model the hierarchical structure inherent in sign language kinematics. By projecting skeletal features derived from Spatio-Temporal Graph Convolutional Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more discriminative embeddings, particularly for fine-grained motions like finger articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet mean aggregation scheme, and a geometric contrastive loss operating directly in hyperbolic space. These components are integrated into an end-to-end translation framework as a regularisation function, to enhance the representations within the language model. This work demonstrates the potential of hyperbolic geometry to improve skeletal representations for Sign Language Translation, improving on SOTA RGB methods while preserving privacy and improving computational efficiency. Code available here: https://github.com/ed-fish/geo-sign.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things</title>
<link>https://arxiv.org/abs/2506.00133</link>
<guid>https://arxiv.org/abs/2506.00133</guid>
<content:encoded><![CDATA[
arXiv:2506.00133v1 Announce Type: cross 
Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low bandwidth, high latency, mobility, and limited energy resources. Traditional routing protocols like RPL, which were designed for land-based networks, do not perform well in these underwater conditions. This paper introduces RL-RPL-UA, a new routing protocol that uses reinforcement learning to improve performance in underwater environments. Each node includes a lightweight RL agent that selects the best parent node based on local information such as packet delivery ratio, buffer level, link quality, and remaining energy. RL-RPL-UA keeps full compatibility with standard RPL messages and adds a dynamic objective function to support real-time decision-making. Simulations using Aqua-Sim show that RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per packet by 14.8%, and extends network lifetime by 80 seconds compared to traditional methods. These results suggest that RL-RPL-UA is a promising and energy-efficient routing solution for underwater networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-QA: A Benchmark for Personalized Long-form Question Answering</title>
<link>https://arxiv.org/abs/2506.00137</link>
<guid>https://arxiv.org/abs/2506.00137</guid>
<content:encoded><![CDATA[
arXiv:2506.00137v1 Announce Type: cross 
Abstract: Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models (LLMs). Our results show that incorporating the personalized context provided leads to performance improvements of up to 39%. The benchmark is publicly released to support future research in this area.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation</title>
<link>https://arxiv.org/abs/2506.00138</link>
<guid>https://arxiv.org/abs/2506.00138</guid>
<content:encoded><![CDATA[
arXiv:2506.00138v1 Announce Type: cross 
Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in sparse reward and reward-free environments, including class of methods known as intrinsic motivation, exhibit inconsistent exploration patterns and thus fail to produce robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in unconstrained, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed to capture robust autonomous exploration observed in animals. Our method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. We demonstrate that artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Profit and Fairness in Risk-Based Pricing Markets</title>
<link>https://arxiv.org/abs/2506.00140</link>
<guid>https://arxiv.org/abs/2506.00140</guid>
<content:encoded><![CDATA[
arXiv:2506.00140v1 Announce Type: cross 
Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer groups from essential resources such as health insurance and consumer credit. We show that a regulator can realign private incentives with social objectives through a learned, interpretable tax schedule. First, we provide a formal proposition that bounding each firm's \emph{local} demographic gap implicitly bounds the \emph{global} opt-out disparity, motivating firm-level penalties. Building on this insight we introduce \texttt{MarketSim} -- an open-source, scalable simulator of heterogeneous consumers and profit-maximizing firms -- and train a reinforcement learning (RL) social planner (SP) that selects a bracketed fairness-tax while remaining close to a simple linear prior via an $\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and easily interpretable. In two empirically calibrated markets, i.e., U.S. health-insurance and consumer-credit, our planner simultaneously raises demand-fairness by up to $16\%$ relative to unregulated Free Market while outperforming a fixed linear schedule in terms of social welfare without explicit coordination. These results illustrate how AI-assisted regulation can convert a competitive social dilemma into a win-win equilibrium, providing a principled and practical framework for fairness-aware market oversight.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures</title>
<link>https://arxiv.org/abs/2506.00165</link>
<guid>https://arxiv.org/abs/2506.00165</guid>
<content:encoded><![CDATA[
arXiv:2506.00165v1 Announce Type: cross 
Abstract: Randomized dimensionality reduction is a widely-used algorithmic technique for speeding up large-scale Euclidean optimization problems. In this paper, we study dimension reduction for a variety of maximization problems, including max-matching, max-spanning tree, max TSP, as well as various measures for dataset diversity. For these problems, we show that the effect of dimension reduction is intimately tied to the \emph{doubling dimension} $\lambda_X$ of the underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of point sets. Specifically, we prove that a target dimension of $O(\lambda_X)$ suffices to approximately preserve the value of any near-optimal solution,which we also show is necessary for some of these problems. This is in contrast to classical dimension reduction results, whose dependence increases with the dataset size $|X|$. We also provide empirical results validating the quality of solutions found in the projected space, as well as speedups due to dimensionality reduction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds</title>
<link>https://arxiv.org/abs/2506.00171</link>
<guid>https://arxiv.org/abs/2506.00171</guid>
<content:encoded><![CDATA[
arXiv:2506.00171v1 Announce Type: cross 
Abstract: We study the problem of estimating eigenpairs of elliptic differential operators from samples of a distribution $\rho$ supported on a manifold $M$. The operators discussed in the paper are relevant in unsupervised learning and in particular are obtained by taking suitable scaling limits of widely used graph Laplacians over data clouds. We study the minimax risk for this eigenpair estimation problem and explore the rates of approximation that can be achieved by commonly used graph Laplacians built from random data. More concretely, assuming that $\rho$ belongs to a certain family of distributions with controlled second derivatives, and assuming that the $d$-dimensional manifold $M$ where $\rho$ is supported has bounded geometry, we prove that the statistical minimax rate for approximating eigenvalues and eigenvectors in the $H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a closely related density estimation problem. We then revisit the literature studying Laplacians over proximity graphs in the large data limit and prove that, under slightly stronger regularity assumptions on the data generating model, eigenpairs of graph Laplacians induce manifold agnostic estimators with an error of approximation that, up to logarithmic corrections, matches our lower bounds. Our analysis allows us to expand the existing literature on graph-based learning in at least two significant ways: 1) we consider stronger norms to measure the error of approximation than the ones that had been analyzed in the past; 2) our rates of convergence are uniform over a family of smooth distributions and do not just apply to densities with special symmetries, and, as a consequence of our lower bounds, are essentially sharp when the connectivity of the graph is sufficiently high.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Validation of the Independent Chip Model</title>
<link>https://arxiv.org/abs/2506.00180</link>
<guid>https://arxiv.org/abs/2506.00180</guid>
<content:encoded><![CDATA[
arXiv:2506.00180v1 Announce Type: cross 
Abstract: The independent chip model (ICM) forms a cornerstone of all modern poker tournament strategy. However, despite its prominence, the ICM's performance in the real world has not been sufficiently scrutinized, especially at a large scale. In this paper, we introduce our new dataset of poker tournaments, consisting of results of over ten thousand events. Then, using this dataset, we perform two experiments as part of a large-scale empirical validation of the ICM. First, we verify that the ICM performs more accurately than a baseline we propose. Second, we obtain empirical evidence of the ICM underestimating the performances of players with larger stacks while overestimating those who are short-stacked. Our contributions may be useful to future researchers developing new algorithms for estimating a player's value in poker tournaments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overfitting has a limitation: a model-independent generalization error bound based on R\'enyi entropy</title>
<link>https://arxiv.org/abs/2506.00182</link>
<guid>https://arxiv.org/abs/2506.00182</guid>
<content:encoded><![CDATA[
arXiv:2506.00182v1 Announce Type: cross 
Abstract: Will further scaling up of machine learning models continue to bring success? A significant challenge in answering this question lies in understanding generalization error, which is the impact of overfitting. Understanding generalization error behavior of increasingly large-scale machine learning models remains a significant area of investigation, as conventional analyses often link error bounds to model complexity, failing to fully explain the success of extremely large architectures. This research introduces a novel perspective by establishing a model-independent upper bound for generalization error applicable to algorithms whose outputs are determined solely by the data's histogram, such as empirical risk minimization or gradient-based methods. Crucially, this bound is shown to depend only on the R\'enyi entropy of the data-generating distribution, suggesting that a small generalization error can be maintained even with arbitrarily large models, provided the data quantity is sufficient relative to this entropy. This framework offers a direct explanation for the phenomenon where generalization performance degrades significantly upon injecting random noise into data, where the performance degrade is attributed to the consequent increase in the data distribution's R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be data-distribution-dependent, demonstrating that an amount of data corresponding to the R\'enyi entropy is indeed essential for successful learning, thereby highlighting the tightness of our proposed generalization bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Limits of Beam Search Decoding for Transducer-based ASR models</title>
<link>https://arxiv.org/abs/2506.00185</link>
<guid>https://arxiv.org/abs/2506.00185</guid>
<content:encoded><![CDATA[
arXiv:2506.00185v1 Announce Type: cross 
Abstract: Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Graph Backdoor Attack</title>
<link>https://arxiv.org/abs/2506.00191</link>
<guid>https://arxiv.org/abs/2506.00191</guid>
<content:encoded><![CDATA[
arXiv:2506.00191v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex, multi-typed relationships across diverse domains, yet their vulnerability to backdoor attacks remains unexplored. To address this gap, we conduct the first investigation into the susceptibility of HGNNs to existing graph backdoor attacks, revealing three critical issues: (1) high attack budget required for effective backdoor injection, (2) inefficient and unreliable backdoor activation, and (3) inaccurate attack effectiveness evaluation. To tackle these issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first backdoor attack specifically designed for HGNNs, introducing a novel relation-based trigger mechanism that establishes specific connections between a strategically selected trigger node and poisoned nodes via the backdoor metapath. HGBA achieves efficient and stealthy backdoor injection with minimal structural modifications and supports easy backdoor activation through two flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally, we improve the ASR measurement protocol, enabling a more accurate assessment of attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses multiple state-of-the-art graph backdoor attacks in black-box settings, efficiently attacking HGNNs with low attack budgets. Ablation studies show that the strength of HBGA benefits from our trigger node selection method and backdoor metapath selection strategy. In addition, HGBA shows superior robustness against node feature perturbations and multiple types of existing graph backdoor defense mechanisms. Finally, extension experiments demonstrate that the relation-based trigger mechanism can effectively extend to tasks in homogeneous graph scenarios, thereby posing severe threats to broader security-critical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs</title>
<link>https://arxiv.org/abs/2506.00197</link>
<guid>https://arxiv.org/abs/2506.00197</guid>
<content:encoded><![CDATA[
arXiv:2506.00197v1 Announce Type: cross 
Abstract: Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring Radiology Reports: Challenging LLMs with Lightweight Models</title>
<link>https://arxiv.org/abs/2506.00200</link>
<guid>https://arxiv.org/abs/2506.00200</guid>
<content:encoded><![CDATA[
arXiv:2506.00200v1 Announce Type: cross 
Abstract: Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)-specifically T5 and BERT2BERT-for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B-70B), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol</title>
<link>https://arxiv.org/abs/2506.00223</link>
<guid>https://arxiv.org/abs/2506.00223</guid>
<content:encoded><![CDATA[
arXiv:2506.00223v1 Announce Type: cross 
Abstract: Accurate prediction of molecular solubility is a cornerstone of early-stage drug discovery, yet conventional machine learning models face significant challenges due to limited labeled data and the high-dimensional nature of molecular descriptors. To address these issues, we propose LatMixSol, a novel latent space augmentation framework that combines autoencoder-based feature compression with guided interpolation to enrich training data. Our approach first encodes molecular descriptors into a low-dimensional latent space using a two-layer autoencoder. Spectral clustering is then applied to group chemically similar molecules, enabling targeted MixUp-style interpolation within clusters. Synthetic samples are generated by blending latent vectors of cluster members and decoding them back to the original feature space. Evaluated on the Huuskonen solubility benchmark, LatMixSol demonstrates consistent improvements across three of four gradient-boosted regressors (CatBoost, LightGBM, HistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared increases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant enhancement with a 7.6% RMSE improvement. Our analysis confirms that cluster-guided latent space augmentation preserves chemical validity while expanding dataset diversity, offering a computationally efficient strategy to enhance predictive models in resource-constrained drug discovery pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Principal Component Analysis</title>
<link>https://arxiv.org/abs/2506.00226</link>
<guid>https://arxiv.org/abs/2506.00226</guid>
<content:encoded><![CDATA[
arXiv:2506.00226v1 Announce Type: cross 
Abstract: This paper proposes an innovative extension of Principal Component Analysis (PCA) that transcends the traditional assumption of data lying in Euclidean space, enabling its application to data on Riemannian manifolds. The primary challenge addressed is the lack of vector space operations on such manifolds. Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA) as a geometric approach to analyze data on Riemannian manifolds, particularly effective for structured datasets like medical images, where the manifold's intrinsic structure is apparent. However, PGA's applicability is limited when dealing with general datasets that lack an implicit local distance notion. In this work, we introduce a generalized framework, termed {\em Riemannian Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with a local distance structure. Specifically, we adapt the PCA methodology to Riemannian manifolds by equipping data tables with local metrics, enabling the incorporation of manifold geometry. This framework provides a unified approach for dimensionality reduction and statistical analysis directly on manifolds, opening new possibilities for datasets with region-specific or part-specific distance notions, ensuring respect for their intrinsic geometric properties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sorrel: A simple and flexible framework for multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2506.00228</link>
<guid>https://arxiv.org/abs/2506.00228</guid>
<content:encoded><![CDATA[
arXiv:2506.00228v1 Announce Type: cross 
Abstract: We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple Python interface for generating and testing new multi-agent reinforcement learning environments. This interface places a high degree of emphasis on simplicity and accessibility, and uses a more psychologically intuitive structure for the basic agent-environment loop, making it a useful tool for social scientists to investigate how learning and social interaction leads to the development and change of group dynamics. In this short paper, we outline the basic design philosophy and features of Sorrel.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment</title>
<link>https://arxiv.org/abs/2506.00238</link>
<guid>https://arxiv.org/abs/2506.00238</guid>
<content:encoded><![CDATA[
arXiv:2506.00238v1 Announce Type: cross 
Abstract: Natural disasters usually affect vast areas and devastate infrastructures. Performing a timely and efficient response is crucial to minimize the impact on affected communities, and data-driven approaches are the best choice. Visual question answering (VQA) models help management teams to achieve in-depth understanding of damages. However, recently published models do not possess the ability to answer open-ended questions and only select the best answer among a predefined list of answers. If we want to ask questions with new additional possible answers that do not exist in the predefined list, the model needs to be fin-tuned/retrained on a new collected and annotated dataset, which is a time-consuming procedure. In recent years, large-scale Vision-Language Models (VLMs) have earned significant attention. These models are trained on extensive datasets and demonstrate strong performance on both unimodal and multimodal vision/language downstream tasks, often without the need for fine-tuning. In this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and investigate the performance of on post-disaster FloodNet dataset. Since the proposed method takes advantage of zero-shot learning, it can be applied on new datasets without fine-tuning. In addition, ZeShot-VQA is able to process and generate answers that has been not seen during the training procedure, which demonstrates its flexibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How hard is learning to cut? Trade-offs and sample complexity</title>
<link>https://arxiv.org/abs/2506.00252</link>
<guid>https://arxiv.org/abs/2506.00252</guid>
<content:encoded><![CDATA[
arXiv:2506.00252v1 Announce Type: cross 
Abstract: In the recent years, branch-and-cut algorithms have been the target of data-driven approaches designed to enhance the decision making in different phases of the algorithm such as branching, or the choice of cutting planes (cuts). In particular, for cutting plane selection two score functions have been proposed in the literature to evaluate the quality of a cut: branch-and-cut tree size and gap closed. In this paper, we present new sample complexity lower bounds, valid for both scores. We show that for a wide family of classes $\mathcal{F}$ that maps an instance to a cut, learning over an unknown distribution of the instances to minimize those scores requires at least (up to multiplicative constants) as many samples as learning from the same class function $\mathcal{F}$ any generic target function (using square loss). Our results also extend to the case of learning from a restricted set of cuts, namely those from the Simplex tableau. To the best of our knowledge, these constitute the first lower bounds for the learning-to-cut framework. We compare our bounds to known upper bounds in the case of neural networks and show they are nearly tight. We illustrate our results with a graph neural network selection evaluated on set covering and facility location integer programming models and we empirically show that the gap closed score is an effective proxy to minimize the branch-and-cut tree size. Although the gap closed score has been extensively used in the integer programming literature, this is the first principled analysis discussing both scores at the same time both theoretically and computationally.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Data Sketching for Varying Coefficient Regression Models</title>
<link>https://arxiv.org/abs/2506.00270</link>
<guid>https://arxiv.org/abs/2506.00270</guid>
<content:encoded><![CDATA[
arXiv:2506.00270v1 Announce Type: cross 
Abstract: Varying coefficient models are popular for estimating nonlinear regression functions in functional data models. Their Bayesian variants have received limited attention in large data applications, primarily due to prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We introduce Bayesian data sketching for varying coefficient models to obviate computational challenges presented by large sample sizes. To address the challenges of analyzing large data, we compress the functional response vector and predictor matrix by a random linear transformation to achieve dimension reduction and conduct inference on the compressed data. Our approach distinguishes itself from several existing methods for analyzing large functional data in that it requires neither the development of new models or algorithms, nor any specialized computational hardware while delivering fully model-based Bayesian inference. Well-established methods and algorithms for varying coefficient regression models can be applied to the compressed data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction</title>
<link>https://arxiv.org/abs/2506.00273</link>
<guid>https://arxiv.org/abs/2506.00273</guid>
<content:encoded><![CDATA[
arXiv:2506.00273v1 Announce Type: cross 
Abstract: This paper introduces SoundSculpt, a neural network designed to extract target sound fields from ambisonic recordings. SoundSculpt employs an ambisonic-in-ambisonic-out architecture and is conditioned on both spatial information (e.g., target direction obtained by pointing at an immersive video) and semantic embeddings (e.g., derived from image segmentation and captioning). Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt demonstrates superior performance compared to various signal processing baselines. Our results further reveal that while spatial conditioning alone can be effective, the combination of spatial and semantic information is beneficial in scenarios where there are secondary sound sources spatially close to the target. Additionally, we compare two different semantic embeddings derived from a text description of the target sound using text encoders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning</title>
<link>https://arxiv.org/abs/2506.00279</link>
<guid>https://arxiv.org/abs/2506.00279</guid>
<content:encoded><![CDATA[
arXiv:2506.00279v1 Announce Type: cross 
Abstract: Despite extensive research on the relationship between sleep and cognition, the connection between sleep microstructure and human performance across specific cognitive domains remains underexplored. This study investigates whether deep learning models can predict executive functions, particularly cognitive adaptability and conceptual reasoning from physiological processes during a night's sleep. To address this, we introduce CogPSGFormer, a multi-scale convolutional-transformer model designed to process multi-modal polysomnographic data. This model integrates one-channel ECG and EEG signals along with extracted features, including EEG power bands and heart rate variability parameters, to capture complementary information across modalities. A thorough evaluation of the CogPSGFormer architecture was conducted to optimize the processing of extended sleep signals and identify the most effective configuration. The proposed framework was evaluated on 817 individuals from the STAGES dataset using cross-validation. The model achieved 80.3\% accuracy in classifying individuals into low vs. high cognitive performance groups on unseen data based on Penn Conditional Exclusion Test (PCET) scores. These findings highlight the effectiveness of our multi-scale feature extraction and multi-modal learning approach in leveraging sleep-derived signals for cognitive performance prediction. To facilitate reproducibility, our code is publicly accessible (https://github.com/boshrakh95/CogPSGFormer.git).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Gaussian Splat Vulnerabilities</title>
<link>https://arxiv.org/abs/2506.00280</link>
<guid>https://arxiv.org/abs/2506.00280</guid>
<content:encoded><![CDATA[
arXiv:2506.00280v1 Announce Type: cross 
Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLM-One: Diffusion Language Models for One-Step Sequence Generation</title>
<link>https://arxiv.org/abs/2506.00290</link>
<guid>https://arxiv.org/abs/2506.00290</guid>
<content:encoded><![CDATA[
arXiv:2506.00290v1 Announce Type: cross 
Abstract: This paper introduces DLM-One, a score-distillation-based framework for one-step sequence generation with continuous diffusion language models (DLMs). DLM-One eliminates the need for iterative refinement by aligning the scores of a student model's outputs in the continuous token embedding space with the score function of a pretrained teacher DLM. We investigate whether DLM-One can achieve substantial gains in sampling efficiency for language modeling. Through comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we show that DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks used to evaluate the teacher models. We further analyze the method's empirical behavior across multiple datasets, providing initial insights into its generality and practical applicability. Our findings position one-step diffusion as a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models operating in embedding space for natural language processing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Aerodynamics for the Control of Flying Humanoid Robots</title>
<link>https://arxiv.org/abs/2506.00305</link>
<guid>https://arxiv.org/abs/2506.00305</guid>
<content:encoded><![CDATA[
arXiv:2506.00305v1 Announce Type: cross 
Abstract: Robots with multi-modal locomotion are an active research field due to their versatility in diverse environments. In this context, additional actuation can provide humanoid robots with aerial capabilities. Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint. The technological contribution includes the mechanical design of iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine integration, and hardware modifications for wind tunnel experiments on humanoid robots for precise aerodynamic forces and surface pressure measurements. The scientific contribution offers a comprehensive approach to model and control aerodynamic forces using classical and learning techniques. Computational Fluid Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the aerodynamic dataset, enabling the training of a Deep Neural Network and a linear regression model. These models are integrated into a simulator for designing aerodynamic-aware controllers, validated through flight simulations and balancing experiments on the iRonCub-Mk1 physical prototype.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Token Sequence Compression via Meta-Tokens</title>
<link>https://arxiv.org/abs/2506.00307</link>
<guid>https://arxiv.org/abs/2506.00307</guid>
<content:encoded><![CDATA[
arXiv:2506.00307v1 Announce Type: cross 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-of-Two (PoT) Weights in Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2506.00315</link>
<guid>https://arxiv.org/abs/2506.00315</guid>
<content:encoded><![CDATA[
arXiv:2506.00315v1 Announce Type: cross 
Abstract: Complexity of Neural Networks is increasing rapidly due to the massive increase in model parameters. Specifically, in Large Language Models (LLMs), the number of model parameters has grown exponentially in the past few years, for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This raises a significant challenge for implementation, especially for Edge devices where memory and processing power are very limited. In this work, we investigate reducing LLM complexity with special type of quantization, power of two (PoT), for linear layers weights and transformer tables. PoT not only provides memory reduction but more importantly provides significant computational reduction through converting multiplication to bit shifting. We obtained preliminary results of PoT quantization on Nano-GPT implementation using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The PoT quantization results are shown to be very promising with cross entropy loss degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent power levels.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</title>
<link>https://arxiv.org/abs/2506.00320</link>
<guid>https://arxiv.org/abs/2506.00320</guid>
<content:encoded><![CDATA[
arXiv:2506.00320v1 Announce Type: cross 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation</title>
<link>https://arxiv.org/abs/2506.00322</link>
<guid>https://arxiv.org/abs/2506.00322</guid>
<content:encoded><![CDATA[
arXiv:2506.00322v1 Announce Type: cross 
Abstract: We propose dpmm, an open-source library for synthetic data generation with Differentially Private (DP) guarantees. It includes three popular marginal models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer richer functionality compared to alternative implementations. Additionally, we adopt best practices to provide end-to-end DP guarantees and address well-known DP-related vulnerabilities. Our goal is to accommodate a wide audience with easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The iNaturalist Sounds Dataset</title>
<link>https://arxiv.org/abs/2506.00343</link>
<guid>https://arxiv.org/abs/2506.00343</guid>
<content:encoded><![CDATA[
arXiv:2506.00343v1 Announce Type: cross 
Abstract: We present the iNaturalist Sounds Dataset (iNatSounds), a collection of 230,000 audio files capturing sounds from over 5,500 species, contributed by more than 27,000 recordists worldwide. The dataset encompasses sounds from birds, mammals, insects, reptiles, and amphibians, with audio and species labels derived from observations submitted to iNaturalist, a global citizen science platform. Each recording in the dataset varies in length and includes a single species annotation. We benchmark multiple backbone architectures, comparing multiclass classification objectives with multilabel objectives. Despite weak labeling, we demonstrate that iNatSounds serves as a useful pretraining resource by benchmarking it on strongly labeled downstream evaluation datasets. The dataset is available as a single, freely accessible archive, promoting accessibility and research in this important domain. We envision models trained on this data powering next-generation public engagement applications, and assisting biologists, ecologists, and land use managers in processing large audio collections, thereby contributing to the understanding of species compositions in diverse soundscapes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings</title>
<link>https://arxiv.org/abs/2506.00348</link>
<guid>https://arxiv.org/abs/2506.00348</guid>
<content:encoded><![CDATA[
arXiv:2506.00348v1 Announce Type: cross 
Abstract: Knowledge of accurate relative skills in any competitive system is essential, but foundational approaches such as ELO discard extremely relevant performance data by concentrating exclusively on binary outcomes. While margin of victory (MOV) extensions exist, they often lack a definitive method for incorporating this information. We introduce Margin of Victory Differential Analysis (MOVDA), a framework that enhances traditional rating systems by using the deviation between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a domain-specific, non-linear function (a scaled hyperbolic tangent that captures saturation effects and home advantage) to predict expected MOV based on rating differentials. Crucially, the $\textit{difference}$ between the true and expected MOV provides a subtle and weighted signal for rating updates, highlighting informative deviations in all levels of contests. Extensive experiments on professional NBA basketball data (from 2013 to 2023, with 13,619 games) show that MOVDA significantly outperforms standard ELO and Bayesian baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly accelerates rating convergence by $13.5\%$, while maintaining the computational efficiency of the original ELO updates. MOVDA offers a theoretically motivated, empirically superior, and computationally lean approach to integrating performance magnitude into skill rating for competitive environments like the NBA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</title>
<link>https://arxiv.org/abs/2506.00358</link>
<guid>https://arxiv.org/abs/2506.00358</guid>
<content:encoded><![CDATA[
arXiv:2506.00358v1 Announce Type: cross 
Abstract: While recent audio-visual models have demonstrated impressive performance, their robustness to distributional shifts at test-time remains not fully understood. Existing robustness benchmarks mainly focus on single modalities, making them insufficient for thoroughly assessing the robustness of audio-visual models. Motivated by real-world scenarios where shifts can occur $\textit{simultaneously}$ in both audio and visual modalities, we introduce $\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the test-time robustness of audio-visual recognition models. $\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets, $\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and $\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through extensive evaluations, we observe that state-of-the-art supervised and self-supervised audio-visual models exhibit declining robustness as corruption severity increases. Furthermore, online test-time adaptation (TTA) methods, on $\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a simple TTA approach enabling on-the-fly cross-modal fusion by penalizing high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective and robust audio-visual TTA approaches. Our code is available $\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-shift robust federated feature screening for high-dimensional classification</title>
<link>https://arxiv.org/abs/2506.00379</link>
<guid>https://arxiv.org/abs/2506.00379</guid>
<content:encoded><![CDATA[
arXiv:2506.00379v1 Announce Type: cross 
Abstract: Distributed and federated learning are important tools for high-dimensional classification of large datasets. To reduce computational costs and overcome the curse of dimensionality, feature screening plays a pivotal role in eliminating irrelevant features during data preprocessing. However, data heterogeneity, particularly label shifting across different clients, presents significant challenges for feature screening. This paper introduces a general framework that unifies existing screening methods and proposes a novel utility, label-shift robust federated feature screening (LR-FFS), along with its federated estimation procedure. The framework facilitates a uniform analysis of methods and systematically characterizes their behaviors under label shift conditions. Building upon this framework, LR-FFS leverages conditional distribution functions and expectations to address label shift without adding computational burdens and remains robust against model misspecification and outliers. Additionally, the federated procedure ensures computational efficiency and privacy protection while maintaining screening effectiveness comparable to centralized processing. We also provide a false discovery rate (FDR) control method for federated feature screening. Experimental results and theoretical analyses demonstrate LR-FFS's superior performance across diverse client environments, including those with varying class distributions, sample sizes, and missing categorical data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2506.00385</link>
<guid>https://arxiv.org/abs/2506.00385</guid>
<content:encoded><![CDATA[
arXiv:2506.00385v1 Announce Type: cross 
Abstract: Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLMs via Adaptive Parallel Decoding</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
arXiv:2506.00413v1 Announce Type: cross 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free</title>
<link>https://arxiv.org/abs/2506.00433</link>
<guid>https://arxiv.org/abs/2506.00433</guid>
<content:encoded><![CDATA[
arXiv:2506.00433v1 Announce Type: cross 
Abstract: High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight framework that enables any latent diffusion model to scale to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces three key components: (1) a scale-consistent variational autoencoder objective that enhances the spectral fidelity of latent representations; (2) wavelet energy maps that identify and localize detail-rich spatial regions within the latent space; and (3) a time-dependent masking strategy that focuses denoising supervision on high-frequency components during training. LWD requires no architectural modifications and incurs no additional computational overhead. Despite its simplicity, it consistently improves perceptual quality and reduces FID in ultra-high-resolution image synthesis, outperforming strong baseline models. These results highlight the effectiveness of frequency-aware, signal-driven supervision as a principled and efficient approach for high-resolution generative modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling</title>
<link>https://arxiv.org/abs/2506.00446</link>
<guid>https://arxiv.org/abs/2506.00446</guid>
<content:encoded><![CDATA[
arXiv:2506.00446v1 Announce Type: cross 
Abstract: Off-policy evaluation (OPE) in ranking settings with large ranking action spaces, which stems from an increase in both the number of unique actions and length of the ranking, is essential for assessing new recommender policies using only logged bandit data from previous versions. To address the high variance issues associated with existing estimators, we introduce two new assumptions: no direct effect on rankings and user behavior model on ranking embedding spaces. We then propose the generalized marginalized inverse propensity score (GMIPS) estimator with statistically desirable properties compared to existing ones. Finally, we demonstrate that the GMIPS achieves the lowest MSE. Notably, among GMIPS variants, the marginalized reward interaction IPS (MRIPS) incorporates a doubly marginalized importance weight based on a cascade behavior assumption on ranking embeddings. MRIPS effectively balances the trade-off between bias and variance, even as the ranking action spaces increase and the above assumptions may not hold, as evidenced by our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DV365: Extremely Long User History Modeling at Instagram</title>
<link>https://arxiv.org/abs/2506.00450</link>
<guid>https://arxiv.org/abs/2506.00450</guid>
<content:encoded><![CDATA[
arXiv:2506.00450v1 Announce Type: cross 
Abstract: Long user history is highly valuable signal for recommendation systems, but effectively incorporating it often comes with high cost in terms of data center power consumption and GPU. In this work, we chose offline embedding over end-to-end sequence length optimization methods to enable extremely long user sequence modeling as a cost-effective solution, and propose a new user embedding learning strategy, multi-slicing and summarization, that generates highly generalizable user representation of user's long-term stable interest. History length we encoded in this embedding is up to 70,000 and on average 40,000. This embedding, named as DV365, is proven highly incremental on top of advanced attentive user sequence models deployed in Instagram. Produced by a single upstream foundational model, it is launched in 15 different models across Instagram and Threads with significant impact, and has been production battle-proven for >1 year since our first launch.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[
arXiv:2506.00455v1 Announce Type: cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address this challenge, we introduce a novel machine learning method using diffusion-based molecular generation to enhance odour localization accuracy that can be used by itself or with automated olfactory dataset construction pipelines with vision-language models (VLMs) This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and the training data of VLMs, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors which emulate human olfactory recognition through electronic sensor arrays. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making in environments where olfactory cues are essential. Our methodology represents a foundational advancement in the field of robotic olfaction, offering a scalable solution to the challenges posed by limited olfactory data and sensor ambiguities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark</title>
<link>https://arxiv.org/abs/2506.00462</link>
<guid>https://arxiv.org/abs/2506.00462</guid>
<content:encoded><![CDATA[
arXiv:2506.00462v1 Announce Type: cross 
Abstract: Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested ``in the wild''. Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation</title>
<link>https://arxiv.org/abs/2506.00471</link>
<guid>https://arxiv.org/abs/2506.00471</guid>
<content:encoded><![CDATA[
arXiv:2506.00471v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for seismic wavefield modeling, yet they typically require time-consuming retraining when applied to different velocity models. Moreover, their training can suffer from slow convergence due to the complexity of of the wavefield solution. To address these challenges, we introduce a latent diffusion-based strategy for rapid and effective PINN initialization. First, we train multiple PINNs to represent frequency-domain scattered wavefields for various velocity models, then flatten each trained network's parameters into a one-dimensional vector, creating a comprehensive parameter dataset. Next, we employ an autoencoder to learn latent representations of these parameter vectors, capturing essential patterns across diverse PINN's parameters. We then train a conditional diffusion model to store the distribution of these latent vectors, with the corresponding velocity models serving as conditions. Once trained, this diffusion model can generate latent vectors corresponding to new velocity models, which are subsequently decoded by the autoencoder into complete PINN parameters. Experimental results indicate that our method significantly accelerates training and maintains high accuracy across in-distribution and out-of-distribution velocity scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00479</link>
<guid>https://arxiv.org/abs/2506.00479</guid>
<content:encoded><![CDATA[
arXiv:2506.00479v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet their significant computational demands hinder practical deployment. While efforts to improve LVLM efficiency are growing, existing methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics. In this work, we systematically evaluate mainstream acceleration techniques for LVLMs, categorized into token and parameter compression. We introduce EffiVLM-Bench, a unified framework for assessing not only absolute performance but also generalization and loyalty, while exploring Pareto-optimal trade-offs. Our extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs. We open-source code and recipes for EffiVLM-Bench to foster future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2506.00483</link>
<guid>https://arxiv.org/abs/2506.00483</guid>
<content:encoded><![CDATA[
arXiv:2506.00483v1 Announce Type: cross 
Abstract: Multi-hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps. We introduce Auto-Patch, a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch selectively modifies internal representations using a learned classifier. Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from 18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to Chain-of-Thought prompting (27.44\%). Our results highlight the potential of dynamic hidden state interventions for advancing complex reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities</title>
<link>https://arxiv.org/abs/2506.00548</link>
<guid>https://arxiv.org/abs/2506.00548</guid>
<content:encoded><![CDATA[
arXiv:2506.00548v1 Announce Type: cross 
Abstract: Existing attacks against multimodal language models (MLLMs) primarily communicate instructions through text accompanied by adversarial images. In contrast, we exploit the capabilities of MLLMs to interpret non-textual instructions, specifically, adversarial images or audio generated by our novel method, Con Instruction. We optimize these adversarial examples to align closely with target instructions in the embedding space, revealing the detrimental implications of MLLMs' sophisticated understanding. Unlike prior work, our method does not require training data or preprocessing of textual instructions. While these non-textual adversarial examples can effectively bypass MLLM safety mechanisms, their combination with various text inputs substantially amplifies attack success. We further introduce a new Attack Response Categorization (ARC) framework, which evaluates both the quality of the model's response and its relevance to the malicious instructions. Experimental results demonstrate that Con Instruction effectively bypasses safety mechanisms in multiple vision- and audio-language models, including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard benchmarks: AdvBench and SafeBench. Specifically, our method achieves the highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On the defense side, we explore various countermeasures against our attacks and uncover a substantial performance gap among existing techniques. Our implementation is made publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score Matching With Missing Data</title>
<link>https://arxiv.org/abs/2506.00557</link>
<guid>https://arxiv.org/abs/2506.00557</guid>
<content:encoded><![CDATA[
arXiv:2506.00557v1 Announce Type: cross 
Abstract: Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification</title>
<link>https://arxiv.org/abs/2506.00589</link>
<guid>https://arxiv.org/abs/2506.00589</guid>
<content:encoded><![CDATA[
arXiv:2506.00589v1 Announce Type: cross 
Abstract: Many core problems in robotics can be framed as constrained optimization problems. Often on these problems, the robotic system has uncertainty, or it would be advantageous to identify multiple high quality feasible solutions. To enable this, we present two novel frameworks for applying principles of constrained optimization to the new variational inference algorithm Stein variational gradient descent. Our general framework supports multiple types of constrained optimizers and can handle arbitrary constraints. We demonstrate on a variety of problems that we are able to learn to approximate distributions without violating constraints. Specifically, we show that we can build distributions of: robot motion plans that exactly avoid collisions, robot arm joint angles on the SE(3) manifold with exact table placement constraints, and object poses from point clouds with table placement constraints.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PackHero: A Scalable Graph-based Approach for Efficient Packer Identification</title>
<link>https://arxiv.org/abs/2506.00659</link>
<guid>https://arxiv.org/abs/2506.00659</guid>
<content:encoded><![CDATA[
arXiv:2506.00659v1 Announce Type: cross 
Abstract: Anti-analysis techniques, particularly packing, challenge malware analysts, making packer identification fundamental. Existing packer identifiers have significant limitations: signature-based methods lack flexibility and struggle against dynamic evasion, while Machine Learning approaches require extensive training data, limiting scalability and adaptability. Consequently, achieving accurate and adaptable packer identification remains an open problem. This paper presents PackHero, a scalable and efficient methodology for identifying packers using a novel static approach. PackHero employs a Graph Matching Network and clustering to match and group Call Graphs from programs packed with known packers. We evaluate our approach on a public dataset of malware and benign samples packed with various packers, demonstrating its effectiveness and scalability across varying sample sizes. PackHero achieves a macro-average F1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100 samples. Notably, PackHero requires fewer samples to achieve stable performance compared to other Machine Learning-based tools. Overall, PackHero matches the performance of State-of-the-art signature-based tools, outperforming them in handling Virtualization-based packers such as Themida/Winlicense, with a recall of 100%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout</title>
<link>https://arxiv.org/abs/2506.00662</link>
<guid>https://arxiv.org/abs/2506.00662</guid>
<content:encoded><![CDATA[
arXiv:2506.00662v1 Announce Type: cross 
Abstract: INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating robust classification from genomic data. METHODS: We developed a transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for uncertainty estimation in AD classification from whole-genome sequencing (WGS). We combined a transformer that preserves single-nucleotide polymorphism (SNP) sequence structure with a concurrent random forest using flattened genotypes. An uncertainty threshold separated samples into an uncertain (high-variance) group and a more certain (low-variance) group. RESULTS: We analyzed 1050 individuals, holding out half for testing. Overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636, respectively. Excluding the uncertain group improved accuracy from 0.6263 to 0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase). DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus improving reliability in AD genomic classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.00664</link>
<guid>https://arxiv.org/abs/2506.00664</guid>
<content:encoded><![CDATA[
arXiv:2506.00664v1 Announce Type: cross 
Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization</title>
<link>https://arxiv.org/abs/2506.00674</link>
<guid>https://arxiv.org/abs/2506.00674</guid>
<content:encoded><![CDATA[
arXiv:2506.00674v1 Announce Type: cross 
Abstract: The Boolean satisfiability (SAT) problem lies at the core of many applications in combinatorial optimization, software verification, cryptography, and machine learning. While state-of-the-art solvers have demonstrated high efficiency in handling conjunctive normal form (CNF) formulas, numerous applications require non-CNF (hybrid) constraints, such as XOR, cardinality, and Not-All-Equal constraints. Recent work leverages polynomial representations to represent such hybrid constraints, but it relies on box constraints that can limit the use of powerful unconstrained optimizers. In this paper, we propose unconstrained continuous optimization formulations for hybrid SAT solving by penalty terms. We provide theoretical insights into when these penalty terms are necessary and demonstrate empirically that unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid benchmarks. Our results highlight the potential of combining continuous optimization and machine-learning-based methods for effective hybrid SAT solving.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Upsample and Upmix Audio in the Latent Domain</title>
<link>https://arxiv.org/abs/2506.00681</link>
<guid>https://arxiv.org/abs/2506.00681</guid>
<content:encoded><![CDATA[
arXiv:2506.00681v1 Announce Type: cross 
Abstract: Neural audio autoencoders create compact latent representations that preserve perceptually important information, serving as the foundation for both modern audio compression systems and generation approaches like next-token prediction and latent diffusion. Despite their prevalence, most audio processing operations, such as spatial and spectral up-sampling, still inefficiently operate on raw waveforms or spectral representations rather than directly on these compressed representations. We propose a framework that performs audio processing operations entirely within an autoencoder's latent space, eliminating the need to decode to raw audio formats. Our approach dramatically simplifies training by operating solely in the latent domain, with a latent L1 reconstruction term, augmented by a single latent adversarial discriminator. This contrasts sharply with raw-audio methods that typically require complex combinations of multi-scale losses and discriminators. Through experiments in bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational efficiency gains of up to 100x while maintaining quality comparable to post-processing on raw audio. This work establishes a more efficient paradigm for audio processing pipelines that already incorporate autoencoders, enabling significantly faster and more resource-efficient workflows across various audio tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments</title>
<link>https://arxiv.org/abs/2506.00694</link>
<guid>https://arxiv.org/abs/2506.00694</guid>
<content:encoded><![CDATA[
arXiv:2506.00694v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Project page: https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Centric Token Interpretation for Vector-Quantized Generative Models</title>
<link>https://arxiv.org/abs/2506.00698</link>
<guid>https://arxiv.org/abs/2506.00698</guid>
<content:encoded><![CDATA[
arXiv:2506.00698v1 Announce Type: cross 
Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for image generation. However, the key component of VQGMs -- the codebook of discrete tokens -- is still not well understood, e.g., which tokens are critical to generate an image of a certain concept? This paper introduces Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting VQGMs by identifying concept-specific token combinations. Our framework employs two methods: (1) a sample-level explanation method that analyzes token importance scores in individual images, and (2) a codebook-level explanation method that explores the entire codebook to find globally relevant tokens. Experimental results demonstrate CORTEX's efficacy in providing clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX is useful in applications such as targeted image editing and shortcut feature detection. Our code is available at https://github.com/YangTianze009/CORTEX.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[
arXiv:2506.00708v1 Announce Type: cross 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[
arXiv:2506.00713v1 Announce Type: cross 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Inpainted Objects In-N-Out of Context</title>
<link>https://arxiv.org/abs/2506.00721</link>
<guid>https://arxiv.org/abs/2506.00721</guid>
<content:encoded><![CDATA[
arXiv:2506.00721v1 Announce Type: cross 
Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel dataset addressing the scarcity of out-of-context examples in existing vision datasets. By systematically replacing objects in COCO images through diffusion-based inpainting, we create 97,722 unique images featuring both contextually coherent and inconsistent scenes, enabling effective context learning. Each inpainted object is meticulously verified and categorized as in- or out-of-context through a multimodal large language model assessment. Our analysis reveals significant patterns in semantic priors that influence inpainting success across object categories. We demonstrate three key tasks enabled by COinCO: (1) training context classifiers that effectively determine whether existing objects belong in their context; (2) a novel Objects-from-Context prediction task that determines which new objects naturally belong in given scenes at both instance and clique levels, and (3) context-enhanced fake detection on state-of-the-art methods without fine-tuning. COinCO provides a controlled testbed with contextual variations, establishing a foundation for advancing context-aware visual understanding in computer vision and image forensics. Our code and data are at: https://github.com/YangTianze009/COinCO.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra</title>
<link>https://arxiv.org/abs/2506.00725</link>
<guid>https://arxiv.org/abs/2506.00725</guid>
<content:encoded><![CDATA[
arXiv:2506.00725v1 Announce Type: cross 
Abstract: Defects are ubiquitous in solids and strongly influence materials' mechanical and functional properties. However, non-destructive characterization and quantification of defects, especially when multiple types coexist, remain a long-standing challenge. Here we introduce DefectNet, a foundation machine learning model that predicts the chemical identity and concentration of substitutional point defects with multiple coexisting elements directly from vibrational spectra, specifically phonon density-of-states (PDoS). Trained on over 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a tailored attention mechanism to identify up to six distinct defect elements at concentrations ranging from 0.2% to 25%. The model generalizes well to unseen crystals across 56 elements and can be fine-tuned on experimental data. Validation using inelastic scattering measurements of SiGe alloys and MgB$_2$ superconductor demonstrates its accuracy and transferability. Our work establishes vibrational spectroscopy as a viable, non-destructive probe for point defect quantification in bulk materials, and highlights the promise of foundation models in data-driven defect engineering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?</title>
<link>https://arxiv.org/abs/2506.00751</link>
<guid>https://arxiv.org/abs/2506.00751</guid>
<content:encoded><![CDATA[
arXiv:2506.00751v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[
arXiv:2506.00785v1 Announce Type: cross 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer</title>
<link>https://arxiv.org/abs/2506.00800</link>
<guid>https://arxiv.org/abs/2506.00800</guid>
<content:encoded><![CDATA[
arXiv:2506.00800v1 Announce Type: cross 
Abstract: Automated Audio Captioning (AAC) aims to describe the semantic contexts of general sounds, including acoustic events and scenes, by leveraging effective acoustic features. To enhance performance, an AAC method, EnCLAP, employed discrete tokens from EnCodec as an effective input for fine-tuning a language model BART. However, EnCodec is designed to reconstruct waveforms rather than capture the semantic contexts of general sounds, which AAC should describe. To address this issue, we propose CLAP-ART, an AAC method that utilizes ``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich discrete tokens from pre-trained audio representations through vector quantization. We experimentally confirmed that CLAP-ART outperforms baseline EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens derived from semantically rich AR are beneficial for AAC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning</title>
<link>https://arxiv.org/abs/2506.00813</link>
<guid>https://arxiv.org/abs/2506.00813</guid>
<content:encoded><![CDATA[
arXiv:2506.00813v1 Announce Type: cross 
Abstract: Tabular-image multimodal learning, which integrates structured tabular data with imaging data, holds great promise for a variety of tasks, especially in medical applications. Yet, two key challenges remain: (1) the lack of a standardized, pretrained representation for tabular data, as is commonly available in vision and language domains; and (2) the difficulty of handling missing values in the tabular modality, which are common in real-world medical datasets. To address these issues, we propose the TabPFN-Integrated Multimodal Engine (TIME), a novel multimodal framework that builds on the recently introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen tabular encoder to generate robust, strong embeddings that are naturally resilient to missing data, and combines them with image features from pretrained vision backbones. We explore a range of fusion strategies and tabular encoders, and evaluate our approach on both natural and medical datasets. Extensive experiments demonstrate that TIME consistently outperforms competitive baselines across both complete and incomplete tabular inputs, underscoring its practical value in real-world multimodal learning scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Linear Markov Decision Process</title>
<link>https://arxiv.org/abs/2506.00818</link>
<guid>https://arxiv.org/abs/2506.00818</guid>
<content:encoded><![CDATA[
arXiv:2506.00818v1 Announce Type: cross 
Abstract: The linear Markov Decision Process (MDP) framework offers a principled foundation for reinforcement learning (RL) with strong theoretical guarantees and sample efficiency. However, its restrictive assumption-that both transition dynamics and reward functions are linear in the same feature space-limits its applicability in real-world domains, where rewards often exhibit nonlinear or discrete structures. Motivated by applications such as healthcare and e-commerce, where data is scarce and reward signals can be binary or count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an extension of the linear MDP framework-that models rewards using generalized linear models (GLMs) while maintaining linear transition dynamics. We establish the Bellman completeness of GLMDPs with respect to a new function class that accommodates nonlinear rewards and develop two offline RL algorithms: Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant (SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our algorithms achieve theoretical guarantees on policy suboptimality and demonstrate improved sample efficiency in settings where reward labels are expensive or limited.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[
arXiv:2506.00826v1 Announce Type: cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System</title>
<link>https://arxiv.org/abs/2506.00828</link>
<guid>https://arxiv.org/abs/2506.00828</guid>
<content:encoded><![CDATA[
arXiv:2506.00828v1 Announce Type: cross 
Abstract: In a single-slot recommendation system, users are only exposed to one item at a time, and the system cannot collect user feedback on multiple items simultaneously. Therefore, only pointwise modeling solutions can be adopted, focusing solely on modeling the likelihood of clicks or conversions for items by users to learn user-item preferences, without the ability to capture the ranking information among different items directly. However, since user-side information is often much more abundant than item-side information, the model can quickly learn the differences in user intrinsic tendencies, which are independent of the items they are exposed to. This can cause these intrinsic tendencies to become a shortcut bias for the model, leading to insufficient mining of the most concerned user-item preferences. To solve this challenge, we introduce the Breaker model. Breaker integrates an auxiliary task of user representation clustering with a multi-tower structure for cluster-specific preference modeling. By clustering user representations, we ensure that users within each cluster exhibit similar characteristics, which increases the complexity of the pointwise recommendation task on the user side. This forces the multi-tower structure with cluster-driven parameter learning to better model user-item preferences, ultimately eliminating shortcut biases related to user intrinsic tendencies. In terms of training, we propose a delayed parameter update mechanism to enhance training stability and convergence, enabling end-to-end joint training of the auxiliary clustering and classification tasks. Both offline and online experiments demonstrate that our method surpasses the baselines. It has already been deployed and is actively serving tens of millions of users daily on Meituan, one of the most popular e-commerce platforms for services.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPKE: Complex Question Answering under Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.00829</link>
<guid>https://arxiv.org/abs/2506.00829</guid>
<content:encoded><![CDATA[
arXiv:2506.00829v1 Announce Type: cross 
Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Path Guiding with Distribution Factorization</title>
<link>https://arxiv.org/abs/2506.00839</link>
<guid>https://arxiv.org/abs/2506.00839</guid>
<content:encoded><![CDATA[
arXiv:2506.00839v1 Announce Type: cross 
Abstract: In this paper, we present a neural path guiding method to aid with Monte Carlo (MC) integration in rendering. Existing neural methods utilize distribution representations that are either fast or expressive, but not both. We propose a simple, but effective, representation that is sufficiently expressive and reasonably fast. Specifically, we break down the 2D distribution over the directional domain into two 1D probability distribution functions (PDF). We propose to model each 1D PDF using a neural network that estimates the distribution at a set of discrete coordinates. The PDF at an arbitrary location can then be evaluated and sampled through interpolation. To train the network, we maximize the similarity of the learned and target distributions. To reduce the variance of the gradient during optimizations and estimate the normalization factor, we propose to cache the incoming radiance using an additional network. Through extensive experiments, we demonstrate that our approach is better than the existing methods, particularly in challenging scenes with complex light transport.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[
arXiv:2506.00854v1 Announce Type: cross 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models</title>
<link>https://arxiv.org/abs/2506.00863</link>
<guid>https://arxiv.org/abs/2506.00863</guid>
<content:encoded><![CDATA[
arXiv:2506.00863v1 Announce Type: cross 
Abstract: Emotion recognition in low-resource languages like Marathi remains challenging due to limited annotated data. We present L3Cube-MahaEmotions, a high-quality Marathi emotion recognition dataset with 11 fine-grained emotion labels. The training data is synthetically annotated using large language models (LLMs), while the validation and test sets are manually labeled to serve as a reliable gold-standard benchmark. Building on the MahaSent dataset, we apply the Chain-of-Translation (CoTR) prompting technique, where Marathi sentences are translated into English and emotion labeled via a single prompt. GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data annotation due to superior label quality. We evaluate model performance using standard metrics and explore label aggregation strategies (e.g., Union, Intersection). While GPT-4 predictions outperform fine-tuned BERT models, BERT-based models trained on synthetic labels fail to surpass GPT-4. This highlights both the importance of high-quality human-labeled data and the inherent complexity of emotion recognition. An important finding of this work is that generic LLMs like GPT-4 and Llama3-405B generalize better than fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection Pursuit Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2506.00866</link>
<guid>https://arxiv.org/abs/2506.00866</guid>
<content:encoded><![CDATA[
arXiv:2506.00866v1 Announce Type: cross 
Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for its broad applications across multiple domains, such as covariate shift adaptation, causal inference, independence tests and beyond. Parametric methods for estimating the density ratio possibly lead to biased results if models are misspecified, while conventional non-parametric methods suffer from the curse of dimensionality when the dimension of data is large. To address these challenges, in this paper, we propose a novel approach for DRE based on the projection pursuit (PP) approximation. The proposed method leverages PP to mitigate the impact of high dimensionality while retaining the model flexibility needed for the accuracy of DRE. We establish the consistency and the convergence rate for the proposed estimator. Experimental results demonstrate that our proposed method outperforms existing alternatives in various applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODEMENV: Benchmarking Large Language Models on Code Migration</title>
<link>https://arxiv.org/abs/2506.00894</link>
<guid>https://arxiv.org/abs/2506.00894</guid>
<content:encoded><![CDATA[
arXiv:2506.00894v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras</title>
<link>https://arxiv.org/abs/2506.00904</link>
<guid>https://arxiv.org/abs/2506.00904</guid>
<content:encoded><![CDATA[
arXiv:2506.00904v1 Announce Type: cross 
Abstract: The construction industry faces significant challenges in optimizing equipment utilization, as underused machinery leads to increased operational costs and project delays. Accurate and timely monitoring of equipment activity is therefore key to identifying idle periods and improving overall efficiency. This paper presents the Edge-IMI framework for detecting idle construction machinery, specifically designed for integration with surveillance camera systems. The proposed solution consists of three components: object detection, tracking, and idle state identification, which are tailored for execution on resource-constrained, CPU-based edge computing devices. The performance of Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS benchmarks. Experimental results confirm that the object detector achieves an F1 score of 71.75%, indicating robust real-world detection capabilities. The logistic regression-based idle identification module reliably distinguishes between active and idle machinery with minimal false positives. Integrating all three modules, Edge-IMI enables efficient on-site inference, reducing reliance on high-bandwidth cloud services and costly hardware accelerators. We also evaluate the performance of object detection models on Raspberry Pi 5 and an Intel NUC platforms, as example edge computing platforms. We assess the feasibility of real-time processing and the impact of model optimization techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search</title>
<link>https://arxiv.org/abs/2506.00925</link>
<guid>https://arxiv.org/abs/2506.00925</guid>
<content:encoded><![CDATA[
arXiv:2506.00925v1 Announce Type: cross 
Abstract: Designing protein sequences that fold into a target 3D structure, known as protein inverse folding, is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by expanding the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise</title>
<link>https://arxiv.org/abs/2506.00933</link>
<guid>https://arxiv.org/abs/2506.00933</guid>
<content:encoded><![CDATA[
arXiv:2506.00933v1 Announce Type: cross 
Abstract: Integral equations are widely used in fields such as applied modeling, medical imaging, and system identification, providing a powerful framework for solving deterministic problems. While parameter identification for differential equations has been extensively studied, the focus on integral equations, particularly stochastic Volterra integral equations, remains limited. This research addresses the parameter identification problem, also known as the equation reconstruction problem, in Volterra integral equations driven by Gaussian noise. We propose an improved deep neural networks framework for estimating unknown parameters in the drift term of these equations. The network represents the primary variables and their integrals, enhancing parameter estimation accuracy by incorporating inter-output relationships into the loss function. Additionally, the framework extends beyond parameter identification to predict the system's behavior outside the integration interval. Prediction accuracy is validated by comparing predicted and true trajectories using a 95% confidence interval. Numerical experiments demonstrate the effectiveness of the proposed deep neural networks framework in both parameter identification and prediction tasks, showing robust performance under varying noise levels and providing accurate solutions for modeling stochastic systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: From Ad-hoc to Proactive Search in Conversations</title>
<link>https://arxiv.org/abs/2506.00983</link>
<guid>https://arxiv.org/abs/2506.00983</guid>
<content:encoded><![CDATA[
arXiv:2506.00983v1 Announce Type: cross 
Abstract: Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Local Intrinsic Dimensions of Contextual Language Models</title>
<link>https://arxiv.org/abs/2506.01034</link>
<guid>https://arxiv.org/abs/2506.01034</guid>
<content:encoded><![CDATA[
arXiv:2506.01034v1 Announce Type: cross 
Abstract: Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[
arXiv:2506.01062v1 Announce Type: cross 
Abstract: We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety</title>
<link>https://arxiv.org/abs/2506.01069</link>
<guid>https://arxiv.org/abs/2506.01069</guid>
<content:encoded><![CDATA[
arXiv:2506.01069v1 Announce Type: cross 
Abstract: Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning DNF through Generalized Fourier Representations</title>
<link>https://arxiv.org/abs/2506.01075</link>
<guid>https://arxiv.org/abs/2506.01075</guid>
<content:encoded><![CDATA[
arXiv:2506.01075v1 Announce Type: cross 
Abstract: The Fourier representation for the uniform distribution over the Boolean cube has found numerous applications in algorithms and complexity analysis. Notably, in learning theory, learnability of Disjunctive Normal Form (DNF) under uniform as well as product distributions has been established through such representations. This paper makes five main contributions. First, it introduces a generalized Fourier expansion that can be used with any distribution $D$ through the representation of the distribution as a Bayesian network (BN). Second, it shows that the main algorithmic tools for learning with the Fourier representation, that use membership queries to approximate functions by recovering their heavy Fourier coefficients, can be used with slight modifications with the generalized expansion. These results hold for any distribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under the new expansion, showing that it is bounded for a class of distributions which can be represented by difference bounded tree BN, where a parent node in the BN representation can change the conditional expectation of a child node by at most $\alpha<0.5$. Lower bounds are presented to show that such constraints are necessary. The fourth contribution uses these results to show the learnability of DNF with membership queries under difference bounded tree BN. The final contribution is to develop an algorithm for learning difference-bounded tree BN distributions, thus extending the DNF learnability result to cases where the distribution is not known in advance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion posterior sampling for informative likelihoods</title>
<link>https://arxiv.org/abs/2506.01083</link>
<guid>https://arxiv.org/abs/2506.01083</guid>
<content:encoded><![CDATA[
arXiv:2506.01083v1 Announce Type: cross 
Abstract: Sequential Monte Carlo (SMC) methods have recently shown successful results for conditional sampling of generative diffusion models. In this paper we propose a new diffusion posterior SMC sampler achieving improved statistical efficiencies, particularly under outlier conditions or highly informative likelihoods. The key idea is to construct an observation path that correlates with the diffusion model and to design the sampler to leverage this correlation for more efficient sampling. Empirical results conclude the efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression</title>
<link>https://arxiv.org/abs/2506.01084</link>
<guid>https://arxiv.org/abs/2506.01084</guid>
<content:encoded><![CDATA[
arXiv:2506.01084v1 Announce Type: cross 
Abstract: Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\%, with significant improvements in inference latency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization</title>
<link>https://arxiv.org/abs/2506.01143</link>
<guid>https://arxiv.org/abs/2506.01143</guid>
<content:encoded><![CDATA[
arXiv:2506.01143v1 Announce Type: cross 
Abstract: Modern machine learning models are often trained in a setting where the number of parameters exceeds the number of training samples. To understand the implicit bias of gradient descent in such overparameterized models, prior work has studied diagonal linear neural networks in the regression setting. These studies have shown that, when initialized with small weights, gradient descent tends to favor solutions with minimal $\ell^1$-norm - an effect known as implicit regularization. In this paper, we investigate implicit regularization in diagonal linear neural networks of depth $D\ge 2$ for overparameterized linear regression problems. We focus on analyzing the approximation error between the limit point of gradient flow trajectories and the solution to the $\ell^1$-minimization problem. By deriving tight upper and lower bounds on the approximation error, we precisely characterize how the approximation error depends on the scale of initialization $\alpha$. Our results reveal a qualitative difference between depths: for $D \ge 3$, the error decreases linearly with $\alpha$, whereas for $D=2$, it decreases at rate $\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly characterized. Interestingly, this parameter is closely linked to so-called null space property constants studied in the sparse recovery literature. We demonstrate the asymptotic tightness of our bounds through explicit examples. Numerical experiments corroborate our theoretical findings and suggest that deeper networks, i.e., $D \ge 3$, may lead to better generalization, particularly for realistic initialization scales.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition</title>
<link>https://arxiv.org/abs/2506.01147</link>
<guid>https://arxiv.org/abs/2506.01147</guid>
<content:encoded><![CDATA[
arXiv:2506.01147v1 Announce Type: cross 
Abstract: System-generated logs are typically converted into categorical log templates through parsing. These templates are crucial for generating actionable insights in various downstream tasks. However, existing parsers often fail to capture fine-grained template details, leading to suboptimal accuracy and reduced utility in downstream tasks requiring precise pattern identification. We propose a character-level log parser utilizing a novel neural architecture that aggregates character embeddings. Our approach estimates a sequence of binary-coded decimals to achieve highly granular log templates extraction. Our low-resource character-level parser, tested on revised Loghub-2k and a manually annotated industrial dataset, matches LLM-based parsers in accuracy while outperforming semantic parsers in efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor</title>
<link>https://arxiv.org/abs/2506.01162</link>
<guid>https://arxiv.org/abs/2506.01162</guid>
<content:encoded><![CDATA[
arXiv:2506.01162v1 Announce Type: cross 
Abstract: Estimating the density of a distribution from its samples is a fundamental problem in statistics. Hypothesis selection addresses the setting where, in addition to a sample set, we are given $n$ candidate distributions -- referred to as hypotheses -- and the goal is to determine which one best describes the underlying data distribution. This problem is known to be solvable very efficiently, requiring roughly $O(\log n)$ samples and running in $\tilde{O}(n)$ time. The quality of the output is measured via the total variation distance to the unknown distribution, and the approximation factor of the algorithm determines how large this distance is compared to the optimal distance achieved by the best candidate hypothesis. It is known that $\alpha = 3$ is the optimal approximation factor for this problem. We study hypothesis selection under the constraint of differential privacy. We propose a differentially private algorithm in the central model that runs in nearly-linear time with respect to the number of hypotheses, achieves the optimal approximation factor, and incurs only a modest increase in sample complexity, which remains polylogarithmic in $n$. This resolves an open question posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work, existing upper bounds required quadratic time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration</title>
<link>https://arxiv.org/abs/2506.01166</link>
<guid>https://arxiv.org/abs/2506.01166</guid>
<content:encoded><![CDATA[
arXiv:2506.01166v1 Announce Type: cross 
Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network DNN accelerators - particularly important for emerging Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate MAC units. The proposed architecture achieves saving by 37% and 68% in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFBench: An Extensive Benchmark for Fatigue Analysis</title>
<link>https://arxiv.org/abs/2506.01173</link>
<guid>https://arxiv.org/abs/2506.01173</guid>
<content:encoded><![CDATA[
arXiv:2506.01173v1 Announce Type: cross 
Abstract: Fatigue-induced crack growth is a leading cause of structural failure across critical industries such as aerospace, civil engineering, automotive, and energy. Accurate prediction of stress intensity factors (SIFs) -- the key parameters governing crack propagation in linear elastic fracture mechanics -- is essential for assessing fatigue life and ensuring structural integrity. While machine learning (ML) has shown great promise in SIF prediction, its advancement has been severely limited by the lack of rich, transparent, well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale benchmark database designed to support ML-based SIF prediction. SIFBench contains over 5 million different crack and component geometries derived from high-fidelity finite element simulations across 37 distinct scenarios, and provides a unified Python interface for seamless data access and customization. We report baseline results using a range of popular ML models -- including random forests, support vector machines, feedforward neural networks, and Fourier neural operators -- alongside comprehensive evaluation metrics and template code for model training, validation, and assessment. By offering a standardized and scalable resource, SIFBench substantially lowers the entry barrier and fosters the development and application of ML methods in damage tolerance design and predictive maintenance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data</title>
<link>https://arxiv.org/abs/2506.01189</link>
<guid>https://arxiv.org/abs/2506.01189</guid>
<content:encoded><![CDATA[
arXiv:2506.01189v1 Announce Type: cross 
Abstract: Despite progress in the rapidly developing field of geometric deep learning, performing statistical analysis on geometric data--where each observation is a shape such as a curve, graph, or surface--remains challenging due to the non-Euclidean nature of shape spaces, which are defined as equivalence classes under invariance groups. Building machine learning frameworks that incorporate such invariances, notably to shape parametrization, is often crucial to ensure generalizability of the trained models to new observations. This work proposes SVarM to exploit varifold representations of shapes as measures and their duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$. This method provides a general framework akin to linear support vector machines but operating instead over the infinite-dimensional space of varifolds. We develop classification and regression models on shape datasets by introducing a neural network-based representation of the trainable test function $h$. This approach demonstrates strong performance and robustness across various shape graph and surface datasets, achieving results comparable to state-of-the-art methods while significantly reducing the number of trainable parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures</title>
<link>https://arxiv.org/abs/2506.01197</link>
<guid>https://arxiv.org/abs/2506.01197</guid>
<content:encoded><![CDATA[
arXiv:2506.01197v1 Announce Type: cross 
Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</title>
<link>https://arxiv.org/abs/2506.01215</link>
<guid>https://arxiv.org/abs/2506.01215</guid>
<content:encoded><![CDATA[
arXiv:2506.01215v1 Announce Type: cross 
Abstract: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Mixed Precision Quantization for Learned Image Compression</title>
<link>https://arxiv.org/abs/2506.01221</link>
<guid>https://arxiv.org/abs/2506.01221</guid>
<content:encoded><![CDATA[
arXiv:2506.01221v1 Announce Type: cross 
Abstract: Despite its improvements in coding performance compared to traditional codecs, Learned Image Compression (LIC) suffers from large computational costs for storage and deployment. Model quantization offers an effective solution to reduce the computational complexity of LIC models. However, most existing works perform fixed-precision quantization which suffers from sub-optimal utilization of resources due to the varying sensitivity to quantization of different layers of a neural network. In this paper, we propose a Flexible Mixed Precision Quantization (FMPQ) method that assigns different bit-widths to different layers of the quantized network using the fractional change in rate-distortion loss as the bit-assignment criterion. We also introduce an adaptive search algorithm which reduces the time-complexity of searching for the desired distribution of quantization bit-widths given a fixed model size. Evaluation of our method shows improved BD-Rate performance under similar model size constraints compared to other works on quantization of LIC models. We have made the source code available at gitlab.com/viper-purdue/fmpq.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN</title>
<link>https://arxiv.org/abs/2506.01226</link>
<guid>https://arxiv.org/abs/2506.01226</guid>
<content:encoded><![CDATA[
arXiv:2506.01226v1 Announce Type: cross 
Abstract: We study parameterizations of stabilizing nonlinear policies for learning-based control. We propose a structure based on a nonlinear version of the Youla-Ku\v{c}era parameterization combined with robust neural networks such as the recurrent equilibrium network (REN). The resulting parameterizations are unconstrained, and hence can be searched over with first-order optimization methods, while always ensuring closed-loop stability by construction. We study the combination of (a) nonlinear dynamics, (b) partial observation, and (c) incremental closed-loop stability requirements (contraction and Lipschitzness). We find that with any two of these three difficulties, a contracting and Lipschitz Youla parameter always leads to contracting and Lipschitz closed loops. However, if all three hold, then incremental stability can be lost with exogenous disturbances. Instead, a weaker condition is maintained, which we call d-tube contraction and Lipschitzness. We further obtain converse results showing that the proposed parameterization covers all contracting and Lipschitz closed loops for certain classes of nonlinear systems. Numerical experiments illustrate the utility of our parameterization when learning controllers with built-in stability certificates for: i) ``economic'' rewards without stabilizing effects; ii) short training horizons; and iii) uncertain systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors</title>
<link>https://arxiv.org/abs/2506.01247</link>
<guid>https://arxiv.org/abs/2506.01247</guid>
<content:encoded><![CDATA[
arXiv:2506.01247v1 Announce Type: cross 
Abstract: Steering vision foundation models at inference time without retraining or access to large labeled datasets is a desirable yet challenging objective, particularly in dynamic or resource-constrained settings. In this paper, we introduce Visual Sparse Steering (VS2), a lightweight, test-time method that guides vision models using steering vectors derived from sparse features learned by top-$k$ Sparse Autoencoders without requiring contrastive data. Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a retrieval-augmented variant that selectively amplifies relevant sparse features using pseudo-labeled neighbors at inference time. With oracle positive/negative sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44% on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2 and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing that sparse steering benefits specific classes by disambiguating visually or taxonomically proximate categories rather than providing a uniform boost. Finally, to better align the sparse features learned through the SAE reconstruction task with those relevant for downstream performance, we propose Prototype-Aligned Sparse Steering (PASS). By incorporating a prototype-alignment loss during SAE training, using labels only during training while remaining fully test-time unsupervised, PASS consistently, though modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100 with ViT-B/32.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence intervals for forced alignment boundaries using model ensembles</title>
<link>https://arxiv.org/abs/2506.01256</link>
<guid>https://arxiv.org/abs/2506.01256</guid>
<content:encoded><![CDATA[
arXiv:2506.01256v1 Announce Type: cross 
Abstract: Forced alignment is a common tool to align audio with orthographic and phonetic transcriptions. Most forced alignment tools provide only a single estimate of a boundary. The present project introduces a method of deriving confidence intervals for these boundaries using a neural network ensemble technique. Ten different segment classifier neural networks were previously trained, and the alignment process is repeated with each model. The alignment ensemble is then used to place the boundary at the median of the boundaries in the ensemble, and 97.85% confidence intervals are constructed using order statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a slight improvement over using just a single model. The confidence intervals are incorporated into Praat TextGrids using a point tier, and they are also output as a table for researchers to analyze separately as diagnostics or to incorporate uncertainty into their analyses.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation</title>
<link>https://arxiv.org/abs/2506.01267</link>
<guid>https://arxiv.org/abs/2506.01267</guid>
<content:encoded><![CDATA[
arXiv:2506.01267v1 Announce Type: cross 
Abstract: Despite tremendous advancements of machine learning models and algorithms in various application domains, they are known to be vulnerable to subtle, natural or intentionally crafted perturbations in future input data, known as adversarial attacks. While numerous adversarial learning methods have been proposed, fundamental questions about their statistical optimality in robust loss remain largely unanswered. In particular, the minimax rate of convergence and the construction of rate-optimal estimators under future $X$-attacks are yet to be worked out.
  In this paper, we address this issue in the context of nonparametric regression, under suitable assumptions on the smoothness of the regression function and the geometric structure of the input perturbation set. We first establish the minimax rate of convergence under adversarial $L_q$-risks with $1 \leq q \leq \infty$ and propose a piecewise local polynomial estimator that achieves the minimax optimality. The established minimax rate elucidates how the smoothness level and perturbation magnitude affect the fundamental limit of adversarial learning under future $X$-attacks. Furthermore, we construct a data-driven adaptive estimator that is shown to achieve, within a logarithmic factor, the optimal rate across a broad scale of nonparametric and adversarial classes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction</title>
<link>https://arxiv.org/abs/2506.01268</link>
<guid>https://arxiv.org/abs/2506.01268</guid>
<content:encoded><![CDATA[
arXiv:2506.01268v1 Announce Type: cross 
Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework's single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \https://github.com/opendilab/CleanS2S.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable In-Context Q-Learning</title>
<link>https://arxiv.org/abs/2506.01299</link>
<guid>https://arxiv.org/abs/2506.01299</guid>
<content:encoded><![CDATA[
arXiv:2506.01299v1 Announce Type: cross 
Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Platform for Investigating Public Health Content with Efficient Concern Classification</title>
<link>https://arxiv.org/abs/2506.01308</link>
<guid>https://arxiv.org/abs/2506.01308</guid>
<content:encoded><![CDATA[
arXiv:2506.01308v1 Announce Type: cross 
Abstract: A recent rise in online content expressing concerns with public health initiatives has contributed to already stalled uptake of preemptive measures globally. Future public health efforts must attempt to understand such content, what concerns it may raise among readers, and how to effectively respond to it. To this end, we present ConcernScope, a platform that uses a teacher-student framework for knowledge transfer between large language models and light-weight classifiers to quickly and effectively identify the health concerns raised in a text corpus. The platform allows uploading massive files directly, automatically scraping specific URLs, and direct text editing. ConcernScope is built on top of a taxonomy of public health concerns. Intended for public health officials, we demonstrate several applications of this platform: guided data exploration to find useful examples of common concerns found in online community datasets, identification of trends in concerns through an example time series analysis of 186,000 samples, and finding trends in topic frequency before and after significant events.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Clustering in Mixture of Markov Chains</title>
<link>https://arxiv.org/abs/2506.01324</link>
<guid>https://arxiv.org/abs/2506.01324</guid>
<content:encoded><![CDATA[
arXiv:2506.01324v1 Announce Type: cross 
Abstract: We study the problem of clustering $T$ trajectories of length $H$, each generated by one of $K$ unknown ergodic Markov chains over a finite state space of size $S$. The goal is to accurately group trajectories according to their underlying generative model. We begin by deriving an instance-dependent, high-probability lower bound on the clustering error rate, governed by the weighted KL divergence between the transition kernels of the chains. We then present a novel two-stage clustering algorithm. In Stage~I, we apply spectral clustering using a new injective Euclidean embedding for ergodic Markov chains -- a contribution of independent interest that enables sharp concentration results. Stage~II refines the initial clusters via a single step of likelihood-based reassignment. Our method achieves a near-optimal clustering error with high probability, under the conditions $H = \tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH = \tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the minimum stationary probability of a state across the $K$ chains and $\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements provide significant improvements, if not at least comparable, to the state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm offers a key practical advantage: unlike existing approach, it requires no prior knowledge of model-specific quantities (e.g., separation between kernels or visitation probabilities). We conclude by discussing the inherent gap between our upper and lower bounds, providing insights into the unique structure of this clustering problem.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01347</link>
<guid>https://arxiv.org/abs/2506.01347</guid>
<content:encoded><![CDATA[
arXiv:2506.01347v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. We train Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum ($k$ up to $256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@$1$ but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@$k$ performance on MATH, AIME 2025, and AMC23. Our code is available at https://github.com/TianHongZXY/RLVR-Decomposed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoBrain: Synergizing Minds and Eyes For Human Action Understanding</title>
<link>https://arxiv.org/abs/2506.01353</link>
<guid>https://arxiv.org/abs/2506.01353</guid>
<content:encoded><![CDATA[
arXiv:2506.01353v1 Announce Type: cross 
Abstract: The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain --the world's first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a muiltimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Scientists Fail Without Strong Implementation Capability</title>
<link>https://arxiv.org/abs/2506.01372</link>
<guid>https://arxiv.org/abs/2506.01372</guid>
<content:encoded><![CDATA[
arXiv:2506.01372v1 Announce Type: cross 
Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Turbulence to Tranquility: AI-Driven Low-Altitude Network</title>
<link>https://arxiv.org/abs/2506.01378</link>
<guid>https://arxiv.org/abs/2506.01378</guid>
<content:encoded><![CDATA[
arXiv:2506.01378v1 Announce Type: cross 
Abstract: Low Altitude Economy (LAE) networks own transformative potential in urban mobility, emergency response, and aerial logistics. However, these networks face significant challenges in spectrum management, interference mitigation, and real-time coordination across dynamic and resource-constrained environments. After addressing these challenges, this study explores three core elements for enabling intelligent LAE networks as follows machine learning-based spectrum sensing and coexistence, artificial intelligence (AI)-optimized resource allocation and trajectory planning, and testbed-driven validation and standardization. We highlight how federated and reinforcement learning techniques support decentralized, adaptive decision-making under mobility and energy constraints. In addition, we discuss the role of real-world platforms such as AERPAW in bridging the gap between simulation and deployment and enabling iterative system refinement under realistic conditions. This study aims to provide a forward-looking roadmap toward developing efficient and interoperable AI-driven LAE ecosystems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Calls for Malware Detection and Classification: Methodologies and Applications</title>
<link>https://arxiv.org/abs/2506.01412</link>
<guid>https://arxiv.org/abs/2506.01412</guid>
<content:encoded><![CDATA[
arXiv:2506.01412v1 Announce Type: cross 
Abstract: As malware continues to become more complex and harder to detect, Malware Analysis needs to continue to evolve to stay one step ahead. One promising key area approach focuses on using system calls and API Calls, the core communication between user applications and the operating system and their kernels. These calls provide valuable insight into how software or programs behaves, making them an useful tool for spotting suspicious or harmful activity of programs and software. This chapter takes a deep down look at how system calls are used in malware detection and classification, covering techniques like static and dynamic analysis, as well as sandboxing. By combining these methods with advanced techniques like machine learning, statistical analysis, and anomaly detection, researchers can analyze system call patterns to tell the difference between normal and malicious behavior. The chapter also explores how these techniques are applied across different systems, including Windows, Linux, and Android, while also looking at the ways sophisticated malware tries to evade detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v1 Announce Type: cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title>
<link>https://arxiv.org/abs/2506.01420</link>
<guid>https://arxiv.org/abs/2506.01420</guid>
<content:encoded><![CDATA[
arXiv:2506.01420v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in sensitive domains, where their ability to infer personal data from seemingly benign text poses emerging privacy risks. While recent LLM-based anonymization methods help mitigate such risks, they often rely on proprietary models (e.g., GPT-4), raising concerns about cost and the potential exposure of sensitive data to untrusted external systems. To address this, we introduce SElf-refining Anonymization with Language model (SEAL), a novel distillation framework for training small language models (SLMs) to perform effective anonymization without relying on external costly models at inference time. We leverage adversarial interactions between an LLM anonymizer and an inference model to collect trajectories of anonymized texts and inferred attributes, which are used to distill anonymization, adversarial inference, and utility evaluation capabilities into SLMs via supervised fine-tuning and preference learning. The resulting models learn to both anonymize text and critique their outputs, enabling iterative improvement of anonymization quality via self-refinement. Experiments on SynthPAI, a dataset of synthetic personal profiles and text comments, demonstrate that SLMs trained with SEAL achieve substantial improvements in anonymization capabilities. Notably, 8B models attain a privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with self-refinement, even surpass it in terms of privacy. These results show the effectiveness of our adversarial distillation framework in training SLMs as efficient anonymizers. To facilitate further research, we release the full dataset used in our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes</title>
<link>https://arxiv.org/abs/2506.01456</link>
<guid>https://arxiv.org/abs/2506.01456</guid>
<content:encoded><![CDATA[
arXiv:2506.01456v1 Announce Type: cross 
Abstract: Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity</title>
<link>https://arxiv.org/abs/2506.01493</link>
<guid>https://arxiv.org/abs/2506.01493</guid>
<content:encoded><![CDATA[
arXiv:2506.01493v1 Announce Type: cross 
Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully scaled to billion-scale large text-to-image datasets. However, training such models entails a high training cost, limiting some applications and research usage. To reduce the cost, one promising direction is the incorporation of pre-trained models. The existing method of utilizing pre-trained models for a generator significantly reduced the training cost compared with the other large-scale GANs, but we found the model loses the diversity of generation for a given prompt by a large margin. To build an efficient and high-fidelity text-to-image GAN without compromise, we propose to use two specialized discriminators with Slicing Adversarial Networks (SANs) adapted for text-to-image tasks. Our proposed model, called SCAD, shows a notable enhancement in diversity for a given prompt with better sample fidelity. We also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID competitive with the latest large-scale GANs at two orders of magnitude less training cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiceMixer - Netlist-Level Circuit Evolution</title>
<link>https://arxiv.org/abs/2506.01497</link>
<guid>https://arxiv.org/abs/2506.01497</guid>
<content:encoded><![CDATA[
arXiv:2506.01497v1 Announce Type: cross 
Abstract: This paper introduces SpiceMixer, a genetic algorithm developed to synthesize novel analog circuits by evolving SPICE netlists. Unlike conventional methods, SpiceMixer operates directly on netlist lines, enabling compatibility with any component or subcircuit type and supporting general-purpose genetic operations. By using a normalized netlist format, the algorithm enhances the effectiveness of its genetic operators: crossover, mutation, and pruning. We show that SpiceMixer achieves superior performance in synthesizing standard cells (inverter, two-input NAND, and latch) and in designing an analog classifier circuit for the Iris dataset, reaching an accuracy of 89% on the test set. Across all evaluated tasks, SpiceMixer consistently outperforms existing synthesis methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing</title>
<link>https://arxiv.org/abs/2506.01566</link>
<guid>https://arxiv.org/abs/2506.01566</guid>
<content:encoded><![CDATA[
arXiv:2506.01566v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. However, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge devices. One promising approach to address this challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense processing of general matrix multiplications (GEMMs). FlexiSAGA supports seven different sparse and dense dataflows, enabling efficient processing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN/HW co-design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Dataset Distillation in the Wild</title>
<link>https://arxiv.org/abs/2506.01586</link>
<guid>https://arxiv.org/abs/2506.01586</guid>
<content:encoded><![CDATA[
arXiv:2506.01586v1 Announce Type: cross 
Abstract: Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading model performance. To these ends, we propose Multi-modal dataset Distillation in the Wild, i.e., MDW, the first framework to distill noisy multi-modal datasets into compact clean ones for effective and efficient model training. Specifically, MDW introduces learnable fine-grained correspondences during distillation and adaptively optimizes distilled data to emphasize correspondence-discriminative regions, thereby enhancing distilled data's information density and efficacy. Moreover, to capture robust cross-modal correspondence prior knowledge from real data, MDW proposes dual-track collaborative learning to avoid the risky data noise, alleviating information loss with certifiable noise tolerance. Extensive experiments validate MDW's theoretical and empirical efficacy with remarkable scalability, surpassing prior methods by over 15% across various compression ratios, highlighting its appealing practicality for applications with diverse efficacy and resource needs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy</title>
<link>https://arxiv.org/abs/2506.01602</link>
<guid>https://arxiv.org/abs/2506.01602</guid>
<content:encoded><![CDATA[
arXiv:2506.01602v1 Announce Type: cross 
Abstract: Word sense analysis is an essential analysis work for interpreting the linguistic and social backgrounds. The word sense change detection is a task of identifying and interpreting shifts in word meanings over time. This paper proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and quantify changes across time periods. This method enables both the identification of words undergoing sense shifts and the explanation of their evolution over multiple historical periods. To my knowledge, this is the first application of MMD to word sense change detection. Empirical assessment results demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title>
<link>https://arxiv.org/abs/2506.01608</link>
<guid>https://arxiv.org/abs/2506.01608</guid>
<content:encoded><![CDATA[
arXiv:2506.01608v1 Announce Type: cross 
Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech</title>
<link>https://arxiv.org/abs/2506.01618</link>
<guid>https://arxiv.org/abs/2506.01618</guid>
<content:encoded><![CDATA[
arXiv:2506.01618v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: https://github.com/idiap/RnV
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents need world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
arXiv:2506.01622v1 Announce Type: cross 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer</title>
<link>https://arxiv.org/abs/2506.01623</link>
<guid>https://arxiv.org/abs/2506.01623</guid>
<content:encoded><![CDATA[
arXiv:2506.01623v1 Announce Type: cross 
Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a related one with minimal relearning. In contrast, reinforcement learning (RL) agents typically require extensive retraining even when new tasks share structural similarities with previously learned ones. In this work, we propose MAGIK, a novel framework that enables RL agents to transfer knowledge to analogous tasks without interacting with the target environment. Our approach leverages an imagination mechanism to map entities in the target task to their analogues in the source domain, allowing the agent to reuse its original policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK achieves effective zero-shot transfer using only a small number of human-labelled examples. We compare our approach to related baselines and highlight how it offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Cooperation in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2506.01624</link>
<guid>https://arxiv.org/abs/2506.01624</guid>
<content:encoded><![CDATA[
arXiv:2506.01624v1 Announce Type: cross 
Abstract: The development of AI agents based on large, open-domain language models (LLMs) has paved the way for the development of general-purpose AI assistants that can support human in tasks such as writing, coding, graphic design, and scientific research. A major challenge with such agents is that, by necessity, they are trained by observing relatively short-term interactions with humans. Such models can fail to generalize to long-term interactions, for example, interactions where a user has repeatedly corrected mistakes on the part of the agent. In this work, we argue that these challenges can be overcome by explicitly modeling humans' social intelligence, that is, their ability to build and maintain long-term relationships with other agents whose behavior cannot always be predicted. By mathematically modeling the strategies humans use to communicate and reason about one another over long periods of time, we may be able to derive new game theoretic objectives against which LLMs and future AI agents may be optimized.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</title>
<link>https://arxiv.org/abs/2506.01635</link>
<guid>https://arxiv.org/abs/2506.01635</guid>
<content:encoded><![CDATA[
arXiv:2506.01635v1 Announce Type: cross 
Abstract: Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees</title>
<link>https://arxiv.org/abs/2506.01641</link>
<guid>https://arxiv.org/abs/2506.01641</guid>
<content:encoded><![CDATA[
arXiv:2506.01641v1 Announce Type: cross 
Abstract: In recent years, deep reinforcement learning (DRL) algorithms have gained traction in home energy management systems. However, their adoption by energy management companies remains limited due to the black-box nature of DRL, which fails to provide transparent decision-making feedback. To address this, explainable reinforcement learning (XRL) techniques have emerged, aiming to make DRL decisions more transparent. Among these, soft differential decision tree (DDT) distillation provides a promising approach due to the clear decision rules they are based on, which can be efficiently computed. However, achieving high performance often requires deep, and completely full, trees, which reduces interpretability. To overcome this, we propose a novel asymmetric soft DDT construction method. Unlike traditional soft DDTs, our approach adaptively constructs trees by expanding nodes only when necessary. This improves the efficient use of decision nodes, which require a predetermined depth to construct full symmetric trees, enhancing both interpretability and performance. We demonstrate the potential of asymmetric DDTs to provide transparent, efficient, and high-performing decision-making in home energy management systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge</title>
<link>https://arxiv.org/abs/2506.01646</link>
<guid>https://arxiv.org/abs/2506.01646</guid>
<content:encoded><![CDATA[
arXiv:2506.01646v1 Announce Type: cross 
Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, "DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engram Memory Encoding and Retrieval: A Neurocomputational Perspective</title>
<link>https://arxiv.org/abs/2506.01659</link>
<guid>https://arxiv.org/abs/2506.01659</guid>
<content:encoded><![CDATA[
arXiv:2506.01659v1 Announce Type: cross 
Abstract: Despite substantial research into the biological basis of memory, the precise mechanisms by which experiences are encoded, stored, and retrieved in the brain remain incompletely understood. A growing body of evidence supports the engram theory, which posits that sparse populations of neurons undergo lasting physical and biochemical changes to support long-term memory. Yet, a comprehensive computational framework that integrates biological findings with mechanistic models remains elusive. This work synthesizes insights from cellular neuroscience and computational modeling to address key challenges in engram research: how engram neurons are identified and manipulated; how synaptic plasticity mechanisms contribute to stable memory traces; and how sparsity promotes efficient, interference-resistant representations. Relevant computational approaches -- such as sparse regularization, engram gating, and biologically inspired architectures like Sparse Distributed Memory and spiking neural networks -- are also examined. Together, these findings suggest that memory efficiency, capacity, and stability emerge from the interaction of plasticity and sparsity constraints. By integrating neurobiological and computational perspectives, this paper provides a comprehensive theoretical foundation for engram research and proposes a roadmap for future inquiry into the mechanisms underlying memory, with implications for the diagnosis and treatment of memory-related disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI Systems Must Be Contestable: Here's How to Make It Happen</title>
<link>https://arxiv.org/abs/2506.01662</link>
<guid>https://arxiv.org/abs/2506.01662</guid>
<content:encoded><![CDATA[
arXiv:2506.01662v1 Announce Type: cross 
Abstract: As AI regulations around the world intensify their focus on system safety, contestability has become a mandatory, yet ill-defined, safeguard. In XAI, "contestability" remains an empty promise: no formal definition exists, no algorithm guarantees it, and practitioners lack concrete guidance to satisfy regulatory requirements. Grounded in a systematic literature review, this paper presents the first rigorous formal definition of contestability in explainable AI, directly aligned with stakeholder requirements and regulatory mandates. We introduce a modular framework of by-design and post-hoc mechanisms spanning human-centered interfaces, technical architectures, legal processes, and organizational workflows. To operationalize our framework, we propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria. Through multiple case studies across diverse application domains, we reveal where state-of-the-art systems fall short and show how our framework drives targeted improvements. By converting contestability from regulatory theory into a practical framework, our work equips practitioners with the tools to embed genuine recourse and accountability into AI systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of discrete-continuous quantum circuits with multimodal diffusion models</title>
<link>https://arxiv.org/abs/2506.01666</link>
<guid>https://arxiv.org/abs/2506.01666</guid>
<content:encoded><![CDATA[
arXiv:2506.01666v1 Announce Type: cross 
Abstract: Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts</title>
<link>https://arxiv.org/abs/2506.01685</link>
<guid>https://arxiv.org/abs/2506.01685</guid>
<content:encoded><![CDATA[
arXiv:2506.01685v1 Announce Type: cross 
Abstract: In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Descriptive and Normative Theory of Human Beliefs in RLHF</title>
<link>https://arxiv.org/abs/2506.01692</link>
<guid>https://arxiv.org/abs/2506.01692</guid>
<content:encoded><![CDATA[
arXiv:2506.01692v1 Announce Type: cross 
Abstract: Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent -- and resulting preferences -- for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the \textit{mismatch} between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signature Maximum Mean Discrepancy Two-Sample Statistical Tests</title>
<link>https://arxiv.org/abs/2506.01718</link>
<guid>https://arxiv.org/abs/2506.01718</guid>
<content:encoded><![CDATA[
arXiv:2506.01718v1 Announce Type: cross 
Abstract: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning research which has gained popularity in recent years as a highly effective tool for comparing (finite-dimensional) distributions. Since it is designed as a kernel-based method, the MMD can be extended to path space valued distributions using the signature kernel. The resulting signature MMD (sig-MMD) can be used to define a metric between distributions on path space. Similarly to the original use case of the MMD as a test statistic within a two-sample testing framework, the sig-MMD can be applied to determine if two sets of paths are drawn from the same stochastic process. This work is dedicated to understanding the possibilities and challenges associated with applying the sig-MMD as a statistical tool in practice. We introduce and explain the sig-MMD, and provide easily accessible and verifiable examples for its practical use. We present examples that can lead to Type 2 errors in the hypothesis test, falsely indicating that samples have been drawn from the same underlying process (which generally occurs in a limited data setting). We then present techniques to mitigate the occurrence of this type of error.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-assimilated model-informed reinforcement learning</title>
<link>https://arxiv.org/abs/2506.01755</link>
<guid>https://arxiv.org/abs/2506.01755</guid>
<content:encoded><![CDATA[
arXiv:2506.01755v1 Announce Type: cross 
Abstract: The control of spatio-temporally chaos is challenging because of high dimensionality and unpredictability. Model-free reinforcement learning (RL) discovers optimal control policies by interacting with the system, typically requiring observations of the full physical state.In practice, sensors often provide only partial and noisy measurements (observations) of the system. The objective of this paper is to develop a framework that enables the control of chaotic systems with partial and noisy observability. The proposed method, data-assimilated model-informed reinforcement learning (DA-MIRL), integrates (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct the model prediction when observations become available; and (iii) an off-policy actor-critic RL algorithm to adaptively learn an optimal control strategy based on the corrected state estimates. We test DA-MIRL on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation. We estimate the full state of the environment with (i) a physics-based model, here, a coarse-grained model; and (ii) a data-driven model, here, the control-aware echo state network, which is proposed in this paper. We show that DA-MIRL successfully estimates and suppresses the chaotic dynamics of the environment in real time from partial observations and approximate models. This work opens opportunities for the control of partially observable chaotic systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs</title>
<link>https://arxiv.org/abs/2506.01770</link>
<guid>https://arxiv.org/abs/2506.01770</guid>
<content:encoded><![CDATA[
arXiv:2506.01770v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning</title>
<link>https://arxiv.org/abs/2506.01778</link>
<guid>https://arxiv.org/abs/2506.01778</guid>
<content:encoded><![CDATA[
arXiv:2506.01778v1 Announce Type: cross 
Abstract: We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An adaptive data sampling strategy for stabilizing dynamical systems via controller inference</title>
<link>https://arxiv.org/abs/2506.01816</link>
<guid>https://arxiv.org/abs/2506.01816</guid>
<content:encoded><![CDATA[
arXiv:2506.01816v1 Announce Type: cross 
Abstract: Learning stabilizing controllers from data is an important task in engineering applications; however, collecting informative data is challenging because unstable systems often lead to rapidly growing or erratic trajectories. In this work, we propose an adaptive sampling scheme that generates data while simultaneously stabilizing the system to avoid instabilities during the data collection. Under mild assumptions, the approach provably generates data sets that are informative for stabilization and have minimal size. The numerical experiments demonstrate that controller inference with the novel adaptive sampling approach learns controllers with up to one order of magnitude fewer data samples than unguided data generation. The results show that the proposed approach opens the door to stabilizing systems in edge cases and limit states where instabilities often occur and data collection is inherently difficult.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fodor and Pylyshyn's Legacy - Still No Human-like Systematic Compositionality in Neural Networks</title>
<link>https://arxiv.org/abs/2506.01820</link>
<guid>https://arxiv.org/abs/2506.01820</guid>
<content:encoded><![CDATA[
arXiv:2506.01820v1 Announce Type: cross 
Abstract: Strong meta-learning capabilities for systematic compositionality are emerging as an important skill for navigating the complex and changing tasks of today's world. However, in presenting models for robust adaptation to novel environments, it is important to refrain from making unsupported claims about the performance of meta-learning systems that ultimately do not stand up to scrutiny. While Fodor and Pylyshyn famously posited that neural networks inherently lack this capacity as they are unable to model compositional representations or structure-sensitive operations, and thus are not a viable model of the human mind, Lake and Baroni recently presented meta-learning as a pathway to compositionality. In this position paper, we critically revisit this claim and highlight limitations in the proposed meta-learning framework for compositionality. Our analysis shows that modern neural meta-learning systems can only perform such tasks, if at all, under a very narrow and restricted definition of a meta-learning setup. We therefore claim that `Fodor and Pylyshyn's legacy' persists, and to date, there is no human-like systematic compositionality learned in neural networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Streaming Discrete Speech Units</title>
<link>https://arxiv.org/abs/2506.01845</link>
<guid>https://arxiv.org/abs/2506.01845</guid>
<content:encoded><![CDATA[
arXiv:2506.01845v1 Announce Type: cross 
Abstract: Discrete speech units (DSUs) are derived from clustering the features of self-supervised speech models (S3Ms). DSUs offer significant advantages for on-device streaming speech applications due to their rich phonetic information, high transmission efficiency, and seamless integration with large language models. However, conventional DSU-based approaches are impractical as they require full-length speech input and computationally expensive S3Ms. In this work, we reduce both the attention window and the model size while preserving the effectiveness of DSUs. Our results demonstrate that we can reduce floating-point operations (FLOPs) by 50% with only a relative increase of 6.5% in character error rate (CER) on the ML-SUPERB 1h dataset. These findings highlight the potential of DSUs for real-time speech processing in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs</title>
<link>https://arxiv.org/abs/2506.01850</link>
<guid>https://arxiv.org/abs/2506.01850</guid>
<content:encoded><![CDATA[
arXiv:2506.01850v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning thermodynamic master equations for open quantum systems</title>
<link>https://arxiv.org/abs/2506.01882</link>
<guid>https://arxiv.org/abs/2506.01882</guid>
<content:encoded><![CDATA[
arXiv:2506.01882v1 Announce Type: cross 
Abstract: The characterization of Hamiltonians and other components of open quantum dynamical systems plays a crucial role in quantum computing and other applications. Scientific machine learning techniques have been applied to this problem in a variety of ways, including by modeling with deep neural networks. However, the majority of mathematical models describing open quantum systems are linear, and the natural nonlinearities in learnable models have not been incorporated using physical principles. We present a data-driven model for open quantum systems that includes learnable, thermodynamically consistent terms. The trained model is interpretable, as it directly estimates the system Hamiltonian and linear components of coupling to the environment. We validate the model on synthetic two and three-level data, as well as experimental two-level data collected from a quantum device at Lawrence Livermore National Laboratory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States</title>
<link>https://arxiv.org/abs/2506.01891</link>
<guid>https://arxiv.org/abs/2506.01891</guid>
<content:encoded><![CDATA[
arXiv:2506.01891v1 Announce Type: cross 
Abstract: Neural Quantum States (NQS) are a class of variational wave functions parametrized by neural networks (NNs) to study quantum many-body systems. In this work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold Networks (KANs), to represent quantum mechanical wave functions as nested univariate functions. We show that \sk wavefunction with learnable sinusoidal activation functions can capture the ground state energies, fidelities and various correlation functions of the 1D Transverse-Field Ising model, Anisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with different chain lengths. In our study of the $J_1-J_2$ model with $L=100$ sites, we find that the SineKAN model outperforms several previously explored neural quantum state ans\"atze, including Restricted Boltzmann Machines (RBMs), Long Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN), when compared to the results obtained from the Density Matrix Renormalization Group (DMRG) algorithm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-Learned Sampling of Conditioned Path Measures</title>
<link>https://arxiv.org/abs/2506.01904</link>
<guid>https://arxiv.org/abs/2506.01904</guid>
<content:encoded><![CDATA[
arXiv:2506.01904v1 Announce Type: cross 
Abstract: We propose algorithms for sampling from posterior path measures $P(C([0, T], \mathbb{R}^d))$ under a general prior process. This leverages ideas from (1) controlled equilibrium dynamics, which gradually transport between two path measures, and (2) optimization in $\infty$-dimensional probability space endowed with a Wasserstein metric, which can be used to evolve a density curve under the specified likelihood. The resulting algorithms are theoretically grounded and can be integrated seamlessly with neural networks for learning the target trajectory ensembles, without access to data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation</title>
<link>https://arxiv.org/abs/2506.01923</link>
<guid>https://arxiv.org/abs/2506.01923</guid>
<content:encoded><![CDATA[
arXiv:2506.01923v1 Announce Type: cross 
Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: https://amink8.github.io/TaxaDiffusion/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title>
<link>https://arxiv.org/abs/2506.01926</link>
<guid>https://arxiv.org/abs/2506.01926</guid>
<content:encoded><![CDATA[
arXiv:2506.01926v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Esoteric Language Models</title>
<link>https://arxiv.org/abs/2506.01928</link>
<guid>https://arxiv.org/abs/2506.01928</guid>
<content:encoded><![CDATA[
arXiv:2506.01928v1 Announce Type: cross 
Abstract: Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Generation from Contextually-Contradictory Prompts</title>
<link>https://arxiv.org/abs/2506.01929</link>
<guid>https://arxiv.org/abs/2506.01929</guid>
<content:encoded><![CDATA[
arXiv:2506.01929v1 Announce Type: cross 
Abstract: Text-to-image diffusion models excel at generating high-quality, diverse images from natural language prompts. However, they often fail to produce semantically accurate results when the prompt contains concept combinations that contradict their learned priors. We define this failure mode as contextual contradiction, where one concept implicitly negates another due to entangled associations learned during training. To address this, we propose a stage-aware prompt decomposition framework that guides the denoising process using a sequence of proxy prompts. Each proxy prompt is constructed to match the semantic content expected to emerge at a specific stage of denoising, while ensuring contextual coherence. To construct these proxy prompts, we leverage a large language model (LLM) to analyze the target prompt, identify contradictions, and generate alternative expressions that preserve the original intent while resolving contextual conflicts. By aligning prompt information with the denoising progression, our method enables fine-grained semantic control and accurate image generation in the presence of contextual contradictions. Experiments across a variety of challenging prompts show substantial improvements in alignment to the textual prompt.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should Decision-Makers Reveal Classifiers in Online Strategic Classification?</title>
<link>https://arxiv.org/abs/2506.01936</link>
<guid>https://arxiv.org/abs/2506.01936</guid>
<content:encoded><![CDATA[
arXiv:2506.01936v1 Announce Type: cross 
Abstract: Strategic classification addresses a learning problem where a decision-maker implements a classifier over agents who may manipulate their features in order to receive favorable predictions. In the standard model of online strategic classification, in each round, the decision-maker implements and publicly reveals a classifier, after which agents perfectly best respond based on this knowledge. However, in practice, whether to disclose the classifier is often debated -- some decision-makers believe that hiding the classifier can prevent misclassification errors caused by manipulation.
  In this paper, we formally examine how limiting the agents' access to the current classifier affects the decision-maker's performance. Specifically, we consider an extended online strategic classification setting where agents lack direct knowledge about the current classifier and instead manipulate based on a weighted average of historically implemented classifiers. Our main result shows that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or $k_{\text{in}}$ times more mistakes compared to the full-knowledge setting, where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph (representing how many distinct feature vectors can be manipulated to appear as a single one), and $\gamma$ is the discount factor indicating agents' memory of past classifiers. Our results demonstrate how withholding access to the classifier can backfire and degrade the decision-maker's performance in online strategic classification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
arXiv:2506.01939v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries</title>
<link>https://arxiv.org/abs/2506.01945</link>
<guid>https://arxiv.org/abs/2506.01945</guid>
<content:encoded><![CDATA[
arXiv:2506.01945v1 Announce Type: cross 
Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia, Nigeria, and T\"urkiye), are gaining influence in global stock markets, although they remain susceptible to the economic conditions of developed countries like the G7 (Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States). This interconnectedness and sensitivity of financial markets make understanding these relationships crucial for investors and policymakers to predict stock price movements accurately. To this end, we examined the main stock market indices of G7 and MINT countries from 2012 to 2024, using a recent graph neural network (GNN) algorithm called multivariate time series forecasting with graph neural network (MTGNN). This method allows for considering complex spatio-temporal connections in multivariate time series. In the implementations, MTGNN revealed that the US and Canada are the most influential G7 countries regarding stock indices in the forecasting process, and Indonesia and T\"urkiye are the most influential MINT countries. Additionally, our results showed that MTGNN outperformed traditional methods in forecasting the prices of stock market indices for MINT and G7 countries. Consequently, the study offers valuable insights into economic blocks' markets and presents a compelling empirical approach to analyzing global stock market dynamics using MTGNN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-ensemble: Mitigating Confidence Distortion for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01951</link>
<guid>https://arxiv.org/abs/2506.01951</guid>
<content:encoded><![CDATA[
arXiv:2506.01951v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) perform well in general fields, they exhibit a confidence distortion problem on multi-choice question-answering (MCQA), particularly as the number of answer choices increases. Specifically, on MCQA with many choices, LLMs suffer from under-confidence in correct predictions and over-confidence in incorrect ones, leading to a substantially degraded performance. To solve this problem, we propose Self-ensemble in this work. Our method splits the choices into several groups and ensembles LLM predictions across these groups to reach a final decision. The advantage of Self-ensemble is its plug-and-play nature, where it can be integrated into existing LLM architecture based on a designed attention mask and positional encoding, without requiring labeled datasets for parameter tuning. Experimental results on three LLMs and datasets demonstrate that Self-ensemble comprehensively addresses the confidence distortion problem of LLMs, outperforming standard inference as well as baseline methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks</title>
<link>https://arxiv.org/abs/2506.01952</link>
<guid>https://arxiv.org/abs/2506.01952</guid>
<content:encoded><![CDATA[
arXiv:2506.01952v1 Announce Type: cross 
Abstract: Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</title>
<link>https://arxiv.org/abs/2506.01954</link>
<guid>https://arxiv.org/abs/2506.01954</guid>
<content:encoded><![CDATA[
arXiv:2506.01954v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Process Image Generation</title>
<link>https://arxiv.org/abs/2506.01955</link>
<guid>https://arxiv.org/abs/2506.01955</guid>
<content:encoded><![CDATA[
arXiv:2506.01955v1 Announce Type: cross 
Abstract: Prior methods for controlling image generation are limited in their ability to be taught new tasks. In contrast, vision-language models, or VLMs, can learn tasks in-context and produce the correct outputs for a given input. We propose a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator. Our general framework enables a wide variety of new control tasks through the same text-and-image based interface. We showcase a handful of applications of this technique for different types of control signals, such as commonsense inferences and visual prompts. With our method, users can implement multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes. Project page: https://dual-process.github.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
arXiv:2205.09337v2 Announce Type: replace 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
arXiv:2205.10538v2 Announce Type: replace 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Aware Pseudo Labeling via Optimal Transport for Entity Alignment</title>
<link>https://arxiv.org/abs/2209.01847</link>
<guid>https://arxiv.org/abs/2209.01847</guid>
<content:encoded><![CDATA[
arXiv:2209.01847v3 Announce Type: replace 
Abstract: Entity alignment aims to discover unique equivalent entity pairs with the same meaning across different knowledge graphs (KGs). Existing models have focused on projecting KGs into a latent embedding space so that inherent semantics between entities can be captured for entity alignment. However, the adverse impacts of alignment conflicts have been largely overlooked during training, thereby limiting the entity alignment performance. To address this issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport model (CPL-OT) for entity alignment. The key idea is to iteratively pseudo-label alignment pairs empowered with conflict-aware optimal transport (OT) modeling to boost the precision of entity alignment. CPL-OT is composed of two key components -- entity embedding learning with global-local aggregation and iterative conflict-aware pseudo labeling -- that mutually reinforce each other. To mitigate alignment conflicts during pseudo labeling, we propose to use optimal transport as an effective means to warrant one-to-one entity alignment between two KGs with the minimal overall transport cost. Extensive experiments on benchmark datasets validate the superiority of CPL-OT over state-of-the-art baselines under both settings with and without prior alignment seeds.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning distributed representations with efficient SoftMax normalization</title>
<link>https://arxiv.org/abs/2303.17475</link>
<guid>https://arxiv.org/abs/2303.17475</guid>
<content:encoded><![CDATA[
arXiv:2303.17475v4 Announce Type: replace 
Abstract: Learning distributed representations, or embeddings, that encode the relational similarity patterns among objects is a relevant task in machine learning. A popular method to learn the embedding matrices $X, Y$ is optimizing a loss function of the term ${\rm SoftMax}(XY^T)$. The complexity required to calculate this term, however, runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time heuristic approximation to compute the normalization constants of ${\rm SoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some pre-trained embedding datasets that the proposed estimation method achieves higher or comparable accuracy with competing methods. From this result, we design an efficient and task-agnostic algorithm that learns the embeddings by optimizing the cross entropy between the softmax and a set of probability distributions given as inputs. The proposed algorithm is interpretable and easily adapted to arbitrary embedding problems. We consider a few use cases and observe similar or higher performances and a lower computational time than similar ``2Vec'' algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming</title>
<link>https://arxiv.org/abs/2304.09276</link>
<guid>https://arxiv.org/abs/2304.09276</guid>
<content:encoded><![CDATA[
arXiv:2304.09276v2 Announce Type: replace 
Abstract: Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with complex structures, we use the Lambda Calculus ({\lambda}-Calculus), a simple, but Turing-Complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. We will introduce the use of integrated neural learning and lambda calculi formalization. Finally, we explore execution of a program in {\lambda}-Calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks, Transformer Model, Sequence-to-Sequence Models, Computational Models
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Motor Symptom Presence and Severity in Parkinson's Disease from Wrist Accelerometer Time Series using ROCKET and InceptionTime</title>
<link>https://arxiv.org/abs/2304.11265</link>
<guid>https://arxiv.org/abs/2304.11265</guid>
<content:encoded><![CDATA[
arXiv:2304.11265v4 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a neurodegenerative condition characterized by frequently changing motor symptoms, necessitating continuous symptom monitoring for more targeted treatment. Classical time series classification and deep learning techniques have demonstrated limited efficacy in monitoring PD symptoms using wearable accelerometer data due to complex PD movement patterns and the small size of available datasets. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) as they are promising for PD symptom monitoring. InceptionTime's high learning capacity is well-suited to modeling complex movement patterns, while ROCKET is suited to small datasets. With random search methodology, we identify the highest-scoring InceptionTime architecture and compare its performance to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motion data from PD patients. Our findings indicate that all approaches can learn to estimate tremor severity and bradykinesia presence with moderate performance but encounter challenges in detecting dyskinesia. Among the presented approaches, ROCKET demonstrates higher scores in identifying dyskinesia, whereas InceptionTime exhibits slightly better performance in tremor and bradykinesia estimation. Notably, both methods outperform the multi-layer perceptron. In conclusion, InceptionTime can classify complex wrist motion time series and holds potential for continuous symptom monitoring in PD with further development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breiman meets Bellman: Non-Greedy Decision Trees with MDPs</title>
<link>https://arxiv.org/abs/2309.12701</link>
<guid>https://arxiv.org/abs/2309.12701</guid>
<content:encoded><![CDATA[
arXiv:2309.12701v5 Announce Type: replace 
Abstract: In supervised learning, decision trees are valued for their interpretability and performance. While greedy decision tree algorithms like CART remain widely used due to their computational efficiency, they often produce sub-optimal solutions with respect to a regularized training loss. Conversely, optimal decision tree methods can find better solutions but are computationally intensive and typically limited to shallow trees or binary features. We present Dynamic Programming Decision Trees (DPDT), a framework that bridges the gap between greedy and optimal approaches. DPDT relies on a Markov Decision Process formulation combined with heuristic split generation to construct near-optimal decision trees with significantly reduced computational complexity. Our approach dynamically limits the set of admissible splits at each node while directly optimizing the tree regularized training loss. Theoretical analysis demonstrates that DPDT can minimize regularized training losses at least as well as CART. Our empirical study shows on multiple datasets that DPDT achieves near-optimal loss with orders of magnitude fewer operations than existing optimal solvers. More importantly, extensive benchmarking suggests statistically significant improvements of DPDT over both CART and optimal decision trees in terms of generalization to unseen data. We demonstrate DPDT practicality through applications to boosting, where it consistently outperforms baselines. Our framework provides a promising direction for developing efficient, near-optimal decision tree algorithms that scale to practical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supercharging Graph Transformers with Advective Diffusion</title>
<link>https://arxiv.org/abs/2310.06417</link>
<guid>https://arxiv.org/abs/2310.06417</guid>
<content:encoded><![CDATA[
arXiv:2310.06417v3 Announce Type: replace 
Abstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Successor Features with Distributed Hebbian Temporal Memory</title>
<link>https://arxiv.org/abs/2310.13391</link>
<guid>https://arxiv.org/abs/2310.13391</guid>
<content:encoded><![CDATA[
arXiv:2310.13391v4 Announce Type: replace 
Abstract: This paper presents a novel approach to address the challenge of online sequence learning for decision making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on the factor graph formalism and a multi-component neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Features (SFs). Inspired by neurophysiological models of the neocortex, the algorithm uses distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning of traditional temporal memory algorithms such as RNN and HMM. Experimental results show that DHTM outperforms LSTM, RWKV and a biologically inspired HMM-like algorithm, CSCG, on non-stationary data sets. Our results suggest that DHTM is a promising approach to address the challenges of online sequence learning and planning in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Conflicts in Neural Networks and Their Implications for Explainability</title>
<link>https://arxiv.org/abs/2310.20363</link>
<guid>https://arxiv.org/abs/2310.20363</guid>
<content:encoded><![CDATA[
arXiv:2310.20363v2 Announce Type: replace 
Abstract: Artificial Neural Networks (ANNs) often represent conflicts between features, arising naturally during training as the network learns to integrate diverse and potentially disagreeing inputs to better predict the target variable. Despite their relevance to the ``reasoning'' processes of these models, the properties and implications of conflicts for understanding and explaining ANNs remain underexplored. In this paper, we develop a rigorous theory of conflicts in ANNs and demonstrate their impact on ANN explainability through two case studies. In the first case study, we use our theory of conflicts to inspire the design of a novel feature attribution method, which we call Conflict-Aware Feature-wise Explanations (CAFE). CAFE separates the positive and negative influences of features and biases, enabling more faithful explanations for models applied to tabular data. In the second case study, we take preliminary steps towards understanding the role of conflicts in out-of-distribution (OOD) scenarios. Through our experiments, we identify potentially useful connections between model conflicts and different kinds of distributional shifts in tabular and image data. Overall, our findings demonstrate the importance of accounting for conflicts in the development of more reliable explanation methods for AI systems, which are crucial for the beneficial use of these systems in the society.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective</title>
<link>https://arxiv.org/abs/2311.16646</link>
<guid>https://arxiv.org/abs/2311.16646</guid>
<content:encoded><![CDATA[
arXiv:2311.16646v2 Announce Type: replace 
Abstract: Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling and Uniqueness Sets in Graphon Signal Processing</title>
<link>https://arxiv.org/abs/2401.06279</link>
<guid>https://arxiv.org/abs/2401.06279</guid>
<content:encoded><![CDATA[
arXiv:2401.06279v2 Announce Type: replace 
Abstract: In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the convergence results to provide an algorithm that obtains approximately close to optimal sampling sets. Performing a set of numerical experiments, we evaluate the quality of these sampling sets. Our results open the door for the efficient computation of optimal sampling sets in graphs of large size.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCFC: Bridging Federated Clustering and Contrastive Learning</title>
<link>https://arxiv.org/abs/2401.06634</link>
<guid>https://arxiv.org/abs/2401.06634</guid>
<content:encoded><![CDATA[
arXiv:2401.06634v2 Announce Type: replace 
Abstract: Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCFC also shows superior performance in handling device failures from a practical viewpoint.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning</title>
<link>https://arxiv.org/abs/2401.13796</link>
<guid>https://arxiv.org/abs/2401.13796</guid>
<content:encoded><![CDATA[
arXiv:2401.13796v3 Announce Type: replace 
Abstract: Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Free Score-Based Sampling Methods with Ensembles</title>
<link>https://arxiv.org/abs/2401.17539</link>
<guid>https://arxiv.org/abs/2401.17539</guid>
<content:encoded><![CDATA[
arXiv:2401.17539v2 Announce Type: replace 
Abstract: Recent developments in generative modeling have utilized score-based methods coupled with stochastic differential equations to sample from complex probability distributions. However, these and other performant sampling methods generally require gradients of the target probability distribution, which can be unavailable or computationally prohibitive in many scientific and engineering applications. Here, we introduce ensembles within score-based sampling methods to develop gradient-free approximate sampling techniques that leverage the collective dynamics of particle ensembles to compute approximate reverse diffusion drifts. We introduce the underlying methodology, emphasizing its relationship with generative diffusion models and the previously introduced F\"ollmer sampler. We demonstrate the efficacy of the ensemble strategies through various examples, ranging from low- to medium-dimensionality sampling problems, including multi-modal and highly non-Gaussian probability distributions, and provide comparisons to traditional methods like the No-U-Turn Sampler. Additionally, we showcase these strategies in the context of a high-dimensional Bayesian inversion problem within the geophysical sciences. Our findings highlight the potential of ensemble strategies for modeling complex probability distributions in situations where gradients are unavailable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</title>
<link>https://arxiv.org/abs/2402.04435</link>
<guid>https://arxiv.org/abs/2402.04435</guid>
<content:encoded><![CDATA[
arXiv:2402.04435v2 Announce Type: replace 
Abstract: Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\method} in IP protection and maintaining high-performance for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Temperature Scaling and Conformal Prediction of Deep Classifiers</title>
<link>https://arxiv.org/abs/2402.05806</link>
<guid>https://arxiv.org/abs/2402.05806</guid>
<content:encoded><![CDATA[
arXiv:2402.05806v4 Announce Type: replace 
Abstract: In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied by some confidence indication. Two popular approaches for that aim are: 1) Calibration: modifies the classifier's softmax values such that the maximal value better estimates the correctness probability; and 2) Conformal Prediction (CP): produces a prediction set of candidate labels that contains the true label with a user-specified probability, guaranteeing marginal coverage but not, e.g., per class coverage. In practice, both types of indications are desirable, yet, so far the interplay between them has not been investigated. Focusing on the ubiquitous Temperature Scaling (TS) calibration, we start this paper with an extensive empirical study of its effect on prominent CP methods. We show that while TS calibration improves the class-conditional coverage of adaptive CP methods, surprisingly, it negatively affects their prediction set sizes. Motivated by this behavior, we explore the effect of TS on CP beyond its calibration application and reveal an intriguing trend under which it allows to trade prediction set size and conditional coverage of adaptive CP methods. Then, we establish a mathematical theory that explains the entire non-monotonic trend. Finally, based on our experiments and theory, we offer simple guidelines for practitioners to effectively combine adaptive CP with calibration, aligned with user-defined goals.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</title>
<link>https://arxiv.org/abs/2402.15734</link>
<guid>https://arxiv.org/abs/2402.15734</guid>
<content:encoded><![CDATA[
arXiv:2402.15734v4 Announce Type: replace 
Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions, and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data_efficient_nopt.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanAgent: Automating Data Standardization with LLM-based Agents</title>
<link>https://arxiv.org/abs/2403.08291</link>
<guid>https://arxiv.org/abs/2403.08291</guid>
<content:encoded><![CDATA[
arXiv:2403.08291v4 Announce Type: replace 
Abstract: Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplifying the LLM's code generation with concise API calls. We first propose Dataprep.Clean, a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists only need to provide their requirements once, allowing for a hands-free process. To demonstrate the practical utility of CleanAgent, we developed a user-friendly web application, allowing users to interact with it using real-world datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-PDE Methods for Rough Signature Kernels</title>
<link>https://arxiv.org/abs/2404.02926</link>
<guid>https://arxiv.org/abs/2404.02926</guid>
<content:encoded><![CDATA[
arXiv:2404.02926v2 Announce Type: replace 
Abstract: Signature kernels, inner products of path signatures, underpin several machine learning algorithms for multivariate time series analysis. For bounded variation paths, signature kernels were recently shown to solve a Goursat PDE. However, existing PDE solvers only use increments as input data, leading to first order approximation errors. These approaches become computationally intractable for highly oscillatory input paths, as they have to be resolved at a fine enough scale to accurately recover their signature kernel, resulting in significant time and memory complexities. In this paper, we extend the analysis to rough paths, and show, leveraging the framework of smooth rough paths, that the resulting rough signature kernels can be approximated by a novel system of PDEs whose coefficients involve higher order iterated integrals of the input rough paths. We show that this system of PDEs admits a unique solution and establish quantitative error bounds yielding a higher order approximation to rough signature kernels.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Four-hour thunderstorm nowcasting using deep diffusion models of satellite</title>
<link>https://arxiv.org/abs/2404.10512</link>
<guid>https://arxiv.org/abs/2404.10512</guid>
<content:encoded><![CDATA[
arXiv:2404.10512v4 Announce Type: replace 
Abstract: Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose deep diffusion models of satellite (DDMS) to establish an AI-based convection nowcasting system. Specifically, DDMS employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time. Additionally, it combines geostationary satellite brightness temperature data and domain knowledge from meteorological experts, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km$^2$), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints</title>
<link>https://arxiv.org/abs/2404.14202</link>
<guid>https://arxiv.org/abs/2404.14202</guid>
<content:encoded><![CDATA[
arXiv:2404.14202v4 Announce Type: replace 
Abstract: In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bypassing Skip-Gram Negative Sampling: Dimension Regularization as a More Efficient Alternative for Graph Embeddings</title>
<link>https://arxiv.org/abs/2405.00172</link>
<guid>https://arxiv.org/abs/2405.00172</guid>
<content:encoded><![CDATA[
arXiv:2405.00172v2 Announce Type: replace 
Abstract: A wide range of graph embedding objectives decompose into two components: one that enforces similarity, attracting the embeddings of nodes that are perceived as similar, and another that enforces dissimilarity, repelling the embeddings of nodes that are perceived as dissimilar. Without repulsion, the embeddings would collapse into trivial solutions. Skip-Gram Negative Sampling (SGNS) is a popular and efficient repulsion approach that prevents collapse by repelling each node from a sample of dissimilar nodes. In this work, we show that when repulsion is most needed and the embeddings approach collapse, SGNS node-wise repulsion is, in the aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are more scalable than node operations and produce a simpler geometric interpretation of the repulsion. Our theoretical result establishes dimension regularization as an effective and more efficient, compared to skip-gram node contrast, approach to enforcing dissimilarity among embeddings of nodes. We use this result to propose a flexible algorithm augmentation framework that improves the scalability of any existing algorithm using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream link-prediction performance while reducing GPU memory usage by up to 33.3% and training time by 23.4%. Moreover, we show that completely removing repulsion (a special case of our augmentation framework) in LINE reduces training time by 70.9% on average, while increasing link prediction performance, especially for graphs that are globally sparse but locally dense. In general, however, repulsion is needed, and dimension regularization provides an efficient alternative to SGNS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to set AdamW's weight decay as you scale model and dataset size</title>
<link>https://arxiv.org/abs/2405.13698</link>
<guid>https://arxiv.org/abs/2405.13698</guid>
<content:encoded><![CDATA[
arXiv:2405.13698v3 Announce Type: replace 
Abstract: The scaling of the optimal AdamW weight decay hyperparameter with model and dataset size is critical as we seek to build larger models, but is poorly understood. We show that weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates. This gives critical insights for how to set the weight decay in AdamW, and how the weight decay should scale with model and dataset size. In particular, the key hyperparameter for an exponential moving average is the EMA timescale. Intuitively, the EMA timescale can be understood as the number of recent iterations the EMA averages over. We find that the optimal timescale, measured in epochs, is roughly constant as we change model and dataset size. Moreover, given a learning rate, there is a one-to-one mapping from the EMA timescale to the weight decay hyperparameter. Thus, if the optimal EMA timescale is constant, that implies that as the dataset size increases, the optimal weight decay should fall and as the model size increases, the optimal weight decay should increase (if we follow the muP recommendation for scaling the learning rate). We validate these scaling rules on ResNet-18 and Vision Transformers trained on CIFAR-10 and ImageNet, and on NanoGPT pre-training on OpenWebText. Finally, we found that as training progresses, muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoAdaLer: Geometric Insights into Adaptive Stochastic Gradient Descent Algorithms</title>
<link>https://arxiv.org/abs/2405.16255</link>
<guid>https://arxiv.org/abs/2405.16255</guid>
<content:encoded><![CDATA[
arXiv:2405.16255v2 Announce Type: replace 
Abstract: The Adam optimization method has achieved remarkable success in addressing contemporary challenges in stochastic optimization. This method falls within the realm of adaptive sub-gradient techniques, yet the underlying geometric principles guiding its performance have remained shrouded in mystery, and have long confounded researchers. In this paper, we introduce GeoAdaLer (Geometric Adaptive Learner), a novel adaptive learning method for stochastic gradient descent optimization, which draws from the geometric properties of the optimization landscape. Beyond emerging as a formidable contender, the proposed method extends the concept of adaptive learning by introducing a geometrically inclined approach that enhances the interpretability and effectiveness in complex optimization scenarios
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-based Optimally Weighted Conformal Prediction Intervals</title>
<link>https://arxiv.org/abs/2405.16828</link>
<guid>https://arxiv.org/abs/2405.16828</guid>
<content:encoded><![CDATA[
arXiv:2405.16828v2 Announce Type: replace 
Abstract: In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real and synthetic time-series data against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models</title>
<link>https://arxiv.org/abs/2405.17829</link>
<guid>https://arxiv.org/abs/2405.17829</guid>
<content:encoded><![CDATA[
arXiv:2405.17829v3 Announce Type: replace 
Abstract: With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this, here we present a novel latent diffusion model dubbed LDMol for text-conditioned molecule generation. By recognizing that the suitable latent space design is the key to the diffusion model performance, we employ a contrastive learning strategy to extract novel feature space from text data that embeds the unique characteristics of the molecule structure. Experiments show that LDMol outperforms the existing autoregressive baselines on the text-to-molecule generation benchmark, being one of the first diffusion models that outperforms autoregressive models in textual data generation with a better choice of the latent domain. Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-guided molecule editing, demonstrating its versatility as a diffusion model.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic and statistical calibration of constitutive models from full-field data with parametric physics-informed neural networks</title>
<link>https://arxiv.org/abs/2405.18311</link>
<guid>https://arxiv.org/abs/2405.18311</guid>
<content:encoded><![CDATA[
arXiv:2405.18311v3 Announce Type: replace 
Abstract: The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities. In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest. However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches. Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated. In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation. In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration. We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data. We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty. A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid. Finally, we consider experimental data and show that the results are in good agreement with a finite element method-based calibration. Due to the fast evaluation of PINNs, calibration can be performed in near real-time. This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Policy Evaluation Uncovers Policies of Generative Flow Networks</title>
<link>https://arxiv.org/abs/2406.02213</link>
<guid>https://arxiv.org/abs/2406.02213</guid>
<content:encoded><![CDATA[
arXiv:2406.02213v3 Announce Type: replace 
Abstract: The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects proportionally to an unnormalized reward function. A number of recent works explored connections between GFlowNets and maximum entropy (MaxEnt) RL, which modifies the standard objective of RL agents by learning an entropy-regularized objective. However, the relationship between GFlowNets and standard RL remains largely unexplored, despite the inherent similarities in their sequential decision-making nature. While GFlowNets can discover diverse solutions through specialized flow-matching objectives, connecting them can simplify their implementation through established RL principles and improve RL's diverse solution discovery capabilities. In this paper, we bridge this gap by revealing a fundamental connection between GFlowNets and one RL's most basic components -- policy evaluation. Surprisingly, we find that the value function obtained from evaluating a uniform policy is closely associated with the flow functions in GFlowNets through the lens of flow iteration under certain structural conditions. Building upon these insights, we introduce a rectified random policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets based on simply evaluating a fixed random policy in these cases, offering a new perspective. Empirical results across extensive benchmarks demonstrate that RPE achieves competitive results compared to previous approaches, shedding light on the previously overlooked connection between (non-MaxEnt) RL and GFlowNets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2406.04328</link>
<guid>https://arxiv.org/abs/2406.04328</guid>
<content:encoded><![CDATA[
arXiv:2406.04328v5 Announce Type: replace 
Abstract: The past few years have seen remarkable progress in the decoding of speech from brain activity, primarily driven by large single-subject datasets. However, due to individual variation, such as anatomy, and differences in task design and scanning hardware, leveraging data across subjects and datasets remains challenging. In turn, the field has not benefited from the growing number of open neural data repositories to exploit large-scale deep learning. To address this, we develop neuroscience-informed self-supervised objectives, together with an architecture, for learning from heterogeneous brain recordings. Scaling to nearly 400 hours of MEG data and 900 subjects, our approach shows generalisation across participants, datasets, tasks, and even to novel subjects. It achieves improvements of 15-27% over state-of-the-art models and matches surgical decoding performance with non-invasive data. These advances unlock the potential for scaling speech decoding models beyond the current frontier.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Explainable Clustering with Differential Privacy</title>
<link>https://arxiv.org/abs/2406.04610</link>
<guid>https://arxiv.org/abs/2406.04610</guid>
<content:encoded><![CDATA[
arXiv:2406.04610v2 Announce Type: replace 
Abstract: This paper presents a novel approach to Explainable AI (XAI) that combines contrastive explanations with differential privacy for clustering algorithms. Focusing on k-median and k-means problems, we calculate contrastive explanations as the utility difference between original clustering and clustering with a centroid fixed to a specific data point. This method provides personalized insights into centroid placement. Our key contribution is demonstrating that these differentially private explanations achieve essentially the same utility bounds as non-private explanations. Experiments across various datasets show that our approach offers meaningful, privacy-preserving, and individually relevant explanations without significantly compromising clustering utility. This work advances privacy-aware machine learning by balancing data protection, explanation quality, and personalization in clustering tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
<link>https://arxiv.org/abs/2406.11569</link>
<guid>https://arxiv.org/abs/2406.11569</guid>
<content:encoded><![CDATA[
arXiv:2406.11569v4 Announce Type: replace 
Abstract: For modern artificial intelligence (AI) applications such as large language models (LLMs), the training paradigm has recently shifted to pre-training followed by fine-tuning. Furthermore, owing to dwindling open repositories of data and thanks to efforts to democratize access to AI models, pre-training is expected to increasingly migrate from the current centralized deployments to federated learning (FL) implementations. Meta-learning provides a general framework in which pre-training and fine-tuning can be formalized. Meta-learning-based personalized FL (meta-pFL) moves beyond basic personalization by targeting generalization to new agents and tasks. This paper studies the generalization performance of meta-pFL for a wireless setting in which the agents participating in the pre-training phase, i.e., meta-learning, are connected via a shared wireless channel to the server. Adopting over-the-air computing, we study the trade-off between generalization to new agents and tasks, on the one hand, and convergence, on the other hand. The trade-off arises from the fact that channel impairments may enhance generalization, while degrading convergence. Extensive numerical results validate the theory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Learning with Quality-Driven Data Selection</title>
<link>https://arxiv.org/abs/2407.00102</link>
<guid>https://arxiv.org/abs/2407.00102</guid>
<content:encoded><![CDATA[
arXiv:2407.00102v2 Announce Type: replace 
Abstract: The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2407.00490</link>
<guid>https://arxiv.org/abs/2407.00490</guid>
<content:encoded><![CDATA[
arXiv:2407.00490v2 Announce Type: replace 
Abstract: We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate $O(1/\sqrt{t})$. This is the first global convergence result for Gaussian mixtures with more than $2$ components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Conditional Probability for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2407.01171</link>
<guid>https://arxiv.org/abs/2407.01171</guid>
<content:encoded><![CDATA[
arXiv:2407.01171v2 Announce Type: replace 
Abstract: We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of com- plex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
arXiv:2407.06533v2 Announce Type: replace 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden State Differential Private Mini-Batch Block Coordinate Descent for Multi-convexity Optimization</title>
<link>https://arxiv.org/abs/2407.08233</link>
<guid>https://arxiv.org/abs/2407.08233</guid>
<content:encoded><![CDATA[
arXiv:2407.08233v2 Announce Type: replace 
Abstract: We investigate the differential privacy (DP) guarantees under the hidden state assumption (HSA) for multi-convex problems. Recent analyses of privacy loss under the hidden state assumption have relied on strong assumptions such as convexity, thereby limiting their applicability to practical problems. In this paper, we introduce the Differential Privacy Mini-Batch Block Coordinate Descent (DP-MBCD) algorithm, accompanied by the privacy loss accounting methods under the hidden state assumption. Our proposed methods apply to a broad range of classical non-convex problems which are or can be converted to multi-convex problems, such as matrix factorization and neural network training. In addition to a tighter bound for privacy loss, our theoretical analysis is also compatible with proximal gradient descent and adaptive calibrated noise scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashNorm: fast normalization for LLMs</title>
<link>https://arxiv.org/abs/2407.09577</link>
<guid>https://arxiv.org/abs/2407.09577</guid>
<content:encoded><![CDATA[
arXiv:2407.09577v3 Announce Type: replace 
Abstract: This paper presents FlashNorm, which is an exact but faster implementation of RMSNorm followed by linear layers. RMSNorm is used by many LLMs such as Llama, Mistral, and OpenELM. FlashNorm also speeds up Layer Normalization and its recently proposed replacement Dynamic Tanh (DyT) arXiv:2503.10622. FlashNorm also reduces the number of parameter tensors by simply merging the normalization weights with the weights of the next linear layer. See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deflated Dynamics Value Iteration</title>
<link>https://arxiv.org/abs/2407.10454</link>
<guid>https://arxiv.org/abs/2407.10454</guid>
<content:encoded><![CDATA[
arXiv:2407.10454v2 Announce Type: replace 
Abstract: The Value Iteration (VI) algorithm is an iterative procedure to compute the value function of a Markov decision process, and is the basis of many reinforcement learning (RL) algorithms as well. As the error convergence rate of VI as a function of iteration $k$ is $O(\gamma^k)$, it is slow when the discount factor $\gamma$ is close to $1$. To accelerate the computation of the value function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses matrix splitting and matrix deflation techniques to effectively remove (deflate) the top $s$ dominant eigen-structure of the transition matrix $\mathcal{P}^{\pi}$. We prove that this leads to a $\tilde{O}(\gamma^k |\lambda_{s+1}|^k)$ convergence rate, where $\lambda_{s+1}$is $(s+1)$-th largest eigenvalue of the dynamics matrix. We then extend DDVI to the RL setting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We empirically show the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning</title>
<link>https://arxiv.org/abs/2408.03819</link>
<guid>https://arxiv.org/abs/2408.03819</guid>
<content:encoded><![CDATA[
arXiv:2408.03819v2 Announce Type: replace 
Abstract: Active Learning (AL) allows models to learn interactively from user feedback. This paper introduces a counterfactual data augmentation approach to AL, particularly addressing the selection of datapoints for user querying, a pivotal concern in enhancing data efficiency. Our approach is inspired by Variation Theory, a theory of human concept learning that emphasizes the essential features of a concept by focusing on what stays the same and what changes. Instead of just querying with existing datapoints, our approach synthesizes artificial datapoints that highlight potential key similarities and differences among labels using a neuro-symbolic pipeline combining large language models (LLMs) and rule-based models. Through an experiment in the example domain of text classification, we show that our approach achieves significantly higher performance when there are fewer annotated data. As the annotated training data gets larger the impact of the generated data starts to diminish showing its capability to address the cold start problem in AL. This research sheds light on integrating theories of human learning into the optimization of AL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On zero-shot learning in neural state estimation of power distribution systems</title>
<link>https://arxiv.org/abs/2408.05787</link>
<guid>https://arxiv.org/abs/2408.05787</guid>
<content:encoded><![CDATA[
arXiv:2408.05787v2 Announce Type: replace 
Abstract: This paper addresses the challenge of neural state estimation in power distribution systems. We identified a research gap in the current state of the art, which lies in the inability of models to adapt to changes in the power grid, such as loss of sensors and branch switching, in a zero-shot fashion. Based on the literature, we identified graph neural networks as the most promising class of models for this use case. Our experiments confirm their robustness to some grid changes and also show that a deeper network does not always perform better. We propose data augmentations to improve performance and conduct a comprehensive grid search of different model configurations for common zero-shot learning scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable k-Medoids Clustering via Whale Optimization Algorithm</title>
<link>https://arxiv.org/abs/2408.16993</link>
<guid>https://arxiv.org/abs/2408.16993</guid>
<content:encoded><![CDATA[
arXiv:2408.16993v2 Announce Type: replace 
Abstract: Unsupervised clustering has emerged as a critical tool for uncovering hidden patterns in vast, unlabeled datasets. However, traditional methods, such as Partitioning Around Medoids (PAM), struggle with scalability owing to their quadratic computational complexity. To address this limitation, we introduce WOA-kMedoids, a novel unsupervised clustering method that incorporates the Whale Optimization Algorithm (WOA), a nature-inspired metaheuristic inspired by the hunting strategies of humpback whales. By optimizing the centroid selection, WOA-kMedoids reduces the computational complexity from quadratic to near-linear with respect to the number of observations, enabling scalability to large datasets while maintaining high clustering accuracy. We evaluated WOA-kMedoids using 25 diverse time-series datasets from the UCR archive. Our empirical results show that WOA-kMedoids achieved a clustering performance comparable to PAM, with an average Rand Index (RI) of 0.731 compared to PAM's 0.739, outperforming PAM on 12 out of 25 datasets. While exhibiting a slightly higher runtime than PAM on small datasets (<300 observations), WOA-kMedoids outperformed PAM on larger datasets, with an average speedup of 1.7x and a maximum of 2.3x. The scalability of WOA-kMedoids, combined with its high accuracy, makes them a promising choice for unsupervised clustering in big data applications. This method has implications for efficient knowledge discovery in massive unlabeled datasets, particularly where traditional k-medoids methods are computationally infeasible, including IoT anomaly detection, biomedical signal analysis, and customer behavior clustering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELO-Rated Sequence Rewards: Advancing Reinforcement Learning Models</title>
<link>https://arxiv.org/abs/2409.03301</link>
<guid>https://arxiv.org/abs/2409.03301</guid>
<content:encoded><![CDATA[
arXiv:2409.03301v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) heavily relies on the careful design of the reward function. However, accurately assigning rewards to each state-action pair in Long-Term Reinforcement Learning (LTRL) tasks remains a significant challenge. As a result, RL agents are often trained under expert guidance. Inspired by the ordinal utility theory in economics, we propose a novel reward estimation algorithm: ELO-Rating based Reinforcement Learning (ERRL). This approach features two key contributions. First, it uses expert preferences over trajectories rather than cardinal rewards (utilities) to compute the ELO rating of each trajectory as its reward. Second, a new reward redistribution algorithm is introduced to alleviate training instability in the absence of a fixed anchor reward. In long-term scenarios (up to 5000 steps), where traditional RL algorithms struggle, our method outperforms several state-of-the-art baselines. Additionally, we conduct a comprehensive analysis of how expert preferences influence the results.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Deep Neural Operator Networks via Randomized Sampling</title>
<link>https://arxiv.org/abs/2409.13280</link>
<guid>https://arxiv.org/abs/2409.13280</guid>
<content:encoded><![CDATA[
arXiv:2409.13280v2 Announce Type: replace 
Abstract: Neural operators (NOs) employ deep neural networks to learn mappings between infinite-dimensional function spaces. Deep operator network (DeepONet), a popular NO architecture, has demonstrated success in the real-time prediction of complex dynamics across various scientific and engineering applications. In this work, we introduce a random sampling technique to be adopted during the training of DeepONet, aimed at improving the generalization ability of the model, while significantly reducing the computational time. The proposed approach targets the trunk network of the DeepONet model that outputs the basis functions corresponding to the spatiotemporal locations of the bounded domain on which the physical system is defined. While constructing the loss function, DeepONet training traditionally considers a uniform grid of spatiotemporal points at which all the output functions are evaluated for each iteration. This approach leads to a larger batch size, resulting in poor generalization and increased memory demands, due to the limitations of the stochastic gradient descent (SGD) optimizer. The proposed random sampling over the inputs of the trunk net mitigates these challenges, improving generalization and reducing memory requirements during training, resulting in significant computational gains. We validate our hypothesis through three benchmark examples, demonstrating substantial reductions in training time while achieving comparable or lower overall test errors relative to the traditional training approach. Our results indicate that incorporating randomization in the trunk network inputs during training enhances the efficiency and robustness of DeepONet, offering a promising avenue for improving the framework's performance in modeling complex physical systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Banking Dataset: Understanding Client Needs through Event Sequences</title>
<link>https://arxiv.org/abs/2409.17587</link>
<guid>https://arxiv.org/abs/2409.17587</guid>
<content:encoded><![CDATA[
arXiv:2409.17587v2 Announce Type: replace 
Abstract: Financial organizations collect a huge amount of temporal (sequential) data about clients, which is typically collected from multiple sources (modalities). Despite the urgent practical need, developing deep learning techniques suitable to handle such data is limited by the absence of large open-source multi-source real-world datasets of event sequences. To fill this gap, which is mainly caused by security reasons, we present the first industrial-scale publicly available multimodal banking dataset, MBD, that contains information on more than 2M corporate clients of a large bank. Clients are represented by several data sources: 950M bank transactions, 1B geo position events, 5M embeddings of dialogues with technical support, and monthly aggregated purchases of four bank products. All entries are properly anonymized from real proprietary bank data, and the experiments confirm that our anonymization still saves all significant information for introduced downstream tasks. Moreover, we introduce a novel multimodal benchmark suggesting several important practical tasks, such as future purchase prediction and modality matching. The benchmark incorporates our MBD and two public financial datasets. We provide numerical results for the state-of-the-art event sequence modeling techniques including large language models and demonstrate the superiority of fusion baselines over single-modal techniques for each task. Thus, MBD provides a valuable resource for future research in financial applications of multimodal event sequence analysis.
  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD
  Github Link: https://github.com/Dzhambo/MBD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data</title>
<link>https://arxiv.org/abs/2409.19078</link>
<guid>https://arxiv.org/abs/2409.19078</guid>
<content:encoded><![CDATA[
arXiv:2409.19078v3 Announce Type: replace 
Abstract: Speech pathology has impacts on communication abilities and quality of life. While deep learning-based models have shown potential in diagnosing these disorders, the use of sensitive data raises critical privacy concerns. Although differential privacy (DP) has been explored in the medical imaging domain, its application in pathological speech analysis remains largely unexplored despite the equally critical privacy concerns. To the best of our knowledge, this study is the first to investigate DP's impact on pathological speech data, focusing on the trade-offs between privacy, diagnostic accuracy, and fairness. Using a large, real-world dataset of 200 hours of recordings from 2,839 German-speaking participants, we observed a maximum accuracy reduction of 3.85% when training with DP with high privacy levels. To highlight real-world privacy risks, we demonstrated the vulnerability of non-private models to gradient inversion attacks, reconstructing identifiable speech samples and showcasing DP's effectiveness in mitigating these risks. To explore the potential generalizability across languages and disorders, we validated our approach on a dataset of Spanish-speaking Parkinson's disease patients, leveraging pretrained models from healthy English-speaking datasets, and demonstrated that careful pretraining on large-scale task-specific datasets can maintain favorable accuracy under DP constraints. A comprehensive fairness analysis revealed minimal gender bias at reasonable privacy levels but underscored the need for addressing age-related disparities. Our results establish that DP can balance privacy and utility in speech disorder detection, while highlighting unique challenges in privacy-fairness trade-offs for speech data. This provides a foundation for refining DP methodologies and improving fairness across diverse patient groups in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Practices for Multi-Fidelity Bayesian Optimization in Materials and Molecular Research</title>
<link>https://arxiv.org/abs/2410.00544</link>
<guid>https://arxiv.org/abs/2410.00544</guid>
<content:encoded><![CDATA[
arXiv:2410.00544v3 Announce Type: replace 
Abstract: Multi-fidelity Bayesian Optimization (MFBO) is a promising framework to speed up materials and molecular discovery as sources of information of different accuracies are at hand at increasing cost. Despite its potential use in chemical tasks, there is a lack of systematic evaluation of the many parameters playing a role in MFBO. In this work, we provide guidelines and recommendations to decide when to use MFBO in experimental settings. We investigate MFBO methods applied to molecules and materials problems. First, we test two different families of acquisition functions in two synthetic problems and study the effect of the informativeness and cost of the approximate function. We use our implementation and guidelines to benchmark three real discovery problems and compare them against their single-fidelity counterparts. Our results may help guide future efforts to implement MFBO as a routine tool in the chemical sciences.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softmax is not Enough (for Sharp Size Generalisation)</title>
<link>https://arxiv.org/abs/2410.01104</link>
<guid>https://arxiv.org/abs/2410.01104</guid>
<content:encoded><![CDATA[
arXiv:2410.01104v3 Announce Type: replace 
Abstract: A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions with increasing problem size, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization</title>
<link>https://arxiv.org/abs/2410.02628</link>
<guid>https://arxiv.org/abs/2410.02628</guid>
<content:encoded><![CDATA[
arXiv:2410.02628v3 Announce Type: replace 
Abstract: Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \sim \pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\textbf{end-to-end}$ learning algorithm to get $\pi^*(\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?</title>
<link>https://arxiv.org/abs/2410.02735</link>
<guid>https://arxiv.org/abs/2410.02735</guid>
<content:encoded><![CDATA[
arXiv:2410.02735v2 Announce Type: replace 
Abstract: Out-of-distribution (OOD) generalization is challenging because distribution shifts come in many forms. Numerous algorithms exist to address specific settings, but choosing the right training algorithm for the right dataset without trial and error is difficult. Indeed, real-world applications often involve multiple types and combinations of shifts that are hard to analyze theoretically.
  Method. This work explores the possibility of learning the selection of a training algorithm for OOD generalization. We propose a proof of concept (OOD-Chameleon) that formulates the selection as a multi-label classification over candidate algorithms, trained on a dataset of datasets representing a variety of shifts. We evaluate the ability of OOD-Chameleon to rank algorithms on unseen shifts and datasets based only on dataset characteristics, i.e., without training models first, unlike traditional model selection.
  Findings. Extensive experiments show that the learned selector identifies high-performing algorithms across synthetic, vision, and language tasks. Further inspection shows that it learns non-trivial decision rules, which provide new insights into the applicability of existing algorithms. Overall, this new approach opens the possibility of better exploiting and understanding the plethora of existing algorithms for OOD generalization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness</title>
<link>https://arxiv.org/abs/2410.03210</link>
<guid>https://arxiv.org/abs/2410.03210</guid>
<content:encoded><![CDATA[
arXiv:2410.03210v2 Announce Type: replace 
Abstract: Frameworks and domain-specific languages for auto-generating code have traditionally depended on human experts to implement rigorous methods ensuring the legality of code transformations. Recently, machine learning (ML) has gained traction for generating code optimized for specific hardware targets. However, ML approaches-particularly black-box neural networks-offer no guarantees on the correctness or legality of the transformations they produce. To address this gap, we introduce Tadashi, an end-to-end system that leverages the polyhedral model to support researchers in curating datasets critical for ML-based code generation. Tadashi provides an end-to-end system capable of applying, verifying, and evaluating candidate transformations on polyhedral schedules with both reliability and practicality. We formally prove that Tadashi guarantees the legality of generated transformations, demonstrate its low runtime overhead, and showcase its broad applicability. Tadashi available at https://github.com/vatai/tadashi/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation</title>
<link>https://arxiv.org/abs/2410.03960</link>
<guid>https://arxiv.org/abs/2410.03960</guid>
<content:encoded><![CDATA[
arXiv:2410.03960v3 Announce Type: replace 
Abstract: LLM inference for enterprise applications, such as summarization, RAG, and code-generation, typically observe much longer prompt than generations, leading to high prefill cost and response latency. We present SwiftKV, a novel model transformation and distillation procedure targeted at reducing the prefill compute (in FLOPs) of prompt tokens while preserving high generation quality. First, SwiftKV prefills later layers' KV cache using an earlier layer's output, allowing prompt tokens to skip those later layers. Second, SwiftKV employs a lightweight knowledge-preserving distillation procedure that can adapt existing LLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV cache compression to improve inference performance in low-memory scenarios. Our comprehensive experiments show that SwiftKV can effectively reduce prefill computation by 25-50% across several LLM families while incurring minimum quality degradation. In the end-to-end inference serving, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at https://github.com/snowflakedb/arctictraining.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Multi-task Policy Fine-tuning</title>
<link>https://arxiv.org/abs/2410.05026</link>
<guid>https://arxiv.org/abs/2410.05026</guid>
<content:encoded><![CDATA[
arXiv:2410.05026v2 Announce Type: replace 
Abstract: Pre-trained generalist policies are rapidly gaining relevance in robot learning due to their promise of fast adaptation to novel, in-domain tasks. This adaptation often relies on collecting new demonstrations for a specific task of interest and applying imitation learning algorithms, such as behavioral cloning. However, as soon as several tasks need to be learned, we must decide which tasks should be demonstrated and how often? We study this multi-task problem and explore an interactive framework in which the agent adaptively selects the tasks to be demonstrated. We propose AMF (Active Multi-task Fine-tuning), an algorithm to maximize multi-task policy performance under a limited demonstration budget by collecting demonstrations yielding the largest information gain on the expert policy. We derive performance guarantees for AMF under regularity assumptions and demonstrate its empirical effectiveness to efficiently fine-tune neural policies in complex and high-dimensional environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Dynamic Client Arrival and Departure: Convergence and Rapid Adaptation via Initial Model Construction</title>
<link>https://arxiv.org/abs/2410.05662</link>
<guid>https://arxiv.org/abs/2410.05662</guid>
<content:encoded><![CDATA[
arXiv:2410.05662v2 Announce Type: replace 
Abstract: Most federated learning (FL) approaches assume a fixed client set. However, real-world scenarios often involve clients dynamically joining or leaving the system based on their needs or interest in specific tasks. This dynamic setting introduces unique challenges: (1) the optimization objective evolves with the active client set, unlike traditional FL with a static objective; and (2) the current global model may no longer serve as an effective initialization for subsequent rounds, potentially hindering adaptation. To address these challenges, we first provide a convergence analysis under a non-convex loss with a dynamic client set, accounting for factors such as gradient noise, local training iterations, and data heterogeneity. Building on this analysis, we propose a model initialization algorithm that enables rapid adaptation to new client sets whenever clients join or leave the system. Our key idea is to compute a weighted average of previous global models, guided by gradient similarity, to prioritize models trained on data distributions that closely align with the current client set, thereby accelerating recovery from distribution shifts. This plug-and-play algorithm is designed to integrate seamlessly with existing FL methods, offering broad applicability in practice. Experimental results on diverse datasets including both image and text domains, varied label distributions, and multiple FL algorithms demonstrate the effectiveness of the proposed approach across a range of scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</title>
<link>https://arxiv.org/abs/2410.06820</link>
<guid>https://arxiv.org/abs/2410.06820</guid>
<content:encoded><![CDATA[
arXiv:2410.06820v3 Announce Type: replace 
Abstract: Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach extends to parametric PDEs. Specifically, we integrate the physical loss gradient with PDE parameters, allowing our method to solve over a distribution of PDE parameters, including coefficients, initial conditions, and boundary conditions. We demonstrate the effectiveness of our approach through empirical experiments on multiple datasets, comparing both training and test-time optimization performance. The code is available at https://github.com/2ailesB/neural-parametric-solver.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroplastic Expansion in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.07994</link>
<guid>https://arxiv.org/abs/2410.07994</guid>
<content:encoded><![CDATA[
arXiv:2410.07994v3 Announce Type: replace 
Abstract: The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, {\it Neuroplastic Expansion} (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (\textit{1}) elastic topology generation based on potential gradients, (\textit{2}) dormant neuron pruning to optimize network expressivity, and (\textit{3}) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</title>
<link>https://arxiv.org/abs/2410.09411</link>
<guid>https://arxiv.org/abs/2410.09411</guid>
<content:encoded><![CDATA[
arXiv:2410.09411v2 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency, and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviors in classification tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2410.11674</link>
<guid>https://arxiv.org/abs/2410.11674</guid>
<content:encoded><![CDATA[
arXiv:2410.11674v2 Announce Type: replace 
Abstract: Time series forecasting remains a challenging task, particularly in the context of complex multiscale temporal patterns. This study presents LLM-Mixer, a framework that improves forecasting accuracy through the combination of multiscale time-series decomposition with pre-trained LLMs (Large Language Models). LLM-Mixer captures both short-term fluctuations and long-term trends by decomposing the data into multiple temporal resolutions and processing them with a frozen LLM, guided by a textual prompt specifically designed for time-series data. Extensive experiments conducted on multivariate and univariate datasets demonstrate that LLM-Mixer achieves competitive performance, outperforming recent state-of-the-art models across various forecasting horizons. This work highlights the potential of combining multiscale analysis and LLMs for effective and scalable time-series forecasting.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FragNet: A Graph Neural Network for Molecular Property Prediction with Four Levels of Interpretability</title>
<link>https://arxiv.org/abs/2410.12156</link>
<guid>https://arxiv.org/abs/2410.12156</guid>
<content:encoded><![CDATA[
arXiv:2410.12156v2 Announce Type: replace 
Abstract: Molecular property prediction is essential in a variety of contemporary scientific fields, such as drug development and designing energy storage materials. Although there are many machine learning models available for this purpose, those that achieve high accuracy while also offering interpretability of predictions are uncommon. We present a graph neural network that not only matches the prediction accuracies of leading models but also provides insights on four levels of molecular substructures. This model helps identify which atoms, bonds, molecular fragments, and connections between fragments are significant for predicting a specific molecular property. Understanding the importance of connections between fragments is particularly valuable for molecules with substructures that do not connect through standard bonds. The model additionally can quantify the impact of specific fragments on the prediction, allowing the identification of fragments that may improve or degrade a property value. These interpretable features are essential for deriving scientific insights from the model's learned relationships between molecular structures and properties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2410.13097</link>
<guid>https://arxiv.org/abs/2410.13097</guid>
<content:encoded><![CDATA[
arXiv:2410.13097v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods typically assume that Large Language Models (LLMs) are trained on data from a single device or client. However, real-world scenarios often require fine-tuning these models on private data distributed across multiple devices. Federated Learning (FL) offers an appealing solution by preserving user privacy, as sensitive data remains on local devices during training. Nonetheless, integrating PEFT methods into FL introduces two main challenges: communication overhead and data heterogeneity. In this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by integrating tensorized adapters into client-side models' encoder/decoder blocks. FedTT is versatile and can be applied to both cross-silo FL and large-scale cross-device FL. FedTT+, an extension of FedTT tailored for cross-silo FL, enhances robustness against data heterogeneity by adaptively freezing portions of tensor factors, further reducing the number of trainable parameters. Experiments on BERT and LLaMA models demonstrate that our proposed methods successfully address data heterogeneity challenges and perform on par or even better than existing federated PEFT approaches while achieving up to 10$\times$ reduction in communication cost.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation</title>
<link>https://arxiv.org/abs/2410.13248</link>
<guid>https://arxiv.org/abs/2410.13248</guid>
<content:encoded><![CDATA[
arXiv:2410.13248v2 Announce Type: replace 
Abstract: Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. The datasets and benchmark implementation are available at: https://github.com/jchanxtarov/sent_xrec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2410.17933</link>
<guid>https://arxiv.org/abs/2410.17933</guid>
<content:encoded><![CDATA[
arXiv:2410.17933v2 Announce Type: replace 
Abstract: One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy preserved. Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation with Estimation of Source Reliability</title>
<link>https://arxiv.org/abs/2410.22954</link>
<guid>https://arxiv.org/abs/2410.22954</guid>
<content:encoded><![CDATA[
arXiv:2410.22954v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) addresses key limitations of large language models (LLMs), such as hallucinations and outdated knowledge, by incorporating external databases. These databases typically consult multiple sources to encompass up-to-date and various information. However, standard RAG methods often overlook the heterogeneous source reliability in the multi-source database and retrieve documents solely based on relevance, making them prone to propagating misinformation. To address this, we propose Reliability-Aware RAG (RA-RAG) which estimates the reliability of multiple sources and incorporates this information into both retrieval and aggregation processes. Specifically, it iteratively estimates source reliability and true answers for a set of queries with no labelling. Then, it selectively retrieves relevant documents from a few of reliable sources and aggregates them using weighted majority voting, where the selective retrieval ensures scalability while not compromising the performance. We also introduce a benchmark designed to reflect real-world scenarios with heterogeneous source reliability and demonstrate the effectiveness of RA-RAG compared to a set of baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison-based Active Preference Learning for Multi-dimensional Personalization</title>
<link>https://arxiv.org/abs/2411.00524</link>
<guid>https://arxiv.org/abs/2411.00524</guid>
<content:encoded><![CDATA[
arXiv:2411.00524v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable success, but aligning them with human preferences remains a core challenge. As individuals have their own, multi-dimensional preferences, recent studies have explored multi-dimensional personalization, which aims to enable models to generate responses personalized to explicit preferences. However, human preferences are often implicit and thus difficult to articulate, limiting the direct application of this approach. To bridge this gap, we propose Active Multi-dimensional Preference Learning (AMPLe), designed to capture implicit user preferences from interactively collected comparative feedback. Building on Bayesian inference, our work introduces a modified posterior update procedure to mitigate estimation bias and potential noise in comparisons. Also, inspired by generalized binary search, we employ an active query selection strategy to minimize the number of required comparisons by a user. Through theoretical analysis and experiments on language generation tasks, we demonstrate feedback efficiency and effectiveness of our framework in personalizing model responses. Our code is publicly available at https://github.com/ml-postech/AMPLe .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Filtering with RKHS Algebras</title>
<link>https://arxiv.org/abs/2411.01341</link>
<guid>https://arxiv.org/abs/2411.01341</guid>
<content:encoded><![CDATA[
arXiv:2411.01341v2 Announce Type: replace 
Abstract: In this paper, we develop a generalized theory of convolutional signal processing and neural networks for Reproducing Kernel Hilbert Spaces (RKHS). Leveraging the theory of algebraic signal processing (ASP), we show that any RKHS allows the formal definition of multiple algebraic convolutional models. We show that any RKHS induces algebras whose elements determine convolutional operators acting on RKHS elements. This approach allows us to achieve scalable filtering and learning as a byproduct of the convolutional model, and simultaneously take advantage of the well-known benefits of processing information in an RKHS. To emphasize the generality and usefulness of our approach, we show how algebraic RKHS can be used to define convolutional signal models on groups, graphons, and traditional Euclidean signal spaces. Furthermore, using algebraic RKHS models, we build convolutional networks, formally defining the notion of pointwise nonlinearities and deriving explicit expressions for the training. Such derivations are obtained in terms of the algebraic representation of the RKHS. We present a set of numerical experiments on real data in which wireless coverage is predicted from measurements captured by unmaned aerial vehicles. This particular real-life scenario emphasizes the benefits of the convolutional RKHS models in neural networks compared to fully connected and standard convolutional operators.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2411.04760</link>
<guid>https://arxiv.org/abs/2411.04760</guid>
<content:encoded><![CDATA[
arXiv:2411.04760v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices. However, SNN model parameters are sensitive to temporal resolution, leading to significant performance drops when the temporal resolution of target data at the edge is not the same with that of the pre-deployment source data used for training, especially when fine-tuning is not possible at the edge. To address this challenge, we propose three novel domain adaptation methods for adapting neuron parameters to account for the change in time resolution without re-training on target time-resolution. The proposed methods are based on a mapping between neuron dynamics in SNNs and State Space Models (SSMs); and are applicable to general neuron models. We evaluate the proposed methods under spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and MSWC as well as the image classification NMINST dataset. Our methods provide an alternative to - and in majority of the cases significantly outperform - the existing reference method that simply scales the time constant. Moreover, our results show that high accuracy on high temporal resolution data can be obtained by time efficient training on lower temporal resolution data and model adaptation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering</title>
<link>https://arxiv.org/abs/2411.09749</link>
<guid>https://arxiv.org/abs/2411.09749</guid>
<content:encoded><![CDATA[
arXiv:2411.09749v2 Announce Type: replace 
Abstract: Differentiable rendering techniques like Gaussian Splatting and Neural Radiance Fields have become powerful tools for generating high-fidelity models of 3D objects and scenes. Their ability to produce both physically plausible and differentiable models of scenes are key ingredient needed to produce physically plausible adversarial attacks on DNNs. However, the adversarial machine learning community has yet to fully explore these capabilities, partly due to differing attack goals (e.g., misclassification, misdetection) and a wide range of possible scene manipulations used to achieve them (e.g., alter texture, mesh). This survey contributes the first framework that unifies diverse goals and tasks, facilitating easy comparison of existing work, identifying research gaps, and highlighting future directions - ranging from expanding attack goals and tasks to account for new modalities, state-of-the-art models, tools, and pipelines, to underscoring the importance of studying real-world threats in complex scenes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models</title>
<link>https://arxiv.org/abs/2411.09837</link>
<guid>https://arxiv.org/abs/2411.09837</guid>
<content:encoded><![CDATA[
arXiv:2411.09837v2 Announce Type: replace 
Abstract: To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization</title>
<link>https://arxiv.org/abs/2411.10958</link>
<guid>https://arxiv.org/abs/2411.10958</guid>
<content:encoded><![CDATA[
arXiv:2411.10958v5 Announce Type: replace 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schemato - An LLM for Netlist-to-Schematic Conversion</title>
<link>https://arxiv.org/abs/2411.13899</link>
<guid>https://arxiv.org/abs/2411.13899</guid>
<content:encoded><![CDATA[
arXiv:2411.13899v2 Announce Type: replace 
Abstract: Machine learning models are advancing circuit design, particularly in analog circuits. They typically generate netlists that lack human interpretability. This is a problem as human designers heavily rely on the interpretability of circuit diagrams or schematics to intuitively understand, troubleshoot, and develop designs. Hence, to integrate domain knowledge effectively, it is crucial to translate ML-generated netlists into interpretable schematics quickly and accurately. We propose Schemato, a large language model (LLM) for netlist-to-schematic conversion. In particular, we consider our approach in converting netlists to .asc files, text-based schematic description used in LTSpice. Experiments on our circuit dataset show that Schemato achieves up to 76% compilation success rate, surpassing 63% scored by the state-of-the-art LLMs. Furthermore, our experiments show that Schemato generates schematics with an average graph edit distance score and mean structural similarity index measure, scaled by the compilation success rate that are 1.8x and 4.3x higher than the best performing LLMs respectively, demonstrating its ability to generate schematics that are more accurately connected and are closer to the reference human design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MC-NEST: Enhancing Mathematical Reasoning in Large Language Models leveraging a Monte Carlo Self-Refine Tree</title>
<link>https://arxiv.org/abs/2411.15645</link>
<guid>https://arxiv.org/abs/2411.15645</guid>
<content:encoded><![CDATA[
arXiv:2411.15645v2 Announce Type: replace 
Abstract: Mathematical reasoning presents significant challenges for large language models (LLMs). To enhance their capabilities, we propose Monte Carlo Self-Refine Tree (MC-NEST), an extension of Monte Carlo Tree Search that integrates LLM-based self-refinement and self-evaluation for improved decision-making in complex reasoning tasks. MC-NEST balances exploration and exploitation using Upper Confidence Bound (UCT) scores combined with diverse selection policies. Through iterative critique and refinement, LLMs learn to reason more strategically. Empirical results demonstrate that MC-NEST with an importance sampling policy substantially improves GPT-4o's performance, achieving state-of-the-art pass@1 scores on Olympiad-level benchmarks. Specifically, MC-NEST attains a pass@1 of 38.6 on AIME and 12.6 on MathOdyssey. The solution quality for MC-NEST using GPT-4o and Phi-3-mini reaches 84.0\% and 82.08\%, respectively, indicating robust consistency across different LLMs. MC-NEST performs strongly across Algebra, Geometry, and Number Theory, benefiting from its ability to handle abstraction, logical deduction, and multi-step reasoning -- core skills in mathematical problem solving.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization of Handwritten Text Recognition Models</title>
<link>https://arxiv.org/abs/2411.17332</link>
<guid>https://arxiv.org/abs/2411.17332</guid>
<content:encoded><![CDATA[
arXiv:2411.17332v2 Announce Type: replace 
Abstract: Recent advances in Handwritten Text Recognition (HTR) have led to significant reductions in transcription errors on standard benchmarks under the i.i.d. assumption, thus focusing on minimizing in-distribution (ID) errors. However, this assumption does not hold in real-world applications, which has motivated HTR research to explore Transfer Learning and Domain Adaptation techniques. In this work, we investigate the unaddressed limitations of HTR models in generalizing to out-of-distribution (OOD) data. We adopt the challenging setting of Domain Generalization, where models are expected to generalize to OOD data without any prior access. To this end, we analyze 336 OOD cases from eight state-of-the-art HTR models across seven widely used datasets, spanning five languages. Additionally, we study how HTR models leverage synthetic data to generalize. We reveal that the most significant factor for generalization lies in the textual divergence between domains, followed by visual divergence. We demonstrate that the error of HTR models in OOD scenarios can be reliably estimated, with discrepancies falling below 10 points in 70\% of cases. We identify the underlying limitations of HTR models, laying the foundation for future research to address this challenge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</title>
<link>https://arxiv.org/abs/2412.08174</link>
<guid>https://arxiv.org/abs/2412.08174</guid>
<content:encoded><![CDATA[
arXiv:2412.08174v3 Announce Type: replace 
Abstract: While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning</title>
<link>https://arxiv.org/abs/2412.08559</link>
<guid>https://arxiv.org/abs/2412.08559</guid>
<content:encoded><![CDATA[
arXiv:2412.08559v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2412.09059</link>
<guid>https://arxiv.org/abs/2412.09059</guid>
<content:encoded><![CDATA[
arXiv:2412.09059v5 Announce Type: replace 
Abstract: Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SB between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SB problems, and various other examples. We also test our approach on an Entropic Optimal Transport (EOT) benchmark problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Generative Modeling with Residual Vector Quantization-Based Tokens</title>
<link>https://arxiv.org/abs/2412.10208</link>
<guid>https://arxiv.org/abs/2412.10208</guid>
<content:encoded><![CDATA[
arXiv:2412.10208v3 Announce Type: replace 
Abstract: We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery</title>
<link>https://arxiv.org/abs/2412.13667</link>
<guid>https://arxiv.org/abs/2412.13667</guid>
<content:encoded><![CDATA[
arXiv:2412.13667v2 Announce Type: replace 
Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Policy Learning under Concept Drifts</title>
<link>https://arxiv.org/abs/2412.14297</link>
<guid>https://arxiv.org/abs/2412.14297</guid>
<content:encoded><![CDATA[
arXiv:2412.14297v2 Announce Type: replace 
Abstract: Distributionally robust policy learning aims to find a policy that performs well under the worst-case distributional shift, and yet most existing methods for robust policy learning consider the worst-case joint distribution of the covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative when we have more information on the source of distributional shifts. This paper studies a more nuanced problem -- robust policy learning under the concept drift, when only the conditional relationship between the outcome and the covariate changes. To this end, we first provide a doubly-robust estimator for evaluating the worst-case average reward of a given policy under a set of perturbed conditional distributions. We show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters are estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the estimated policy value within a given policy class $\Pi$, and show that the sub-optimality gap of the proposed algorithm is of the order $\kappa(\Pi)n^{-1/2}$, where $\kappa(\Pi)$ is the entropy integral of $\Pi$ under the Hamming distance and $n$ is the sample size. A matching lower bound is provided to show the optimality of the rate. The proposed methods are implemented and evaluated in numerical studies, demonstrating substantial improvement compared with existing benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principal-Agent Bandit Games with Self-Interested and Exploratory Learning Agents</title>
<link>https://arxiv.org/abs/2412.16318</link>
<guid>https://arxiv.org/abs/2412.16318</guid>
<content:encoded><![CDATA[
arXiv:2412.16318v2 Announce Type: replace 
Abstract: We study the repeated principal-agent bandit game, where the principal indirectly interacts with the unknown environment by proposing incentives for the agent to play arms. Most existing work assumes the agent has full knowledge of the reward means and always behaves greedily, but in many online marketplaces, the agent needs to learn the unknown environment and sometimes explore. Motivated by such settings, we model a self-interested learning agent with exploration behaviors who iteratively updates reward estimates and either selects an arm that maximizes the estimated reward plus incentive or explores arbitrarily with a certain probability. As a warm-up, we first consider a self-interested learning agent without exploration. We propose algorithms for both i.i.d. and linear reward settings with bandit feedback in a finite horizon $T$, achieving regret bounds of $\widetilde{O}(\sqrt{T})$ and $\widetilde{O}( T^{2/3} )$, respectively. Specifically, these algorithms are established upon a novel elimination framework coupled with newly-developed search algorithms which accommodate the uncertainty arising from the learning behavior of the agent. We then extend the framework to handle the exploratory learning agent and develop an algorithm to achieve a $\widetilde{O}(T^{2/3})$ regret bound in i.i.d. reward setup by enhancing the robustness of our elimination framework to the potential agent exploration. Finally, when reducing our agent behaviors to the one studied in (Dogan et al., 2023a), we propose an algorithm based on our robust framework, which achieves a $\widetilde{O}(\sqrt{T})$ regret bound, significantly improving upon their $\widetilde{O}(T^{11/12})$ bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating Flow Matching ODE Dynamics with Respect to Data Geometries</title>
<link>https://arxiv.org/abs/2412.18730</link>
<guid>https://arxiv.org/abs/2412.18730</guid>
<content:encoded><![CDATA[
arXiv:2412.18730v3 Announce Type: replace 
Abstract: Flow matching (FM) models extend ODE sampler based diffusion models into a general framework, significantly reducing sampling steps through learned vector fields. However, the theoretical understanding of FM models, particularly how their sample trajectories interact with underlying data geometry, remains underexplored. A rigorous theoretical analysis of FM ODE is essential for sample quality, stability, and broader applicability. In this paper, we advance the theory of FM models through a comprehensive analysis of sample trajectories. Central to our theory is the discovery that the denoiser, a key component of FM models, guides ODE dynamics through attracting and absorbing behaviors that adapt to the data geometry. We identify and analyze the three stages of ODE evolution: in the initial and intermediate stages, trajectories move toward the mean and local clusters of the data. At the terminal stage, we rigorously establish the convergence of FM ODE under weak assumptions, addressing scenarios where the data lie on a low-dimensional submanifold-cases that previous results could not handle. Our terminal stage analysis offers insights into the memorization phenomenon and establishes equivariance properties of FM ODEs. These findings bridge critical gaps in understanding flow matching models, with practical implications for optimizing sampling strategies and architectures guided by the intrinsic geometry of data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph</title>
<link>https://arxiv.org/abs/2501.00659</link>
<guid>https://arxiv.org/abs/2501.00659</guid>
<content:encoded><![CDATA[
arXiv:2501.00659v2 Announce Type: replace 
Abstract: Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is 'no' provided they have more than one layer -- they can distinguish sequences with permuted tokens without the need for explicit PEs. This follows from the fact that a cascade of (permutation invariant) set processors can collectively exhibit sequence-sensitive behavior in the autoregressive setting. This property has been known since early efforts (contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated, leading to recent rediscoveries. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2/3, but perhaps also due to the lack of a clear explanation in prior work, despite being commonly understood by practitioners in the past. Here we review the long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their inputs), as well as the origin of this result, and hope to re-establish it as a common knowledge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorGRaD: Tensor Gradient Robust Decomposition for Memory-Efficient Neural Operator Training</title>
<link>https://arxiv.org/abs/2501.02379</link>
<guid>https://arxiv.org/abs/2501.02379</guid>
<content:encoded><![CDATA[
arXiv:2501.02379v2 Announce Type: replace 
Abstract: Scientific problems require resolving multi-scale phenomena across different resolutions and learning solution operators in infinite-dimensional function spaces. Neural operators provide a powerful framework for this, using tensor-parameterized layers to capture complex, multi-dimensional relationships. However, scaling neural operators to high-resolution problems leads to significant computational demands, making the training of industrial-scale models prohibitive. In this work, we introduce \textbf{TensorGRaD}, a novel method that directly addresses the memory challenges associated with optimizing large tensor-structured weights. Our approach, based on a \texit{robust tensor decomposition}, factorizes gradients as the sum of a low-rank tensor and a sparse one to efficiently capture information within optimizer states, including outliers. Additionally, we provide a recipe for mixed precision training of TensorGRaD, achieving further memory savings without sacrificing accuracy. We showcase the effectiveness of TensorGRaD on Fourier Neural Operators, a class of models crucial for solving partial differential equations (PDE). We provide theoretical guarantees for TensorGRaD, demonstrating its fundamental advantage over matrix-based gradient compression methods. We empirically demonstrate large improvements across various PDE tasks, including the challenging turbulent Navier-Stokes case at a Reynolds number of $10^5$. TensorGRaD reduces total memory usage by over $50\%$ while maintaining and sometimes even improving accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?</title>
<link>https://arxiv.org/abs/2501.05000</link>
<guid>https://arxiv.org/abs/2501.05000</guid>
<content:encoded><![CDATA[
arXiv:2501.05000v4 Announce Type: replace 
Abstract: Energy communities (ECs) play a key role in enabling local demand shifting and enhancing self-sufficiency, as energy systems transition toward decentralized structures with high shares of renewable generation. To optimally operate them, accurate short-term load forecasting is essential, particularly for implementing demand-side management strategies. With the recent rise of deep learning methods, data-driven forecasting has gained significant attention, however, it remains insufficiently explored in many practical contexts. Therefore, this study evaluates the effectiveness of state-of-the-art deep learning models -- including LSTM, xLSTM, and Transformer architectures -- compared to traditional benchmarks such as K-Nearest Neighbors (KNN) and persistence forecasting, across varying community size, historical data availability, and model complexity. Additionally, we assess the benefits of transfer learning using publicly available synthetic load profiles. On average, transfer learning improves the normalized mean absolute error by 1.97%pt when only two months of training data are available. Interestingly, for less than six months of training data, simple persistence models outperform deep learning architectures in forecast accuracy. The practical value of improved forecasting is demonstrated using a mixed-integer linear programming optimization for ECs with a shared battery energy storage system. The most accurate deep learning model achieves an average reduction in financial energy costs of 8.06%. Notably, a simple KNN approach achieves average savings of 8.01%, making it a competitive and robust alternative. All implementations are publicly available to facilitate reproducibility. These findings offer actionable insights for ECs, and they highlight when the additional complexity of deep learning is warranted by performance gains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</title>
<link>https://arxiv.org/abs/2501.08219</link>
<guid>https://arxiv.org/abs/2501.08219</guid>
<content:encoded><![CDATA[
arXiv:2501.08219v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks, leading to widespread adoption in both research and industry. However, their inference workloads are computationally and energy intensive, raising concerns about sustainability and environmental impact. As LLMs continue to scale, it becomes essential to identify and optimize the factors that influence their runtime efficiency without compromising performance. In this work, we systematically investigate the energy-performance trade-offs of LLMs during inference. We benchmark models of varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1, LLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question answering, commonsense reasoning, and factual generation. We analyze the effect of input characteristics, such as sequence length, entropy, named entity density and so on. Furthermore, we examine the impact of hardware-level optimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring how different GPU clock settings affect latency and power consumption. Our empirical findings show that model architecture, input complexity, and clock configuration significantly influence inference efficiency. By correlating input features with energy metrics and evaluating DVFS behavior, we identify practical strategies that reduce energy consumption by up to 30% while preserving model quality. This study provides actionable insights for designing energy-efficient and sustainable LLM inference systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Seismic Artificial Intelligence with Uncertainty</title>
<link>https://arxiv.org/abs/2501.14809</link>
<guid>https://arxiv.org/abs/2501.14809</guid>
<content:encoded><![CDATA[
arXiv:2501.14809v2 Announce Type: replace 
Abstract: Artificial intelligence has transformed the seismic community with deep learning models (DLMs) that are trained to complete specific tasks within workflows. However, there is still lack of robust evaluation frameworks for evaluating and comparing DLMs. We address this gap by designing an evaluation framework that jointly incorporates two crucial aspects: performance uncertainty and learning efficiency. To target these aspects, we meticulously construct the training, validation, and test splits using a clustering method tailored to seismic data and enact an expansive training design to segregate performance uncertainty arising from stochastic training processes and random data sampling. The framework's ability to guard against misleading declarations of model superiority is demonstrated through evaluation of PhaseNet [1], a popular seismic phase picking DLM, under 3 training approaches. Our framework helps practitioners choose the best model for their problem and set performance expectations by explicitly analyzing model performance with uncertainty at varying budgets of training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIP: Perturbation-based Iterative Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2501.15278</link>
<guid>https://arxiv.org/abs/2501.15278</guid>
<content:encoded><![CDATA[
arXiv:2501.15278v2 Announce Type: replace 
Abstract: The rapid increase in the parameter counts of Large Language Models (LLMs), reaching billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To ease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in environments with constrained resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Low-Rank Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2501.15361</link>
<guid>https://arxiv.org/abs/2501.15361</guid>
<content:encoded><![CDATA[
arXiv:2501.15361v4 Announce Type: replace 
Abstract: While parameter-efficient fine-tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) offer computationally efficient adaptations of Large Language Models (LLMs), their practical deployment often assumes centralized data and training environments. However, real-world scenarios frequently involve distributed, privacy-sensitive datasets that require decentralized solutions. Federated learning (FL) addresses data privacy by coordinating model updates across clients, but it is typically based on centralized aggregation through a parameter server, which can introduce bottlenecks and communication constraints. Decentralized learning, in contrast, eliminates this dependency by enabling direct collaboration between clients, improving scalability and efficiency in distributed environments. Despite its advantages, decentralized LLM fine-tuning remains underexplored. In this work, we propose Dec-LoRA, a decentralized fine-tuning algorithm for LLMs based on LoRA. Through extensive experiments on BERT and LLaMA-2 models, we demonstrate that Dec-LoRA achieves performance comparable to centralized LoRA under various conditions, including data heterogeneity and quantization constraints. Additionally, we provide a rigorous theoretical guarantee proving the convergence of our algorithm to a stationary point for non-convex and smooth loss functions. These findings highlight the potential of Dec-LoRA for scalable LLM fine-tuning in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Machine Learning Potentials via Difference Vectors Based on Local Atomic Environments</title>
<link>https://arxiv.org/abs/2501.16398</link>
<guid>https://arxiv.org/abs/2501.16398</guid>
<content:encoded><![CDATA[
arXiv:2501.16398v2 Announce Type: replace 
Abstract: Constructing efficient and diverse datasets is essential for the development of accurate machine learning potentials (MLPs) in atomistic simulations. However, existing approaches often suffer from data redundancy and high computational costs. Herein, we propose a new method--Difference Vectors based on Local Atomic Environments (DV-LAE)--that encodes structural differences via histogram-based descriptors and enables visual analysis through t-SNE dimensionality reduction. This approach facilitates redundancy detection and dataset optimization while preserving structural diversity. We demonstrate that DV-LAE significantly reduces dataset size and training time across various materials systems, including high-pressure hydrogen, iron-hydrogen binaries, magnesium hydrides, and carbon allotropes, with minimal compromise in prediction accuracy. For instance, in the $\alpha$-Fe/H system, maintaining a highly similar MLP accuracy, the dataset size was reduced by 56%, and the training time per iteration dropped by over 50%. Moreover, we show how visualizing the DV-LAE representation aids in identifying out-of-distribution data by examining the spatial distribution of high-error prediction points, providing a robust reliability metric for new structures during simulations. Our results highlight the utility of local environment visualization not only as an interpretability tool but also as a practical means for accelerating MLP development and ensuring data efficiency in large-scale atomistic modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.17077</link>
<guid>https://arxiv.org/abs/2501.17077</guid>
<content:encoded><![CDATA[
arXiv:2501.17077v2 Announce Type: replace 
Abstract: Interpretability is crucial for ensuring RL systems align with human values. However, it remains challenging to achieve in complex decision making domains. Existing methods frequently attempt interpretability at the level of fundamental model units, such as neurons or decision nodes: an approach which scales poorly to large models. Here, we instead propose an approach to interpretability at the level of functional modularity. We show how encouraging sparsity and locality in network weights leads to the emergence of functional modules in RL policy networks. To detect these modules, we develop an extended Louvain algorithm which uses a novel `correlation alignment' metric to overcome the limitations of standard network analysis techniques when applied to neural network architectures. Applying these methods to 2D and 3D MiniGrid environments reveals the consistent emergence of distinct navigational modules for different axes, and we further demonstrate how these functions can be validated through direct interventions on network weights prior to inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Provably Improves the Convergence of Gradient Descent</title>
<link>https://arxiv.org/abs/2501.18092</link>
<guid>https://arxiv.org/abs/2501.18092</guid>
<content:encoded><![CDATA[
arXiv:2501.18092v4 Announce Type: replace 
Abstract: Learn to Optimize (L2O) trains deep neural network based solvers for optimization, achieving success in accelerating convex problems and improving non-convex solutions. However, L2O lacks rigorous theoretical backing for its own training convergence, as existing analyses often use unrealistic assumptions -- a gap this work highlights empirically. We bridge this gap by proving the training convergence of L2O models that learn Gradient Descent (GD) hyperparameters for quadratic programming, leveraging the Neural Tangent Kernel (NTK) theory. We propose a deterministic initialization strategy to support our theoretical results and promote stable training over extended optimization horizons by mitigating gradient explosion. Our L2O framework demonstrates over 50\% better optimality against GD and superior robustness over state-of-the-art L2O methods on synthetic datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algebra Unveils Deep Learning -- An Invitation to Neuroalgebraic Geometry</title>
<link>https://arxiv.org/abs/2501.18915</link>
<guid>https://arxiv.org/abs/2501.18915</guid>
<content:encoded><![CDATA[
arXiv:2501.18915v2 Announce Type: replace 
Abstract: In this position paper, we promote the study of function spaces parameterized by machine learning models through the lens of algebraic geometry. To this end, we focus on algebraic models, such as neural networks with polynomial activations, whose associated function spaces are semi-algebraic varieties. We outline a dictionary between algebro-geometric invariants of these varieties, such as dimension, degree, and singularities, and fundamental aspects of machine learning, such as sample complexity, expressivity, training dynamics, and implicit bias. Along the way, we review the literature and discuss ideas beyond the algebraic domain. This work lays the foundations of a research direction bridging algebraic geometry and deep learning, that we refer to as neuroalgebraic geometry.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabFSBench: Tabular Benchmark for Feature Shifts in Open Environments</title>
<link>https://arxiv.org/abs/2501.18935</link>
<guid>https://arxiv.org/abs/2501.18935</guid>
<content:encoded><![CDATA[
arXiv:2501.18935v3 Announce Type: replace 
Abstract: Tabular data is widely utilized in various machine learning tasks. Current tabular learning research predominantly focuses on closed environments, while in real-world applications, open environments are often encountered, where distribution and feature shifts occur, leading to significant degradation in model performance. Previous research has primarily concentrated on mitigating distribution shifts, whereas feature shifts, a distinctive and unexplored challenge of tabular data, have garnered limited attention. To this end, this paper conducts the first comprehensive study on feature shifts in tabular data and introduces the first tabular feature-shift benchmark (TabFSBench). TabFSBench evaluates impacts of four distinct feature-shift scenarios on four tabular model categories across various datasets and assesses the performance of large language models (LLMs) and tabular LLMs in the tabular benchmark for the first time. Our study demonstrates three main observations: (1) most tabular models have the limited applicability in feature-shift scenarios; (2) the shifted feature set importance has a linear relationship with model performance degradation; (3) model performance in closed environments correlates with feature-shift performance. Future research direction is also explored for each observation.
  Benchmark: https://github.com/LAMDASZ-ML/TabFSBench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected</title>
<link>https://arxiv.org/abs/2501.19107</link>
<guid>https://arxiv.org/abs/2501.19107</guid>
<content:encoded><![CDATA[
arXiv:2501.19107v2 Announce Type: replace 
Abstract: Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling and even exceed fully connected baselines in zero-shot tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking</title>
<link>https://arxiv.org/abs/2501.19358</link>
<guid>https://arxiv.org/abs/2501.19358</guid>
<content:encoded><![CDATA[
arXiv:2501.19358v3 Announce Type: replace 
Abstract: This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of Diffusion Models with Application to Schedule Design</title>
<link>https://arxiv.org/abs/2502.00180</link>
<guid>https://arxiv.org/abs/2502.00180</guid>
<content:encoded><![CDATA[
arXiv:2502.00180v2 Announce Type: replace 
Abstract: Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity assumption, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged to design a noise schedule that aligns effectively with the characteristics of the data. The spectral perspective also provides insights into the underlying dynamics and sheds light on the relationship between spectral properties and noise schedule structure. Our results lead to scheduling curves that are dependent on the spectral content of the data, offering a theoretical justification for some of the heuristics taken by practitioners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Abstraction Learning based on the Semantic Embedding Principle</title>
<link>https://arxiv.org/abs/2502.00407</link>
<guid>https://arxiv.org/abs/2502.00407</guid>
<content:encoded><![CDATA[
arXiv:2502.00407v2 Announce Type: replace 
Abstract: Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is semantic embedding, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the Stiefel manifold. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Over-parameterized Matrix Sensing from Noisy Measurements via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2502.00463</link>
<guid>https://arxiv.org/abs/2502.00463</guid>
<content:encoded><![CDATA[
arXiv:2502.00463v3 Announce Type: replace 
Abstract: We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$ of the target matrix $X_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorization $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r > r_\star $, with $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + \lambda I)^{-1} $ and $ (R^\top R + \lambda I)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $\lambda$ and are sensitive to step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter $\lambda$ and enabling faster convergence with larger step sizes. We theoretically prove that APGD convergences to a near-optimal error at a linear rate. We further show that APGD can be extended to deal with other low-rank matrix estimation tasks, also with a theoretical guarantee of linear convergence. To validate the effectiveness and scalability of the proposed APGD, we conduct simulated and real-world experiments on a wide range of low-rank estimation problems, including noisy matrix sensing, weighted PCA, 1-bit matrix completion, and matrix completion. The extensive results demonstrate that APGD consistently achieves the fastest convergence and the lowest computation time compared to the existing alternatives.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Safety Alignment is Divergence Estimation in Disguise</title>
<link>https://arxiv.org/abs/2502.00657</link>
<guid>https://arxiv.org/abs/2502.00657</guid>
<content:encoded><![CDATA[
arXiv:2502.00657v2 Announce Type: replace 
Abstract: We present a theoretical framework showing that popular LLM alignment methods, including RLHF and its variants, can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance-refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science</title>
<link>https://arxiv.org/abs/2502.01159</link>
<guid>https://arxiv.org/abs/2502.01159</guid>
<content:encoded><![CDATA[
arXiv:2502.01159v2 Announce Type: replace 
Abstract: The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Transformer World Models for Data-Efficient RL</title>
<link>https://arxiv.org/abs/2502.01591</link>
<guid>https://arxiv.org/abs/2502.01591</guid>
<content:encoded><![CDATA[
arXiv:2502.01591v2 Announce Type: replace 
Abstract: We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model</title>
<link>https://arxiv.org/abs/2502.01989</link>
<guid>https://arxiv.org/abs/2502.01989</guid>
<content:encoded><![CDATA[
arXiv:2502.01989v3 Announce Type: replace 
Abstract: Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel LRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\times15$, while standard diffusion model completely fails.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer Drug Response Prediction</title>
<link>https://arxiv.org/abs/2502.04034</link>
<guid>https://arxiv.org/abs/2502.04034</guid>
<content:encoded><![CDATA[
arXiv:2502.04034v2 Announce Type: replace 
Abstract: The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug--trained solely on in vitro cell line data without access to target-domain data--consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</title>
<link>https://arxiv.org/abs/2502.04420</link>
<guid>https://arxiv.org/abs/2502.04420</guid>
<content:encoded><![CDATA[
arXiv:2502.04420v4 Announce Type: replace 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans Coexist, So Must Embodied Artificial Agents</title>
<link>https://arxiv.org/abs/2502.04809</link>
<guid>https://arxiv.org/abs/2502.04809</guid>
<content:encoded><![CDATA[
arXiv:2502.04809v3 Announce Type: replace 
Abstract: This paper introduces the concept of coexistence for embodied artificial agents and argues that it is a prerequisite for long-term, in-the-wild interaction with humans. Contemporary embodied artificial agents excel in static, predefined tasks but fall short in dynamic and long-term interactions with humans. On the other hand, humans can adapt and evolve continuously, exploiting the situated knowledge embedded in their environment and other agents, thus contributing to meaningful interactions. We take an interdisciplinary approach at different levels of organization, drawing from biology and design theory, to understand how human and non-human organisms foster entities that coexist within their specific environments. Finally, we propose key research directions for the artificial intelligence community to develop coexisting embodied agents, focusing on the principles, hardware and learning methods responsible for shaping them.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</title>
<link>https://arxiv.org/abs/2502.06051</link>
<guid>https://arxiv.org/abs/2502.06051</guid>
<content:encoded><![CDATA[
arXiv:2502.06051v2 Announce Type: replace 
Abstract: Although many popular reinforcement learning algorithms are underpinned by $f$-divergence regularization, their sample complexity with respect to the \emph{regularized objective} still lacks a tight characterization. In this paper, we analyze $f$-divergence-regularized offline policy learning. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we give the first $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for contextual bandits, surpassing existing $\tilde{O}(\epsilon^{-1})$ bound under all-policy concentrability and $\tilde{O}(\epsilon^{-2})$ bound under single-policy concentrability. Our analysis for general function approximation leverages the principle of pessimism in the face of uncertainty to refine a mean-value-type argument to its extreme. This in turn leads to a novel moment-based technique, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. We further propose a lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the strong convexity of reverse KL. In addition, for $f$-divergences with strongly convex $f$, to which reverse KL \emph{does not} belong, we show that the sharp sample complexity $\tilde{\Theta}(\epsilon^{-1})$ is achievable even without single-policy concentrability. In this case, the algorithm design can get rid of pessimistic estimators. We further extend our analysis to dueling bandits, and we believe these results take a significant step toward a comprehensive understanding of $f$-divergence-regularized policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes a good feedforward computational graph?</title>
<link>https://arxiv.org/abs/2502.06751</link>
<guid>https://arxiv.org/abs/2502.06751</guid>
<content:encoded><![CDATA[
arXiv:2502.06751v2 Announce Type: replace 
Abstract: As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logarithmic Regret for Online KL-Regularized Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.07460</link>
<guid>https://arxiv.org/abs/2502.07460</guid>
<content:encoded><![CDATA[
arXiv:2502.07460v5 Announce Type: replace 
Abstract: Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy amplification by random allocation</title>
<link>https://arxiv.org/abs/2502.08202</link>
<guid>https://arxiv.org/abs/2502.08202</guid>
<content:encoded><![CDATA[
arXiv:2502.08202v3 Announce Type: replace 
Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization [Chua et al., 2024a, Choquette-Choo et al., 2024] and is also motivated by communication-efficient high-dimensional private aggregation [Asi et al., 2025]. Existing analyses of this scheme either rely on privacy amplification by shuffling which leads to overly conservative bounds or require Monte Carlo simulations that are computationally prohibitive in most practical scenarios.
  We give the first theoretical guarantees and numerical estimation algorithms for this sampling scheme. In particular, we demonstrate that the privacy guarantees of random $k$-out-of-$t$ allocation can be upper bounded by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+o(1))k/t$. Further, we provide two additional analysis techniques that lead to numerical improvements in several parameter regimes. Altogether, our bounds give efficiently-computable and nearly tight numerical results for random allocation applied to Gaussian noise addition.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Total-Variance and Signal-to-Noise-Ratio Improves Diffusion Models</title>
<link>https://arxiv.org/abs/2502.08598</link>
<guid>https://arxiv.org/abs/2502.08598</guid>
<content:encoded><![CDATA[
arXiv:2502.08598v2 Announce Type: replace 
Abstract: The long sampling time of diffusion models remains a significant bottleneck, which can be mitigated by reducing the number of diffusion time steps. However, the quality of samples with fewer steps is highly dependent on the noise schedule, i.e., the specific manner in which noise is introduced and the signal is reduced at each step. Although prior work has improved upon the original variance-preserving and variance-exploding schedules, these approaches $\textit{passively}$ adjust the total variance, without direct control over it. In this work, we propose a novel total-variance/signal-to-noise-ratio disentangled (TV/SNR) framework, where TV and SNR can be controlled independently. Our approach reveals that schedules where the TV explodes exponentially can often be improved by adopting a constant TV schedule while preserving the same SNR schedule. Furthermore, generalizing the SNR schedule of the optimal transport flow matching significantly improves the generation performance. Our findings hold across various reverse diffusion solvers and diverse applications, including molecular structure and image generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative</title>
<link>https://arxiv.org/abs/2502.08942</link>
<guid>https://arxiv.org/abs/2502.08942</guid>
<content:encoded><![CDATA[
arXiv:2502.08942v2 Announce Type: replace 
Abstract: While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. With recent progress in large language models and time series learning, we revisit the integration of paired texts with time series through the Platonic Representation Hypothesis, which posits that representations of different modalities converge to shared spaces. In this context, we identify that time-series-paired texts may naturally exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance without modifying model architectures. Code available at https://github.com/iDEA-iSAIL-Lab-UIUC/TaTS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2502.09254</link>
<guid>https://arxiv.org/abs/2502.09254</guid>
<content:encoded><![CDATA[
arXiv:2502.09254v2 Announce Type: replace 
Abstract: Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Path Structural Encoding for Graph Transformers</title>
<link>https://arxiv.org/abs/2502.09365</link>
<guid>https://arxiv.org/abs/2502.09365</guid>
<content:encoded><![CDATA[
arXiv:2502.09365v2 Announce Type: replace 
Abstract: Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation</title>
<link>https://arxiv.org/abs/2502.10762</link>
<guid>https://arxiv.org/abs/2502.10762</guid>
<content:encoded><![CDATA[
arXiv:2502.10762v2 Announce Type: replace 
Abstract: User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
arXiv:2502.11196v2 Announce Type: replace 
Abstract: Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Selection to Generation: A Survey of LLM-based Active Learning</title>
<link>https://arxiv.org/abs/2502.11767</link>
<guid>https://arxiv.org/abs/2502.11767</guid>
<content:encoded><![CDATA[
arXiv:2502.11767v2 Announce Type: replace 
Abstract: Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation</title>
<link>https://arxiv.org/abs/2502.13576</link>
<guid>https://arxiv.org/abs/2502.13576</guid>
<content:encoded><![CDATA[
arXiv:2502.13576v2 Announce Type: replace 
Abstract: Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning</title>
<link>https://arxiv.org/abs/2502.15296</link>
<guid>https://arxiv.org/abs/2502.15296</guid>
<content:encoded><![CDATA[
arXiv:2502.15296v2 Announce Type: replace 
Abstract: Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. We introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. We benchmark EVTSF performance using three real-world datasets and compare it against three potential solutions employing SOTA MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, particularly on expanding variables. Notably, STEV, with only 5% of observations from the expanding period, is on par with SOTA MTSF models trained with complete observations. Further exploration of various expanding strategies underscores the generalizability of STEV in real-world applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning</title>
<link>https://arxiv.org/abs/2502.15455</link>
<guid>https://arxiv.org/abs/2502.15455</guid>
<content:encoded><![CDATA[
arXiv:2502.15455v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) is computationally expensive, and Low-Rank Adaptation (LoRA) provides a cost-effective solution by approximating weight updates through low-rank matrices. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Dropout and Multi-Head Random Initialization, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Our approach not only improves performance in MTL but also reduces GPU memory usage and training time. Experiments show that R-LoRA's gains stem from increased diversity in the head matrices, demonstrating its effectiveness for multi-task learning. The code is available at https://github.com/jinda-liu/R-LoRA
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title>
<link>https://arxiv.org/abs/2502.15798</link>
<guid>https://arxiv.org/abs/2502.15798</guid>
<content:encoded><![CDATA[
arXiv:2502.15798v2 Announce Type: replace 
Abstract: Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS, consistently reducing overconfidence while preserving richer feature representations. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems</title>
<link>https://arxiv.org/abs/2502.17019</link>
<guid>https://arxiv.org/abs/2502.17019</guid>
<content:encoded><![CDATA[
arXiv:2502.17019v2 Announce Type: replace 
Abstract: Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, PDE solving, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
arXiv:2502.18137v3 Announce Type: replace 
Abstract: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training</title>
<link>https://arxiv.org/abs/2502.19726</link>
<guid>https://arxiv.org/abs/2502.19726</guid>
<content:encoded><![CDATA[
arXiv:2502.19726v2 Announce Type: replace 
Abstract: Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose \methodname, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases</title>
<link>https://arxiv.org/abs/2502.20317</link>
<guid>https://arxiv.org/abs/2502.20317</guid>
<content:encoded><![CDATA[
arXiv:2502.20317v4 Announce Type: replace 
Abstract: Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach</title>
<link>https://arxiv.org/abs/2503.00206</link>
<guid>https://arxiv.org/abs/2503.00206</guid>
<content:encoded><![CDATA[
arXiv:2503.00206v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) methods frequently assume that each new observation completely reflects the environment's state, thereby guaranteeing Markovian (one-step) transitions. In practice, partial observability or sensor/actuator noise often invalidates this assumption. This paper proposes a systematic methodology for detecting such violations, combining a partial correlation-based causal discovery process (PCMCI) with a novel Markov Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property.
  Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even substantial observation noise sometimes fails to induce strong multi-lag dependencies in certain domains (e.g., Acrobot). In contrast, dimension-dropping investigations show that excluding some state variables (e.g., angular velocities in CartPole and Pendulum) significantly reduces returns and increases MVS, while removing other dimensions has minimal impact.
  These findings emphasize the importance of locating and safeguarding the most causally essential dimensions in order to preserve effective single-step learning. By integrating partial correlation tests with RL performance outcomes, the proposed approach precisely identifies when and where the Markov assumption is violated. This framework offers a principled mechanism for developing robust policies, informing representation learning, and addressing partial observability in real-world RL scenarios. All code and experimental logs are accessible for reproducibility (https://github.com/ucsb/markovianess).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[
arXiv:2503.01450v4 Announce Type: replace 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models</title>
<link>https://arxiv.org/abs/2503.01461</link>
<guid>https://arxiv.org/abs/2503.01461</guid>
<content:encoded><![CDATA[
arXiv:2503.01461v2 Announce Type: replace 
Abstract: Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems</title>
<link>https://arxiv.org/abs/2503.01891</link>
<guid>https://arxiv.org/abs/2503.01891</guid>
<content:encoded><![CDATA[
arXiv:2503.01891v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \textbf{63.77\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Randomness in Model and Data Partitioning for Privacy Amplification</title>
<link>https://arxiv.org/abs/2503.03043</link>
<guid>https://arxiv.org/abs/2503.03043</guid>
<content:encoded><![CDATA[
arXiv:2503.03043v2 Announce Type: replace 
Abstract: We study how inherent randomness in the training process -- where each sample (or client in federated learning) contributes only to a randomly selected portion of training -- can be leveraged for privacy amplification. This includes (1) data partitioning, where a sample participates in only a subset of training iterations, and (2) model partitioning, where a sample updates only a subset of the model parameters. We apply our framework to model parallelism in federated learning, where each client updates a randomly selected subnetwork to reduce memory and computational overhead, and show that existing methods, e.g. model splitting or dropout, provide a significant privacy amplification gain not captured by previous privacy analysis techniques. Additionally, we introduce Balanced Iteration Subsampling, a new data partitioning method where each sample (or client) participates in a fixed number of training iterations. We show that this method yields stronger privacy amplification than Poisson (i.i.d.) sampling of data (or clients). Our results demonstrate that randomness in the training process, which is structured rather than i.i.d. and interacts with data in complex ways, can be systematically leveraged for significant privacy amplification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Reliable Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2503.04363</link>
<guid>https://arxiv.org/abs/2503.04363</guid>
<content:encoded><![CDATA[
arXiv:2503.04363v2 Announce Type: replace 
Abstract: Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRACode: LoRA Adapters for Code Embeddings</title>
<link>https://arxiv.org/abs/2503.05315</link>
<guid>https://arxiv.org/abs/2503.05315</guid>
<content:encoded><![CDATA[
arXiv:2503.05315v2 Announce Type: replace 
Abstract: Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations. To foster research in this area, we make our code and pre-trained models publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOPO: Neural Combinatorial Optimization via Best-anchored and Objective-guided Preference Optimization</title>
<link>https://arxiv.org/abs/2503.07580</link>
<guid>https://arxiv.org/abs/2503.07580</guid>
<content:encoded><![CDATA[
arXiv:2503.07580v3 Announce Type: replace 
Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising approach for NP-hard problems. However, prevailing RL-based methods suffer from low sample efficiency due to sparse rewards and underused solutions. We propose Best-anchored and Objective-guided Preference Optimization (BOPO), a training paradigm that leverages solution preferences via objective values. It introduces: (1) a best-anchored preference pair construction for better explore and exploit solutions, and (2) an objective-guided pairwise loss function that adaptively scales gradients via objective differences, removing reliance on reward models or reference policies. Experiments on Job-shop Scheduling Problem (JSP), Traveling Salesman Problem (TSP), and Flexible Job-shop Scheduling Problem (FJSP) show BOPO outperforms state-of-the-art neural methods, reducing optimality gaps impressively with efficient inference. BOPO is architecture-agnostic, enabling seamless integration with existing NCO models, and establishes preference optimization as a principled framework for combinatorial optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability</title>
<link>https://arxiv.org/abs/2503.09532</link>
<guid>https://arxiv.org/abs/2503.09532</guid>
<content:encoded><![CDATA[
arXiv:2503.09532v3 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across eight diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across eight recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance. For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at: www.neuronpedia.org/sae-bench
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance</title>
<link>https://arxiv.org/abs/2503.13509</link>
<guid>https://arxiv.org/abs/2503.13509</guid>
<content:encoded><![CDATA[
arXiv:2503.13509v2 Announce Type: replace 
Abstract: We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection</title>
<link>https://arxiv.org/abs/2503.16072</link>
<guid>https://arxiv.org/abs/2503.16072</guid>
<content:encoded><![CDATA[
arXiv:2503.16072v2 Announce Type: replace 
Abstract: Most toxicity detection models treat toxicity as an intrinsic property of text, overlooking the role of context in shaping its impact. Drawing on interdisciplinary research, we reconceptualise toxicity as a socially emergent stress signal. We introduce a new framework for toxicity detection, including a formal definition and metric, and validate our approach on a novel dataset, demonstrating improved contextual sensitivity and adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</title>
<link>https://arxiv.org/abs/2504.00010</link>
<guid>https://arxiv.org/abs/2504.00010</guid>
<content:encoded><![CDATA[
arXiv:2504.00010v2 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present $\textbf{LayerCraft}$, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) $\textit{structured generation}$ from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) $\textit{layered object integration}$, allowing users to insert and customize objects -- such as characters or props -- across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the $\textbf{ChainArchitect}$ for CoT-driven layout planning, and the $\textbf{Object Integration Network (OIN)}$ for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released at https://github.com/PeterYYZhang/LayerCraft.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-guided Representation Learning for Multi-Label Recognition</title>
<link>https://arxiv.org/abs/2504.03801</link>
<guid>https://arxiv.org/abs/2504.03801</guid>
<content:encoded><![CDATA[
arXiv:2504.03801v2 Announce Type: replace 
Abstract: Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at https://github.com/MVL-Lab/SigRL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2504.04799</link>
<guid>https://arxiv.org/abs/2504.04799</guid>
<content:encoded><![CDATA[
arXiv:2504.04799v3 Announce Type: replace 
Abstract: Given two boundary distributions, the Schr\"odinger Bridge (SB) problem seeks the ``most likely`` random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the Topological Schr\"odinger Bridge problem (TSBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable topology-aware stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a closed-form topological SB (TSB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop TSB-based models for matching topological signals by parameterizing the unknowns in the optimal process as (topological) neural networks and learning them through likelihood training. We validate the theoretical results and demonstrate the practical applications of TSB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of TSB-based models to other emerging models, and outline future directions for topological signal matching.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An All-Atom Generative Model for Designing Protein Complexes</title>
<link>https://arxiv.org/abs/2504.13075</link>
<guid>https://arxiv.org/abs/2504.13075</guid>
<content:encoded><![CDATA[
arXiv:2504.13075v2 Announce Type: replace 
Abstract: Proteins typically exist in complexes, interacting with other proteins or biomolecules to perform their specific biological roles. Research on single-chain protein modeling has been extensively and deeply explored, with advancements seen in models like the series of ESM and AlphaFold2. Despite these developments, the study and modeling of multi-chain proteins remain largely uncharted, though they are vital for understanding biological functions. Recognizing the importance of these interactions, we introduce APM (All-Atom Protein Generative Model), a model specifically designed for modeling multi-chain proteins. By integrating atom-level information and leveraging data on multi-chain proteins, APM is capable of precisely modeling inter-chain interactions and designing protein complexes with binding capabilities from scratch. It also performs folding and inverse-folding tasks for multi-chain proteins. Moreover, APM demonstrates versatility in downstream applications: it achieves enhanced performance through supervised fine-tuning (SFT) while also supporting zero-shot sampling in certain tasks, achieving state-of-the-art results. We released our code at https://github.com/bytedance/apm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research</title>
<link>https://arxiv.org/abs/2504.13101</link>
<guid>https://arxiv.org/abs/2504.13101</guid>
<content:encoded><![CDATA[
arXiv:2504.13101v2 Announce Type: replace 
Abstract: Self-Supervised Learning (SSL) powers many current AI systems. As research interest and investment grow, the SSL design space continues to expand. The Platonic view of SSL, following the Platonic Representation Hypothesis (PRH), suggests that despite different methods and engineering approaches, all representations converge to the same Platonic ideal. However, this phenomenon lacks precise theoretical explanation. By synthesizing evidence from Identifiability Theory (IT), we show that the PRH can emerge in SSL. However, current IT cannot explain SSL's empirical success. To bridge the gap between theory and practice, we propose expanding IT into what we term Singular Identifiability Theory (SITh), a broader theoretical framework encompassing the entire SSL pipeline. SITh would allow deeper insights into the implicit data assumptions in SSL and advance the field towards learning more interpretable and generalizable representations. We highlight three critical directions for future research: 1) training dynamics and convergence properties of SSL; 2) the impact of finite samples, batch size, and data diversity; and 3) the role of inductive biases in architecture, augmentations, initialization schemes, and optimizers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17004</link>
<guid>https://arxiv.org/abs/2504.17004</guid>
<content:encoded><![CDATA[
arXiv:2504.17004v2 Announce Type: replace 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19583</link>
<guid>https://arxiv.org/abs/2504.19583</guid>
<content:encoded><![CDATA[
arXiv:2504.19583v2 Announce Type: replace 
Abstract: This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[
arXiv:2505.00812v3 Announce Type: replace 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2505.01199</link>
<guid>https://arxiv.org/abs/2505.01199</guid>
<content:encoded><![CDATA[
arXiv:2505.01199v2 Announce Type: replace 
Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in clinical diagnosis. However, analyzing these signals remains challenging: traditional methods rely on handcrafted features or supervised deep learning models that demand extensive labeled datasets, limiting their scalability and applicability. To address these issues, we propose CaReAQA, an audio-language model that integrates a foundation audio model with the reasoning capabilities of large language models, enabling clinically relevant, open-ended diagnostic responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of annotated medical audio recordings enriched with metadata and paired question-answer examples, intended to drive progress in diagnostic reasoning research. Evaluation results show that CaReAQA achieves 86.2% accuracy on open-ended diagnostic reasoning tasks, outperforming baseline models. It also generalizes well to closed-ended classification tasks, achieving an average accuracy of 56.9% on unseen datasets. Our findings show how audio-language integration and reasoning advances medical diagnostics, enabling efficient AI systems for clinical decision support.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
arXiv:2505.01892v2 Announce Type: replace 
Abstract: With over 700 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.02515</link>
<guid>https://arxiv.org/abs/2505.02515</guid>
<content:encoded><![CDATA[
arXiv:2505.02515v2 Announce Type: replace 
Abstract: Traditional domain generalization approaches predominantly focus on leveraging target domain-aware features while overlooking the critical role of source domain-specific characteristics, particularly in federated settings with inherent data isolation. To address this gap, we propose the Federated Source Domain Awareness Framework (FedSDAF), the first method to systematically exploit source domain-aware features for enhanced federated domain generalization (FedDG). The FedSDAF framework consists of two synergistic components: the Domain-Invariant Adapter, which preserves critical domain-invariant features, and the Domain-Aware Adapter, which extracts and integrates source domain-specific knowledge using a Multihead Self-Attention mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge distillation mechanism that fosters knowledge sharing among clients while safeguarding privacy. Our approach represents the first systematic exploitation of source domain-aware features, resulting in significant advancements in model generalization capability.Extensive experiments on four standard benchmarks (OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently surpasses state-of-the-art federated domain generalization approaches, with accuracy gains of 5.2-13.8%. The source code is available at https://github.com/pizzareapers/FedSDAF.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales</title>
<link>https://arxiv.org/abs/2505.04608</link>
<guid>https://arxiv.org/abs/2505.04608</guid>
<content:encoded><![CDATA[
arXiv:2505.04608v3 Announce Type: replace 
Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but also continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Methods for nonparametric sequential testing -- especially conformal test martingales (CTMs) and anytime-valid inference -- offer promising tools for this monitoring task. However, existing approaches are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (e.g., detecting data shifts that violate certain exchangeability or IID assumptions), do not allow for online adaptation in response to shifts, and/or cannot diagnose the cause of degradation or alarm. In this paper, we address these limitations by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution), quickly detect harmful shifts, and diagnose those harmful shifts as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning</title>
<link>https://arxiv.org/abs/2505.05702</link>
<guid>https://arxiv.org/abs/2505.05702</guid>
<content:encoded><![CDATA[
arXiv:2505.05702v2 Announce Type: replace 
Abstract: The absence of intrinsic adjacency relations and orientation systems in hypergraphs creates fundamental challenges for constructing sheaf Laplacians of arbitrary degrees. We resolve these limitations through symmetric simplicial sets derived directly from hypergraphs, which encode all possible oriented subrelations within each hyperedge as ordered tuples. This construction canonically defines adjacency via facet maps while inherently preserving hyperedge provenance. We establish that the normalized degree zero sheaf Laplacian on our induced symmetric simplicial set reduces exactly to the traditional graph normalized sheaf Laplacian when restricted to graphs, validating its mathematical consistency with prior graph-based sheaf theory. Furthermore, the induced structure preserves all structural information from the original hypergraph, ensuring that every multi-way relational detail is faithfully retained. Leveraging this framework, we introduce Hypergraph Neural Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf Laplacians over symmetric simplicial sets, resolving orientation ambiguity and adjacency sparsity inherent to hypergraph learning. Experimental evaluations demonstrate HNSD's competitive performance across established benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance</title>
<link>https://arxiv.org/abs/2505.07004</link>
<guid>https://arxiv.org/abs/2505.07004</guid>
<content:encoded><![CDATA[
arXiv:2505.07004v2 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</title>
<link>https://arxiv.org/abs/2505.08740</link>
<guid>https://arxiv.org/abs/2505.08740</guid>
<content:encoded><![CDATA[
arXiv:2505.08740v3 Announce Type: replace 
Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</title>
<link>https://arxiv.org/abs/2505.10930</link>
<guid>https://arxiv.org/abs/2505.10930</guid>
<content:encoded><![CDATA[
arXiv:2505.10930v2 Announce Type: replace 
Abstract: Auto-regressive partial differential equation (PDE) foundation models have shown great potential in handling time-dependent data. However, these models suffer from the shortcut problem deeply rooted in auto-regressive prediction, causing error accumulation. The challenge becomes particularly evident for out-of-distribution data, as the pretraining performance may approach random model initialization for downstream tasks with long-term dynamics. To deal with this problem, we propose physics-informed temporal alignment (PITA), a self-supervised learning framework inspired by inverse problem solving. Specifically, PITA aligns the physical dynamics discovered at different time steps on each given PDE trajectory by integrating physics-informed constraints into the self-supervision signal. The alignment is derived from observation data without relying on known physics priors, indicating strong generalization ability to the out-of-distribution data. Extensive experiments show that PITA significantly enhances the accuracy and robustness of existing foundation models on diverse time-dependent PDE data. The code is available at https://github.com/SCAILab-USTC/PITA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2505.11197</link>
<guid>https://arxiv.org/abs/2505.11197</guid>
<content:encoded><![CDATA[
arXiv:2505.11197v2 Announce Type: replace 
Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial for understanding complex cellular processes and behavior. Existing methods leverage optimal transport, Schr\"odinger bridge theory, or their variants to simultaneously infer stochastic, unbalanced dynamics from snapshot data. However, these approaches remain limited in their ability to account for cell-cell interactions. This integration is essential in real-world scenarios since intercellular communications are fundamental life processes and can influence cell state-transition dynamics. To address this challenge, we formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to model unbalanced stochastic interaction dynamics from snapshot data. Inspired by this framework, we further propose CytoBridge, a deep learning algorithm designed to approximate the UMFSB problem. By explicitly modeling cellular transitions, proliferation, and interactions through neural networks, CytoBridge offers the flexibility to learn these processes directly from data. The effectiveness of our method has been extensively validated using both synthetic gene regulatory data and real scRNA-seq datasets. Compared to existing methods, CytoBridge identifies growth, transition, and interaction patterns, eliminates false transitions, and reconstructs the developmental landscape with greater accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Counterfactual Evidences for Node Classification</title>
<link>https://arxiv.org/abs/2505.11396</link>
<guid>https://arxiv.org/abs/2505.11396</guid>
<content:encoded><![CDATA[
arXiv:2505.11396v2 Announce Type: replace 
Abstract: Counterfactual learning is emerging as an important paradigm, rooted in causality, which promises to alleviate common issues of graph neural networks (GNNs), such as fairness and interpretability. However, as in many real-world application domains where conducting randomized controlled trials is impractical, one has to rely on available observational (factual) data to detect counterfactuals. In this paper, we introduce and tackle the problem of searching for counterfactual evidences for the GNN-based node classification task. A counterfactual evidence is a pair of nodes such that, regardless they exhibit great similarity both in the features and in their neighborhood subgraph structures, they are classified differently by the GNN. We develop effective and efficient search algorithms and a novel indexing solution that leverages both node features and structural information to identify counterfactual evidences, and generalizes beyond any specific GNN. Through various downstream applications, we demonstrate the potential of counterfactual evidences to enhance fairness and accuracy of GNNs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games</title>
<link>https://arxiv.org/abs/2505.16401</link>
<guid>https://arxiv.org/abs/2505.16401</guid>
<content:encoded><![CDATA[
arXiv:2505.16401v3 Announce Type: replace 
Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.17155</link>
<guid>https://arxiv.org/abs/2505.17155</guid>
<content:encoded><![CDATA[
arXiv:2505.17155v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu Pro MoE, Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
arXiv:2505.17370v3 Announce Type: replace 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Approach for Child Reading Assessment in the Xhosa Language</title>
<link>https://arxiv.org/abs/2505.17371</link>
<guid>https://arxiv.org/abs/2505.17371</guid>
<content:encoded><![CDATA[
arXiv:2505.17371v2 Announce Type: replace 
Abstract: Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</title>
<link>https://arxiv.org/abs/2505.17662</link>
<guid>https://arxiv.org/abs/2505.17662</guid>
<content:encoded><![CDATA[
arXiv:2505.17662v2 Announce Type: replace 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Electron Cloud in Protein-Ligand Interaction Learning</title>
<link>https://arxiv.org/abs/2505.19014</link>
<guid>https://arxiv.org/abs/2505.19014</guid>
<content:encoded><![CDATA[
arXiv:2505.19014v2 Announce Type: replace 
Abstract: The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulation and signal transduction. Most deep-learning-based prediction approaches focus on structures of atoms or fragments. However, quantum chemical properties, such as electronic structures, are the key to unveiling interaction patterns but remain largely underexplored. To bridge this gap, we propose ECBind, a method for tokenizing electron cloud signals into quantized embeddings, enabling their integration into downstream tasks such as binding affinity prediction. By incorporating electron densities, ECBind helps uncover binding modes that cannot be fully represented by atom-level models. Specifically, to remove the redundancy inherent in electron cloud signals, a structure-aware transformer and hierarchical codebooks encode 3D binding sites enriched with electron structures into tokens. These tokenized codes are then used for specific tasks with labels. To extend its applicability to a wider range of scenarios, we utilize knowledge distillation to develop an electron-cloud-agnostic prediction model. Experimentally, ECBind demonstrates state-of-the-art performance across multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure Pearson and Spearman correlation coefficients, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Graph Learning Over Sets of Temporally-Sparse Data</title>
<link>https://arxiv.org/abs/2505.19193</link>
<guid>https://arxiv.org/abs/2505.19193</guid>
<content:encoded><![CDATA[
arXiv:2505.19193v2 Announce Type: replace 
Abstract: Real-world medical data often includes measurements from multiple signals that are collected at irregular and asynchronous time intervals. For example, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling of different attributes occur in other domains, such as monitoring of large systems using event log files or the spread of fake news on social networks. Effectively learning from such data requires models that can handle sets of temporally sparse and heterogeneous signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design model for learning over irregular sets of temporal signals. Our method achieves state-of-the-art performance in real-world medical tasks, including a 4-point increase in the AUROC score of in-hospital mortality prediction, compared to existing methods. We further showcase GMAN's flexibility by applying it to a fake news detection task. We demonstrate how its interpretability capabilities, including node-level, graph-level, and subset-level importance, allow for transition phases detection and gaining medical insights with real-world high-stakes implications. Finally, we provide theoretical insights on GMAN expressive power.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title>
<link>https://arxiv.org/abs/2505.19433</link>
<guid>https://arxiv.org/abs/2505.19433</guid>
<content:encoded><![CDATA[
arXiv:2505.19433v2 Announce Type: replace 
Abstract: Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On scalable and efficient training of diffusion samplers</title>
<link>https://arxiv.org/abs/2505.19552</link>
<guid>https://arxiv.org/abs/2505.19552</guid>
<content:encoded><![CDATA[
arXiv:2505.19552v2 Announce Type: replace 
Abstract: We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and the sampling space is high-dimensional. To address this limitation, we propose a scalable and sample-efficient framework that properly harmonizes the powerful classical sampling method and the diffusion sampler. Specifically, we utilize Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy as a Searcher to collect off-policy samples, using an auxiliary energy function to compensate for exploring modes the diffusion sampler rarely visits. These off-policy samples are then combined with on-policy data to train the diffusion sampler, thereby expanding its coverage of the energy landscape. Furthermore, we identify primacy bias, i.e., the preference of samplers for early experience during training, as the main cause of mode collapse during training, and introduce a periodic re-initialization trick to resolve this issue. Our method significantly improves sample efficiency on standard benchmarks for diffusion samplers and also excels at higher-dimensional problems and real-world molecular conformer generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling</title>
<link>https://arxiv.org/abs/2505.19669</link>
<guid>https://arxiv.org/abs/2505.19669</guid>
<content:encoded><![CDATA[
arXiv:2505.19669v2 Announce Type: replace 
Abstract: Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^\pi$-Realizable MDPs</title>
<link>https://arxiv.org/abs/2505.19946</link>
<guid>https://arxiv.org/abs/2505.19946</guid>
<content:encoded><![CDATA[
arXiv:2505.19946v2 Announce Type: replace 
Abstract: We study the problem of offline imitation learning in Markov decision processes (MDPs), where the goal is to learn a well-performing policy given a dataset of state-action pairs generated by an expert policy. Complementing a recent line of work on this topic that assumes the expert belongs to a tractable class of known policies, we approach this problem from a new angle and leverage a different type of structural assumption about the environment. Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a new algorithm called saddle-point offline imitation learning (\SPOIL), which is guaranteed to match the performance of any expert up to an additive error $\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover, we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$. Finally, our analysis suggests a new loss function for training critic networks from expert data in deep imitation learning. Empirical evaluations on standard benchmarks demonstrate that the neural net implementation of \SPOIL is superior to behavior cloning and competitive with state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
arXiv:2505.20734v2 Announce Type: replace 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Transformers Learn Variable Binding in Symbolic Programs?</title>
<link>https://arxiv.org/abs/2505.20896</link>
<guid>https://arxiv.org/abs/2505.20896</guid>
<content:encoded><![CDATA[
arXiv:2505.20896v2 Announce Type: replace 
Abstract: Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches. To facilitate reproducible research, we developed Variable Scope, an interactive web platform for exploring our findings at https://variablescope.org
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Value-Function Uncertainties</title>
<link>https://arxiv.org/abs/2505.21119</link>
<guid>https://arxiv.org/abs/2505.21119</guid>
<content:encoded><![CDATA[
arXiv:2505.21119v2 Announce Type: replace 
Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution</title>
<link>https://arxiv.org/abs/2207.06418</link>
<guid>https://arxiv.org/abs/2207.06418</guid>
<content:encoded><![CDATA[
arXiv:2207.06418v2 Announce Type: replace-cross 
Abstract: Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the WorldStrat dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels and Sentinel2 imagery are CC BY, and the source code and pre-trained models under BSD. The dataset is available at https://zenodo.org/record/6810791 and the software package at https://github.com/worldstrat/worldstrat .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining marginal properness in the external validation of survival models with squared and logarithmic losses</title>
<link>https://arxiv.org/abs/2212.05260</link>
<guid>https://arxiv.org/abs/2212.05260</guid>
<content:encoded><![CDATA[
arXiv:2212.05260v3 Announce Type: replace-cross 
Abstract: Scoring rules promote rational and honest decision-making, which is important for model evaluation and becoming increasingly important for automated procedures such as `AutoML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis, with a focus on their theoretical and empirical properness. We introduce a marginal definition of properness and show that both the Integrated Survival Brier Score (ISBS) and the Right-Censored Log-Likelihood (RCLL) are theoretically improper under this definition. We also investigate a new class of losses that may inform future survival scoring rules. Simulation experiments reveal that both the ISBS and RCLL behave as proper scoring rules in practice. The RCLL showed no violations across all settings, while ISBS exhibited only minor, negligible violations at extremely small sample sizes, suggesting one can trust results from historical experiments. As such we advocate for both the RCLL and ISBS in external validation of models, including in automated procedures. However, we note practical challenges in estimating these losses including estimation of censoring distributions and densities; as such further research is required to advance development of robust and honest evaluation in survival analysis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel $\epsilon$-Greedy for Multi-Armed Bandits with Covariates</title>
<link>https://arxiv.org/abs/2306.17329</link>
<guid>https://arxiv.org/abs/2306.17329</guid>
<content:encoded><![CDATA[
arXiv:2306.17329v2 Announce Type: replace-cross 
Abstract: We consider the $\epsilon$-greedy strategy for the multi-arm bandit with covariates (MABC) problem, where the mean reward functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). We propose to estimate the unknown mean reward functions using an online weighted kernel ridge regression estimator, and show the resultant estimator to be consistent under appropriate decay rates of the exploration probability sequence, $\{\epsilon_t\}_t$, and regularization parameter, $\{\lambda_t\}_t$. Moreover, we show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Streaming of Large-Scale Medical Image Datasets for Deep Learning</title>
<link>https://arxiv.org/abs/2307.00438</link>
<guid>https://arxiv.org/abs/2307.00438</guid>
<content:encoded><![CDATA[
arXiv:2307.00438v3 Announce Type: replace-cross 
Abstract: Large-scale medical imaging datasets have accelerated deep learning (DL) for medical image analysis. However, the large scale of these datasets poses a challenge for researchers, resulting in increased storage and bandwidth requirements for hosting and accessing them. Since different researchers have different use cases and require different resolutions or formats for DL, it is neither feasible to anticipate every researcher's needs nor practical to store data in multiple resolutions and formats. To that end, we propose the Medical Image Streaming Toolkit (MIST), a format-agnostic database that enables streaming of medical images at different resolutions and formats from a single high-resolution copy. We evaluated MIST across eight popular, large-scale medical imaging datasets spanning different body parts, modalities, and formats. Our results showed that our framework reduced the storage and bandwidth requirements for hosting and downloading datasets without impacting image quality. We demonstrate that MIST addresses the challenges posed by large-scale medical imaging datasets by building a data-efficient and format-agnostic database to meet the diverse needs of researchers and reduce barriers to DL research in medical imaging.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Checking of AI-Generated Reports</title>
<link>https://arxiv.org/abs/2307.14634</link>
<guid>https://arxiv.org/abs/2307.14634</guid>
<content:encoded><![CDATA[
arXiv:2307.14634v2 Announce Type: replace-cross 
Abstract: With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resampled Confidence Regions with Exponential Shrinkage for the Regression Function of Binary Classification</title>
<link>https://arxiv.org/abs/2308.01835</link>
<guid>https://arxiv.org/abs/2308.01835</guid>
<content:encoded><![CDATA[
arXiv:2308.01835v2 Announce Type: replace-cross 
Abstract: The regression function is one of the key objects of binary classification, since it not only determines a Bayes optimal classifier, hence, defines an optimal decision boundary, but also encodes the conditional distribution of the output given the input. In this paper we build distribution-free confidence regions for the regression function for any user-chosen confidence level and any finite sample size based on a resampling test. These regions are abstract, as the model class can be almost arbitrary, e.g., it does not have to be finitely parameterized. We prove the strong uniform consistency of a new empirical risk minimization based approach for model classes with finite pseudo-dimensions and inverse Lipschitz parameterizations. We provide exponential probably approximately correct bounds on the $L_2$ sizes of these regions, and demonstrate the ideas on specific models. Additionally, we also consider a k-nearest neighbors based method, for which we prove strong pointwise bounds on the probability of exclusion. Finally, the constructions are illustrated on a logistic model class and compared to the asymptotic ellipsoids of the maximum likelihood estimator.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Differential Operators for Hybrid Neural Fields</title>
<link>https://arxiv.org/abs/2312.05984</link>
<guid>https://arxiv.org/abs/2312.05984</guid>
<content:encoded><![CDATA[
arXiv:2312.05984v2 Announce Type: replace-cross 
Abstract: Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield accurate spatial derivatives needed for these downstream applications. In this work, we propose two ways to circumvent these challenges. Our first approach is a post hoc operator that uses local polynomial fitting to obtain more accurate derivatives from pre-trained hybrid neural fields. Additionally, we also propose a self-supervised fine-tuning approach that refines the hybrid neural field to yield accurate derivatives directly while preserving the initial signal. We show applications of our method to rendering, collision simulation, and solving PDEs. We observe that using our approach yields more accurate derivatives, reducing artifacts and leading to more accurate simulations in downstream applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Meta-Prompting</title>
<link>https://arxiv.org/abs/2312.06562</link>
<guid>https://arxiv.org/abs/2312.06562</guid>
<content:encoded><![CDATA[
arXiv:2312.06562v3 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You</title>
<link>https://arxiv.org/abs/2401.16092</link>
<guid>https://arxiv.org/abs/2401.16092</guid>
<content:encoded><![CDATA[
arXiv:2401.16092v4 Announce Type: replace-cross 
Abstract: Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment, and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this technology. However, our results show that multilingual models suffer from significant gender biases just as monolingual models do. Furthermore, the natural expectation that multilingual models will provide similar results across languages does not hold up. Instead, there are important differences between languages. We propose a novel benchmark, MAGBIG, intended to foster research on gender bias in multilingual models. We use MAGBIG to investigate the effect of multilingualism on gender bias in T2I models. To this end, we construct multilingual prompts requesting portraits of people with a certain occupation or trait. Our results show that not only do models exhibit strong gender biases but they also behave differently across languages. Furthermore, we investigate prompt engineering strategies, such as indirect, neutral formulations, to mitigate these biases. Unfortunately, these approaches have limited success and result in worse text-to-image alignment. Consequently, we call for more research into diverse representations across languages in image generators, as well as into steerability to address biased model behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation</title>
<link>https://arxiv.org/abs/2402.07723</link>
<guid>https://arxiv.org/abs/2402.07723</guid>
<content:encoded><![CDATA[
arXiv:2402.07723v3 Announce Type: replace-cross 
Abstract: Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. We support our theory with experiments conducted in a variety of settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbative partial moment matching and gradient-flow adaptive importance sampling transformations for Bayesian leave one out cross-validation</title>
<link>https://arxiv.org/abs/2402.08151</link>
<guid>https://arxiv.org/abs/2402.08151</guid>
<content:encoded><![CDATA[
arXiv:2402.08151v3 Announce Type: replace-cross 
Abstract: Importance sampling (IS) allows one to approximate leave one out (LOO) cross-validation for a Bayesian model, without refitting, by inverting the Bayesian update equation to subtract a given data point from a model posterior. For each data point, one computes expectations under the corresponding LOO posterior by weighted averaging over the full data posterior. This task sometimes requires weight stabilization in the form of adapting the posterior distribution via transformation. So long as one is successful in finding a suitable transformation, one avoids refitting. To this end, we motivate the use of bijective perturbative transformations of the form $T(\boldsymbol{\theta})=\boldsymbol{\theta} + h Q(\boldsymbol{\theta}),$ for $0<h\ll 1,$ and introduce two classes of such transformations: 1) partial moment matching and 2) gradient flow evolution. The former extends prior literature on moment-matching under the recognition that adaptation for LOO is a small perturbation on the full data posterior. The latter class of methods define transformations based on relaxing various statistical objectives: in our case the variance of the IS estimator and the KL divergence between the transformed distribution and the statistics of the LOO fold. Being model-specific, the gradient flow transformations require evaluating Jacobian determinants. While these quantities are generally readily available through auto-differentiation, we derive closed-form expressions in the case of logistic regression and shallow ReLU activated neural networks. We tested the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Control of Linear Systems under Unbounded Noise</title>
<link>https://arxiv.org/abs/2402.10252</link>
<guid>https://arxiv.org/abs/2402.10252</guid>
<content:encoded><![CDATA[
arXiv:2402.10252v2 Announce Type: replace-cross 
Abstract: This paper investigates the problem of controlling a linear system under possibly unbounded stochastic noise with unknown convex cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we show that an $ \tilde{O}(\sqrt{T}) $ high-probability regret can be achieved under unbounded noise, where $ T $ denotes the time horizon. Notably, the noise is only required to have a finite fourth moment. Moreover, when the costs are strongly convex and the noise is sub-Gaussian, we establish an $ O({\rm poly} (\log T)) $ regret bound.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Robustness to Clean-Label Poisoning Using Diffusion Denoising</title>
<link>https://arxiv.org/abs/2403.11981</link>
<guid>https://arxiv.org/abs/2403.11981</guid>
<content:encoded><![CDATA[
arXiv:2403.11981v2 Announce Type: replace-cross 
Abstract: We present a certified defense to clean-label poisoning attacks under $\ell_2$-norm. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $randomized$ $smoothing$, we show how an off-the-shelf diffusion denoising model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks in both $\ell_2$ and $\ell_{\infty}$-norms and reduce their attack success to 0-16% with only a negligible drop in the test accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs</title>
<link>https://arxiv.org/abs/2404.10304</link>
<guid>https://arxiv.org/abs/2404.10304</guid>
<content:encoded><![CDATA[
arXiv:2404.10304v2 Announce Type: replace-cross 
Abstract: Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in software testing. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80x, 2.65x, and 1.66x those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions</title>
<link>https://arxiv.org/abs/2405.03205</link>
<guid>https://arxiv.org/abs/2405.03205</guid>
<content:encoded><![CDATA[
arXiv:2405.03205v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the "logit lens" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Accuracy in Generative Models via Knowledge Transfer</title>
<link>https://arxiv.org/abs/2405.16837</link>
<guid>https://arxiv.org/abs/2405.16837</guid>
<content:encoded><![CDATA[
arXiv:2405.16837v3 Announce Type: replace-cross 
Abstract: This paper investigates the accuracy of generative models and the impact of knowledge transfer on their generation precision. Specifically, we examine a generative model for a target task, fine-tuned using a pre-trained model from a source task. Building on the "Shared Embedding" concept, which bridges the source and target tasks, we introduce a novel framework for transfer learning under distribution metrics such as the Kullback-Leibler divergence. This framework underscores the importance of leveraging inherent similarities between diverse tasks despite their distinct data distributions. Our theory suggests that the shared structures can augment the generation accuracy for a target task, reliant on the capability of a source model to identify shared structures and effective knowledge transfer from source to target learning. To demonstrate the practical utility of this framework, we explore the theoretical implications for two specific generative models: diffusion and normalizing flows. The results show enhanced performance in both models over their non-transfer counterparts, indicating advancements for diffusion models and providing fresh insights into normalizing flows in transfer and non-transfer settings. These results highlight the significant contribution of knowledge transfer in boosting the generation capabilities of these models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine</title>
<link>https://arxiv.org/abs/2406.02394</link>
<guid>https://arxiv.org/abs/2406.02394</guid>
<content:encoded><![CDATA[
arXiv:2406.02394v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Low Rank Neural Representation of Entropy Solutions</title>
<link>https://arxiv.org/abs/2406.05694</link>
<guid>https://arxiv.org/abs/2406.05694</guid>
<content:encoded><![CDATA[
arXiv:2406.05694v2 Announce Type: replace-cross 
Abstract: We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension. The representation is a generalization of the method of characteristics and posseses a compositional form. While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear. This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure. Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2406.11753</link>
<guid>https://arxiv.org/abs/2406.11753</guid>
<content:encoded><![CDATA[
arXiv:2406.11753v3 Announce Type: replace-cross 
Abstract: Finetuning language models (LMs) is crucial for adapting the models to downstream data and tasks. However, full finetuning is usually costly. Existing work, such as parameter-efficient finetuning (PEFT), often focuses on \textit{how to finetune} but neglects the issue of \textit{where to finetune}. As a pioneering work on reducing the cost of backpropagation (at the layer level) by answering where to finetune, we conduct a semantic analysis of the LM inference process. We first propose using transition traces of the latent representation to compute deviations (or loss). Then, using a derived formula of scaling law, we estimate the gain of each layer in reducing deviation (or loss). Further, we narrow down the scope for finetuning, and also, study the cost-benefit balance of LM finetuning. We perform extensive experiments across well-known LMs and datasets. The results show that our approach is effective and efficient, and outperforms the existing baselines. Our approach is orthogonal to other techniques for improving finetuning efficiency, such as PEFT methods, offering practical values on LM finetuning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks</title>
<link>https://arxiv.org/abs/2406.13945</link>
<guid>https://arxiv.org/abs/2406.13945</guid>
<content:encoded><![CDATA[
arXiv:2406.13945v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \textit{CityData} to integrate the diverse urban data and \textit{CitySimu} to simulate fine-grained urban dynamics. Based on \textit{CityData} and \textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityGPT: Empowering Urban Spatial Cognition of Large Language Models</title>
<link>https://arxiv.org/abs/2406.13948</link>
<guid>https://arxiv.org/abs/2406.13948</guid>
<content:encoded><![CDATA[
arXiv:2406.13948v2 Announce Type: replace-cross 
Abstract: Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit{CityInstruction} by \textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit{CityEval}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Foundation Models with Black-Box Visual Prompting</title>
<link>https://arxiv.org/abs/2407.17491</link>
<guid>https://arxiv.org/abs/2407.17491</guid>
<content:encoded><![CDATA[
arXiv:2407.17491v2 Announce Type: replace-cross 
Abstract: With a surge of large-scale pre-trained models, parameter-efficient transfer learning (PETL) of large models has garnered significant attention. While promising, they commonly rely on two optimistic assumptions: 1) full access to the parameters of a PTM, and 2) sufficient memory capacity to cache all intermediate activations for gradient computation. However, in most real-world applications, PTMs serve as black-box APIs or proprietary software without full parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. This work proposes black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge of their architectures or parameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs input-dependent visual prompts, which allow the target PTM to adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to update Coordinator. Besides, we introduce a variant, BlackVIP-SE, which significantly reduces the runtime and computational cost of BlackVIP. Extensive experiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation to diverse domains and tasks with minimal memory requirements. We further provide a theoretical analysis on the generalization of visual prompting methods by presenting their connection to the certified robustness of randomized smoothing, and presenting an empirical support for improved robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and cross validation in ridge regression with correlated samples</title>
<link>https://arxiv.org/abs/2408.04607</link>
<guid>https://arxiv.org/abs/2408.04607</guid>
<content:encoded><![CDATA[
arXiv:2408.04607v4 Announce Type: replace-cross 
Abstract: Recent years have seen substantial advances in our understanding of high-dimensional ridge regression, but existing theories assume that training examples are independent. By leveraging techniques from random matrix theory and free probability, we provide sharp asymptotics for the in- and out-of-sample risks of ridge regression when the data points have arbitrary correlations. We demonstrate that in this setting, the generalized cross validation estimator (GCV) fails to correctly predict the out-of-sample risk. However, in the case where the noise residuals have the same correlations as the data points, one can modify the GCV to yield an efficiently-computable unbiased estimator that concentrates in the high-dimensional limit, which we dub CorrGCV. We further extend our asymptotic analysis to the case where the test point has nontrivial correlations with the training set, a setting often encountered in time series forecasting. Assuming knowledge of the correlation structure of the time series, this again yields an extension of the GCV estimator, and sharply characterizes the degree to which such test points yield an overly optimistic prediction of long-time risk. We validate the predictions of our theory across a variety of high dimensional data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADetBench: Towards Benchmarking Physical Attacks against Object Detection</title>
<link>https://arxiv.org/abs/2408.09181</link>
<guid>https://arxiv.org/abs/2408.09181</guid>
<content:encoded><![CDATA[
arXiv:2408.09181v3 Announce Type: replace-cross 
Abstract: Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing realistic simulation to thoroughly and rigorously benchmark physical attacks with fairness under controlled physical dynamics and cross-domain transformation. This resolves the problem of capturing identical adversarial images that cannot be achieved in the real world. Our benchmark includes 20 physical attack methods, 48 object detectors, comprehensive physical dynamics, and evaluation metrics. We also provide end-to-end pipelines for dataset generation, detection, evaluation, and further analysis. In addition, we perform 8064 groups of evaluation based on our benchmark, which includes both overall evaluation and further detailed ablation studies for controlled physical dynamics. Through these experiments, we provide in-depth analyses of physical attack performance and physical adversarial robustness, draw valuable observations, and discuss potential directions for future research.
  Codebase: https://github.com/JiaweiLian/Benchmarking_Physical_Attack
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANVIL: Anomaly-based Vulnerability Identification without Labelled Training Data</title>
<link>https://arxiv.org/abs/2408.16028</link>
<guid>https://arxiv.org/abs/2408.16028</guid>
<content:encoded><![CDATA[
arXiv:2408.16028v3 Announce Type: replace-cross 
Abstract: Supervised-learning-based vulnerability detectors often fall short due to limited labelled training data. In contrast, Large Language Models (LLMs) like GPT-4 are trained on vast unlabelled code corpora, yet perform only marginally better than coin flips when directly prompted to detect vulnerabilities. In this paper, we reframe vulnerability detection as anomaly detection, based on the premise that vulnerable code is rare and thus anomalous relative to patterns learned by LLMs. We introduce ANVIL, which performs a masked code reconstruction task: the LLM reconstructs a masked line of code, and deviations from the original are scored as anomalies. We propose a hybrid anomaly score that combines exact match, cross-entropy loss, prediction confidence, and structural complexity. We evaluate our approach across multiple LLM families, scoring methods, and context sizes, and against vulnerabilities after the LLM's training cut-off. On the PrimeVul dataset, ANVIL outperforms state-of-the-art supervised detectors-LineVul, LineVD, and LLMAO-achieving up to 2x higher Top-3 accuracy, 75% better Normalized MFR, and a significant improvement on ROC-AUC. Finally, by integrating ANVIL with fuzzers, we uncover two previously unknown vulnerabilities, demonstrating the practical utility of anomaly-guided detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Privacy Leakage in Dimensionality Reduction Methods via Reconstruction Attack</title>
<link>https://arxiv.org/abs/2408.17151</link>
<guid>https://arxiv.org/abs/2408.17151</guid>
<content:encoded><![CDATA[
arXiv:2408.17151v3 Announce Type: replace-cross 
Abstract: This study investigates privacy leakage in dimensionality reduction methods through a novel machine learning-based reconstruction attack. Employing an informed adversary threat model, we develop a neural network capable of reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: principal component analysis (PCA), sparse random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-distributed stochastic neighbor embedding ($t$-SNE), and uniform manifold approximation and projection (UMAP). Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative analysis to identify key factors affecting reconstruction quality. Furthermore, we assess the effectiveness of an additive noise mechanism in mitigating these reconstruction attacks. Our experimental results on both datasets reveal that the attack is effective against deterministic methods (PCA and Isomap). but ineffective against methods that employ random initialization (SRP, MDS, $t$-SNE and UMAP). The experimental results also show that, for PCA and Isomap, our reconstruction network produces higher quality outputs compared to a previously proposed network.
  We also study the effect of additive noise mechanism to prevent the reconstruction attack. Our experiment shows that, when adding the images with large noises before performing PCA or Isomap, the attack produced severely distorted reconstructions. In contrast, for the other four methods, the reconstructions still show some recognizable features, though they bear little resemblance to the original images. The code is available at https://github.com/Chayadon/Reconstruction_attack_on_DR
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content Moderation by LLM: From Accuracy to Legitimacy</title>
<link>https://arxiv.org/abs/2409.03219</link>
<guid>https://arxiv.org/abs/2409.03219</guid>
<content:encoded><![CDATA[
arXiv:2409.03219v2 Announce Type: replace-cross 
Abstract: One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy -- the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs' real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs' role in content moderation and redirect relevant research in this field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>View-Invariant Policy Learning via Zero-Shot Novel View Synthesis</title>
<link>https://arxiv.org/abs/2409.03685</link>
<guid>https://arxiv.org/abs/2409.03685</guid>
<content:encoded><![CDATA[
arXiv:2409.03685v3 Announce Type: replace-cross 
Abstract: Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks</title>
<link>https://arxiv.org/abs/2409.04459</link>
<guid>https://arxiv.org/abs/2409.04459</guid>
<content:encoded><![CDATA[
arXiv:2409.04459v2 Announce Type: replace-cross 
Abstract: Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs</title>
<link>https://arxiv.org/abs/2409.05806</link>
<guid>https://arxiv.org/abs/2409.05806</guid>
<content:encoded><![CDATA[
arXiv:2409.05806v4 Announce Type: replace-cross 
Abstract: Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title>
<link>https://arxiv.org/abs/2409.10289</link>
<guid>https://arxiv.org/abs/2409.10289</guid>
<content:encoded><![CDATA[
arXiv:2409.10289v4 Announce Type: replace-cross 
Abstract: Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach</title>
<link>https://arxiv.org/abs/2409.19458</link>
<guid>https://arxiv.org/abs/2409.19458</guid>
<content:encoded><![CDATA[
arXiv:2409.19458v3 Announce Type: replace-cross 
Abstract: We study the problem of fine-tuning a language model (LM) for a target task by optimally using the information from $n$ auxiliary tasks. This problem has broad applications in NLP, such as targeted instruction tuning and data selection in chain-of-thought fine-tuning. The key challenge of this problem is that not all auxiliary tasks are beneficial in improving the performance of the target task. Thus, selecting the right subset of auxiliary tasks is crucial. Conventional subset selection methods, such as forward and backward stepwise selection, are unsuitable for LM fine-tuning because they require repeated training on subsets of auxiliary tasks. This paper introduces a new algorithm for estimating model fine-tuning performance without requiring repeated training. Our algorithm first performs multitask training using data from all tasks to obtain a meta initialization. Then, we approximate the model fine-tuning loss of a subset using functional values and gradients from the meta initialization. Empirically, we find that this gradient-based approximation holds with remarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate fine-tuning performances on CPUs within a few seconds. Finally, we fine-tune the pretrained base model once on the selected subset of tasks. We conduct extensive experiments to validate this approach, delivering a speedup of $30\times$ over conventional subset selection while incurring only $1\%$ error of the true fine-tuning performances. In downstream evaluations involving both instruction tuning and chain-of-thought fine-tuning, this loss-based selection approach improves over prior gradient or representation similarity-based methods for subset selection by up to $3.8\%$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title>
<link>https://arxiv.org/abs/2410.03026</link>
<guid>https://arxiv.org/abs/2410.03026</guid>
<content:encoded><![CDATA[
arXiv:2410.03026v2 Announce Type: replace-cross 
Abstract: Language models (LMs) rely on their parametric knowledge augmented with relevant contextual knowledge for certain tasks, such as question answering. However, the contextual knowledge can contain private information that may be leaked when answering queries, and estimating this privacy leakage is not well understood. A straightforward approach of directly comparing an LM's output to the contexts can overestimate the privacy risk, since the LM's parametric knowledge might already contain the augmented contextual knowledge. To this end, we introduce $\emph{context influence}$, a metric that builds on differential privacy, a widely-adopted privacy notion, to estimate the privacy leakage of contextual knowledge during decoding. Our approach effectively measures how each subset of the context influences an LM's response while separating the specific parametric knowledge of the LM. Using our context influence metric, we demonstrate that context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge. Moreover, we experimentally demonstrate how context influence properly attributes the privacy leakage to augmented contexts, and we evaluate how factors-- such as model size, context size, generation position, etc.-- affect context privacy leakage. The practical implications of our results will inform practitioners of the privacy risk associated with augmented contextual knowledge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title>
<link>https://arxiv.org/abs/2410.07176</link>
<guid>https://arxiv.org/abs/2410.07176</guid>
<content:encoded><![CDATA[
arXiv:2410.07176v2 Announce Type: replace-cross 
Abstract: Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees</title>
<link>https://arxiv.org/abs/2410.08934</link>
<guid>https://arxiv.org/abs/2410.08934</guid>
<content:encoded><![CDATA[
arXiv:2410.08934v4 Announce Type: replace-cross 
Abstract: Personalized federated learning (PFL) offers a flexible framework for aggregating information across distributed clients with heterogeneous data. This work considers a personalized federated learning setting that simultaneously learns global and local models. While purely local training has no communication cost, collaborative learning among the clients can leverage shared knowledge to improve statistical accuracy, presenting an accuracy-communication trade-off in personalized federated learning. However, the theoretical analysis of how personalization quantitatively influences sample and algorithmic efficiency and their inherent trade-off is largely unexplored. This paper makes a contribution towards filling this gap, by providing a quantitative characterization of the personalization degree on the tradeoff. The results further offers theoretical insights for choosing the personalization degree. As a side contribution, we establish the minimax optimality in terms of statistical accuracy for a widely studied PFL formulation. The theoretical result is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-light Uncertainty Set Merging with Admissibility: Synthetics, Aggregation, and Test Inversion</title>
<link>https://arxiv.org/abs/2410.12201</link>
<guid>https://arxiv.org/abs/2410.12201</guid>
<content:encoded><![CDATA[
arXiv:2410.12201v2 Announce Type: replace-cross 
Abstract: This article introduces a Synthetics, Aggregation, and Test inversion (SAT) approach for merging diverse and potentially dependent uncertainty sets into a single unified set. The procedure is data-light, relying only on initial sets and control levels, and it adapts to any user-specified initial uncertainty sets, accommodating potentially varying coverage levels. SAT is motivated by the challenge of integrating uncertainty sets when only the initial sets and their control levels are available - for example, when merging confidence sets from distributed sites under communication constraints or combining conformal prediction sets generated by different algorithms or data splits. To address this, SAT constructs and aggregates novel synthetic test statistics, and then derive merged sets through test inversion. Our method leverages the duality between set estimation and hypothesis testing, ensuring reliable coverage in dependent scenarios. A key theoretical contribution is a rigorous analysis of SAT's properties, including a proof of its admissibility in the context of deterministic set merging. Both theoretical analyses and empirical results confirm the method's finite-sample coverage validity and desirable set sizes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Model Kinship for Merging Large Language Models</title>
<link>https://arxiv.org/abs/2410.12613</link>
<guid>https://arxiv.org/abs/2410.12613</guid>
<content:encoded><![CDATA[
arXiv:2410.12613v2 Announce Type: replace-cross 
Abstract: Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[
arXiv:2410.13098v2 Announce Type: replace-cross 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised contrastive learning performs non-linear system identification</title>
<link>https://arxiv.org/abs/2410.14673</link>
<guid>https://arxiv.org/abs/2410.14673</guid>
<content:encoded><![CDATA[
arXiv:2410.14673v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models</title>
<link>https://arxiv.org/abs/2410.20445</link>
<guid>https://arxiv.org/abs/2410.20445</guid>
<content:encoded><![CDATA[
arXiv:2410.20445v2 Announce Type: replace-cross 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning</title>
<link>https://arxiv.org/abs/2410.21119</link>
<guid>https://arxiv.org/abs/2410.21119</guid>
<content:encoded><![CDATA[
arXiv:2410.21119v3 Announce Type: replace-cross 
Abstract: One-Shot Federated Learning (OSFL) restricts communication between the server and clients to a single round, significantly reducing communication costs and minimizing privacy leakage risks compared to traditional Federated Learning (FL), which requires multiple rounds of communication. However, existing OSFL frameworks remain vulnerable to distributional heterogeneity, as they primarily focus on model heterogeneity while neglecting data heterogeneity. To bridge this gap, we propose FedHydra, a unified, data-free, OSFL framework designed to effectively address both model and data heterogeneity. Unlike existing OSFL approaches, FedHydra introduces a novel two-stage learning mechanism. Specifically, it incorporates model stratification and heterogeneity-aware stratified aggregation to mitigate the challenges posed by both model and data heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with five SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous OSFL methods in both homogeneous and heterogeneous settings. The code is available at https://github.com/Jun-B0518/FedHydra.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</title>
<link>https://arxiv.org/abs/2411.00355</link>
<guid>https://arxiv.org/abs/2411.00355</guid>
<content:encoded><![CDATA[
arXiv:2411.00355v2 Announce Type: replace-cross 
Abstract: In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowCoder-X: Boosting Multilingual Information Extraction via Code</title>
<link>https://arxiv.org/abs/2411.04794</link>
<guid>https://arxiv.org/abs/2411.04794</guid>
<content:encoded><![CDATA[
arXiv:2411.04794v3 Announce Type: replace-cross 
Abstract: Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactLens: Benchmarking Fine-Grained Fact Verification</title>
<link>https://arxiv.org/abs/2411.05980</link>
<guid>https://arxiv.org/abs/2411.05980</guid>
<content:encoded><![CDATA[
arXiv:2411.05980v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2411.18688</link>
<guid>https://arxiv.org/abs/2411.18688</guid>
<content:encoded><![CDATA[
arXiv:2411.18688v4 Announce Type: replace-cross 
Abstract: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Operators for Predictor Feedback Control of Nonlinear Delay Systems</title>
<link>https://arxiv.org/abs/2411.18964</link>
<guid>https://arxiv.org/abs/2411.18964</guid>
<content:encoded><![CDATA[
arXiv:2411.18964v3 Announce Type: replace-cross 
Abstract: Predictor feedback designs are critical for delay-compensating controllers in nonlinear systems. However, these designs are limited in practical applications as predictors cannot be directly implemented, but require numerical approximation schemes, which become computationally prohibitive when system dynamics are expensive to compute. To address this challenge, we recast the predictor design as an operator learning problem, and learn the predictor mapping via a neural operator. We prove the existence of an arbitrarily accurate neural operator approximation of the predictor operator. Under the approximated predictor, we achieve semiglobal practical stability of the closed-loop nonlinear delay system. The estimate is semiglobal in a unique sense - one can enlarge the set of initial states as desired, though this increases the difficulty of training a neural operator, which appears practically in the stability estimate. Furthermore, our analysis holds for any black-box predictor satisfying the universal approximation error bound. We demonstrate the approach by controlling a 5-link robotic manipulator with different neural operator models, achieving significant speedups compared to classic predictor feedback schemes while maintaining closed-loop stability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Consistent $k$-Center Clustering with Optimal Recourse</title>
<link>https://arxiv.org/abs/2412.03238</link>
<guid>https://arxiv.org/abs/2412.03238</guid>
<content:encoded><![CDATA[
arXiv:2412.03238v2 Announce Type: replace-cross 
Abstract: Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem? This question has received attention in recent years under the name consistent clustering.
  Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson [SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions. In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion. Before our work, {\L}\k{a}cki, Haeupler, Grunau, Rozho\v{n}, and Jayaram [SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.
  In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update. Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure. Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update. Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani [STOC '97]. Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Samudra: An AI Global Ocean Emulator for Climate</title>
<link>https://arxiv.org/abs/2412.03795</link>
<guid>https://arxiv.org/abs/2412.03795</guid>
<content:encoded><![CDATA[
arXiv:2412.03795v4 Announce Type: replace-cross 
Abstract: AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long climate simulations with skill across a range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multi-depth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remain stable, requiring further work.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Sample Generation of Diffusion Models using Noise Level Correction</title>
<link>https://arxiv.org/abs/2412.05488</link>
<guid>https://arxiv.org/abs/2412.05488</guid>
<content:encoded><![CDATA[
arXiv:2412.05488v3 Announce Type: replace-cross 
Abstract: The denoising process of diffusion models can be interpreted as an approximate projection of noisy samples onto the data manifold. Moreover, the noise level in these samples approximates their distance to the underlying manifold. Building on this insight, we propose a novel method to enhance sample generation by aligning the estimated noise level with the true distance of noisy samples to the manifold. Specifically, we introduce a noise level correction network, leveraging a pre-trained denoising network, to refine noise level estimates during the denoising process. Additionally, we extend this approach to various image restoration tasks by integrating task-specific constraints, including inpainting, deblurring, super-resolution, colorization, and compressed sensing. Experimental results demonstrate that our method significantly improves sample quality in both unconstrained and constrained generation scenarios. Notably, the proposed noise level correction framework is compatible with existing denoising schedulers (e.g., DDIM), offering additional performance improvements.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLZero: Direct Policy Inference from Language Without In-Domain Supervision</title>
<link>https://arxiv.org/abs/2412.05718</link>
<guid>https://arxiv.org/abs/2412.05718</guid>
<content:encoded><![CDATA[
arXiv:2412.05718v2 Announce Type: replace-cross 
Abstract: The reward hypothesis states that all goals and purposes can be understood as the maximization of a received scalar reward signal. However, in practice, defining such a reward signal is notoriously difficult, as humans are often unable to predict the optimal behavior corresponding to a reward function. Natural language offers an intuitive alternative for instructing reinforcement learning (RL) agents, yet previous language-conditioned approaches either require costly supervision or test-time training given a language instruction. In this work, we present a new approach that uses a pretrained RL agent trained using only unlabeled, offline interactions--without task-specific supervision or labeled trajectories--to get zero-shot test-time policy inference from arbitrary natural language instructions. We introduce a framework comprising three steps: imagine, project, and imitate. First, the agent imagines a sequence of observations corresponding to the provided language description using video generative models. Next, these imagined observations are projected into the target environment domain. Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution. To the best of our knowledge, our method, RLZero, is the first approach to show direct language-to-behavior generation abilities on a variety of tasks and environments without any in-domain supervision. We further show that components of RLZero can be used to generate policies zero-shot from cross-embodied videos, such as those available on YouTube, even for complex embodiments like humanoids.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dark side of the forces: assessing non-conservative force models for atomistic machine learning</title>
<link>https://arxiv.org/abs/2412.11569</link>
<guid>https://arxiv.org/abs/2412.11569</guid>
<content:encoded><![CDATA[
arXiv:2412.11569v4 Announce Type: replace-cross 
Abstract: The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, has revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training. This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for. The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Bayesian deep reinforcement learning</title>
<link>https://arxiv.org/abs/2412.11743</link>
<guid>https://arxiv.org/abs/2412.11743</guid>
<content:encoded><![CDATA[
arXiv:2412.11743v2 Announce Type: replace-cross 
Abstract: Bayesian reinforcement learning (BRL) is a method that merges principles from Bayesian statistics and reinforcement learning to make optimal decisions in uncertain environments. As a model-based RL method, it has two key components: (1) inferring the posterior distribution of the model for the data-generating process (DGP) and (2) policy learning using the learned posterior. We propose to model the dynamics of the unknown environment through deep generative models, assuming Markov dependence. In the absence of likelihood functions for these models, we train them by learning a generalized predictive-sequential (or prequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC) samplers to draw samples from this generalized Bayesian posterior distribution. In conjunction, to achieve scalability in the high-dimensional parameter space of the neural networks, we use the gradient-based Markov kernels within SMC. To justify the use of the prequential scoring rule posterior, we prove a Bernstein-von Mises-type theorem. For policy learning, we propose expected Thompson sampling (ETS) to learn the optimal policy by maximising the expected value function with respect to the posterior distribution. This improves upon traditional Thompson sampling (TS) and its extensions, which utilize only one sample drawn from the posterior distribution. This improvement is studied both theoretically and using simulation studies, assuming a discrete action space. Finally, we successfully extended our setup for a challenging problem with a continuous action space without theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective</title>
<link>https://arxiv.org/abs/2412.12276</link>
<guid>https://arxiv.org/abs/2412.12276</guid>
<content:encoded><![CDATA[
arXiv:2412.12276v3 Announce Type: replace-cross 
Abstract: Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., "Finding the first noun in a sentence.") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic interior-point methods for smooth conic optimization with applications</title>
<link>https://arxiv.org/abs/2412.12987</link>
<guid>https://arxiv.org/abs/2412.12987</guid>
<content:encoded><![CDATA[
arXiv:2412.12987v2 Announce Type: replace-cross 
Abstract: Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the iteration complexity of our proposed SIPMs, which, up to a polylogarithmic factor, match the best-known results in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Modality Generalization: A Benchmark and Prospective Analysis</title>
<link>https://arxiv.org/abs/2412.18277</link>
<guid>https://arxiv.org/abs/2412.18277</guid>
<content:encoded><![CDATA[
arXiv:2412.18277v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has achieved remarkable success by integrating information from various modalities, achieving superior performance in tasks like recognition and retrieval compared to uni-modal approaches. However, real-world scenarios often present novel modalities that are unseen during training due to resource and privacy constraints, a challenge current methods struggle to address. This paper introduces Modality Generalization (MG), which focuses on enabling models to generalize to unseen modalities. We define two cases: Weak MG, where both seen and unseen modalities can be mapped into a joint embedding space via existing perceptors, and Strong MG, where no such mappings exist. To facilitate progress, we propose a comprehensive benchmark featuring multi-modal algorithms and adapt existing methods that focus on generalization. Extensive experiments highlight the complexity of MG, exposing the limitations of existing methods and identifying key directions for future research. Our work provides a foundation for advancing robust and adaptable multi-modal models, enabling them to handle unseen modalities in realistic scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Budget-Aware LLM Reasoning</title>
<link>https://arxiv.org/abs/2412.18547</link>
<guid>https://arxiv.org/abs/2412.18547</guid>
<content:encoded><![CDATA[
arXiv:2412.18547v5 Announce Type: replace-cross 
Abstract: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging</title>
<link>https://arxiv.org/abs/2412.20070</link>
<guid>https://arxiv.org/abs/2412.20070</guid>
<content:encoded><![CDATA[
arXiv:2412.20070v2 Announce Type: replace-cross 
Abstract: Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback</title>
<link>https://arxiv.org/abs/2501.01377</link>
<guid>https://arxiv.org/abs/2501.01377</guid>
<content:encoded><![CDATA[
arXiv:2501.01377v2 Announce Type: replace-cross 
Abstract: Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Vision Model in Street Scene Semantic Understanding through Leveraging Posterior Optimization Trajectory</title>
<link>https://arxiv.org/abs/2501.01710</link>
<guid>https://arxiv.org/abs/2501.01710</guid>
<content:encoded><![CDATA[
arXiv:2501.01710v2 Announce Type: replace-cross 
Abstract: To improve the generalization of the autonomous driving (AD) perception model, vehicles need to update the model over time based on the continuously collected data. As time progresses, the amount of data fitted by the AD model expands, which helps to improve the AD model generalization substantially. However, such ever-expanding data is a double-edged sword for the AD model. Specifically, as the fitted data volume grows to exceed the the AD model's fitting capacities, the AD model is prone to under-fitting. To address this issue, we propose to use a pretrained Large Vision Models (LVMs) as backbone coupled with downstream perception head to understand AD semantic information. This design can not only surmount the aforementioned under-fitting problem due to LVMs' powerful fitting capabilities, but also enhance the perception generalization thanks to LVMs' vast and diverse training data. On the other hand, to mitigate vehicles' computational burden of training the perception head while running LVM backbone, we introduce a Posterior Optimization Trajectory (POT)-Guided optimization scheme (POTGui) to accelerate the convergence. Concretely, we propose a POT Generator (POTGen) to generate posterior (future) optimization direction in advance to guide the current optimization iteration, through which the model can generally converge within 10 epochs. Extensive experiments demonstrate that the proposed method improves the performance by over 66.48\% and converges faster over 6 times, compared to the existing state-of-the-art approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Equivariance and Symmetry Breaking in Convolutional Networks</title>
<link>https://arxiv.org/abs/2501.01999</link>
<guid>https://arxiv.org/abs/2501.01999</guid>
<content:encoded><![CDATA[
arXiv:2501.01999v3 Announce Type: replace-cross 
Abstract: In this work, we explore the trade-offs of explicit structural priors, particularly group equivariance. We address this through theoretical analysis and a comprehensive empirical study. To enable controlled and fair comparisons, we introduce \texttt{Rapidash}, a unified group convolutional architecture that allows for different variants of equivariant and non-equivariant models. Our results suggest that more constrained equivariant models outperform less constrained alternatives when aligned with the geometry of the task, and increasing representation capacity does not fully eliminate performance gaps. We see improved performance of models with equivariance and symmetry-breaking through tasks like segmentation, regression, and generation across diverse datasets. Explicit \textit{symmetry breaking} via geometric reference frames consistently improves performance, while \textit{breaking equivariance} through geometric input features can be helpful when aligned with task geometry. Our results provide task-specific performance trends that offer a more nuanced way for model selection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?</title>
<link>https://arxiv.org/abs/2501.02669</link>
<guid>https://arxiv.org/abs/2501.02669</guid>
<content:encoded><![CDATA[
arXiv:2501.02669v2 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) are impressive at visual question answering and image captioning. But they underperform on multi-step visual reasoning -- even compared to LLMs on the same tasks presented in text form -- giving rise to perceptions of modality imbalance or brittleness. Towards a systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning, comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We propose strategies for training on the SIMPLE version of tasks that improve performance on the corresponding HARD task, i.e., simple-to-hard (S2H) generalization. This controlled setup, where each task also has an equivalent text-only version, allows a quantification of the modality imbalance and how it is impacted by training strategy. We show that 1) explicit image-to-text conversion is important in promoting S2H generalization on images, by transferring reasoning from text; 2) conversion can be internalized at test time. We also report results of mechanistic study of this phenomenon. We identify measures of gradient alignment that can identify training strategies that promote better S2H generalization. Ablations highlight the importance of chain-of-thought.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge</title>
<link>https://arxiv.org/abs/2501.04292</link>
<guid>https://arxiv.org/abs/2501.04292</guid>
<content:encoded><![CDATA[
arXiv:2501.04292v3 Announce Type: replace-cross 
Abstract: The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Early Prediction of Self-Supervised Speech Model Performance</title>
<link>https://arxiv.org/abs/2501.05966</link>
<guid>https://arxiv.org/abs/2501.05966</guid>
<content:encoded><![CDATA[
arXiv:2501.05966v2 Announce Type: replace-cross 
Abstract: In Self-Supervised Learning (SSL), pre-training and evaluation are resource intensive. In the speech domain, current indicators of the quality of SSL models during pre-training, such as the loss, do not correlate well with downstream performance. Consequently, it is often difficult to gauge the final downstream performance in a cost efficient manner during pre-training. In this work, we propose unsupervised efficient methods that give insights into the quality of the pre-training of SSL speech models, namely, measuring the cluster quality and rank of the embeddings of the SSL model. Results show that measures of cluster quality and rank correlate better with downstream performance than the pre-training loss with only one hour of unlabeled audio, reducing the need for GPU hours and labeled data in SSL model evaluation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing</title>
<link>https://arxiv.org/abs/2501.07985</link>
<guid>https://arxiv.org/abs/2501.07985</guid>
<content:encoded><![CDATA[
arXiv:2501.07985v2 Announce Type: replace-cross 
Abstract: Robotic systems are increasingly employed for industrial automation, with contact-rich tasks like polishing requiring dexterity and compliant behaviour. These tasks are difficult to model, making classical control challenging. Deep reinforcement learning (RL) offers a promising solution by enabling the learning of models and control policies directly from data. However, its application to real-world problems is limited by data inefficiency and unsafe exploration. Adaptive hybrid RL methods blend classical control and RL adaptively, combining the strengths of both: structure from control and learning from RL. This has led to improvements in data efficiency and exploration safety. However, their potential for hardware applications remains underexplored, with no evaluations on physical systems to date. Such evaluations are critical to fully assess the practicality and effectiveness of these methods in real-world settings. This work presents an experimental demonstration of the hybrid RL algorithm CHEQ for robotic polishing with variable impedance, a task requiring precise force and velocity tracking. In simulation, we show that variable impedance enhances polishing performance. We compare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves effective learning while adhering to safety constraints. On hardware, CHEQ achieves effective polishing behaviour, requiring only eight hours of training and incurring just five failures. These results highlight the potential of adaptive hybrid RL for real-world, contact-rich tasks trained directly on hardware.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction</title>
<link>https://arxiv.org/abs/2501.13125</link>
<guid>https://arxiv.org/abs/2501.13125</guid>
<content:encoded><![CDATA[
arXiv:2501.13125v3 Announce Type: replace-cross 
Abstract: In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior</title>
<link>https://arxiv.org/abs/2501.13134</link>
<guid>https://arxiv.org/abs/2501.13134</guid>
<content:encoded><![CDATA[
arXiv:2501.13134v2 Announce Type: replace-cross 
Abstract: Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models</title>
<link>https://arxiv.org/abs/2501.13772</link>
<guid>https://arxiv.org/abs/2501.13772</guid>
<content:encoded><![CDATA[
arXiv:2501.13772v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant security risks, as models can be exploited to generate harmful or inappropriate content through jailbreak attack. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce \textbf{Jailbreak-AudioBench}, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations</title>
<link>https://arxiv.org/abs/2501.15056</link>
<guid>https://arxiv.org/abs/2501.15056</guid>
<content:encoded><![CDATA[
arXiv:2501.15056v2 Announce Type: replace-cross 
Abstract: Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multimodal Learning via Cross-Modal Proxy Tokens</title>
<link>https://arxiv.org/abs/2501.17823</link>
<guid>https://arxiv.org/abs/2501.17823</guid>
<content:encoded><![CDATA[
arXiv:2501.17823v3 Announce Type: replace-cross 
Abstract: Multimodal models often experience a significant performance drop when one or more modalities are missing during inference. To address this challenge, we propose a simple yet effective approach that enhances robustness to missing modalities while maintaining strong performance when all modalities are available. Our method introduces cross-modal proxy tokens (CMPTs), which approximate the class token of a missing modality by attending only to the tokens of the available modality without requiring explicit modality generation or auxiliary networks. To efficiently learn these approximations with minimal computational overhead, we employ low-rank adapters in frozen unimodal encoders and jointly optimize an alignment loss with a task-specific loss. Extensive experiments on five multimodal datasets show that our method outperforms state-of-the-art baselines across various missing rates while achieving competitive results in complete-modality settings. Overall, our method offers a flexible and efficient solution for robust multimodal learning. The code and pretrained models will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization</title>
<link>https://arxiv.org/abs/2501.18756</link>
<guid>https://arxiv.org/abs/2501.18756</guid>
<content:encoded><![CDATA[
arXiv:2501.18756v2 Announce Type: replace-cross 
Abstract: Bayesian optimization is a widely used method for optimizing expensive black-box functions, with Expected Improvement being one of the most commonly used acquisition functions. In contrast, information-theoretic acquisition functions aim to reduce uncertainty about the function's optimum and are often considered fundamentally distinct from EI. In this work, we challenge this prevailing perspective by introducing a unified theoretical framework, Variational Entropy Search, which reveals that EI and information-theoretic acquisition functions are more closely related than previously recognized. We demonstrate that EI can be interpreted as a variational inference approximation of the popular information-theoretic acquisition function, named Max-value Entropy Search. Building on this insight, we propose VES-Gamma, a novel acquisition function that balances the strengths of EI and MES. Extensive empirical evaluations across both low- and high-dimensional synthetic and real-world benchmarks demonstrate that VES-Gamma is competitive with state-of-the-art acquisition functions and in many cases outperforms EI and MES.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Generative Model Comparison</title>
<link>https://arxiv.org/abs/2501.18897</link>
<guid>https://arxiv.org/abs/2501.18897</guid>
<content:encoded><![CDATA[
arXiv:2501.18897v2 Announce Type: replace-cross 
Abstract: Generative models have recently achieved remarkable empirical performance in various applications, however, their evaluations yet lack uncertainty quantification. In this paper, we propose a method to compare two generative models with statistical confidence based on an unbiased estimator of their relative performance gap. Theoretically, our estimator achieves parametric convergence rates and admits asymptotic normality, which enables valid inference. Empirically, on simulated datasets, our approach effectively controls type I error without compromising its power. In addition, on real image and language datasets, we demonstrate our method's performance in comparing generative models with statistical guarantees.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection-Window Decoding: Text Generation with Selective Refinement</title>
<link>https://arxiv.org/abs/2502.03678</link>
<guid>https://arxiv.org/abs/2502.03678</guid>
<content:encoded><![CDATA[
arXiv:2502.03678v3 Announce Type: replace-cross 
Abstract: The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Deepening Sampling as Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.05449</link>
<guid>https://arxiv.org/abs/2502.05449</guid>
<content:encoded><![CDATA[
arXiv:2502.05449v2 Announce Type: replace-cross 
Abstract: Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms</title>
<link>https://arxiv.org/abs/2502.06231</link>
<guid>https://arxiv.org/abs/2502.06231</guid>
<content:encoded><![CDATA[
arXiv:2502.06231v2 Announce Type: replace-cross 
Abstract: A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and semi-synthetic data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-oriented Data Augmentation with Large Language Models</title>
<link>https://arxiv.org/abs/2502.11671</link>
<guid>https://arxiv.org/abs/2502.11671</guid>
<content:encoded><![CDATA[
arXiv:2502.11671v2 Announce Type: replace-cross 
Abstract: Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data \textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). % \(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</title>
<link>https://arxiv.org/abs/2502.13775</link>
<guid>https://arxiv.org/abs/2502.13775</guid>
<content:encoded><![CDATA[
arXiv:2502.13775v2 Announce Type: replace-cross 
Abstract: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[
arXiv:2502.13943v2 Announce Type: replace-cross 
Abstract: Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Lower Bounds for Stochastic Non-Convex Optimization through Divergence Decomposition</title>
<link>https://arxiv.org/abs/2502.14060</link>
<guid>https://arxiv.org/abs/2502.14060</guid>
<content:encoded><![CDATA[
arXiv:2502.14060v2 Announce Type: replace-cross 
Abstract: We study fundamental limits of first-order stochastic optimization in a range of nonconvex settings, including L-smooth functions satisfying Quasar-Convexity (QC), Quadratic Growth (QG), and Restricted Secant Inequalities (RSI). While the convergence properties of standard algorithms are well-understood in deterministic regimes, significantly fewer results address the stochastic case, where only unbiased and noisy gradients are available. We establish new lower bounds on the number of noisy gradient queries to minimize these classes of functions, also showing that they are tight (up to a logarithmic factor) in all the relevant quantities characterizing each class. Our approach reformulates the optimization task as a function identification problem, leveraging divergence decomposition arguments to construct a challenging subclass that leads to sharp lower bounds. Furthermore, we present a specialized algorithm in the one-dimensional setting that achieves faster rates, suggesting that certain dimensional thresholds are intrinsic to the complexity of non-convex stochastic optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIS-CO: Discovering Copyrighted Content in VLMs Training Data</title>
<link>https://arxiv.org/abs/2502.17358</link>
<guid>https://arxiv.org/abs/2502.17358</guid>
<content:encoded><![CDATA[
arXiv:2502.17358v3 Announce Type: replace-cross 
Abstract: How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs</title>
<link>https://arxiv.org/abs/2502.17701</link>
<guid>https://arxiv.org/abs/2502.17701</guid>
<content:encoded><![CDATA[
arXiv:2502.17701v2 Announce Type: replace-cross 
Abstract: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Experts</title>
<link>https://arxiv.org/abs/2502.18530</link>
<guid>https://arxiv.org/abs/2502.18530</guid>
<content:encoded><![CDATA[
arXiv:2502.18530v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents have emerged as a promising solution to automate the workflow of machine learning, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves overall model performance. We also provide some theoretical edvience of the superior properties of this Iterative Refinement. Further, we implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency</title>
<link>https://arxiv.org/abs/2502.19163</link>
<guid>https://arxiv.org/abs/2502.19163</guid>
<content:encoded><![CDATA[
arXiv:2502.19163v2 Announce Type: replace-cross 
Abstract: Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2502.19765</link>
<guid>https://arxiv.org/abs/2502.19765</guid>
<content:encoded><![CDATA[
arXiv:2502.19765v2 Announce Type: replace-cross 
Abstract: We propose EdiText, a controllable text editing method that modifies the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at a broad range of levels across various tasks, including toxicity control and sentiment control.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Collaborative Anti-Money Laundering Among Financial Institutions</title>
<link>https://arxiv.org/abs/2502.19952</link>
<guid>https://arxiv.org/abs/2502.19952</guid>
<content:encoded><![CDATA[
arXiv:2502.19952v2 Announce Type: replace-cross 
Abstract: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[
arXiv:2502.20727v4 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization</title>
<link>https://arxiv.org/abs/2503.05750</link>
<guid>https://arxiv.org/abs/2503.05750</guid>
<content:encoded><![CDATA[
arXiv:2503.05750v2 Announce Type: replace-cross 
Abstract: A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists' workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification</title>
<link>https://arxiv.org/abs/2503.05763</link>
<guid>https://arxiv.org/abs/2503.05763</guid>
<content:encoded><![CDATA[
arXiv:2503.05763v3 Announce Type: replace-cross 
Abstract: Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \textbf{Graph Masked Language Model (GMLM)}, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \textbf{dynamic active node selection} strategy for scalable PLM text processing; (ii) a GNN-specific \textbf{contrastive pretraining stage} using soft masking with a learnable graph \texttt{[MASK]} token for robust structural representations; and (iii) a \textbf{dedicated fusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \& DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \textbf{4.7\%} on Cornell and over \textbf{2.0\%} on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization</title>
<link>https://arxiv.org/abs/2503.15704</link>
<guid>https://arxiv.org/abs/2503.15704</guid>
<content:encoded><![CDATA[
arXiv:2503.15704v3 Announce Type: replace-cross 
Abstract: The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization has been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our implementations are able to obtain a full schedule of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning For Repairable Hardware Systems With Partial Coverage</title>
<link>https://arxiv.org/abs/2503.16315</link>
<guid>https://arxiv.org/abs/2503.16315</guid>
<content:encoded><![CDATA[
arXiv:2503.16315v2 Announce Type: replace-cross 
Abstract: Identifying the optimal diagnostic test and hardware system instance to infer reliability characteristics using field data is challenging, especially when constrained by fixed budgets and minimal maintenance cycles. Active Learning (AL) has shown promise for parameter inference with limited data and budget constraints in machine learning/deep learning tasks. However, AL for reliability model parameter inference remains underexplored for repairable hardware systems. It requires specialized AL Acquisition Functions (AFs) that consider hardware aging and the fact that a hardware system consists of multiple sub-systems, which may undergo only partial testing during a given diagnostic test. To address these challenges, we propose a relaxed Mixed Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing budgets. Furthermore, we design empirical-based simulation experiments focusing on two diagnostic testing scenarios: (1) partial tests of a hardware system with overlapping subsystem coverage, and (2) partial tests where one diagnostic test fully subsumes the subsystem coverage of another. We evaluate our proposed approach against the most widely used AL AF in the literature (entropy), as well as several intuitive AL AFs tailored for reliability model parameter inference. Our proposed AF ranked best on average among the alternative AFs across 6,000 experimental configurations, with respect to Area Under the Curve (AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error (MSE) curves, with statistical significance calculated at a 0.05 alpha level using a Friedman hypothesis test.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of Frontier Data Governance With Synthetic Data</title>
<link>https://arxiv.org/abs/2503.17414</link>
<guid>https://arxiv.org/abs/2503.17414</guid>
<content:encoded><![CDATA[
arXiv:2503.17414v2 Announce Type: replace-cross 
Abstract: Synthetic data, or data generated by machine learning models, is increasingly emerging as a solution to the data access problem. However, its use introduces significant governance and accountability challenges, and potentially debases existing governance paradigms, such as compute and data governance. In this paper, we identify 3 key governance and accountability challenges that synthetic data poses - it can enable the increased emergence of malicious actors, spontaneous biases and value drift. We thus craft 3 technical mechanisms to address these specific challenges, finding applications for synthetic data towards adversarial training, bias mitigation and value reinforcement. These could not only counteract the risks of synthetic data, but serve as critical levers for governance of the frontier in the future.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
arXiv:2503.18938v4 Announce Type: replace-cross 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations</title>
<link>https://arxiv.org/abs/2503.19449</link>
<guid>https://arxiv.org/abs/2503.19449</guid>
<content:encoded><![CDATA[
arXiv:2503.19449v2 Announce Type: replace-cross 
Abstract: Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RxRx3-core: Benchmarking drug-target interactions in High-Content Microscopy</title>
<link>https://arxiv.org/abs/2503.20158</link>
<guid>https://arxiv.org/abs/2503.20158</guid>
<content:encoded><![CDATA[
arXiv:2503.20158v2 Announce Type: replace-cross 
Abstract: High Content Screening (HCS) microscopy datasets have transformed the ability to profile cellular responses to genetic and chemical perturbations, enabling cell-based inference of drug-target interactions (DTI). However, the adoption of representation learning methods for HCS data has been hindered by the lack of accessible datasets and robust benchmarks. To address this gap, we present RxRx3-core, a curated and compressed subset of the RxRx3 dataset, and an associated DTI benchmarking task. At just 18GB, RxRx3-core significantly reduces the size barrier associated with large-scale HCS datasets while preserving critical data necessary for benchmarking representation learning models against a zero-shot DTI prediction task. RxRx3-core includes 222,601 microscopy images spanning 736 CRISPR knockouts and 1,674 compounds at 8 concentrations. RxRx3-core is available on HuggingFace and Polaris, along with pre-trained embeddings and benchmarking code, ensuring accessibility for the research community. By providing a compact dataset and robust benchmarks, we aim to accelerate innovation in representation learning methods for HCS data and support the discovery of novel biological insights.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
arXiv:2503.20756v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced simulation paradigm of human behaviour unveils complex financial systemic projection</title>
<link>https://arxiv.org/abs/2503.20787</link>
<guid>https://arxiv.org/abs/2503.20787</guid>
<content:encoded><![CDATA[
arXiv:2503.20787v2 Announce Type: replace-cross 
Abstract: The high-order complexity of human behaviour is likely the root cause of extreme difficulty in financial market projections. We consider that behavioural simulation can unveil systemic dynamics to support analysis. Simulating diverse human groups must account for the behavioural heterogeneity, especially in finance. To address the fidelity of simulated agents, on the basis of agent-based modeling, we propose a new paradigm of behavioural simulation where each agent is supported and driven by a hierarchical knowledge architecture. This architecture, integrating language and professional models, imitates behavioural processes in specific scenarios. Evaluated on futures markets, our simulator achieves a 13.29% deviation in simulating crisis scenarios whose price increase rate reaches 285.34%. Under normal conditions, our simulator also exhibits lower mean square error in predicting futures price of specific commodities. This technique bridges non-quantitative information with diverse market behaviour, offering a promising platform to simulate investor behaviour and its impact on market dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language and Reasoning Models are Shallow Disjunctive Reasoners</title>
<link>https://arxiv.org/abs/2503.23487</link>
<guid>https://arxiv.org/abs/2503.23487</guid>
<content:encoded><![CDATA[
arXiv:2503.23487v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
arXiv:2504.03561v3 Announce Type: replace-cross 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Adaptive Self-Attention for Quantum Transformer Models</title>
<link>https://arxiv.org/abs/2504.05336</link>
<guid>https://arxiv.org/abs/2504.05336</guid>
<content:encoded><![CDATA[
arXiv:2504.05336v2 Announce Type: replace-cross 
Abstract: Transformer models have revolutionized sequential learning across various domains, yet their self-attention mechanism incurs quadratic computational cost, posing limitations for real-time and resource-constrained tasks. To address this, we propose Quantum Adaptive Self-Attention (QASA), a novel hybrid architecture that enhances classical Transformer models with a quantum attention mechanism. QASA replaces dot-product attention with a parameterized quantum circuit (PQC) that adaptively captures inter-token relationships in the quantum Hilbert space. Additionally, a residual quantum projection module is introduced before the feedforward network to further refine temporal features. Our design retains classical efficiency in earlier layers while injecting quantum expressiveness in the final encoder block, ensuring compatibility with current NISQ hardware. Experiments on synthetic time-series tasks demonstrate that QASA achieves faster convergence and superior generalization compared to both standard Transformers and reduced classical variants. Preliminary complexity analysis suggests potential quantum advantages in gradient computation, opening new avenues for efficient quantum deep learning models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories</title>
<link>https://arxiv.org/abs/2504.16449</link>
<guid>https://arxiv.org/abs/2504.16449</guid>
<content:encoded><![CDATA[
arXiv:2504.16449v2 Announce Type: replace-cross 
Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage.This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</title>
<link>https://arxiv.org/abs/2504.19475</link>
<guid>https://arxiv.org/abs/2504.19475</guid>
<content:encoded><![CDATA[
arXiv:2504.19475v2 Announce Type: replace-cross 
Abstract: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.00763</link>
<guid>https://arxiv.org/abs/2505.00763</guid>
<content:encoded><![CDATA[
arXiv:2505.00763v2 Announce Type: replace-cross 
Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to understand the structure of dark matter halos. However, the kinematic information of these stars is often limited to celestial positions and line-of-sight velocities, making full phase space analysis challenging. Conventional methods rely on projected analytic phase space density models with several parameters and infer dark matter halo structures by solving the spherical Jeans equation. In this paper, we introduce an unsupervised machine learning method for solving the spherical Jeans equation in a model-independent way as a first step toward model-independent analysis of dwarf spheroidal galaxies. Using equivariant continuous normalizing flows, we demonstrate that spherically symmetric stellar phase space densities and velocity dispersions can be estimated without model assumptions. As a proof of concept, we apply our method to Gaia challenge datasets for spherical models and measure dark matter mass densities for given velocity anisotropy profiles. Our method can identify halo structures accurately, even with a small number of tracer stars.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Descriptor: C++ Self-Admitted Technical Debt Dataset (CppSATD)</title>
<link>https://arxiv.org/abs/2505.01136</link>
<guid>https://arxiv.org/abs/2505.01136</guid>
<content:encoded><![CDATA[
arXiv:2505.01136v2 Announce Type: replace-cross 
Abstract: In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Integrated Gradients for Feature Attribution</title>
<link>https://arxiv.org/abs/2505.03201</link>
<guid>https://arxiv.org/abs/2505.03201</guid>
<content:encoded><![CDATA[
arXiv:2505.03201v2 Announce Type: replace-cross 
Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique for assessing the significance of feature attributes of the input on model outputs by evaluating contributions from a baseline input to the current input. The choice of the baseline input significantly influences the resulting explanation. While the traditional Expected Gradients (EG) method assumes baselines can be uniformly sampled and averaged with equal weights, this study argues that baselines should not be treated equivalently. We introduce Weighted Integrated Gradients (WG), a novel approach that unsupervisedly evaluates baseline suitability and incorporates a strategy for selecting effective baselines. Theoretical analysis demonstrates that WG satisfies essential explanation method criteria and offers greater stability than prior approaches. Experimental results further confirm that WG outperforms EG across diverse scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by evaluating baselines, our method can filter a subset of effective baselines for each input to calculate explanations, maintaining high accuracy while reducing computational cost. The code is available at: https://github.com/tamnt240904/weighted_ig.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors</title>
<link>https://arxiv.org/abs/2505.04792</link>
<guid>https://arxiv.org/abs/2505.04792</guid>
<content:encoded><![CDATA[
arXiv:2505.04792v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence has advanced significantly in recent years thanks to innovations in the design and training of artificial neural networks (ANNs). Despite these advancements, we still understand relatively little about how elementary forms of ANNs learn, fail to learn, and generate false information without the intent to deceive, a phenomenon known as `confabulation'. To provide some foundational insight, in this paper we analyse how confabulation occurs in reservoir computers (RCs): a dynamical system in the form of an ANN. RCs are particularly useful to study as they are known to confabulate in a well-defined way: when RCs are trained to reconstruct the dynamics of a given attractor, they sometimes construct an attractor that they were not trained to construct, a so-called `untrained attractor' (UA). This paper sheds light on the role played by UAs when reconstruction fails and their influence when modelling transitions between reconstructed attractors. Based on our results, we conclude that UAs are an intrinsic feature of learning systems whose state spaces are bounded, and that this means of confabulation may be present in systems beyond RCs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Algorithms for Boolean Satisfiability</title>
<link>https://arxiv.org/abs/2505.06146</link>
<guid>https://arxiv.org/abs/2505.06146</guid>
<content:encoded><![CDATA[
arXiv:2505.06146v2 Announce Type: replace-cross 
Abstract: Learning-augmented algorithms are a prominent recent development in beyond worst-case analysis. In this framework, a problem instance is provided with a prediction (``advice'') from a machine-learning oracle, which provides partial information about an optimal solution, and the goal is to design algorithms that leverage this advice to improve worst-case performance. We study the classic Boolean satisfiability (SAT) decision and optimization problems within this framework using two forms of advice. ``Subset advice" provides a random $\epsilon$ fraction of the variables from an optimal assignment, whereas ``label advice" provides noisy predictions for all variables in an optimal assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate the exponential running time of the PPSZ family of algorithms due to Paturi, Pudlak, Saks and Zane, which currently represent the state of the art in the worst case. We accelerate the running time by a multiplicative factor of $2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and $k$. For the optimization problem, we show how to incorporate subset advice in a black-box fashion with any $\alpha$-approximation algorithm, improving the approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 + \Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT. Moreover, for label advice, we obtain near-optimal approximation for instances with large average degree, thereby generalizing recent results on MAX-CUT and MAX-$2$-LIN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measurement to Meaning: A Validity-Centered Framework for AI Evaluation</title>
<link>https://arxiv.org/abs/2505.10573</link>
<guid>https://arxiv.org/abs/2505.10573</guid>
<content:encoded><![CDATA[
arXiv:2505.10573v2 Announce Type: replace-cross 
Abstract: While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions, which provide a limited and potentially misleading assessment. We provide a structured approach for reasoning about the types of evaluative claims that can be made given the available evidence. For instance, our framework helps determine whether performance on a mathematical benchmark is an indication of the ability to solve problems on math tests or instead indicates a broader ability to reason. Our framework is well-suited for the contemporary paradigm in machine learning, where various stakeholders provide measurements and evaluations that downstream users use to validate their claims and decisions. At the same time, our framework also informs the construction of evaluations designed to speak to the validity of the relevant claims. By leveraging psychometrics' breakdown of validity, evaluations can prioritize the most critical facets for a given claim, improving empirical utility and decision-making efficacy. We illustrate our framework through detailed case studies of vision and language model evaluations, highlighting how explicitly considering validity strengthens the connection between evaluation evidence and the claims being made.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective</title>
<link>https://arxiv.org/abs/2505.12185</link>
<guid>https://arxiv.org/abs/2505.12185</guid>
<content:encoded><![CDATA[
arXiv:2505.12185v2 Announce Type: replace-cross 
Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A3 : an Analytical Low-Rank Approximation Framework for Attention</title>
<link>https://arxiv.org/abs/2505.12942</link>
<guid>https://arxiv.org/abs/2505.12942</guid>
<content:encoded><![CDATA[
arXiv:2505.12942v2 Announce Type: replace-cross 
Abstract: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL</title>
<link>https://arxiv.org/abs/2505.15791</link>
<guid>https://arxiv.org/abs/2505.15791</guid>
<content:encoded><![CDATA[
arXiv:2505.15791v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.16415</link>
<guid>https://arxiv.org/abs/2505.16415</guid>
<content:encoded><![CDATA[
arXiv:2505.16415v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models. Our code is available at https://github.com/ruizheliUOA/ARC_JSD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective</title>
<link>https://arxiv.org/abs/2505.16900</link>
<guid>https://arxiv.org/abs/2505.16900</guid>
<content:encoded><![CDATA[
arXiv:2505.16900v4 Announce Type: replace-cross 
Abstract: During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[
arXiv:2505.18174v2 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
<link>https://arxiv.org/abs/2505.18361</link>
<guid>https://arxiv.org/abs/2505.18361</guid>
<content:encoded><![CDATA[
arXiv:2505.18361v3 Announce Type: replace-cross 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM $\times$ DATA</title>
<link>https://arxiv.org/abs/2505.18458</link>
<guid>https://arxiv.org/abs/2505.18458</guid>
<content:encoded><![CDATA[
arXiv:2505.18458v3 Announce Type: replace-cross 
Abstract: The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPCAHUS: Subspace Clustering for Heteroscedastic Data</title>
<link>https://arxiv.org/abs/2505.18918</link>
<guid>https://arxiv.org/abs/2505.18918</guid>
<content:encoded><![CDATA[
arXiv:2505.18918v2 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. Various methods have been proposed to extend PCA to the union of subspace (UoS) setting for clustering data that come from multiple subspaces like K-Subspaces (KSS). However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a heteroscedastic-focused subspace clustering method, named ALPCAHUS, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace bases associated with the low-rank structure of the data. This clustering algorithm builds on K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS setting. Simulations and real-data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing clustering algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Representation Learning Approach for Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2505.19110</link>
<guid>https://arxiv.org/abs/2505.19110</guid>
<content:encoded><![CDATA[
arXiv:2505.19110v2 Announce Type: replace-cross 
Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models. In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74% and shows a better disentanglement than the 3D representation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Articulatory Inversion Models for Inter-Speaker Consistency</title>
<link>https://arxiv.org/abs/2505.20529</link>
<guid>https://arxiv.org/abs/2505.20529</guid>
<content:encoded><![CDATA[
arXiv:2505.20529v2 Announce Type: replace-cross 
Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse mapping from speech to articulation. Exact articulatory prediction from speech alone may be impossible, as speakers can choose different forms of articulation seemingly without reference to their vocal tract structure. However, once a speaker has selected an articulatory form, their productions vary minimally. Recent works in AAI have proposed adapting Self-Supervised Learning (SSL) models to single-speaker datasets, claiming that these single-speaker models provide a universal articulatory template. In this paper, we investigate whether SSL-adapted models trained on single and multi-speaker data produce articulatory targets which are consistent across speaker identities for English and Russian. We do this through the use of a novel evaluation method which extracts articulatory targets using minimal pair sets. We also present a training method which can improve interspeaker consistency using only speech data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
arXiv:2505.22232v2 Announce Type: replace-cross 
Abstract: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval</title>
<link>https://arxiv.org/abs/2505.22238</link>
<guid>https://arxiv.org/abs/2505.22238</guid>
<content:encoded><![CDATA[
arXiv:2505.22238v2 Announce Type: replace-cross 
Abstract: We present Yambda-5B, a large-scale open dataset sourced from the Yandex Music streaming platform. Yambda-5B contains 4.79 billion user-item interactions from 1 million users across 9.39 million tracks. The dataset includes two primary types of interactions: implicit feedback (listening events) and explicit feedback (likes, dislikes, unlikes and undislikes). In addition, we provide audio embeddings for most tracks, generated by a convolutional neural network trained on audio spectrograms. A key distinguishing feature of Yambda-5B is the inclusion of the is_organic flag, which separates organic user actions from recommendation-driven events. This distinction is critical for developing and evaluating machine learning algorithms, as Yandex Music relies on recommender systems to personalize track selection for users. To support rigorous benchmarking, we introduce an evaluation protocol based on a Global Temporal Split, allowing recommendation algorithms to be assessed in conditions that closely mirror real-world use. We report benchmark results for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using a variety of evaluation metrics. By releasing Yambda-5B to the community, we aim to provide a readily accessible, industrial-scale resource to advance research, foster innovation, and promote reproducible results in recommender systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
arXiv:2505.22642v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
<link>https://arxiv.org/abs/2502.18807</link>
<guid>https://arxiv.org/abs/2502.18807</guid>
<content:encoded><![CDATA[
<div> Dataset, battery life prediction, time series data, benchmark, neural networks  
Summary:  
The research addresses challenges in Battery Life Prediction (BLP) by proposing BatteryLife, a comprehensive dataset and benchmark for battery degradation tests. BatteryLife integrates 16 datasets, offering a larger sample size and more diverse battery life data compared to existing resources. The dataset includes batteries from various formats, chemical systems, operating temperatures, and charge/discharge protocols. Additionally, BatteryLife includes data from zinc-ion and sodium-ion batteries, as well as industry-tested large-capacity lithium-ion batteries. The study evaluates the effectiveness of popular baselines in both the BLP field and other time series fields, highlighting the need for specialized models. A plug-in technique called CyclePatch is introduced to enhance neural network performance for BLP. Benchmarking of 18 methods demonstrates the potential for improving model performance with CyclePatch across different aging conditions and domains. The dataset and research findings are available at the provided GitHub link.  
<br /><br />Summary: <div>
arXiv:2502.18807v5 Announce Type: replace 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.5 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 59 chemical systems, 9 operating temperatures, and 421 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in various neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
<div> interaction, data heterogeneity, federated learning, decision boundary, forgetting<br />
<br />Summary: 
This paper addresses the challenges of data heterogeneity in federated learning and its impact on the global decision boundary. Existing methods often suffer from forgetting, where clients only learn local decision boundaries and forget the global one. Even with pre-trained optimal weights, this issue persists. The proposed FedProj framework aims to robustly learn and retain the global decision boundary during local training. It introduces a server-side ensemble knowledge transfer loss for better ensemble knowledge fusion and utilizes an episodic memory of average ensemble logits from a public unlabeled dataset to regulate gradient updates. Experimental results demonstrate that FedProj significantly outperforms state-of-the-art methods in learning and retaining the global decision boundary, highlighting its effectiveness in addressing the challenges of data heterogeneity in federated learning. <div>
arXiv:2505.20485v2 Announce Type: replace 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, informational complexity, algorithmic information theory, Binarized Neural Networks, Block Decomposition Method

Summary:
Neural networks' informational complexity is a crucial challenge in machine learning, affecting generalization and optimization. Current approaches often use entropy-based metrics that do not capture underlying algorithmic regularities. This study proposes utilizing algorithmic information theory, specifically with Binarized Neural Networks (BNNs), to analyze learning dynamics. By applying the Block Decomposition Method (BDM) based on algorithmic complexity, the study shows that it better reflects structural changes during training than entropy. The BDM has stronger correlations with training loss for various model sizes and training runs, indicating that learning involves algorithmic compression of structured regularities. This work provides a method to estimate learning progression and suggests a framework for complexity-aware learning and regularization based on information theory and computability principles.

Summary: <div>
arXiv:2505.20646v2 Announce Type: replace 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control</title>
<link>https://arxiv.org/abs/2505.23857</link>
<guid>https://arxiv.org/abs/2505.23857</guid>
<content:encoded><![CDATA[
<div> MDP, reinforcement learning, output-feedback, observation histories, policy expressiveness <br />
<br />
Reinforcement learning often involves scenarios where the agent only receives partial state information, known as output-feedback settings. To address this challenge, the Output-Feedback Markov Decision Process (OPMDP) is proposed, extending the standard MDP formulation to incorporate decision-making based on observation histories. A new algorithm, Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), is introduced to encode historical observations using depthwise separable convolution and multi-head attention, maintaining policy expressiveness without the instability of recurrent models. Experimental results on continuous control tasks show that DATD3 outperforms existing memory-based and recurrent methods in both partial and full observability scenarios.<br /><br />Summary: <div>
arXiv:2505.23857v1 Announce Type: new 
Abstract: Reinforcement learning in real-world applications often involves output-feedback settings, where the agent receives only partial state information. To address this challenge, we propose the Output-Feedback Markov Decision Process (OPMDP), which extends the standard MDP formulation to accommodate decision-making based on observation histories. Building on this framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise separable convolution and multi-head attention to encode historical observations. DATD3 maintains policy expressiveness while avoiding the instability of recurrent models. Extensive experiments on continuous control tasks demonstrate that DATD3 outperforms existing memory-based and recurrent baselines under both partial and full observability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration</title>
<link>https://arxiv.org/abs/2505.23859</link>
<guid>https://arxiv.org/abs/2505.23859</guid>
<content:encoded><![CDATA[
<div> model merging, multi-task, feature drift, LOT Merging, optimization<br />
<br />
Summary:<br />
The article introduces Layer-wise Optimal Task Vector Merging (LOT Merging), a novel technique for consolidating knowledge from multiple fine-tuned task-specific experts into a unified model. LOT Merging aims to minimize feature drift between task-specific experts and the unified model, which is identified as a key factor in performance degradation during model merging. This technique can be formulated as a convex quadratic optimization problem, allowing for efficient model consolidation through basic matrix operations. Experimental results across vision and vision-language benchmarks show that LOT Merging outperforms existing methods, achieving improvements of up to 4.4% over state-of-the-art approaches. LOT Merging provides a promising approach for multi-task model merging without the performance gaps seen in parameter-level methods or the costly secondary training procedures of task-loss approaches. <div>
arXiv:2505.23859v1 Announce Type: new 
Abstract: Multi-task model merging aims to consolidate knowledge from multiple fine-tuned task-specific experts into a unified model while minimizing performance degradation. Existing methods primarily approach this by minimizing differences between task-specific experts and the unified model, either from a parameter-level or a task-loss perspective. However, parameter-level methods exhibit a significant performance gap compared to the upper bound, while task-loss approaches entail costly secondary training procedures. In contrast, we observe that performance degradation closely correlates with feature drift, i.e., differences in feature representations of the same sample caused by model merging. Motivated by this observation, we propose Layer-wise Optimal Task Vector Merging (LOT Merging), a technique that explicitly minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner. LOT Merging can be formulated as a convex quadratic optimization problem, enabling us to analytically derive closed-form solutions for the parameters of linear and normalization layers. Consequently, LOT Merging achieves efficient model consolidation through basic matrix operations. Extensive experiments across vision and vision-language benchmarks demonstrate that LOT Merging significantly outperforms baseline methods, achieving improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiBLDR: Bidirectional Behavior Learning for Drug Repositioning</title>
<link>https://arxiv.org/abs/2505.23861</link>
<guid>https://arxiv.org/abs/2505.23861</guid>
<content:encoded><![CDATA[
<div> Drug repositioning, deep learning, bidirectional behavior learning, BiBLDR, state-of-the-art performance <br />
Summary: <br />
Drug repositioning aims to find new uses for existing drugs efficiently. Existing deep learning methods struggle with novel drugs in cold-start scenarios. BiBLDR proposes a bidirectional behavior learning strategy by constructing bidirectional behavioral sequences and utilizing a two-stage strategy. The model captures drug-disease interaction patterns more robustly, achieving state-of-the-art performance and outperforming previous methods in cold-start scenarios. The code is available on GitHub. <div>
arXiv:2505.23861v1 Announce Type: new 
Abstract: Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in https://github.com/Renyeeah/BiBLDR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting</title>
<link>https://arxiv.org/abs/2505.23863</link>
<guid>https://arxiv.org/abs/2505.23863</guid>
<content:encoded><![CDATA[
<div> framework, chaotic systems, forecasting, physics-informed, PhyxMamba
Summary:
PhyxMamba introduces a novel framework for long-term forecasting of chaotic systems using a Mamba-based state-space model and physics-informed principles. By reconstructing attractor manifolds from short-term observations, the framework captures essential dynamical features for accurate forecasting. A generative training scheme allows Mamba to replicate the physical process with multi-token prediction and attractor geometry regularization, ensuring prediction accuracy and maintaining key statistical invariants. Extensive evaluations on simulated and real-world chaotic systems demonstrate PhyxMamba's superior forecasting capabilities and ability to capture essential dynamical invariants from limited data. This framework has broad implications in fields such as climate science, neuroscience, and epidemiology, providing a reliable means of predicting chaotic systems under observation-scarce conditions. The open-source code for PhyxMamba is available at https://github.com/tsinghua-fib-lab/PhyxMamba. 

<br /><br />Summary: <div>
arXiv:2505.23863v1 Announce Type: new 
Abstract: Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections</title>
<link>https://arxiv.org/abs/2505.23864</link>
<guid>https://arxiv.org/abs/2505.23864</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, graph-structured data, personalized subgraph, auxiliary projections, node embeddings 
Summary: 
FedAux is a novel framework for federated learning on graph-structured data, addressing challenges of non-IID scenarios where clients have distinct subgraphs. It incorporates personalized models through the use of auxiliary projection vectors (APVs) that capture client-specific information without sharing raw data or embeddings. These APVs are trained alongside local graph neural networks (GNNs), enabling inter-client similarities to be computed based on the APVs for knowledge transfer and aggregation of models at the server. The design of FedAux is supported by rigorous theoretical analysis, ensuring convergence and rationality. Empirical evaluations on various graph benchmarks demonstrate significant improvements in accuracy and personalization compared to existing methods. FedAux represents a promising approach for achieving effective federated learning on graph data while maintaining individual client customization. 
<br /><br />Summary: <div>
arXiv:2505.23864v1 Announce Type: new 
Abstract: Federated learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration</title>
<link>https://arxiv.org/abs/2505.23865</link>
<guid>https://arxiv.org/abs/2505.23865</guid>
<content:encoded><![CDATA[
<div> Keywords: Precision agriculture, Autonomous systems, Deep learning, Exploration efficiency, Crop monitoring

Summary: 
This study focuses on the development of efficient autonomous systems for crop monitoring in precision agriculture. The researchers propose a two-stage deep learning framework that incorporates a belief model to update a probabilistic map of the environment and prioritize informative regions. Three agent architectures were evaluated, with the Double-CNN DQN agent demonstrating superior exploration efficiency, especially in larger environments. The study highlights the importance of uncertainty-aware policies that leverage entropy, belief states, and visibility tracking for robust and scalable exploration. Future work will explore curriculum learning, multi-agent cooperation, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization. <br /><br />Summary: <div>
arXiv:2505.23865v1 Announce Type: new 
Abstract: Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments.
  Results show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2505.23866</link>
<guid>https://arxiv.org/abs/2505.23866</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, calibration, overconfidence, sharpness-aware minimization (SAM), CSAM 

Summary: 
Deep neural networks are commonly used in critical applications like medical diagnosis and autonomous driving, but they are often poorly calibrated and tend to be overconfident, leading to potential risks. Traditional training methods like stochastic gradient descent may exacerbate this issue. However, the sharpness-aware minimization (SAM) technique has been shown to mitigate overconfidence by implicitly maximizing the entropy of the predictive distribution. Building on SAM, a new approach called CSAM is proposed to further enhance model calibration. Experimental results on various datasets, including ImageNet-1K, demonstrate the effectiveness of SAM in reducing calibration error. CSAM outperforms SAM and other methods consistently in reducing calibration error, highlighting its potential for improving the reliability of deep neural networks in safety-critical applications.<br /><br />Summary: <div>
arXiv:2505.23866v1 Announce Type: new 
Abstract: Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title>
<link>https://arxiv.org/abs/2505.23868</link>
<guid>https://arxiv.org/abs/2505.23868</guid>
<content:encoded><![CDATA[
<div> Keywords: parameter-efficient fine-tuning, pre-trained language models, noise-robust adaptation, LoRA poisoning experts, noisy data <br />
Summary: 
The article introduces a new method, LoRA poisoning experts (LoPE), for adapting pre-trained language models to downstream tasks while improving robustness to noisy data. The conventional noise-handling approaches often require laborious data pre-processing or model architecture modifications susceptible to error accumulation. In contrast, LoPE enhances model robustness solely through generated noisy data. By incorporating a dedicated poisoning expert in an asymmetric LoRA configuration, LoPE strategically injects noise during fine-tuning to improve noise discrimination and processing. During inference, the dedicated poisoning expert is selectively masked to utilize purified knowledge from normal experts, resulting in noise-robust output. Extensive experiments show that LoPE achieves strong performance without the need for data cleaning, demonstrating its effectiveness in handling noisy data. <br /><br />Summary: <div>
arXiv:2505.23868v1 Announce Type: new 
Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2505.23870</link>
<guid>https://arxiv.org/abs/2505.23870</guid>
<content:encoded><![CDATA[
<div> adaptation, MaCP, cosine projection, model efficiency, accuracy

Summary: 
MaCP is a new adaptation method that utilizes cosine projection to achieve outstanding performance with minimal parameters and memory requirements for fine-tuning large foundation models. By projecting weight changes from low-rank adaptation into discrete cosine space, MaCP effectively enhances energy compaction and decorrelation properties. The weight change is then segmented across different levels of the discrete cosine spectrum, selecting the most crucial frequency components from each partition. Various experiments across single and multi-modality tasks, including natural language understanding, generation, text summarization, image classification, and video understanding, demonstrate MaCP's superior accuracy, reduced computational complexity, and lower memory demands compared to existing methods. MaCP proves to be a promising approach for enhancing model efficiency and accuracy in various tasks. 

<br /><br />Summary: <div>
arXiv:2505.23870v1 Announce Type: new 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23871</link>
<guid>https://arxiv.org/abs/2505.23871</guid>
<content:encoded><![CDATA[
<div> Keywords: offline reinforcement learning, data corruption, diffusion models, denoising, dataset recovery<br />
<br />
Summary: <br />
The article introduces a novel approach called Ambient Diffusion-Guided Dataset Recovery (ADG) to address data corruption in offline reinforcement learning (RL). It leverages Ambient Denoising Diffusion Probabilistic Models (DDPM) to handle noise in high-dimensional state spaces and multiple corrupted elements simultaneously. The ADG method first uses Ambient DDPM to distinguish between clean and corrupted data and then trains a standard DDPM on the clean subset. This trained standard DDPM is then used to refine the previously identified corrupted data, improving data quality for offline RL training. ADG is versatile and can be seamlessly integrated with any offline RL algorithm, offering strong denoising capabilities and theoretical guarantees. Experiments on various benchmarks show that ADG effectively mitigates the impact of corrupted data and enhances the robustness of offline RL, achieving state-of-the-art results. <br /> <div>
arXiv:2505.23871v1 Announce Type: new 
Abstract: Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants</title>
<link>https://arxiv.org/abs/2505.23875</link>
<guid>https://arxiv.org/abs/2505.23875</guid>
<content:encoded><![CDATA[
<div> benchmark, graph regression, program graphs, RelSC, graph neural network architectures

Summary:
RelSC is a new graph-regression dataset created from program graphs, offering a diverse and challenging benchmark for graph regression models. The dataset includes two variants, RelSC-H with rich node features and RelSC-M with multi-relational structures. The labels are execution-time costs of programs, different from existing benchmarks. Various graph neural network architectures are evaluated on both variants, showing consistent performance differences based on the structural representation chosen. This highlights the significance of representation choice in model behavior and indicates the value of RelSC in advancing graph regression methods.<br /><br />Summary: <div>
arXiv:2505.23875v1 Announce Type: new 
Abstract: Graph-level regression underpins many real-world applications, yet public benchmarks remain heavily skewed toward molecular graphs and citation networks. This limited diversity hinders progress on models that must generalize across both homogeneous and heterogeneous graph structures. We introduce RelSC, a new graph-regression dataset built from program graphs that combine syntactic and semantic information extracted from source code. Each graph is labelled with the execution-time cost of the corresponding program, providing a continuous target variable that differs markedly from those found in existing benchmarks. RelSC is released in two complementary variants. RelSC-H supplies rich node features under a single (homogeneous) edge type, while RelSC-M preserves the original multi-relational structure, connecting nodes through multiple edge types that encode distinct semantic relationships. Together, these variants let researchers probe how representation choice influences model behaviour. We evaluate a diverse set of graph neural network architectures on both variants of RelSC. The results reveal consistent performance differences between the homogeneous and multi-relational settings, emphasising the importance of structural representation. These findings demonstrate RelSC's value as a challenging and versatile benchmark for advancing graph regression methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size</title>
<link>https://arxiv.org/abs/2505.23876</link>
<guid>https://arxiv.org/abs/2505.23876</guid>
<content:encoded><![CDATA[
<div> neural networks, multilayer perceptron, metric recognition methods, MNIST dataset, training 

Summary:
The paper explores the effectiveness of utilizing multilayer perceptron neural networks with pre-calculated weights in metric recognition tasks. The study compares the performance of neural networks trained with pre-calculated weights against those with randomly initialized weights on various MNIST dataset sizes. Results indicate that neural networks with pre-calculated weights exhibit quicker training times and greater robustness when faced with reduced training datasets. This suggests that the analytical calculation of weights can significantly enhance the efficiency and reliability of multilayer perceptrons for metric recognition applications. <div>
arXiv:2505.23876v1 Announce Type: new 
Abstract: The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actor-Critic based Online Data Mixing For Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2505.23878</link>
<guid>https://arxiv.org/abs/2505.23878</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data mixing methods, online data mixing, actor-critic network, pretraining

Summary:<br /><br />
The paper introduces a new method called actor-critic based online data mixing (AC-ODM) to improve the pretraining process of Large Language Models (LLMs). By leveraging auxiliary actor-critic networks, AC-ODM dynamically adjusts domain weights and considers intra-domain interactions during data mixing. The method transfers the sampling strategy from a small proxy LLM to a larger target LLM, increasing efficiency and accelerating convergence. Numerical results show that AC-ODM-410M, using a proxy LLM with 410M parameters, achieves optimal validation perplexity 71% faster and improves performance on benchmarks like zero-shot MMLU and HumanEval. This approach addresses the limitations of previous data mixing techniques by adapting to training dynamics and optimizing data composition for better generalization in LLMs. <div>
arXiv:2505.23878v1 Announce Type: new 
Abstract: The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data</title>
<link>https://arxiv.org/abs/2505.23879</link>
<guid>https://arxiv.org/abs/2505.23879</guid>
<content:encoded><![CDATA[
<div> CNN-LSTM, deep learning, COVID-19 severity, spike protein sequences, clinical data
<br />
Summary:
A hybrid CNN-LSTM deep learning model was developed to predict COVID-19 severity using spike protein sequences and clinical metadata from South American patients. The model achieved high performance metrics, with an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%. Training stability was achieved at 85% accuracy with minimal overfitting. The prevalent viral lineages and clades aligned with regional epidemiological trends, indicating potential associations between viral genetics and clinical outcomes. The study demonstrates the effectiveness of AI in predicting disease severity and highlights the importance of genomic surveillance for precision public health interventions. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
<br />  <div>
arXiv:2505.23879v1 Announce Type: new 
Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Training Done Right</title>
<link>https://arxiv.org/abs/2505.23884</link>
<guid>https://arxiv.org/abs/2505.23884</guid>
<content:encoded><![CDATA[
<div> Large Chunk Test-Time Training, Context dependencies, Fast weights, State capacity, Scaling, 

Summary:
The paper introduces a new approach called Large Chunk Test-Time Training (LaCT) to handle context dependencies in deep learning models. LaCT adapts part of the model's weights (fast weights) during inference, similar to recurrent states in RNNs, but with larger chunk sizes ranging from 2K to 1M tokens. This approach significantly improves hardware utilization and allows for scaling of nonlinear state sizes without requiring complex kernel implementations. By using large chunk updates, LaCT enhances state capacity and enables efficient handling of long-context data in various tasks such as language models, auto-regressive video diffusion, and novel view synthesis. The method showcases successful experiments on tasks with sequences ranging up to 56K tokens, including novel view synthesis with a context length of 1 million. LaCT opens new possibilities for long-context modeling and test-time training in deep learning research. 

<br /><br />Summary: <div>
arXiv:2505.23884v1 Announce Type: new 
Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Website: https://tianyuanzhang.com/projects/ttt-done-right
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling</title>
<link>https://arxiv.org/abs/2505.23913</link>
<guid>https://arxiv.org/abs/2505.23913</guid>
<content:encoded><![CDATA[
<div> surrogate model, acquisition function, Bayesian optimization, deep generative model, efficiency<br />
<br />
Summary: 
This article presents a new approach to optimizing expensive black-box functions using Bayesian optimization (BO). The traditional BO process involves fitting a surrogate model and optimizing an acquisition function at each iteration. However, this study proposes a zero-shot solution that eliminates the need for surrogate fitting or acquisition function optimization. By using a pre-trained deep generative model to sample from the posterior over the optimum point, equivalent to Thompson sampling, this approach achieves significant efficiency gains. The method outperforms Gaussian process-based BO, offering a more cost-effective and time-efficient solution for high-throughput optimization tasks. This innovative approach enables efficient parallel and distributed BO, making it a promising advancement in the field of optimization. <div>
arXiv:2505.23913v1 Announce Type: new 
Abstract: The optimization of expensive black-box functions is ubiquitous in science and engineering. A common solution to this problem is Bayesian optimization (BO), which is generally comprised of two components: (i) a surrogate model and (ii) an acquisition function, which generally require expensive re-training and optimization steps at each iteration, respectively. Although recent work enabled in-context surrogate models that do not require re-training, virtually all existing BO methods still require acquisition function maximization to select the next observation, which introduces many knobs to tune, such as Monte Carlo samplers and multi-start optimizers. In this work, we propose a completely in-context, zero-shot solution for BO that does not require surrogate fitting or acquisition function optimization. This is done by using a pre-trained deep generative model to directly sample from the posterior over the optimum point. We show that this process is equivalent to Thompson sampling and demonstrate the capabilities and cost-effectiveness of our foundation model on a suite of real-world benchmarks. We achieve an efficiency gain of more than 35x in terms of wall-clock time when compared with Gaussian process-based BO, enabling efficient parallel and distributed BO, e.g., for high-throughput optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thompson Sampling in Online RLHF with General Function Approximation</title>
<link>https://arxiv.org/abs/2505.23927</link>
<guid>https://arxiv.org/abs/2505.23927</guid>
<content:encoded><![CDATA[
<div> Thompson sampling, reinforcement learning, human feedback, language models, regret bound  
Summary:  
- The study focuses on the theoretical efficiency of reinforcement learning from human feedback (RLHF) algorithms in aligning large language models with human preference.  
- A model-free posterior sampling algorithm for online RLHF is proposed, inspired by Thompson sampling, with an O(sqrt{T}) regret bound.  
- The Bellman eluder dimension is used as the complexity measure, and the algorithm's performance depends on the horizon, BE dimension, and log-bracketing number of the function class.  
- The concentration-type inequality of the squared Bellman error bound is established based on the maximum likelihood estimator generalization bound, crucial for obtaining the eluder-type regret bound.  
- The theoretical analysis provides insights into the statistical efficiency of RLHF algorithms in the online setting, contributing to the understanding of reinforcement learning with human feedback.  
<br /><br />Summary: <div>
arXiv:2505.23927v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has achieved great empirical success in aligning large language models (LLMs) with human preference, and it is of great importance to study the statistical efficiency of RLHF algorithms from a theoretical perspective. In this work, we consider the online RLHF setting where the preference data is revealed during the learning process and study action value function approximation. We design a model-free posterior sampling algorithm for online RLHF inspired by Thompson sampling and provide its theoretical guarantee. Specifically, we adopt Bellman eluder (BE) dimension as the complexity measure of the function class and establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other multiplicative factor depending on the horizon, BE dimension and the $log$-bracketing number of the function class. Further, in the analysis, we first establish the concentration-type inequality of the squared Bellman error bound based on the maximum likelihood estimator (MLE) generalization bound, which plays the crucial rules in obtaining the eluder-type regret bound and may be of independent interest.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIRD: Behavior Induction via Representation-structure Distillation</title>
<link>https://arxiv.org/abs/2505.23933</link>
<guid>https://arxiv.org/abs/2505.23933</guid>
<content:encoded><![CDATA[
<div> Keywords: Behavior Induction, Deep Learning, Robustness, Transfer Learning, Representation Structure

Summary:
BIRD (Behavior Induction via Representation-structure Distillation) is a framework designed to transfer human-aligned behavioral properties, such as robustness, fairness, and honesty, from a teacher model to a student model. This framework aims to address the challenge of forgetting aligned behavior during fine-tuning and the high cost of collecting task-specific data. BIRD has shown improved performance in out-of-distribution robustness in image classification, outperforming traditional methods like fine-tuning and transfer learning. It is effective even when the teacher model is simpler and significantly smaller than the student model. By analyzing the teacher's internal representation structure, BIRD can identify key properties that contribute to successful transfer of aligned behavior, offering practical guidance for teacher selection and design. This framework transforms small, well-aligned models into scalable alignment seeds, removing a bottleneck in deploying safe AI systems in real-world scenarios.
<br /><br />Summary: <div>
arXiv:2505.23933v1 Announce Type: new 
Abstract: Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searching Neural Architectures for Sensor Nodes on IoT Gateways</title>
<link>https://arxiv.org/abs/2505.23939</link>
<guid>https://arxiv.org/abs/2505.23939</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Networks, Edge Computing, Internet of Things, Privacy, Machine Learning

Summary: 
Neural Networks designing method at the edge for IoT applications, enabling ML access without data sharing. The approach runs on IoT gateways, designing custom NNs for sensor nodes on the local network, suitable for HIoT and IIoT. This method facilitates personalized healthcare and industrial services like quality control and predictive maintenance. The process prevents data disclosure to protect sensitive information, maintaining industrial secrets and personal data privacy. Experimental results on the Visual Wake Words dataset demonstrate state-of-the-art performance achieved in less than 10 hours on the Raspberry Pi Zero 2. 
<br /><br />Summary: <div>
arXiv:2505.23939v1 Announce Type: new 
Abstract: This paper presents an automatic method for the design of Neural Networks (NNs) at the edge, enabling Machine Learning (ML) access even in privacy-sensitive Internet of Things (IoT) applications. The proposed method runs on IoT gateways and designs NNs for connected sensor nodes without sharing the collected data outside the local network, keeping the data in the site of collection. This approach has the potential to enable ML for Healthcare Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing hardware-friendly and custom NNs at the edge for personalized healthcare and advanced industrial services such as quality control, predictive maintenance, or fault diagnosis. By preventing data from being disclosed to cloud services, this method safeguards sensitive information, including industrial secrets and personal data. The outcomes of a thorough experimental session confirm that -- on the Visual Wake Words dataset -- the proposed approach can achieve state-of-the-art results by exploiting a search procedure that runs in less than 10 hours on the Raspberry Pi Zero 2.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Language Models are Biased</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, vision language models, biases, counting accuracy, automated framework

Summary:
State-of-the-art vision language models (VLMs) are found to be strongly biased, impacting their accuracy on visual tasks such as counting and identification. These biases are evident across various domains including animals, logos, chess, board games, optical illusions, and patterned grids. Even when instructed to double-check results or rely solely on image details, VLMs only show a slight improvement in counting accuracy. The study showcases how prior knowledge encoded in large language models can lead to inaccurate responses in visual tasks. An automated framework for testing biases in VLMs is introduced, highlighting the need to address and mitigate these biases for better model performance. The research sheds light on a significant failure mode in VLMs and emphasizes the importance of assessing and rectifying biases in model development. <div>
arXiv:2505.23941v1 Announce Type: new 
Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations</title>
<link>https://arxiv.org/abs/2505.23942</link>
<guid>https://arxiv.org/abs/2505.23942</guid>
<content:encoded><![CDATA[
<div> Keywords: SG-Blend, activation functions, deep neural networks, SSwish, GELU

Summary:
SG-Blend is a novel activation function designed to optimize deep neural networks by blending SSwish, a first-order symmetric variant of Swish, and the established GELU through dynamic interpolation. The goal is to combine the controlled non-monotonicity and symmetry of SSwish with the smooth, probabilistic profile of GELU to achieve a more universally robust balance between model expressivity and gradient stability. Empirical evaluations across various natural language and computer vision tasks and models show performance improvements with negligible computational overhead. SG-Blend serves as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code for SG-Blend is available for reference. 

<br /><br />Summary: <div>
arXiv:2505.23942v1 Announce Type: new 
Abstract: The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at https://anonymous.4open.science/r/SGBlend-6CBC.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Future of Bayesian Prediction Is Prior-Fitted</title>
<link>https://arxiv.org/abs/2505.23947</link>
<guid>https://arxiv.org/abs/2505.23947</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, prior-data fitted networks, Bayesian models, amortized inference, data-scarce problems <br />
Summary: 
Prior-Data Fitted Networks (PFNs) are neural network methods trained on artificial datasets to capture prior distributions. They offer efficient pre-training in low-data scenarios and are increasingly used in complex domains. PFNs and amortized inference are seen as the future of Bayesian inference due to their ability to tackle data-scarce problems. The paper argues for the importance of PFNs in research and explores their potential and limitations. With computational resources expanding and real-world data generation stagnant, PFNs are highlighted as a crucial tool in various applications. The field has evolved from small tasks to handling larger datasets, indicating their versatility and relevance in modern Bayesian modeling. The paper advocates for continued exploration and development of PFNs to address the evolving challenges in Bayesian inference. 

<br /><br />Summary: <div>
arXiv:2505.23947v1 Announce Type: new 
Abstract: Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks</title>
<link>https://arxiv.org/abs/2505.23949</link>
<guid>https://arxiv.org/abs/2505.23949</guid>
<content:encoded><![CDATA[
<div> Pruning, Neural Networks, N:M Sparsity, Transposable Masks, Efficient Solver <br />
<br />
Summary: Network pruning reduces computational requirements in neural networks by utilizing N:M sparsity. However, this sparsity pattern does not efficiently accelerate both forward and backward passes during training. A new method is introduced to efficiently generate transposable N:M sparse masks that scale to large models. The approach formulates mask generation as optimal transport problems, utilizing entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Implementing this approach using tensors leverages GPU parallelism, achieving significant speedup with minimal error compared to existing methods. The generated transposable masks can be integrated with various pruning frameworks to produce models with arbitrary N:M values. Experimental results show that models with transposable 16:32 sparsity maintain performance close to their standard N:M counterparts and outperform standard 2:4 sparse models, demonstrating the practical value of the proposed approach. <br /> <div>
arXiv:2505.23949v1 Announce Type: new 
Abstract: Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective</title>
<link>https://arxiv.org/abs/2505.23954</link>
<guid>https://arxiv.org/abs/2505.23954</guid>
<content:encoded><![CDATA[
<div> allocation, resources, ML models, misreporting, causally-motivated

Summary:
- ML models are used for resource allocation, where affected agents may strategically modify their features.
- Distinguishing between deceptive changes and genuine modifications is challenging.
- A causally-motivated approach is proposed to identify and quantify misreporting by analyzing causal effects.
- Misreported features do not causally affect downstream variables, allowing for the detection of deception.
- The approach is validated using semi-synthetic and real Medicare datasets, proving its effectiveness in identifying misreporting in real-world scenarios.

<br /><br />Summary: <div>
arXiv:2505.23954v1 Announce Type: new 
Abstract: In settings where ML models are used to inform the allocation of resources, agents affected by the allocation decisions might have an incentive to strategically change their features to secure better outcomes. While prior work has studied strategic responses broadly, disentangling misreporting from genuine modification remains a fundamental challenge. In this paper, we propose a causally-motivated approach to identify and quantify how much an agent misreports on average by distinguishing deceptive changes in their features from genuine modification. Our key insight is that, unlike genuine modification, misreported features do not causally affect downstream variables (i.e., causal descendants). We exploit this asymmetry by comparing the causal effect of misreported features on their causal descendants as derived from manipulated datasets against those from unmanipulated datasets. We formally prove identifiability of the misreporting rate and characterize the variance of our estimator. We empirically validate our theoretical results using a semi-synthetic and real Medicare dataset with misreported data, demonstrating that our approach can be employed to identify misreporting in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</title>
<link>https://arxiv.org/abs/2505.23960</link>
<guid>https://arxiv.org/abs/2505.23960</guid>
<content:encoded><![CDATA[
<div> quantitative methods, representational spaces, systematic structure, deep-learning models, multi-agent reinforcement learning models <br />
Summary: This thesis introduces quantitative methods to analyze representational spaces in large-scale neural networks. It identifies structural primitives and information theoretic quantifications to understand how deep-learning models learn to represent information and what structures drive generalization. The research focuses on multi-agent reinforcement learning models, sequence-to-sequence models, and Large Language Models, shedding light on how large-scale distributed models of cognition learn. The experiments demonstrate parallels between the structures of language and those driving contemporary neural networks' performance. Additionally, a novel approach to estimating vector space entropy is introduced, allowing for analysis across models of varying parameter sizes from one million to 12 billion. This analysis aims to enhance our understanding of how neural networks learn and their similarities to human cognitive systems. <br /><br />Summary: <div>
arXiv:2505.23960v1 Announce Type: new 
Abstract: Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Approximations for Hard Graph Problems using Predictions</title>
<link>https://arxiv.org/abs/2505.23967</link>
<guid>https://arxiv.org/abs/2505.23967</guid>
<content:encoded><![CDATA[
<div> prediction model, graph problems, approximation algorithms, MaxCut, Vertex Cover, Set Cover<br />
Summary: <br />
The article introduces improved approximation algorithms for NP-hard graph problems by incorporating predictions based on past data. Building upon the $\varepsilon$-prediction framework, the edge-based model uses two bits of information per edge to make predictions about optimal solutions. Even with weak correlations, these predictions help break approximation barriers in MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems. The algorithms developed improve approximation ratios by satisfying constraints related to high degree vertices using predictions and low-degree vertices without predictions, and combining the results effectively. <div>
arXiv:2505.23967v1 Announce Type: new 
Abstract: We design improved approximation algorithms for NP-hard graph problems by incorporating predictions (e.g., learned from past data). Our prediction model builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad, d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based version of this model, where each edge provides two bits of information, corresponding to predictions about whether each of its endpoints belong to an optimal solution. Even with weak predictions where each bit is only $\varepsilon$-correlated with the true solution, this information allows us to break approximation barriers in the standard setting. We develop algorithms with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems (among others). Across these problems, our algorithms share a unifying theme, where we separately satisfy constraints related to high degree vertices (using predictions) and low-degree vertices (without using predictions) and carefully combine the answers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training</title>
<link>https://arxiv.org/abs/2505.23971</link>
<guid>https://arxiv.org/abs/2505.23971</guid>
<content:encoded><![CDATA[
<div> keywords: batch size, language models, training, gradient noise, CBS <br />
Summary: 
The paper discusses the importance of determining the right batch size for training language models at scale. McCandlish et al. proposed a method to estimate the critical batch size (CBS) based on gradient noise scale, but this method has limitations. The authors introduce an empirical approach to directly measure the CBS and observe how it evolves during training. They find that the CBS starts near 0, increases rapidly initially, then plateaus as training progresses. This trend is consistent across different model sizes (1B and 7B), suggesting that insights from smaller training runs can inform larger-scale training runs. The study advocates for batch size warmup as a method to train language models reliably at large batch sizes. By implementing batch size warmup, they successfully train a model to achieve better loss with fewer gradient steps. This approach demonstrates the potential to enhance data parallelism without sacrificing performance. <br /> <br />Summary: <div>
arXiv:2505.23971v1 Announce Type: new 
Abstract: The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is too large will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a critical batch size (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to directly measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate batch size warmup as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Deadline and Batch Layered Synchronized Federated Learning</title>
<link>https://arxiv.org/abs/2505.23973</link>
<guid>https://arxiv.org/abs/2505.23973</guid>
<content:encoded><![CDATA[
<div> Framework, Federated Learning, Optimization, Convergence Analysis, Heterogeneous Conditions
Summary:
ADEL-FL introduces a novel framework for Federated Learning that optimally adjusts per-round deadlines and user-specific batch sizes. By jointly optimizing these parameters, the approach minimizes latency bottlenecks caused by slower devices (stragglers) in synchronous FL settings. A constrained optimization problem is formulated to minimize the expected L2 distance to the global optimum under total training time and global rounds. Convergence analysis under exponential compute models shows that ADEL-FL provides unbiased updates with bounded variance. Extensive experiments demonstrate ADEL-FL's superior performance in convergence rate and final accuracy compared to alternative methods in the presence of device heterogeneity. The approach effectively addresses the challenges posed by strict time constraints in Federated Learning settings, offering a promising solution for collaborative model training across distributed edge devices while preserving data privacy.<br /><br />Summary: <div>
arXiv:2505.23973v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed edge devices while preserving data privacy, and typically operates in a round-based synchronous manner. However, synchronous FL suffers from latency bottlenecks due to device heterogeneity, where slower clients (stragglers) delay or degrade global updates. Prior solutions, such as fixed deadlines, client selection, and layer-wise partial aggregation, alleviate the effect of stragglers, but treat round timing and local workload as static parameters, limiting their effectiveness under strict time constraints. We propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines and user-specific batch sizes for layer-wise aggregation. Our approach formulates a constrained optimization problem minimizing the expected L2 distance to the global optimum under total training time and global rounds. We provide a convergence analysis under exponential compute models and prove that ADEL-FL yields unbiased updates with bounded variance. Extensive experiments demonstrate that ADEL-FL outperforms alternative methods in both convergence rate and final accuracy under heterogeneous conditions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization</title>
<link>https://arxiv.org/abs/2505.23987</link>
<guid>https://arxiv.org/abs/2505.23987</guid>
<content:encoded><![CDATA[
<div> Keywords: molecule optimization, drug design, instruction-tuning, property-specific objectives, LLMs

Summary:
The study introduces C-MuMOInstruct, a dataset aimed at multi-property optimization in drug design with explicit, property-specific objectives. Using this dataset, the researchers developed GeLLMO-Cs, a series of instruction-tuned language models (LLMs) capable of targeted property-specific optimization. The experiments conducted across various tasks demonstrated that GeLLMO-Cs outperform strong baselines consistently, achieving success rates up to 126% higher. Moreover, the GeLLMO-Cs exhibit exceptional 0-shot generalization to new optimization tasks and unseen instructions, indicating their potential to support diverse and realistic optimizations with property-specific goals. The dataset C-MuMOInstruct and the code for GeLLMO-Cs are publicly available for further research and applications. The study signifies a significant advancement in addressing the complex requirements of molecule optimization in drug design, offering a promising solution for achieving pharmaceutically relevant molecular properties efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2505.23987v1 Announce Type: new 
Abstract: In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24003</link>
<guid>https://arxiv.org/abs/2505.24003</guid>
<content:encoded><![CDATA[
<div> Keywords: time series, multi-modal views, long-term forecasting, large vision models, trend-seasonal decomposition

Summary:
DMMV introduces a novel approach to enhance long-term time series forecasting by leveraging multi-modal views of numerical sequences in the form of images and texts. By integrating trend-seasonal decomposition and a unique backcast residual method, DMMV outperforms existing models in predicting future trends across diverse datasets. The use of large vision models (LVMs) for forecasting introduces a bias towards forecasting periods, which DMMV addresses effectively. Comparative evaluations show that DMMV achieves the best mean squared error on the majority of benchmark datasets, highlighting its superior performance in long-term time series forecasting. This innovative framework presents a promising direction for utilizing multi-modal views to improve forecasting accuracy. <br /><br />Summary: <div>
arXiv:2505.24003v1 Announce Type: new 
Abstract: Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, applying LVMs to LTSF poses an inductive bias towards "forecasting periods". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art (SOTA) models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How far away are truly hyperparameter-free learning algorithms?</title>
<link>https://arxiv.org/abs/2505.24005</link>
<guid>https://arxiv.org/abs/2505.24005</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperparameter tuning, machine learning systems, deep neural networks, optimization methods, learning-rate-free methods 

Summary: 
Despite advances in methodology, hyperparameter tuning remains crucial for machine learning system development. Deep neural networks have numerous optimization and regularization hyperparameters requiring careful tuning per workload. The ideal scenario would involve training algorithms with default settings that perform well across various workloads without the need for workload-specific tuning. Recent research focuses on reducing hyperparameters, particularly the learning rate and its schedule. This paper evaluates the potential of learning-rate-free methods as components of hyperparameter-free methods. Testing against the AlgoPerf benchmark revealed that default settings performed poorly, necessitating a search for configurations that achieved high performance across all workloads simultaneously. While the best learning-rate-free methods showed improved performance, they still slightly lagged behind a NadamW baseline. This study highlights the need for further enhancement of learning-rate-free methods and underscores the importance of benchmarking against strong, workload-agnostic baselines. 

<br /><br />Summary: <div>
arXiv:2505.24005v1 Announce Type: new 
Abstract: Despite major advances in methodology, hyperparameter tuning remains a crucial (and expensive) part of the development of machine learning systems. Even ignoring architectural choices, deep neural networks have a large number of optimization and regularization hyperparameters that need to be tuned carefully per workload in order to obtain the best results. In a perfect world, training algorithms would not require workload-specific hyperparameter tuning, but would instead have default settings that performed well across many workloads. Recently, there has been a growing literature on optimization methods which attempt to reduce the number of hyperparameters -- particularly the learning rate and its accompanying schedule. Given these developments, how far away is the dream of neural network training algorithms that completely obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as components of hyperparameter-free methods. We freeze their (non-learning rate) hyperparameters to default values, and score their performance using the recently-proposed AlgoPerf: Training Algorithms benchmark. We found that literature-supplied default settings performed poorly on the benchmark, so we performed a search for hyperparameter configurations that performed well across all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free methods had much improved performance but still lagged slightly behind a similarly calibrated NadamW baseline in overall benchmark score. Our results suggest that there is still much room for improvement for learning-rate-free methods, and that testing against a strong, workload-agnostic baseline is important to improve hyperparameter reduction techniques.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rich and the Simple: On the Implicit Bias of Adam and SGD</title>
<link>https://arxiv.org/abs/2505.24022</link>
<guid>https://arxiv.org/abs/2505.24022</guid>
<content:encoded><![CDATA[
<div> Adam, optimization algorithm, deep learning, implicit bias, gradient descent<br />
<br />
Summary: <br />
The paper explores the implicit bias of Adam, a popular optimization algorithm in deep learning, compared to standard first-order methods like gradient descent (GD). While neural networks trained with SGD tend to favor simple solutions, Adam is shown to be more resistant to simplicity bias. By analyzing the biases of Adam and GD when training two-layer ReLU neural networks, the study reveals that GD exhibits a simplicity bias, leading to suboptimal linear decision boundaries, while Adam creates richer, nonlinear boundaries closer to optimal solutions. Theoretical analysis and empirical results demonstrate that Adam's diversity in features leads to higher test accuracy and better generalization across datasets with spurious correlations, where networks trained with SGD struggle. <div>
arXiv:2505.24022v1 Announce Type: new 
Abstract: Adam is the de facto optimization algorithm for several deep learning applications, but an understanding of its implicit bias and how it differs from other algorithms, particularly standard first-order methods such as (stochastic) gradient descent (GD), remains limited. In practice, neural networks trained with SGD are known to exhibit simplicity bias -- a tendency to find simple solutions. In contrast, we show that Adam is more resistant to such simplicity bias. To demystify this phenomenon, in this paper, we investigate the differences in the implicit biases of Adam and GD when training two-layer ReLU neural networks on a binary classification task involving synthetic data with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting in a linear decision boundary with a suboptimal margin, whereas Adam leads to much richer and more diverse features, producing a nonlinear boundary that is closer to the Bayes' optimal predictor. This richer decision boundary also allows Adam to achieve higher test accuracy both in-distribution and under certain distribution shifts. We theoretically prove these results by analyzing the population gradients. To corroborate our theoretical findings, we present empirical results showing that this property of Adam leads to superior generalization across datasets with spurious correlations where neural networks trained with SGD are known to show simplicity bias and don't generalize well under certain distributional shifts.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</title>
<link>https://arxiv.org/abs/2505.24030</link>
<guid>https://arxiv.org/abs/2505.24030</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, Large Language Models (LLMs), Large Vision Models (LVMs), time series analysis, multi-modality <br />
<br />Summary:
The study investigates the utility of Large Vision Models (LVMs) in time series analysis, comparing their performance on classification and forecasting tasks using various datasets and baselines. The findings suggest that LVMs are effective for time series classification but face challenges in forecasting. While some LVMs show promise as forecasters, they are limited in their ability to utilize long look-back windows and exhibit a bias toward forecasting periods. The study highlights the need for further research to enhance LVMs and explore multimodal solutions for diverse time series tasks. <div>
arXiv:2505.24030v1 Announce Type: new 
Abstract: Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin</title>
<link>https://arxiv.org/abs/2505.24034</link>
<guid>https://arxiv.org/abs/2505.24034</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Distributed Training, Asynchronous RL Framework, GPU Clusters

Summary:
LlamaRL is a new asynchronous RL framework designed for efficiently training large-scale Language Models (LLMs) on GPU clusters. It is optimized for models with billions of parameters, ranging from 8B to 405B. The framework is built on PyTorch and features a streamlined, single-controller architecture for easy scalability and modularity. The asynchronous design of LlamaRL ensures strict RL speed-up, leading to efficiency gains of up to 10.7x compared to existing systems like DeepSpeed-Chat for a 405B-parameter model. By utilizing techniques such as colocated model offloading and distributed weight synchronization, LlamaRL demonstrates significant efficiency advantages that increase with model scale. This makes it well-suited for future large-scale RL training endeavors.

<br /><br />Summary: LlamaRL is an asynchronous RL framework optimized for training large-scale LLMs on GPU clusters. Built on PyTorch, it offers modularity and scalability, achieving up to 10.7x speed-up compared to existing systems. Through techniques like colocated model offloading and distributed weight synchronization, LlamaRL demonstrates superior efficiency gains that grow with model size, proving to be a promising solution for large-scale RL training. <div>
arXiv:2505.24034v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronTune: Towards Self-Guided Spurious Bias Mitigation</title>
<link>https://arxiv.org/abs/2505.24048</link>
<guid>https://arxiv.org/abs/2505.24048</guid>
<content:encoded><![CDATA[
<div> NeuronTune, spurious bias, deep neural networks, post hoc method, self-guided mitigation<br />
<br />
Summary:<br />
Deep neural networks often exhibit spurious bias, leading to reliance on non-essential features for predictions. Existing methods to mitigate this bias require external annotations, which may not be practical. NeuronTune is a post hoc method that intervenes in a model's decision process to regulate neurons causing spurious behaviors, without needing correlation annotations. Theoretical justification and experiments across various architectures and data modalities show that NeuronTune effectively reduces spurious bias in a self-guided manner, improving model robustness. <div>
arXiv:2505.24048v1 Announce Type: new 
Abstract: Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Gated Self-Attention</title>
<link>https://arxiv.org/abs/2505.24054</link>
<guid>https://arxiv.org/abs/2505.24054</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Multihead Differential Gated Self-Attention, noise resilience, contrast enhancement, lateral inhibition

Summary: 
The study introduces Multihead Differential Gated Self-Attention (M-DGSA) as a solution to address the vulnerability of Transformers to corrupted inputs. Inspired by biological neural circuits, M-DGSA incorporates an input-dependent gating mechanism grounded in lateral inhibition. The model includes excitatory and inhibitory branches per head that dynamically suppress attention noise. By fusing dual softmax maps with a sigmoid gate predicted from token embeddings, M-DGSA achieves context-aware contrast enhancement. This innovative approach seamlessly integrates into existing Transformer stacks with minimal computational overhead. Experimental results on vision and language benchmarks showcase the superior robustness of M-DGSA compared to vanilla Transformer, Vision Transformer, and Differential Transformer baselines. The study contributes a novel method for noise resilience in self-attention, combining principles of contrast enhancement and self-attention theory, backed by comprehensive cross-domain experiments. 

<br /><br />Summary: <div>
arXiv:2505.24054v1 Announce Type: new 
Abstract: Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self-attention treats all query-key interactions uniformly. Inspired by lateral inhibition in biological neural circuits and building on the recent use by the Differential Transformer's use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self-attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast-enhancement and self-attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs</title>
<link>https://arxiv.org/abs/2505.24055</link>
<guid>https://arxiv.org/abs/2505.24055</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, unsupervised domain adaptation, link prediction, message-passing mechanism, identity-preserving learning<br />
<br />Summary:
This paper introduces a novel framework for unsupervised domain adaptation in graph neural networks. It addresses the challenge of domain shift by leveraging link prediction to connect nodes between source and target graphs. This facilitates message-passing between the graphs and ensures that target nodes have "in-distribution" neighborhoods with the source domain. The framework modifies the target graph on the input level to reduce deviation from the source domain in the embedding space. It also includes an identity-preserving learning objective to prevent the loss of discriminative information in the target graph. Experimental results on real-world datasets demonstrate the effectiveness of this approach. <div>
arXiv:2505.24055v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have shown great ability for node classification on graphs. However, the success of GNNs relies on abundant labeled data, while obtaining high-quality labels is costly and challenging, especially for newly emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a classifier on the labeled source graph and adapts it to the unlabeled target graph, is attracting increasing attention. Various approaches have been proposed to alleviate the distribution shift between the source and target graphs to facilitate the classifier adaptation. However, most of them simply adopt existing UDA techniques developed for independent and identically distributed data to gain domain-invariant node embeddings for graphs, which do not fully consider the graph structure and message-passing mechanism of GNNs during the adaptation and will fail when label distribution shift exists among domains. In this paper, we proposed a novel framework that adopts link prediction to connect nodes between source and target graphs, which can facilitate message-passing between the source and target graphs and augment the target nodes to have ``in-distribution'' neighborhoods with the source domain. This strategy modified the target graph on the input level to reduce its deviation from the source domain in the embedding space and is insensitive to disproportional label distributions across domains. To prevent the loss of discriminative information in the target graph, we further design a novel identity-preserving learning objective, which guides the learning of the edge insertion module together with reconstruction and adaptation losses. Experimental results on real-world datasets demonstrate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition</title>
<link>https://arxiv.org/abs/2505.24059</link>
<guid>https://arxiv.org/abs/2505.24059</guid>
<content:encoded><![CDATA[
<div> Audio, video, phoneme recognition, multimodal learning, MRI corpus  
Summary:  
- Study developed models for phoneme recognition using single-speaker MRI data, aiming to understand the relationship between acoustics and articulation in speech.  
- Unimodal audio and video models as well as multimodal models were compared, showing similar performance in phonetic manner classes but differences in places of articulation.  
- Latent space analysis revealed similar encoding of phonetic space across audio and multimodal models.  
- Attention weights of the models highlighted differences in acoustic and articulatory timing for certain phonemes.  
- Previous multi-speaker corpus studies were limited in understanding detailed relationships between acoustics and articulation due to cross-speaker variability, emphasizing the importance of single-speaker data in multimodal learning research.  

<br /><br />Summary: <div>
arXiv:2505.24059v1 Announce Type: new 
Abstract: Although many previous studies have carried out multimodal learning with real-time MRI data that captures the audio-visual kinematics of the vocal tract during speech, these studies have been limited by their reliance on multi-speaker corpora. This prevents such models from learning a detailed relationship between acoustics and articulation due to considerable cross-speaker variability. In this study, we develop unimodal audio and video models as well as multimodal models for phoneme recognition using a long-form single-speaker MRI corpus, with the goal of disentangling and interpreting the contributions of each modality. Audio and multimodal models show similar performance on different phonetic manner classes but diverge on places of articulation. Interpretation of the models' latent space shows similar encoding of the phonetic space across audio and multimodal models, while the models' attention weights highlight differences in acoustic and articulatory timing for certain phonemes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterising the Inductive Biases of Neural Networks on Boolean Data</title>
<link>https://arxiv.org/abs/2505.24060</link>
<guid>https://arxiv.org/abs/2505.24060</guid>
<content:encoded><![CDATA[
<div> Neural networks, overparameterization, generalization, NTK task-model alignment, feature learning <br />
<br />
Summary: This article explores the ability of deep neural networks to generalize well across various tasks despite being heavily overparameterized. By studying the training dynamics of depth-2 discrete fully connected networks on Boolean functions, the authors establish a connection between the network's inductive prior, training dynamics, feature learning, and eventual generalization. Through a Monte Carlo learning algorithm, predictable training dynamics and interpretable features emerge, shedding light on how inductive bias and feature formation drive generalization. This end-to-end case study offers insights into the mechanisms underlying deep neural network generalization and highlights the importance of understanding the interplay between inductive priors, training dynamics, and feature learning in achieving robust generalization performance. <br /><br /> <div>
arXiv:2505.24060v1 Announce Type: new 
Abstract: Deep neural networks are renowned for their ability to generalise well across diverse tasks, even when heavily overparameterized. Existing works offer only partial explanations (for example, the NTK-based task-model alignment explanation neglects feature learning). Here, we provide an end-to-end, analytically tractable case study that links a network's inductive prior, its training dynamics including feature learning, and its eventual generalisation. Specifically, we exploit the one-to-one correspondence between depth-2 discrete fully connected networks and disjunctive normal form (DNF) formulas by training on Boolean functions. Under a Monte Carlo learning algorithm, our model exhibits predictable training dynamics and the emergence of interpretable features. This framework allows us to trace, in detail, how inductive bias and feature formation drive generalisation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning</title>
<link>https://arxiv.org/abs/2505.24061</link>
<guid>https://arxiv.org/abs/2505.24061</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, neuronal activity loss, tau-dormant neuron ratio, GraMa, ReGraMa <br />
Summary: <br />
1. Deep reinforcement learning agents often experience neuronal activity loss hindering their ability to adapt and learn continuously. <br />
2. The traditional method to address this issue, tau-dormant neuron ratio, lacks effectiveness in complex architectures. <br />
3. Maintaining a neuron's learning capacity is crucial for advanced RL agents, leading to the introduction of GraMa, a metric focusing on gradient-based adaptability. <br />
4. GraMa successfully identifies inactive neurons in diverse architectures and facilitates improved learning through ReGraMa, a reset process guided by GraMa. <br />
5. The application of ReGraMa enhances learning performance across various deep RL algorithms and benchmarks, including MuJoCo and the DeepMind Control Suite. <br /> <div>
arXiv:2505.24061v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical power in more complex architectures. To address this, we argue that in advanced RL agents, maintaining a neuron's learning capacity, its ability to adapt via gradient updates, is more critical than preserving its expressive ability. Based on this insight, we shift the statistical objective from activations to gradients, and introduce GraMa (Gradient Magnitude Neural Activity Metric), a lightweight, architecture-agnostic metric for quantifying neuron-level learning capacity. We show that GraMa effectively reveals persistent neuron inactivity across diverse architectures, including residual networks, diffusion models, and agents with varied activation functions. Moreover, resetting neurons guided by GraMa (ReGraMa) consistently improves learning performance across multiple deep RL algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Primal-Dual Neural Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2505.24067</link>
<guid>https://arxiv.org/abs/2505.24067</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Algorithmic Reasoning, primal-dual paradigm, Graph Neural Networks, approximation algorithms, real-world datasets

Summary: 
Neural Algorithmic Reasoning (NAR) is a framework that trains neural networks to simulate classical algorithms, allowing for structured and interpretable reasoning on complex data. This study introduces a general NAR framework based on the primal-dual paradigm, a method for developing efficient approximation algorithms. By connecting primal and dual variables in a bipartite representation, the model aligns primal-dual algorithms with Graph Neural Networks. Additionally, optimal solutions from small instances are incorporated to improve the model's reasoning capabilities. Empirical results show that the model not only simulates but also surpasses approximation algorithms for various tasks, demonstrating robust generalization to larger and out-of-distribution graphs. The framework's practical utility is exemplified by integrating it with commercial solvers and applying it to real-world datasets. <br /><br />Summary: <div>
arXiv:2505.24067v1 Announce Type: new 
Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures</title>
<link>https://arxiv.org/abs/2505.24069</link>
<guid>https://arxiv.org/abs/2505.24069</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, structural reasoning, data structures, benchmark, evaluation

Summary:
Large language models (LLMs) are increasingly used in real-world tasks that require data manipulation. A new benchmark called DSR-Bench has been introduced to evaluate LLMs' structural reasoning abilities through data structures, providing interpretable representations of data relationships. This benchmark includes various data structures, operations, and problem instances for detailed analysis. The evaluation process is automated and deterministic, ensuring objectivity and scalability. Results show that instruction-tuned models struggle with basic reasoning tasks, while reasoning-oriented models perform better but still face challenges with complex structures. Models often struggle with multi-dimensional data and natural language tasks, indicating a gap for real-world applications. These findings highlight the need for further improvement in LLMs' reasoning capabilities for practical deployment.

<br /><br />Summary: <div>
arXiv:2505.24069v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals</title>
<link>https://arxiv.org/abs/2505.24085</link>
<guid>https://arxiv.org/abs/2505.24085</guid>
<content:encoded><![CDATA[
<div> deep learning, atrial fibrillation, gradient boosting, autoencoder, AF detection

Summary:
The study introduces a novel hybrid methodology for atrial fibrillation (AF) detection, combining unsupervised deep learning with boosting models. A deep convolutional autoencoder (DCAE) is integrated with three boosting classifiers - AdaBoost, XGBoost, and LightGBM - to improve AF identification without manual feature extraction. The DCAE-LGBM model achieves impressive performance metrics, including an F1-score of 95.20% and a sensitivity of 99.99%, with a low inference latency of four seconds. This hybrid approach outperforms existing methods and meets the requirements for clinical deployment. By combining DCAE with gradient boosting, the system provides a reliable tool for automated AF detection in healthcare settings. <div>
arXiv:2505.24085v1 Announce Type: new 
Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with elevated health risks, where timely detection is pivotal for mitigating stroke-related morbidity. This study introduces an innovative hybrid methodology integrating unsupervised deep learning and gradient boosting models to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM (LGBM)-to harness their complementary advantages while addressing individual limitations. The proposed framework uniquely combines DCAE with gradient boosting, enabling end-to-end AF identification devoid of manual feature extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of 99.99%, and inference latency of four seconds, outperforming existing methods and aligning with clinical deployment requirements. The DCAE integration significantly enhances boosting models, positioning this hybrid system as a reliable tool for automated AF detection in clinical settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting</title>
<link>https://arxiv.org/abs/2505.24088</link>
<guid>https://arxiv.org/abs/2505.24088</guid>
<content:encoded><![CDATA[
<div> Proxy-FDA, fine-tuning, knowledge preservation, feature space, concept forgetting <br />
Summary: <br />
Proxy-FDA is a novel regularization method that aims to mitigate concept forgetting during fine-tuning of vision foundation models. It preserves structural knowledge in feature space through Feature Distribution Alignment using nearest neighbor graphs. By incorporating informative proxies to increase data diversity, Proxy-FDA significantly reduces forgetting and aligns feature distributions between pre-trained and fine-tuned models. The method demonstrates benefits across various fine-tuning settings and tasks like image classification, captioning, and VQA. A strong correlation between forgetting and distributional distance metrics is observed, highlighting the effectiveness of Proxy-FDA in maintaining knowledge while adapting models to new tasks. <div>
arXiv:2505.24088v1 Announce Type: new 
Abstract: Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. Experiments show that Proxy-FDA significantly reduces concept forgetting during fine-tuning, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end, few-shot and continual tuning) and across different tasks like image classification, captioning and VQA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Bayes-Optimal Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2505.24089</link>
<guid>https://arxiv.org/abs/2505.24089</guid>
<content:encoded><![CDATA[
<div> Bayes-optimal, membership inference attacks, i.i.d. data, graph-structured data, node-level MIAs, graph neural networks, BASE, G-BASE, classifier-based attacks, computational efficiency, optimal query strategies, RMIA, LiRA, Bayesian decision-theoretic framework<br />
Summary:<br />
The study presents membership inference attacks (MIAs) for both i.i.d. data and graph-structured data. By leveraging the Bayesian decision-theoretic framework, the researchers derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks. They introduce BASE and G-BASE, efficient approximations of the Bayes-optimal attack, with G-BASE demonstrating superior performance compared to existing classifier-based attacks. BASE, applicable to non-graph data as well, achieves performance on par or better than prior state-of-the-art MIAs like LiRA and RMIA at a lower computational cost. The study also establishes the equivalence between BASE and RMIA under a specific hyperparameter setting, offering a principled, Bayes-optimal rationale for the RMIA attack. <div>
arXiv:2505.24089v1 Announce Type: new 
Abstract: We develop practical and theoretically grounded membership inference attacks (MIAs) against both independent and identically distributed (i.i.d.) data and graph-structured data. Building on the Bayesian decision-theoretic framework of Sablayrolles et al., we derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks, addressing key open questions about optimal query strategies in the graph setting. We introduce BASE and G-BASE, computationally efficient approximations of the Bayes-optimal attack. G-BASE achieves superior performance compared to previously proposed classifier-based node-level MIA attacks. BASE, which is also applicable to non-graph data, matches or exceeds the performance of prior state-of-the-art MIAs, such as LiRA and RMIA, at a significantly lower computational cost. Finally, we show that BASE and RMIA are equivalent under a specific hyperparameter setting, providing a principled, Bayes-optimal justification for the RMIA attack.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke</title>
<link>https://arxiv.org/abs/2505.24101</link>
<guid>https://arxiv.org/abs/2505.24101</guid>
<content:encoded><![CDATA[
<div> Keywords: LOS prediction, acute stroke, machine learning, interpretable model, feature selection <br />
Summary: 
An interpretable multi-level stacking ensemble model was developed to predict length of stay (LOS) in acute stroke patients, using data from the Stroke Foundation Acute Audit in Australia. The model, developed separately for ischaemic and haemorrhagic stroke, outperformed logistic regression in predicting prolonged LOS in ischaemic stroke but not in haemorrhagic stroke. Candidate predictors were categorized into patient, clinical, and system domains, with key predictors refined using correlation-based approaches. Shared predictors for both types of stroke included rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and involvement of stroke care coordinators. The model achieved superior performance in ischaemic stroke, with further validation needed for haemorrhagic stroke in larger cohorts. SHAP analysis provided insights into the importance of these predictors in predicting LOS.<br /><br /> <div>
arXiv:2505.24101v1 Announce Type: new 
Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving care planning. Existing machine learning models have shown suboptimal predictive performance, limited generalisability, and have overlooked system-level factors. We aimed to enhance model efficiency, performance, and interpretability by refining predictors and developing an interpretable multi-level stacking ensemble model. Data were accessed from the biennial Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models were developed for ischaemic and haemorrhagic stroke separately. The outcome was prolonged LOS (the LOS above the 75th percentile). Candidate predictors (ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical, and system domains. Feature selection with correlation-based approaches was used to refine key predictors. The evaluation of models included discrimination (AUC), calibration curves, and interpretability (SHAP plots). In ischaemic stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance [AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However, the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for haemorrhagic. SHAP analysis identified shared predictors for both types of stroke: rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and stroke care coordinators involvement. An explainable ensemble model effectively predicted the prolonged LOS in ischaemic stroke. Further validation in larger cohorts is needed for haemorrhagic stroke.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs</title>
<link>https://arxiv.org/abs/2505.24110</link>
<guid>https://arxiv.org/abs/2505.24110</guid>
<content:encoded><![CDATA[
<div> Nondeterministic finite automata, ReLU neural networks, equivalence, symbolic simulation, regular language <br />
<br />
Summary: 
This study establishes the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks. It shows that ReLU activations can mimic nondeterministic branching, subset construction, and -closures precisely. The research proves that a three-layer ReLU network can accurately recognize any regular language accepted by an NFA of n states. Additionally, it demonstrates that gradient descent over these networks preserves symbolic semantics and acceptance behavior. Through extensive experiments, including tasks like parallel path tracking and acceptance classification, the study achieves high alignment with ground-truth automata. This work provides a complete symbolic simulation of NFAs within deep learning architectures, bridging automata theory with neural computation using ReLU dynamics. <div>
arXiv:2505.24110v1 Announce Type: new 
Abstract: We present a formal and constructive framework establishing the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks. By encoding automaton states as binary vectors and transitions as sparse linear layers, we show that ReLU activations simulate nondeterministic branching, subset construction, and $\epsilon$-closures in a mathematically precise manner. Our core theoretical results prove that a three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any regular language accepted by an $n$-state NFA-without recurrence, memory, or approximation. Furthermore, we show that gradient descent over structure-preserving networks preserves symbolic semantics and acceptance behavior. Extensive experiments across multiple validation tasks-including parallel path tracking, symbolic subset construction, $\epsilon$-closure convergence, acceptance classification, structural training invariants, and functional equivalence-achieve perfect or near-perfect empirical alignment with ground-truth automata. This work provides the first provably complete symbolic simulation of NFAs within standard deep learning architectures, uniting automata theory with neural computation through ReLU dynamics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</title>
<link>https://arxiv.org/abs/2505.24138</link>
<guid>https://arxiv.org/abs/2505.24138</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog/Mixed-Signal circuits, Multi-modal Large Language Models, AMSbench, circuit design, benchmark

Summary: 

Analog/Mixed-Signal (AMS) circuits are crucial in the IC industry, but automating their design has been challenging. Multi-modal Large Language Models (MLLMs) show promise in supporting AMS circuit analysis and design. To evaluate MLLM performance across AMS tasks, a benchmark suite called AMSbench was introduced. It includes 8000 test questions across circuit schematic perception, analysis, and design, assessing eight models. Results indicate limitations in complex reasoning and design tasks for current MLLMs. Advancements are needed to narrow the performance gap relative to human expertise and move towards fully automated AMS circuit design workflows. The data from this evaluation is available at the provided link. <br /><br />Summary: <div>
arXiv:2505.24138v1 Announce Type: new 
Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at https://huggingface.co/datasets/wwhhyy/AMSBench
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction</title>
<link>https://arxiv.org/abs/2505.24145</link>
<guid>https://arxiv.org/abs/2505.24145</guid>
<content:encoded><![CDATA[
<div> predictive modeling, computational fluid dynamics, machine learning, turbulent flows, stochastic differential equation

Summary:
The article introduces a novel conditional score-based diffusion model for multi-scenario fluid flow prediction. This model integrates an energy constraint based on statistical properties of turbulent flows, improving prediction quality with minimal training. The model has a simple and versatile architecture that does not require specific design, supporting plug-and-play enhancements for efficient solution generation. It features an efficient conditioning mechanism for training across different scenarios without model redesign. Various stochastic differential equation formulations are explored to demonstrate performance enhancements. Extensive experiments on complex fluid dynamics datasets show that the model consistently delivers stable, robust, and physically accurate predictions, even under turbulent conditions. With tuned parameters, it achieves accuracy across multiple scenarios while preserving key physical and statistical properties. The study provides a thorough analysis of the impact of stochastic differential equations and discusses the methodology across a range of fluid mechanics tasks. 

Summary: <br /><br /> <div>
arXiv:2505.24145v1 Announce Type: new 
Abstract: Building on recent advances in scientific machine learning and generative modeling for computational fluid dynamics, we propose a conditional score-based diffusion model designed for multi-scenarios fluid flow prediction. Our model integrates an energy constraint rooted in the statistical properties of turbulent flows, improving prediction quality with minimal training, while enabling efficient sampling at low cost. The method features a simple and general architecture that requires no problem-specific design, supports plug-and-play enhancements, and enables fast and flexible solution generation. It also demonstrates an efficient conditioning mechanism that simplifies training across different scenarios without demanding a redesign of existing models. We further explore various stochastic differential equation formulations to demonstrate how thoughtful design choices enhance performance. We validate the proposed methodology through extensive experiments on complex fluid dynamics datasets encompassing a variety of flow regimes and configurations. Results demonstrate that our model consistently achieves stable, robust, and physically faithful predictions, even under challenging turbulent conditions. With properly tuned parameters, it achieves accurate results across multiple scenarios while preserving key physical and statistical properties. We present a comprehensive analysis of stochastic differential equation impact and discuss our approach across diverse fluid mechanics tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</title>
<link>https://arxiv.org/abs/2505.24149</link>
<guid>https://arxiv.org/abs/2505.24149</guid>
<content:encoded><![CDATA[
<div> dynamic model update, concept drift, resource constraints, Lyapunov drift-plus-penalty framework, real-time ML deployments

Summary: 
RCCDA is proposed as a dynamic model update policy for machine learning algorithms facing concept drift in real-world environments. The policy optimizes training dynamics while ensuring strict adherence to predefined resource constraints. By utilizing past loss information and a tunable drift threshold, RCCDA minimizes computational overhead and provides theoretical performance assurances. The policy is developed within a Lyapunov drift-plus-penalty framework that limits update frequency and cost based on accumulated loss thresholds. Experimental results on domain generalization datasets show that RCCDA outperforms baseline methods in inference accuracy while meeting resource constraints under various concept drift scenarios. This lightweight policy is well-suited for real-time ML deployments, offering a practical and effective solution for maintaining model performance in changing environments. 

<br /><br />Summary: <div>
arXiv:2505.24149v1 Announce Type: new 
Abstract: Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24155</link>
<guid>https://arxiv.org/abs/2505.24155</guid>
<content:encoded><![CDATA[
arXiv:2505.24155v1 Announce Type: new 
Abstract: Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents</title>
<link>https://arxiv.org/abs/2505.24157</link>
<guid>https://arxiv.org/abs/2505.24157</guid>
<content:encoded><![CDATA[
arXiv:2505.24157v1 Announce Type: new 
Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem</title>
<link>https://arxiv.org/abs/2505.24178</link>
<guid>https://arxiv.org/abs/2505.24178</guid>
<content:encoded><![CDATA[
arXiv:2505.24178v1 Announce Type: new 
Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at https://github.com/kthrn22/OOD-Linker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</title>
<link>https://arxiv.org/abs/2505.24179</link>
<guid>https://arxiv.org/abs/2505.24179</guid>
<content:encoded><![CDATA[
arXiv:2505.24179v1 Announce Type: new 
Abstract: Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeV-R1: Reasoning-Enhanced Verilog Generation</title>
<link>https://arxiv.org/abs/2505.24183</link>
<guid>https://arxiv.org/abs/2505.24183</guid>
<content:encoded><![CDATA[
arXiv:2505.24183v1 Announce Type: new 
Abstract: Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling</title>
<link>https://arxiv.org/abs/2505.24185</link>
<guid>https://arxiv.org/abs/2505.24185</guid>
<content:encoded><![CDATA[
arXiv:2505.24185v1 Announce Type: new 
Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
arXiv:2505.24189v1 Announce Type: new 
Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Improving Generalization of Few-Shot Models with Synthetic Data</title>
<link>https://arxiv.org/abs/2505.24190</link>
<guid>https://arxiv.org/abs/2505.24190</guid>
<content:encoded><![CDATA[
arXiv:2505.24190v1 Announce Type: new 
Abstract: Few-shot image classification remains challenging due to the scarcity of labeled training examples. Augmenting them with synthetic data has emerged as a promising way to alleviate this issue, but models trained on synthetic samples often face performance degradation due to the inherent gap between real and synthetic distributions. To address this limitation, we develop a theoretical framework that quantifies the impact of such distribution discrepancies on supervised learning, specifically in the context of image classification. More importantly, our framework suggests practical ways to generate good synthetic samples and to train a predictor with high generalization ability. Building upon this framework, we propose a novel theoretical-based algorithm that integrates prototype learning to optimize both data partitioning and model training, effectively bridging the gap between real few-shot data and synthetic data. Extensive experiments results show that our approach demonstrates superior performance compared to state-of-the-art methods, outperforming them across multiple datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback</title>
<link>https://arxiv.org/abs/2505.24193</link>
<guid>https://arxiv.org/abs/2505.24193</guid>
<content:encoded><![CDATA[
arXiv:2505.24193v1 Announce Type: new 
Abstract: We study the multi-armed bandit problem with adversarially chosen delays in the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal performance in both stochastic and adversarial environments. While prior work has made progress toward this goal, existing algorithms suffer from significant gaps to the known lower bounds, especially in the stochastic settings. Our main contribution is a new algorithm that, up to logarithmic factors, matches the known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of $\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is the cumulative delay. In the stochastic case, we provide a regret bound which scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum \Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$ and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to simultaneously match the lower bounds in both stochastic and adversarial regimes in delayed environment. Moreover, even beyond the BoBW setting, our stochastic regret bound is the first to match the known lower bound under adversarial delays, improving the second term over the best known result by a factor of $K$.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks</title>
<link>https://arxiv.org/abs/2505.24205</link>
<guid>https://arxiv.org/abs/2505.24205</guid>
<content:encoded><![CDATA[
arXiv:2505.24205v1 Announce Type: new 
Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that $\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion</title>
<link>https://arxiv.org/abs/2505.24231</link>
<guid>https://arxiv.org/abs/2505.24231</guid>
<content:encoded><![CDATA[
arXiv:2505.24231v1 Announce Type: new 
Abstract: Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Informed Flows for Bayesian Inference of Probabilistic Programs</title>
<link>https://arxiv.org/abs/2505.24243</link>
<guid>https://arxiv.org/abs/2505.24243</guid>
<content:encoded><![CDATA[
arXiv:2505.24243v1 Announce Type: new 
Abstract: Variational inference often struggles with the posterior geometry exhibited by complex hierarchical Bayesian models. Recent advances in flow-based variational families and Variationally Inferred Parameters (VIP) each address aspects of this challenge, but their formal relationship is unexplored. Here, we prove that the combination of VIP and a full-rank Gaussian can be represented exactly as a forward autoregressive flow augmented with a translation term and input from the model's prior. Guided by this theoretical insight, we introduce the Model-Informed Flow (MIF) architecture, which adds the necessary translation mechanism, prior information, and hierarchical ordering. Empirically, MIF delivers tighter posterior approximations and matches or exceeds state-of-the-art performance across a suite of hierarchical and non-hierarchical benchmarks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Continual Learning with Progressive Neural Collapse</title>
<link>https://arxiv.org/abs/2505.24254</link>
<guid>https://arxiv.org/abs/2505.24254</guid>
<content:encoded><![CDATA[
arXiv:2505.24254v1 Announce Type: new 
Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a sequence of tasks, where a key challenge, namely Catastrophic Forgetting, persists due to the potential knowledge interference among different tasks. On the other hand, deep neural networks (DNNs) are shown to converge to a terminal state termed Neural Collapse during training, where all class prototypes geometrically form a static simplex equiangular tight frame (ETF). These maximally and equally separated class prototypes make the ETF an ideal target for model learning in CL to mitigate knowledge interference. Thus inspired, several studies have emerged very recently to leverage a fixed global ETF in CL, which however suffers from key drawbacks, such as impracticability and limited performance.To address these challenges and fully unlock the potential of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework that completely removes the need of a fixed global ETF in CL. Specifically, ProNC progressively expands the ETF target in a principled way by adding new class prototypes as vertices for new tasks, ensuring maximal separability across all encountered classes with minimal shifts from the previous ETF. We next develop a new CL framework by plugging ProNC into commonly used CL algorithm designs, where distillation is further leveraged to balance between target shifting for old classes and target aligning for new classes. Extensive experiments show that our approach significantly outperforms related baselines while maintaining superior flexibility, simplicity, and efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining</title>
<link>https://arxiv.org/abs/2505.24261</link>
<guid>https://arxiv.org/abs/2505.24261</guid>
<content:encoded><![CDATA[
arXiv:2505.24261v1 Announce Type: new 
Abstract: Data attribution methods, which quantify the influence of individual training data points on a machine learning model, have gained increasing popularity in data-centric applications in modern AI. Despite a recent surge of new methods developed in this space, the impact of hyperparameter tuning in these methods remains under-explored. In this work, we present the first large-scale empirical study to understand the hyperparameter sensitivity of common data attribution methods. Our results show that most methods are indeed sensitive to certain key hyperparameters. However, unlike typical machine learning algorithms -- whose hyperparameters can be tuned using computationally-cheap validation metrics -- evaluating data attribution performance often requires retraining models on subsets of training data, making such metrics prohibitively costly for hyperparameter tuning. This poses a critical open challenge for the practical application of data attribution methods. To address this challenge, we advocate for better theoretical understandings of hyperparameter behavior to inform efficient tuning strategies. As a case study, we provide a theoretical analysis of the regularization term that is critical in many variants of influence function methods. Building on this analysis, we propose a lightweight procedure for selecting the regularization value without model retraining, and validate its effectiveness across a range of standard data attribution benchmarks. Overall, our study identifies a fundamental yet overlooked challenge in the practical application of data attribution, and highlights the importance of careful discussion on hyperparameter selection in future method development.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fairness of Task Arithmetic: The Role of Task Vectors</title>
<link>https://arxiv.org/abs/2505.24262</link>
<guid>https://arxiv.org/abs/2505.24262</guid>
<content:encoded><![CDATA[
arXiv:2505.24262v1 Announce Type: new 
Abstract: Model editing techniques, particularly task arithmetic using task vectors, have shown promise in efficiently modifying pre-trained models through arithmetic operations like task addition and negation. Despite computational advantages, these methods may inadvertently affect model fairness, creating risks in sensitive applications like hate speech detection. However, the fairness implications of task arithmetic remain largely unexplored, presenting a critical gap in the existing literature. We systematically examine how manipulating task vectors affects fairness metrics, including Demographic Parity and Equalized Odds. To rigorously assess these effects, we benchmark task arithmetic against full fine-tuning, a costly but widely used baseline, and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning method. Additionally, we explore merging task vectors from models fine-tuned on demographic subgroups vulnerable to hate speech, investigating whether fairness outcomes can be controlled by adjusting task vector coefficients, potentially enabling tailored model behavior. Our results offer novel insights into the fairness implications of model editing and establish a foundation for fairness-aware and responsible model editing practices.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradPower: Powering Gradients for Faster Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2505.24275</link>
<guid>https://arxiv.org/abs/2505.24275</guid>
<content:encoded><![CDATA[
arXiv:2505.24275v1 Announce Type: new 
Abstract: We propose GradPower, a lightweight gradient-transformation technique for accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$, GradPower first applies the elementwise sign-power transformation: $\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds the transformed gradient into a base optimizer. Notably, GradPower requires only a single-line code change and no modifications to the base optimizer's internal logic, including the hyperparameters. When applied to Adam (termed AdamPower), GradPower consistently achieves lower terminal loss across diverse architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements. Finally, we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlights the influence of gradient noise.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Locally Linear Mappings</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[
arXiv:2505.24293v1 Announce Type: new 
Abstract: We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
arXiv:2505.24298v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective</title>
<link>https://arxiv.org/abs/2505.24313</link>
<guid>https://arxiv.org/abs/2505.24313</guid>
<content:encoded><![CDATA[
arXiv:2505.24313v1 Announce Type: new 
Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong student model, trained on a dataset labeled by a weak teacher, ultimately outperforms the teacher on the target task. Recent studies attribute this performance gain to the prediction misfit between the student and teacher models. In this work, we theoretically investigate the emergence of W2SG through a generalized bias-variance decomposition of Bregman divergence. Specifically, we show that the expected population risk gap between the student and teacher is quantified by the expected misfit between the two models. While this aligns with previous results, our analysis removes several restrictive assumptions, most notably, the convexity of the student's hypothesis class, required in earlier works. Moreover, we show that W2SG is more likely to emerge when the student model approximates its posterior mean teacher, rather than mimicking an individual teacher. Using a concrete example, we demonstrate that if the student model has significantly larger capacity than the teacher, it can indeed converge to this posterior mean. Our analysis also suggests that avoiding overfitting to the teacher's supervision and reducing the entropy of student's prediction further facilitate W2SG. In addition, we show that the reverse cross-entropy loss, unlike the standard forward cross-entropy, is less sensitive to the predictive uncertainty of the teacher. Finally, we empirically verify our theoretical insights and demonstrate that incorporating the reverse cross-entropy loss consistently improves student performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.24317</link>
<guid>https://arxiv.org/abs/2505.24317</guid>
<content:encoded><![CDATA[
arXiv:2505.24317v1 Announce Type: new 
Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation</title>
<link>https://arxiv.org/abs/2505.24324</link>
<guid>https://arxiv.org/abs/2505.24324</guid>
<content:encoded><![CDATA[
arXiv:2505.24324v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cartan Networks: Group theoretical Hyperbolic Deep Learning</title>
<link>https://arxiv.org/abs/2505.24353</link>
<guid>https://arxiv.org/abs/2505.24353</guid>
<content:encoded><![CDATA[
arXiv:2505.24353v1 Announce Type: new 
Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces to develop efficient and informative embeddings of hierarchical data. Here, we focus on the solvable group structure of hyperbolic spaces, which follows naturally from their construction as symmetric spaces. This dual nature of Lie group and Riemannian manifold allows us to propose a new class of hyperbolic deep learning algorithms where group homomorphisms are interleaved with metric-preserving diffeomorphisms. The resulting algorithms, which we call Cartan networks, show promising results on various benchmark data sets and open the way to a novel class of hyperbolic deep learning architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration</title>
<link>https://arxiv.org/abs/2505.24357</link>
<guid>https://arxiv.org/abs/2505.24357</guid>
<content:encoded><![CDATA[
arXiv:2505.24357v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: https://github.com/XIANGLONGYAN/ReCalKV.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning</title>
<link>https://arxiv.org/abs/2505.24360</link>
<guid>https://arxiv.org/abs/2505.24360</guid>
<content:encoded><![CDATA[
arXiv:2505.24360v1 Announce Type: new 
Abstract: Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm</title>
<link>https://arxiv.org/abs/2505.24365</link>
<guid>https://arxiv.org/abs/2505.24365</guid>
<content:encoded><![CDATA[
arXiv:2505.24365v1 Announce Type: new 
Abstract: This paper introduces a unified approach to cluster refinement and anomaly detection in datasets. We propose a novel algorithm that iteratively reduces the intra-cluster variance of N clusters until a global minimum is reached, yielding tighter clusters than the standard k-means algorithm. We evaluate the method using intrinsic measures for unsupervised learning, including the silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and extend it to anomaly detection by identifying points whose assignment causes a significant variance increase. External validation on synthetic data and the UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity score, V-measure, and F1 score. Results show variance reductions of 18.7% and 88.1% on the synthetic and Wine Quality datasets, respectively, along with accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Preference Learning for Robust LLM Alignment</title>
<link>https://arxiv.org/abs/2505.24369</link>
<guid>https://arxiv.org/abs/2505.24369</guid>
<content:encoded><![CDATA[
arXiv:2505.24369v1 Announce Type: new 
Abstract: Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</title>
<link>https://arxiv.org/abs/2505.24378</link>
<guid>https://arxiv.org/abs/2505.24378</guid>
<content:encoded><![CDATA[
arXiv:2505.24378v1 Announce Type: new 
Abstract: Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.24379</link>
<guid>https://arxiv.org/abs/2505.24379</guid>
<content:encoded><![CDATA[
arXiv:2505.24379v1 Announce Type: new 
Abstract: Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightSAM: Parameter-Agnostic Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2505.24399</link>
<guid>https://arxiv.org/abs/2505.24399</guid>
<content:encoded><![CDATA[
arXiv:2505.24399v1 Announce Type: new 
Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization ability of the machine learning model by exploring the flat minima landscape through weight perturbations. Despite its empirical success, SAM introduces an additional hyper-parameter, the perturbation radius, which causes the sensitivity of SAM to it. Moreover, it has been proved that the perturbation radius and learning rate of SAM are constrained by problem-dependent parameters to guarantee convergence. These limitations indicate the requirement of parameter-tuning in practical applications. In this paper, we propose the algorithm LightSAM which sets the perturbation radius and learning rate of SAM adaptively, thus extending the application scope of SAM. LightSAM employs three popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to replace the SGD optimizer for weight perturbation and model updating, reducing sensitivity to parameters. Theoretical results show that under weak assumptions, LightSAM could converge ideally with any choices of perturbation radius and learning rate, thus achieving parameter-agnostic. We conduct preliminary experiments on several deep learning tasks, which together with the theoretical findings validate the the effectiveness of LightSAM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets</title>
<link>https://arxiv.org/abs/2505.24403</link>
<guid>https://arxiv.org/abs/2505.24403</guid>
<content:encoded><![CDATA[
arXiv:2505.24403v1 Announce Type: new 
Abstract: The Lipschitz constant of a neural network is connected to several important properties of the network such as its robustness and generalization. It is thus useful in many settings to estimate the Lipschitz constant of a model. Prior work has focused mainly on estimating the Lipschitz constant of multi-layer perceptrons and convolutional neural networks. Here we focus on data modeled as sets or multisets of vectors and on neural networks that can handle such data. These models typically apply some permutation invariant aggregation function, such as the sum, mean or max operator, to the input multisets to produce a single vector for each input sample. In this paper, we investigate whether these aggregation functions are Lipschitz continuous with respect to three distance functions for unordered multisets, and we compute their Lipschitz constants. In the general case, we find that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Then, we build on these results to derive upper bounds on the Lipschitz constant of neural networks that can process multisets of vectors, while we also study their stability to perturbations and generalization under distribution shifts. To empirically verify our theoretical analysis, we conduct a series of experiments on datasets from different domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data</title>
<link>https://arxiv.org/abs/2505.24413</link>
<guid>https://arxiv.org/abs/2505.24413</guid>
<content:encoded><![CDATA[
arXiv:2505.24413v1 Announce Type: new 
Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool to solve multiple learning tasks simultaneously and has been successfully applied to healthcare, marketing, and biomedical fields. However, in order to borrow information across different tasks effectively, it is essential to utilize both homogeneous and heterogeneous information. Among the extensive literature on MTL, various forms of heterogeneity are presented in MTL problems, such as block-wise, distribution, and posterior heterogeneity. Existing methods, however, struggle to tackle these forms of heterogeneity simultaneously in a unified framework. In this paper, we propose a two-step learning strategy for MTL which addresses the aforementioned heterogeneity. First, we impute the missing blocks using shared representations extracted from homogeneous source across different tasks. Next, we disentangle the mappings between input features and responses into a shared component and a task-specific component, respectively, thereby enabling information borrowing through the shared component. Our numerical experiments and real-data analysis from the ADNI database demonstrate the superior MTL performance of the proposed method compared to other competing methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation</title>
<link>https://arxiv.org/abs/2505.24415</link>
<guid>https://arxiv.org/abs/2505.24415</guid>
<content:encoded><![CDATA[
arXiv:2505.24415v1 Announce Type: new 
Abstract: Automated evaluation of movement quality holds significant potential for enhancing physiotherapeutic treatments and sports training by providing objective, real-time feedback. However, the effectiveness of deep learning models in assessing movements captured by inertial measurement units (IMUs) is often hampered by limited data availability, class imbalance, and label ambiguity. In this work, we present a novel data augmentation method that generates realistic IMU data using musculoskeletal simulations integrated with systematic modifications of movement trajectories. Crucially, our approach ensures biomechanical plausibility and allows for automatic, reliable labeling by combining inverse kinematic parameters with a knowledge-based evaluation strategy. Extensive evaluations demonstrate that augmented variants closely resembles real-world data, significantly improving the classification accuracy and generalization capability of neural network models. Additionally, we highlight the benefits of augmented data for patient-specific fine-tuning scenarios, particularly when only limited subject-specific training examples are available. Our findings underline the practicality and efficacy of this augmentation method in overcoming common challenges faced by deep learning applications in physiotherapeutic exercise evaluation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.24424</link>
<guid>https://arxiv.org/abs/2505.24424</guid>
<content:encoded><![CDATA[
arXiv:2505.24424v1 Announce Type: new 
Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot capabilities in classification and retrieval. However, these models often struggle with compositional reasoning - the ability to understand the relationships between concepts. A recent benchmark, SugarCrepe++, reveals that previous works on improving compositionality have mainly improved lexical sensitivity but neglected semantic understanding. In addition, downstream retrieval performance often deteriorates, although one would expect that improving compositionality should enhance retrieval. In this work, we introduce CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a novel training technique combining multiple images and their associated captions. CLIC improves compositionality across architectures as well as differently pre-trained CLIP models, both in terms of lexical and semantic understanding, and achieves consistent gains in retrieval performance. This even applies to the recent CLIPS, which achieves SOTA retrieval performance. Nevertheless, the short fine-tuning with CLIC leads to an improvement in retrieval and to the best compositional CLIP model on SugarCrepe++. All our models and code are available at https://clic-compositional-clip.github.io
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields</title>
<link>https://arxiv.org/abs/2505.24434</link>
<guid>https://arxiv.org/abs/2505.24434</guid>
<content:encoded><![CDATA[
arXiv:2505.24434v1 Announce Type: new 
Abstract: Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves Fr\'echet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs</title>
<link>https://arxiv.org/abs/2505.24438</link>
<guid>https://arxiv.org/abs/2505.24438</guid>
<content:encoded><![CDATA[
arXiv:2505.24438v1 Announce Type: new 
Abstract: An important characteristic of temporal graphs is how the directed arrow of time influences their causal topology, i.e., which nodes can possibly influence each other causally via time-respecting paths. The resulting patterns are often neglected by temporal graph neural networks (TGNNs). To formally analyze the expressive power of TGNNs, we lack a generalization of graph isomorphism to temporal graphs that fully captures their causal topology. Addressing this gap, we introduce the notion of consistent event graph isomorphism, which utilizes a time-unfolded representation of time-respecting paths in temporal graphs. We compare this definition with existing notions of temporal graph isomorphisms. We illustrate and highlight the advantages of our approach and develop a temporal generalization of the Weisfeiler-Leman algorithm to heuristically distinguish non-isomorphic temporal graphs. Building on this theoretical foundation, we derive a novel message passing scheme for temporal graph neural networks that operates on the event graph representation of temporal graphs. An experimental evaluation shows that our approach performs well in a temporal graph classification experiment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Safety Constraints for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24445</link>
<guid>https://arxiv.org/abs/2505.24445</guid>
<content:encoded><![CDATA[
arXiv:2505.24445v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepsize anything: A unified learning rate schedule for budgeted-iteration training</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
arXiv:2505.24452v1 Announce Type: new 
Abstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between $\varphi$ and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of $\varphi$.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA \textit{consistently surpasses} the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logits-Based Finetuning</title>
<link>https://arxiv.org/abs/2505.24461</link>
<guid>https://arxiv.org/abs/2505.24461</guid>
<content:encoded><![CDATA[
arXiv:2505.24461v1 Announce Type: new 
Abstract: The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at https://github.com/JulietLJY/MOOD.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Model Compression without Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.24469</link>
<guid>https://arxiv.org/abs/2505.24469</guid>
<content:encoded><![CDATA[
arXiv:2505.24469v1 Announce Type: new 
Abstract: Compressing and pruning large machine learning models has become a critical step towards their deployment in real-world applications. Standard pruning and compression techniques are typically designed without taking the structure of the network's weights into account, limiting their effectiveness. We explore the impact of smooth regularization on neural network training and model compression. By applying nuclear norm, first- and second-order derivative penalties of the weights during training, we encourage structured smoothness while preserving predictive performance on par with non-smooth models. We find that standard pruning methods often perform better when applied to these smooth models. Building on this observation, we apply a Singular-Value-Decomposition-based compression method that exploits the underlying smooth structure and approximates the model's weight tensors by smaller low-rank tensors. Our approach enables state-of-the-art compression without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18 on CIFAR-10 with $70\%$ fewer parameters.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy</title>
<link>https://arxiv.org/abs/2505.24473</link>
<guid>https://arxiv.org/abs/2505.24473</guid>
<content:encoded><![CDATA[
arXiv:2505.24473v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Centric Concept Bottlenecks</title>
<link>https://arxiv.org/abs/2505.24492</link>
<guid>https://arxiv.org/abs/2505.24492</guid>
<content:encoded><![CDATA[
arXiv:2505.24492v1 Announce Type: new 
Abstract: Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem</title>
<link>https://arxiv.org/abs/2505.24498</link>
<guid>https://arxiv.org/abs/2505.24498</guid>
<content:encoded><![CDATA[
arXiv:2505.24498v1 Announce Type: new 
Abstract: Recent work in online speech spectrogram inversion effectively combines Deep Learning with the Gradient Theorem to predict phase derivatives directly from magnitudes. Then, phases are estimated from their derivatives via least squares, resulting in a high quality reconstruction. In this work, we introduce three innovations that drastically reduce computational cost, while maintaining high quality: Firstly, we introduce a novel neural network architecture with just 8k parameters, 30 times smaller than previous state of the art. Secondly, increasing latency by 1 hop size allows us to further halve the cost of the neural inference step. Thirdly, we we observe that the least squares problem features a tridiagonal matrix and propose a linear-complexity solver for the least squares step that leverages tridiagonality and positive-semidefiniteness, achieving a speedup of several orders of magnitude. We release samples online.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset</title>
<link>https://arxiv.org/abs/2505.24505</link>
<guid>https://arxiv.org/abs/2505.24505</guid>
<content:encoded><![CDATA[
arXiv:2505.24505v1 Announce Type: new 
Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in power system operations, ensuring voltage stability and minimizing power losses. Recent advances in machine learning, particularly within the ``learning to optimize'' framework, have enabled fast and efficient approximations of ORPD solutions, typically by training models on precomputed optimization results. While these approaches have demonstrated promising performance on synthetic datasets, their effectiveness under real-world grid conditions remains largely unexplored. This paper makes two key contributions. First, we introduce a publicly available power system dataset that includes both the structural characteristics of Uruguay's electrical grid and nearly two years of real-world operational data, encompassing actual demand and generation profiles. Given Uruguay's high penetration of renewable energy, the ORPD problem has become the primary optimization challenge in its power network. Second, we assess the impact of real-world data on learning-based ORPD solutions, revealing a significant increase in prediction errors when transitioning from synthetic to actual demand and generation inputs. Our results highlight the limitations of existing models in learning under the complex statistical properties of real grid conditions and emphasize the need for more expressive architectures. By providing this dataset, we aim to facilitate further research into robust learning-based optimization techniques for power system management.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v1 Announce Type: new 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airborne Neural Network</title>
<link>https://arxiv.org/abs/2505.24513</link>
<guid>https://arxiv.org/abs/2505.24513</guid>
<content:encoded><![CDATA[
arXiv:2505.24513v1 Announce Type: new 
Abstract: Deep Learning, driven by neural networks, has led to groundbreaking advancements in Artificial Intelligence by enabling systems to learn and adapt like the human brain. These models have achieved remarkable results, particularly in data-intensive domains, supported by massive computational infrastructure. However, deploying such systems in Aerospace, where real time data processing and ultra low latency are critical, remains a challenge due to infrastructure limitations. This paper proposes a novel concept: the Airborne Neural Network a distributed architecture where multiple airborne devices each host a subset of neural network neurons. These devices compute collaboratively, guided by an airborne network controller and layer specific controllers, enabling real-time learning and inference during flight. This approach has the potential to revolutionize Aerospace applications, including airborne air traffic control, real-time weather and geographical predictions, and dynamic geospatial data processing. By enabling large-scale neural network operations in airborne environments, this work lays the foundation for the next generation of AI powered Aerospace systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Are Universally Consistent</title>
<link>https://arxiv.org/abs/2505.24531</link>
<guid>https://arxiv.org/abs/2505.24531</guid>
<content:encoded><![CDATA[
arXiv:2505.24531v1 Announce Type: new 
Abstract: Despite their central role in the success of foundational models and large-scale language modeling, the theoretical foundations governing the operation of Transformers remain only partially understood. Contemporary research has largely focused on their representational capacity for language comprehension and their prowess in in-context learning, frequently under idealized assumptions such as linearized attention mechanisms. Initially conceived to model sequence-to-sequence transformations, a fundamental and unresolved question is whether Transformers can robustly perform functional regression over sequences of input tokens. This question assumes heightened importance given the inherently non-Euclidean geometry underlying real-world data distributions. In this work, we establish that Transformers equipped with softmax-based nonlinear attention are uniformly consistent when tasked with executing Ordinary Least Squares (OLS) regression, provided both the inputs and outputs are embedded in hyperbolic space. We derive deterministic upper bounds on the empirical error which, in the asymptotic regime, decay at a provable rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens and $d$ the embedding dimensionality. Notably, our analysis subsumes the Euclidean setting as a special case, recovering analogous convergence guarantees parameterized by the intrinsic dimensionality of the data manifold. These theoretical insights are corroborated through empirical evaluations on real-world datasets involving both continuous and categorical response variables.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators</title>
<link>https://arxiv.org/abs/2505.24533</link>
<guid>https://arxiv.org/abs/2505.24533</guid>
<content:encoded><![CDATA[
arXiv:2505.24533v1 Announce Type: new 
Abstract: We introduce a novel framework consisting of a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions by defining per-axis composition operators subject to non-commutativity and a global interchange law. These structures, defined recursively from a base case of vector-matrix pairs, model directional composition in multiple dimensions while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms in signal processing and data analysis. In this framework, data indices are embedded into a composite structure that decomposes into simpler components. We show that classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform are special cases of our algebraic structure. The framework provides a systematic way to derive these transforms by appropriately choosing vector and matrix pairs. By subsuming classical transforms within a common structure, the framework also enables the development of learnable transformations tailored to specific data modalities and tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.24534</link>
<guid>https://arxiv.org/abs/2505.24534</guid>
<content:encoded><![CDATA[
arXiv:2505.24534v1 Announce Type: new 
Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in time-evolving simplicial complexes. While traditional graph anomaly detection techniques have been extensively studied, they often fail to capture changes in higher-order interactions that are crucial for identifying complex structural anomalies. These higher-order interactions can arise either directly from the underlying data itself or through graph lifting techniques. Our approach leverages the spectral properties of Hodge Laplacians of simplicial complexes to effectively model multi-way interactions among data points. By incorporating higher-dimensional simplicial structures into our method, our method enhances both detection accuracy and computational efficiency. Through comprehensive experiments on both synthetic and real-world datasets, we demonstrate that our approach outperforms existing graph methods in detecting both events and change points.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Linear Steering: Unified Multi-Attribute Control for Language Models</title>
<link>https://arxiv.org/abs/2505.24535</link>
<guid>https://arxiv.org/abs/2505.24535</guid>
<content:encoded><![CDATA[
arXiv:2505.24535v1 Announce Type: new 
Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems</title>
<link>https://arxiv.org/abs/2505.24578</link>
<guid>https://arxiv.org/abs/2505.24578</guid>
<content:encoded><![CDATA[
arXiv:2505.24578v1 Announce Type: new 
Abstract: Complex piezoelectric systems are foundational in industrial applications. Their performance, however, is challenged by the nonlinear voltage-displacement hysteretic relationships. Efficient characterization methods are, therefore, essential for reliable design, monitoring, and maintenance. Recently proposed neural operator methods serve as surrogates for system characterization but face two pressing issues: interpretability and generalizability. State-of-the-art (SOTA) neural operators are black-boxes, providing little insight into the learned operator. Additionally, generalizing them to novel voltages and predicting displacement profiles beyond the training domain is challenging, limiting their practical use. To address these limitations, this paper proposes a neuro-symbolic operator (NSO) framework that derives the analytical operators governing hysteretic relationships. NSO first learns a Fourier neural operator mapping voltage fields to displacement profiles, followed by a library-based sparse model discovery method, generating white-box parsimonious models governing the underlying hysteresis. These models enable accurate and interpretable prediction of displacement profiles across varying and out-of-distribution voltage fields, facilitating generalizability. The potential of NSO is demonstrated by accurately predicting voltage-displacement hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts displacement profiles even for noisy and low-fidelity voltage data, emphasizing its robustness. The results highlight the advantages of NSO compared to SOTA neural operators and model discovery methods on several evaluation metrics. Consequently, NSO contributes to characterizing complex piezoelectric systems while improving the interpretability and generalizability of neural operators, essential for design, monitoring, maintenance, and other real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservation-preserved Fourier Neural Operator through Adaptive Correction</title>
<link>https://arxiv.org/abs/2505.24579</link>
<guid>https://arxiv.org/abs/2505.24579</guid>
<content:encoded><![CDATA[
arXiv:2505.24579v1 Announce Type: new 
Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and efficient approach for learning the numerical solutions to partial differential equations (PDEs) from data. However, standard FNO often fails to preserve key conservation laws, such as mass conservation, momentum conservation, norm conservation, etc., which are crucial for accurately modeling physical systems. Existing methods for incorporating these conservation laws into Fourier neural operators are achieved by designing related loss function or incorporating post-processing method at the training time. None of them can both exactly and adaptively correct the outputs to satisfy conservation laws, and our experiments show that these methods can lead to inferior performance while preserving conservation laws. In this work, we propose a novel adaptive correction approach to ensure the conservation of fundamental quantities. Our method introduces a learnable matrix to adaptively adjust the solution to satisfy the conservation law during training. It ensures that the outputs exactly satisfy the goal conservation law and allow for more flexibility and adaptivity for the model to correct the outputs. We theoretically show that applying our adaptive correction to an unconstrained FNO yields a solution with data loss no worse than that of the best conservation-satisfying FNO. We compare our approach with existing methods on a range of representative PDEs. Experiment results show that our method consistently outperform other methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams</title>
<link>https://arxiv.org/abs/2505.24584</link>
<guid>https://arxiv.org/abs/2505.24584</guid>
<content:encoded><![CDATA[
arXiv:2505.24584v1 Announce Type: new 
Abstract: Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[
arXiv:2505.24592v1 Announce Type: new 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Cumulative Encoding meets Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24595</link>
<guid>https://arxiv.org/abs/2505.24595</guid>
<content:encoded><![CDATA[
arXiv:2505.24595v1 Announce Type: new 
Abstract: Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches</title>
<link>https://arxiv.org/abs/2505.24603</link>
<guid>https://arxiv.org/abs/2505.24603</guid>
<content:encoded><![CDATA[
arXiv:2505.24603v1 Announce Type: new 
Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random Gaussian matrix, is a widely used technique for multiple problems in data science and machine learning, with applications spanning computationally efficient optimization, coded computing, and federated learning. This operation also provides differential privacy guarantees due to its inherent randomness. In this work, we revisit this operation through the lens of Renyi Differential Privacy (RDP), providing a refined privacy analysis that yields significantly tighter bounds than prior results. We then demonstrate how this improved analysis leads to performance improvement in different linear regression settings, establishing theoretical utility guarantees. Empirically, our methods improve performance across multiple datasets and, in several cases, reduce runtime.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-criteria Rank-based Aggregation for Explainable AI</title>
<link>https://arxiv.org/abs/2505.24612</link>
<guid>https://arxiv.org/abs/2505.24612</guid>
<content:encoded><![CDATA[
arXiv:2505.24612v1 Announce Type: new 
Abstract: Explainability is crucial for improving the transparency of black-box machine learning models. With the advancement of explanation methods such as LIME and SHAP, various XAI performance metrics have been developed to evaluate the quality of explanations. However, different explainers can provide contrasting explanations for the same prediction, introducing trade-offs across conflicting quality metrics. Although available aggregation approaches improve robustness, reducing explanations' variability, very limited research employed a multi-criteria decision-making approach. To address this gap, this paper introduces a multi-criteria rank-based weighted aggregation method that balances multiple quality metrics simultaneously to produce an ensemble of explanation models. Furthermore, we propose rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations. Extensive experiments on publicly available datasets demonstrate the robustness of the proposed model across these metrics. Comparative analyses of various multi-criteria decision-making and rank aggregation algorithms showed that TOPSIS and WSUM are the best candidates for this use case.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.24623</link>
<guid>https://arxiv.org/abs/2505.24623</guid>
<content:encoded><![CDATA[
arXiv:2505.24623v1 Announce Type: new 
Abstract: To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. Notably, HDD is seamlessly compatible with most existing DM methods, and extensive experiments on different datasets validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees</title>
<link>https://arxiv.org/abs/2505.24627</link>
<guid>https://arxiv.org/abs/2505.24627</guid>
<content:encoded><![CDATA[
arXiv:2505.24627v1 Announce Type: new 
Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and proposes a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performances on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks</title>
<link>https://arxiv.org/abs/2505.24629</link>
<guid>https://arxiv.org/abs/2505.24629</guid>
<content:encoded><![CDATA[
arXiv:2505.24629v1 Announce Type: new 
Abstract: Penalties are fraught and game-changing moments in soccer games that teams explicitly prepare for. Consequently, there has been substantial interest in analyzing them in order to provide advice to practitioners. From a data science perspective, such analyses suffer from a significant limitation: they make the unrealistic simplifying assumption that goalkeepers and takers select their action -- where to dive and where to the place the kick -- independently of each other. In reality, the choices that some goalkeepers make depend on the taker's movements and vice-versa. This adds substantial complexity to the problem because not all players have the same action capacities, that is, only some players are capable of basing their decisions on their opponent's movements. However, the small sample sizes on the player level mean that one may have limited insights into a specific opponent's capacities. We address these challenges by developing a player-agnostic simulation framework that can evaluate the efficacy of different goalkeeper strategies. It considers a rich set of choices and incorporates information about a goalkeeper's skills. Our work is grounded in a large dataset of penalties that were annotated by penalty experts and include aspects of both kicker and goalkeeper strategies. We show how our framework can be used to optimize goalkeeper policies in real-world situations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WILTing Trees: Interpreting the Distance Between MPNN Embeddings</title>
<link>https://arxiv.org/abs/2505.24642</link>
<guid>https://arxiv.org/abs/2505.24642</guid>
<content:encoded><![CDATA[
arXiv:2505.24642v1 Announce Type: new 
Abstract: We investigate the distance function learned by message passing neural networks (MPNNs) in specific tasks, aiming to capture the functional distance between prediction targets that MPNNs implicitly learn. This contrasts with previous work, which links MPNN distances on arbitrary tasks to structural distances on graphs that ignore task-specific information. To address this gap, we distill the distance between MPNN embeddings into an interpretable graph distance. Our method uses optimal transport on the Weisfeiler Leman Labeling Tree (WILT), where the edge weights reveal subgraphs that strongly influence the distance between embeddings. This approach generalizes two well-known graph kernels and can be computed in linear time. Through extensive experiments, we demonstrate that MPNNs define the relative position of embeddings by focusing on a small set of subgraphs that are known to be functionally important in the domain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Distributions over Permutations and Rankings with Factorized Representations</title>
<link>https://arxiv.org/abs/2505.24664</link>
<guid>https://arxiv.org/abs/2505.24664</guid>
<content:encoded><![CDATA[
arXiv:2505.24664v1 Announce Type: new 
Abstract: Learning distributions over permutations is a fundamental problem in machine learning, with applications in ranking, combinatorial optimization, structured prediction, and data association. Existing methods rely on mixtures of parametric families or neural networks with expensive variational inference procedures. In this work, we propose a novel approach that leverages alternative representations for permutations, including Lehmer codes, Fisher-Yates draws, and Insertion-Vectors. These representations form a bijection with the symmetric group, allowing for unconstrained learning using conventional deep learning techniques, and can represent any probability distribution over permutations. Our approach enables a trade-off between expressivity of the model family and computational requirements. In the least expressive and most computationally efficient case, our method subsumes previous families of well established probabilistic models over permutations, including Mallow's and the Repeated Insertion Model. Experiments indicate our method significantly outperforms current approaches on the jigsaw puzzle benchmark, a common task for permutation learning. However, we argue this benchmark is limited in its ability to assess learning probability distributions, as the target is a delta distribution (i.e., a single correct solution exists). We therefore propose two additional benchmarks: learning cyclic permutations and re-ranking movies based on user preference. We show that our method learns non-trivial distributions even in the least expressive mode, while traditional models fail to even generate valid permutations in this setting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning geometry and topology via multi-chart flows</title>
<link>https://arxiv.org/abs/2505.24665</link>
<guid>https://arxiv.org/abs/2505.24665</guid>
<content:encoded><![CDATA[
arXiv:2505.24665v1 Announce Type: new 
Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in high-dimensional spaces. This motivates learning degenerate normalizing flows that map between the ambient space and a low-dimensional latent space. However, if the manifold has a non-trivial topology, it can never be correctly learned using a single flow. Instead multiple flows must be `glued together'. In this paper, we first propose the general training scheme for learning such a collection of flows, and secondly we develop the first numerical algorithms for computing geodesics on such manifolds. Empirically, we demonstrate that this leads to highly significant improvements in topology estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning</title>
<link>https://arxiv.org/abs/2505.24676</link>
<guid>https://arxiv.org/abs/2505.24676</guid>
<content:encoded><![CDATA[
arXiv:2505.24676v1 Announce Type: new 
Abstract: Despite well-documented consequences of the U.S. government's 1930s housing policies on racial wealth disparities, scholars have struggled to quantify its precise financial effects due to the inaccessibility of historical property appraisal records. Many counties still store these records in physical formats, making large-scale quantitative analysis difficult. We present an approach scholars can use to digitize historical housing assessment data, applying it to build and release a dataset for one county. Starting from publicly available scanned documents, we manually annotated property cards for over 12,000 properties to train and validate our methods. We use OCR to label data for an additional 50,000 properties, based on our two-stage approach combining classical computer vision techniques with deep learning-based OCR. For cases where OCR cannot be applied, such as when scanned documents are not available, we show how a regression model based on building feature data can estimate the historical values, and test the generalizability of this model to other counties. With these cost-effective tools, scholars, community activists, and policy makers can better analyze and understand the historical impacts of redlining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs</title>
<link>https://arxiv.org/abs/2505.24684</link>
<guid>https://arxiv.org/abs/2505.24684</guid>
<content:encoded><![CDATA[
arXiv:2505.24684v1 Announce Type: new 
Abstract: Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting "V"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms</title>
<link>https://arxiv.org/abs/2505.24692</link>
<guid>https://arxiv.org/abs/2505.24692</guid>
<content:encoded><![CDATA[
arXiv:2505.24692v1 Announce Type: new 
Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary reward environment where the size of the action space (number of arms) is small. More recently developed methods typically relax only one of these assumptions: existing non-stationary bandit policies are designed for a small number of arms, while Lipschitz, linear, and Gaussian process bandit policies are designed to handle a large (or infinite) number of arms in stationary reward environments under constraints on the reward function. In this manuscript, we propose a novel policy to learn reward environments over a continuous space using Gaussian interpolation. We show that our method efficiently learns continuous Lipschitz reward functions with $\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally extends to non-stationary problems with a simple modification. We finally demonstrate that our method is computationally favorable (100-10000x faster) and experimentally outperforms sliding Gaussian process policies on datasets with non-stationarity and an extremely large number of arms.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Symmetric Losses for Robust Policy Optimization with Noisy Preferences</title>
<link>https://arxiv.org/abs/2505.24709</link>
<guid>https://arxiv.org/abs/2505.24709</guid>
<content:encoded><![CDATA[
arXiv:2505.24709v1 Announce Type: new 
Abstract: Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting</title>
<link>https://arxiv.org/abs/2505.24710</link>
<guid>https://arxiv.org/abs/2505.24710</guid>
<content:encoded><![CDATA[
arXiv:2505.24710v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRet: Improved Retriever for Code Editing</title>
<link>https://arxiv.org/abs/2505.24715</link>
<guid>https://arxiv.org/abs/2505.24715</guid>
<content:encoded><![CDATA[
arXiv:2505.24715v1 Announce Type: new 
Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations</title>
<link>https://arxiv.org/abs/2505.24717</link>
<guid>https://arxiv.org/abs/2505.24717</guid>
<content:encoded><![CDATA[
arXiv:2505.24717v1 Announce Type: new 
Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach</title>
<link>https://arxiv.org/abs/2505.24721</link>
<guid>https://arxiv.org/abs/2505.24721</guid>
<content:encoded><![CDATA[
arXiv:2505.24721v1 Announce Type: new 
Abstract: Memristor-based hardware offers new possibilities for energy-efficient machine learning (ML) by providing analog in-memory matrix multiplication. Current hardware prototypes cannot fit large neural networks, and related literature covers only small ML models for tasks like MNIST or single word recognition. Simulation can be used to explore how hardware properties affect larger models, but existing software assumes simplified hardware. We propose a PyTorch-based library based on "Synaptogen" to simulate neural network execution with accurately captured memristor hardware properties. For the first time, we show how an ML system with millions of parameters would behave on memristor hardware, using a Conformer trained on the speech recognition task TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the relative degradation in word error rate to 25% when using a 3-bit weight precision to execute linear operations via simulated analog computation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</title>
<link>https://arxiv.org/abs/2505.24722</link>
<guid>https://arxiv.org/abs/2505.24722</guid>
<content:encoded><![CDATA[
arXiv:2505.24722v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Federated Learning against Model Perturbation in Edge Networks</title>
<link>https://arxiv.org/abs/2505.24728</link>
<guid>https://arxiv.org/abs/2505.24728</guid>
<content:encoded><![CDATA[
arXiv:2505.24728v1 Announce Type: new 
Abstract: Federated Learning (FL) is a promising paradigm for realizing edge intelligence, allowing collaborative learning among distributed edge devices by sharing models instead of raw data. However, the shared models are often assumed to be ideal, which would be inevitably violated in practice due to various perturbations, leading to significant performance degradation. To overcome this challenge, we propose a novel method, termed Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL), which aims to improve model robustness against perturbations by exploring the geometrical property of the model landscape. Specifically, SMRFL solves a min-max optimization problem that promotes model convergence towards a flat minimum by minimizing the maximum loss within a neighborhood of the model parameters. In this way, model sensitivity to perturbations is reduced, and robustness is enhanced since models in the neighborhood of the flat minimum also enjoy low loss values. The theoretical result proves that SMRFL can converge at the same rate as FL without perturbations. Extensive experimental results show that SMRFL significantly enhances robustness against perturbations compared to three baseline methods on two real-world datasets under three perturbation scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Attribution from First Principles</title>
<link>https://arxiv.org/abs/2505.24729</link>
<guid>https://arxiv.org/abs/2505.24729</guid>
<content:encoded><![CDATA[
arXiv:2505.24729v1 Announce Type: new 
Abstract: Feature attribution methods are a popular approach to explain the behavior of machine learning models. They assign importance scores to each input feature, quantifying their influence on the model's prediction. However, evaluating these methods empirically remains a significant challenge. To bypass this shortcoming, several prior works have proposed axiomatic frameworks that any feature attribution method should satisfy. In this work, we argue that such axioms are often too restrictive, and propose in response a new feature attribution framework, built from the ground up. Rather than imposing axioms, we start by defining attributions for the simplest possible models, i.e., indicator functions, and use these as building blocks for more complex models. We then show that one recovers several existing attribution methods, depending on the choice of atomic attribution. Subsequently, we derive closed-form expressions for attribution of deep ReLU networks, and take a step toward the optimization of evaluation metrics with respect to feature attributions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning</title>
<link>https://arxiv.org/abs/2505.24737</link>
<guid>https://arxiv.org/abs/2505.24737</guid>
<content:encoded><![CDATA[
arXiv:2505.24737v1 Announce Type: new 
Abstract: This paper studies the problem of differentially private empirical risk minimization (DP-ERM) for binary linear classification. We obtain an efficient $(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of $\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} + \frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and $\gamma$ is the margin of linear separation of the remaining data points (after $S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic terms. In the agnostic case, we improve the existing results when the number of outliers is small. Our algorithm is highly adaptive because it does not require knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We also derive a utility bound for the advanced private hyperparameter tuning algorithm.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training</title>
<link>https://arxiv.org/abs/2505.24749</link>
<guid>https://arxiv.org/abs/2505.24749</guid>
<content:encoded><![CDATA[
arXiv:2505.24749v1 Announce Type: new 
Abstract: Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.24760</link>
<guid>https://arxiv.org/abs/2505.24760</guid>
<content:encoded><![CDATA[
arXiv:2505.24760v1 Announce Type: new 
Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption</title>
<link>https://arxiv.org/abs/2505.24773</link>
<guid>https://arxiv.org/abs/2505.24773</guid>
<content:encoded><![CDATA[
arXiv:2505.24773v1 Announce Type: new 
Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Symbolic Regression</title>
<link>https://arxiv.org/abs/2505.24776</link>
<guid>https://arxiv.org/abs/2505.24776</guid>
<content:encoded><![CDATA[
arXiv:2505.24776v1 Announce Type: new 
Abstract: Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlightened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative processes with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation</title>
<link>https://arxiv.org/abs/2505.24779</link>
<guid>https://arxiv.org/abs/2505.24779</guid>
<content:encoded><![CDATA[
arXiv:2505.24779v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features -- particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QGAN-based data augmentation for hybrid quantum-classical neural networks</title>
<link>https://arxiv.org/abs/2505.24780</link>
<guid>https://arxiv.org/abs/2505.24780</guid>
<content:encoded><![CDATA[
arXiv:2505.24780v1 Announce Type: new 
Abstract: Quantum neural networks converge faster and achieve higher accuracy than classical models. However, data augmentation in quantum machine learning remains underexplored. To tackle data scarcity, we integrate quantum generative adversarial networks (QGANs) with hybrid quantum-classical neural networks (HQCNNs) to develop an augmentation framework. We propose two strategies: a general approach to enhance data processing and classification across HQCNNs, and a customized strategy that dynamically generates samples tailored to the HQCNN's performance on specific data categories, improving its ability to learn from complex datasets. Simulation experiments on the MNIST dataset demonstrate that QGAN outperforms traditional data augmentation methods and classical GANs. Compared to baseline DCGAN, QGAN achieves comparable performance with half the parameters, balancing efficiency and effectiveness. This suggests that QGANs can simplify models and generate high-quality data, enhancing HQCNN accuracy and performance. These findings pave the way for applying quantum data augmentation techniques in machine learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding</title>
<link>https://arxiv.org/abs/2505.24791</link>
<guid>https://arxiv.org/abs/2505.24791</guid>
<content:encoded><![CDATA[
arXiv:2505.24791v1 Announce Type: new 
Abstract: Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByzFL: Research Framework for Robust Federated Learning</title>
<link>https://arxiv.org/abs/2505.24802</link>
<guid>https://arxiv.org/abs/2505.24802</guid>
<content:encoded><![CDATA[
arXiv:2505.24802v1 Announce Type: new 
Abstract: We present ByzFL, an open-source Python library for developing and benchmarking robust federated learning (FL) algorithms. ByzFL provides a unified and extensible framework that includes implementations of state-of-the-art robust aggregators, a suite of configurable attacks, and tools for simulating a variety of FL scenarios, including heterogeneous data distributions, multiple training algorithms, and adversarial threat models. The library enables systematic experimentation via a single JSON-based configuration file and includes built-in utilities for result visualization. Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to facilitate reproducible research and rapid prototyping of robust FL solutions. ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on GitHub: https://github.com/LPD-EPFL/byzfl.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24823</link>
<guid>https://arxiv.org/abs/2505.24823</guid>
<content:encoded><![CDATA[
arXiv:2505.24823v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24835</link>
<guid>https://arxiv.org/abs/2505.24835</guid>
<content:encoded><![CDATA[
arXiv:2505.24835v1 Announce Type: new 
Abstract: Fund allocation has been an increasingly important problem in the financial domain. In reality, we aim to allocate the funds to buy certain assets within a certain future period. Naive solutions such as prediction-only or Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the introduction of the SOTA time series forecasting model inevitably introduces additional uncertainty in the predicted result. To solve both problems mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate (RTS-PnO) framework, which holds no prior assumption on the forecasting models. Such a framework contains three features: (i) end-to-end training with objective alignment measurement, (ii) adaptive forecasting uncertainty calibration, and (iii) agnostic towards forecasting models. The evaluation of RTS-PnO is conducted over both online and offline experiments. For offline experiments, eight datasets from three categories of financial applications are used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other competitive baselines. The online experiment is conducted on the Cross-Border Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed when compared with the product-line approach. The code for the offline experiment is available at https://github.com/fuyuanlyu/RTS-PnO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascading Adversarial Bias from Injection to Distillation in Language Models</title>
<link>https://arxiv.org/abs/2505.24842</link>
<guid>https://arxiv.org/abs/2505.24842</guid>
<content:encoded><![CDATA[
arXiv:2505.24842v1 Announce Type: new 
Abstract: Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching</title>
<link>https://arxiv.org/abs/2505.24843</link>
<guid>https://arxiv.org/abs/2505.24843</guid>
<content:encoded><![CDATA[
arXiv:2505.24843v1 Announce Type: new 
Abstract: Spurious correlations can cause model performance to degrade in new environments. Prior causality-inspired works aim to learn invariant representations (e.g., IRM) but typically underperform empirical risk minimization (ERM). Recent alternatives improve robustness by leveraging test-time data, but such data may be unavailable in practice. To address these issues, we take a data-centric approach by leveraging invariant data pairs, pairs of samples that would have the same prediction with the optimally robust classifier. We prove that certain counterfactual pairs will naturally satisfy this invariance property and introduce noisy counterfactual matching (NCM), a simple constraint-based method for leveraging invariant pairs for enhanced robustness, even with a small set of noisy pairs-in the ideal case, each pair can eliminate one spurious feature. For linear causal models, we prove that the test domain error can be upper bounded by the in-domain error and a term that depends on the counterfactuals' diversity and quality. We validate on a synthetic dataset and demonstrate on real-world benchmarks that linear probing on a pretrained backbone improves robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning</title>
<link>https://arxiv.org/abs/2505.24844</link>
<guid>https://arxiv.org/abs/2505.24844</guid>
<content:encoded><![CDATA[
arXiv:2505.24844v1 Announce Type: new 
Abstract: Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at https://github.com/LIONS-EPFL/Chameleon.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
arXiv:2505.24850v1 Announce Type: new 
Abstract: Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking</title>
<link>https://arxiv.org/abs/2505.24857</link>
<guid>https://arxiv.org/abs/2505.24857</guid>
<content:encoded><![CDATA[
arXiv:2505.24857v1 Announce Type: new 
Abstract: Recent masked diffusion models (MDMs) have shown competitive performance compared to autoregressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization</title>
<link>https://arxiv.org/abs/2505.24859</link>
<guid>https://arxiv.org/abs/2505.24859</guid>
<content:encoded><![CDATA[
arXiv:2505.24859v1 Announce Type: new 
Abstract: Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. So far, steering vectors have predominantly been evaluated in multiple-choice settings, while their effectiveness in free-form generation tasks remains understudied. Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries of the NEWTS dataset. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Compared to steering, prompting offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results underscore the practical trade-off between control strength and text quality preservation when applying steering vectors to free-form generation tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models</title>
<link>https://arxiv.org/abs/2505.24874</link>
<guid>https://arxiv.org/abs/2505.24874</guid>
<content:encoded><![CDATA[
arXiv:2505.24874v1 Announce Type: new 
Abstract: Neuro-symbolic learning was proposed to address challenges with training neural networks for complex reasoning tasks with the added benefits of interpretability, reliability, and efficiency. Neuro-symbolic learning methods traditionally train neural models in conjunction with symbolic programs, but they face significant challenges that limit them to simplistic problems. On the other hand, purely-neural foundation models now reach state-of-the-art performance through prompting rather than training, but they are often unreliable and lack interpretability. Supplementing foundation models with symbolic programs, which we call neuro-symbolic prompting, provides a way to use these models for complex reasoning tasks. Doing so raises the question: What role does specialized model training as part of neuro-symbolic learning have in the age of foundation models? To explore this question, we highlight three pitfalls of traditional neuro-symbolic learning with respect to the compute, data, and programs leading to generalization problems. This position paper argues that foundation models enable generalizable neuro-symbolic solutions, offering a path towards achieving the original goals of neuro-symbolic learning without the downsides of training from scratch.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset</title>
<link>https://arxiv.org/abs/2411.14207</link>
<guid>https://arxiv.org/abs/2411.14207</guid>
<content:encoded><![CDATA[
arXiv:2411.14207v2 Announce Type: cross 
Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse Responses (HOA-RIRs), created using the Image Source Method. By employing higher-order Ambisonics, our dataset enables precise spatial audio reproduction, a critical requirement for realistic immersive audio applications. Leveraging the virtual simulation, we present a unique microphone configuration, based on the superposition principle, designed to optimize sound field coverage while addressing the limitations of traditional microphone arrays. The presented 64-microphone configuration allows us to capture RIRs directly in the Spherical Harmonics domain. The dataset features a wide range of room configurations, encompassing variations in room geometry, acoustic absorption materials, and source-receiver distances. A detailed description of the simulation setup is provided alongside for an accurate reproduction. The dataset serves as a vital resource for researchers working on spatial audio, particularly in applications involving machine learning to improve room acoustics modeling and sound field synthesis. It further provides a very high level of spatial resolution and realism crucial for tasks such as source localization, reverberation prediction, and immersive sound reproduction.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams</title>
<link>https://arxiv.org/abs/2505.05880</link>
<guid>https://arxiv.org/abs/2505.05880</guid>
<content:encoded><![CDATA[
arXiv:2505.05880v1 Announce Type: cross 
Abstract: Monitoring and analyzing process traces is a critical task for modern companies and organizations. In scenarios where there is a gap between trace events and reference business activities, this entails an interpretation problem, amounting to translating each event of any ongoing trace into the corresponding step of the activity instance. Building on a recent approach that frames the interpretation problem as an acceptance problem within an Abstract Argumentation Framework (AAF), one can elegantly analyze plausible event interpretations (possibly in an aggregated form), as well as offer explanations for those that conflict with prior process knowledge. Since, in settings where event-to-activity mapping is highly uncertain (or simply under-specified) this reasoning-based approach may yield lowly-informative results and heavy computation, one can think of discovering a sequencetagging model, trained to suggest highly-probable candidate event interpretations in a context-aware way. However, training such a model optimally may require using a large amount of manually-annotated example traces. Considering the urgent need of developing Green AI solutions enabling environmental and societal sustainability (with reduced labor/computational costs and carbon footprint), we propose a data/computation-efficient neuro-symbolic approach to the problem, where the candidate interpretations returned by the example-driven sequence tagger is refined by the AAF-based reasoner. This allows us to also leverage prior knowledge to compensate for the scarcity of example data, as confirmed by experimental results; clearly, this property is particularly useful in settings where data annotation and model optimization costs are subject to stringent constraints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection</title>
<link>https://arxiv.org/abs/2505.18754</link>
<guid>https://arxiv.org/abs/2505.18754</guid>
<content:encoded><![CDATA[
arXiv:2505.18754v1 Announce Type: cross 
Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection (59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Parallels Between Evolutionary Theory and the State of AI</title>
<link>https://arxiv.org/abs/2505.23774</link>
<guid>https://arxiv.org/abs/2505.23774</guid>
<content:encoded><![CDATA[
arXiv:2505.23774v1 Announce Type: cross 
Abstract: This article critically examines the foundational principles of contemporary AI methods, exploring the limitations that hinder its potential. We draw parallels between the modern AI landscape and the 20th-century Modern Synthesis in evolutionary biology, and highlight how advancements in evolutionary theory that augmented the Modern Synthesis, particularly those of Evolutionary Developmental Biology, offer insights that can inform a new design paradigm for AI. By synthesizing findings across AI and evolutionary theory, we propose a pathway to overcome existing limitations, enabling AI to achieve its aspirational goals.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified AI for Accurate Audio Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.23781</link>
<guid>https://arxiv.org/abs/2505.23781</guid>
<content:encoded><![CDATA[
arXiv:2505.23781v1 Announce Type: cross 
Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly detection by integrating advanced noise reduction, feature extraction, and machine learning modeling techniques. The approach combines spectral subtraction and adaptive filtering to enhance audio quality, followed by feature extraction using traditional methods like MFCCs and deep embeddings from pre-trained models such as OpenL3. The modeling pipeline incorporates classical models (SVM, Random Forest), deep learning architectures (CNNs), and ensemble methods to boost robustness and accuracy. Evaluated on benchmark datasets including TORGO and LibriSpeech, the proposed framework demonstrates superior performance in precision, recall, and classification of slurred vs. normal speech. This work addresses challenges in noisy environments and real-time applications and provides a scalable solution for audio-based anomaly detection.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning</title>
<link>https://arxiv.org/abs/2505.23783</link>
<guid>https://arxiv.org/abs/2505.23783</guid>
<content:encoded><![CDATA[
arXiv:2505.23783v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases, but also enables the ability to alter and even completely reverse the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques: context-invariance and directional trust-region. The former is designed to tackle the instability issue in ICL, while the latter controls the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Normal Patterns in Musical Loops</title>
<link>https://arxiv.org/abs/2505.23784</link>
<guid>https://arxiv.org/abs/2505.23784</guid>
<content:encoded><![CDATA[
arXiv:2505.23784v1 Announce Type: cross 
Abstract: This paper introduces an unsupervised framework for detecting audio patterns in musical samples (loops) through anomaly detection techniques, addressing challenges in music information retrieval (MIR). Existing methods are often constrained by reliance on handcrafted features, domain-specific limitations, or dependence on iterative user interaction. We address these limitations through an architecture combining deep feature extraction with unsupervised anomaly detection. Our approach leverages a pre-trained Hierarchical Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion Mechanism (FFM), to generate representations from variable-length audio loops. These embeddings are processed using one-class Deep Support Vector Data Description (Deep SVDD), which learns normative audio patterns by mapping them to a compact latent hypersphere. Evaluations on curated bass and guitar datasets compare standard and residual autoencoder variants against baselines like Isolation Forest (IF) and and principle component analysis (PCA) methods. Results show our Deep SVDD models, especially the residual autoencoder variant, deliver improved anomaly separation, particularly for larger variations. This research contributes a flexible, fully unsupervised solution for processing diverse audio samples, overcoming previous structural and input limitations while enabling effective pattern identification through distance-based latent space scoring.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: A Practical Attack on GGUF Quantization</title>
<link>https://arxiv.org/abs/2505.23786</link>
<guid>https://arxiv.org/abs/2505.23786</guid>
<content:encoded><![CDATA[
arXiv:2505.23786v1 Announce Type: cross 
Abstract: With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Exploration of Literature Landscape with LitChat</title>
<link>https://arxiv.org/abs/2505.23789</link>
<guid>https://arxiv.org/abs/2505.23789</guid>
<content:encoded><![CDATA[
arXiv:2505.23789v1 Announce Type: cross 
Abstract: We are living in an era of "big literature", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering "comprehensive, objective, open and transparent" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.23791</link>
<guid>https://arxiv.org/abs/2505.23791</guid>
<content:encoded><![CDATA[
arXiv:2505.23791v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Suicidal Risk on Social Media: A Hybrid Model</title>
<link>https://arxiv.org/abs/2505.23797</link>
<guid>https://arxiv.org/abs/2505.23797</guid>
<content:encoded><![CDATA[
arXiv:2505.23797v1 Announce Type: cross 
Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</title>
<link>https://arxiv.org/abs/2505.23799</link>
<guid>https://arxiv.org/abs/2505.23799</guid>
<content:encoded><![CDATA[
arXiv:2505.23799v1 Announce Type: cross 
Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks</title>
<link>https://arxiv.org/abs/2505.23801</link>
<guid>https://arxiv.org/abs/2505.23801</guid>
<content:encoded><![CDATA[
arXiv:2505.23801v1 Announce Type: cross 
Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies</title>
<link>https://arxiv.org/abs/2505.23804</link>
<guid>https://arxiv.org/abs/2505.23804</guid>
<content:encoded><![CDATA[
arXiv:2505.23804v1 Announce Type: cross 
Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions</title>
<link>https://arxiv.org/abs/2505.23811</link>
<guid>https://arxiv.org/abs/2505.23811</guid>
<content:encoded><![CDATA[
arXiv:2505.23811v1 Announce Type: cross 
Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance.It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content</title>
<link>https://arxiv.org/abs/2505.23812</link>
<guid>https://arxiv.org/abs/2505.23812</guid>
<content:encoded><![CDATA[
arXiv:2505.23812v1 Announce Type: cross 
Abstract: The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and 17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and 11.18\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLMs by Predicting Preferences from User Writing Samples</title>
<link>https://arxiv.org/abs/2505.23815</link>
<guid>https://arxiv.org/abs/2505.23815</guid>
<content:encoded><![CDATA[
arXiv:2505.23815v1 Announce Type: cross 
Abstract: Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\% improvement over ICL alone.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs</title>
<link>https://arxiv.org/abs/2505.23816</link>
<guid>https://arxiv.org/abs/2505.23816</guid>
<content:encoded><![CDATA[
arXiv:2505.23816v1 Announce Type: cross 
Abstract: Despite advances in large language models (LLMs) on reasoning and instruction-following benchmarks, it remains unclear whether they can reliably produce outputs aligned with a broad variety of user goals, a concept we refer to as steerability. The abundance of methods proposed to modify LLM behavior makes it unclear whether current LLMs are already steerable, or require further intervention. In particular, LLMs may exhibit (i) poor coverage, where rare user goals are underrepresented; (ii) miscalibration, where models overshoot requests; and (iii) side effects, where changes to one dimension of text inadvertently affect others. To systematically evaluate these failures, we introduce a framework based on a multi-dimensional goal space that models user goals and LLM outputs as vectors with dimensions corresponding to text attributes (e.g., reading difficulty). Applied to a text-rewriting task, we find that current LLMs struggle with steerability, as side effects are persistent. Interventions to improve steerability, such as prompt engineering, best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying effectiveness, yet side effects remain problematic. Our findings suggest that even strong LLMs struggle with steerability, and existing alignment strategies may be insufficient. We open-source our steerability evaluation framework at https://github.com/MLD3/steerability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses</title>
<link>https://arxiv.org/abs/2505.23834</link>
<guid>https://arxiv.org/abs/2505.23834</guid>
<content:encoded><![CDATA[
arXiv:2505.23834v1 Announce Type: cross 
Abstract: Lung sound classification is vital for early diagnosis of respiratory diseases. However, biomedical signals often exhibit inter-patient variability even among patients with the same symptoms, requiring a learning approach that considers individual differences. We propose a Patient-Aware Feature Alignment (PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL) and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same patient while separating those from other patients to capture patient variability, whereas GPAL draws each patient's centroid toward a global center, preventing feature space fragmentation. Our method achieves outstanding results on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for two-class classification. These findings highlight PAFA's ability to capture individualized patterns and demonstrate performance gains in distinct patient clusters, offering broader applications for patient-centered healthcare.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks</title>
<link>https://arxiv.org/abs/2505.23843</link>
<guid>https://arxiv.org/abs/2505.23843</guid>
<content:encoded><![CDATA[
arXiv:2505.23843v1 Announce Type: cross 
Abstract: Multi-round incomplete information tasks are crucial for evaluating the lateral thinking capabilities of large language models (LLMs). Currently, research primarily relies on multiple benchmarks and automated evaluation metrics to assess these abilities. However, our study reveals novel insights into the limitations of existing methods, as they often yield misleading results that fail to uncover key issues, such as shortcut-taking behaviors, rigid patterns, and premature task termination. These issues obscure the true reasoning capabilities of LLMs and undermine the reliability of evaluations. To address these limitations, we propose a refined set of evaluation standards, including inspection of reasoning paths, diversified assessment metrics, and comparative analyses with human performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models</title>
<link>https://arxiv.org/abs/2505.23848</link>
<guid>https://arxiv.org/abs/2505.23848</guid>
<content:encoded><![CDATA[
arXiv:2505.23848v1 Announce Type: cross 
Abstract: We introduce a method to reduce refusal rates of large language models (LLMs) on sensitive content without modifying model weights or prompts. Motivated by the observation that refusals in certain models were often preceded by the specific token sequence of a token marking the beginning of the chain-of-thought (CoT) block () followed by a double newline token (\n\n), we investigate the impact of two simple formatting adjustments during generation: suppressing \n\n after  and suppressing the end-of-sequence token after the end of the CoT block (). Our method requires no datasets, parameter changes, or training, relying solely on modifying token probabilities during generation. In our experiments with official DeepSeek-R1 distillations, these interventions increased the proportion of substantive answers to sensitive prompts without affecting performance on standard benchmarks. Our findings suggest that refusal behaviors can be circumvented by blocking refusal subspaces at specific points in the generation process.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2505.23849</link>
<guid>https://arxiv.org/abs/2505.23849</guid>
<content:encoded><![CDATA[
arXiv:2505.23849v1 Announce Type: cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves privacy and security of the client's data by not exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a novel framework that allows users to define custom data readiness (DR) standards, metrics, rules, and remedies tailored to specific FL tasks. Our framework generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are optimally prepared for FL while preserving privacy. We demonstrate the framework's practical application by integrating it into an existing PPFL framework. We conducted experiments across six diverse datasets, addressing seven different DR issues. The results illustrate the framework's versatility and effectiveness in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources by identifying and addressing data-related issues before the training phase.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Uncertainty Estimation and Calibration of Large Language Models</title>
<link>https://arxiv.org/abs/2505.23854</link>
<guid>https://arxiv.org/abs/2505.23854</guid>
<content:encoded><![CDATA[
arXiv:2505.23854v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
arXiv:2505.23856v1 Announce Type: cross 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE</title>
<link>https://arxiv.org/abs/2505.23858</link>
<guid>https://arxiv.org/abs/2505.23858</guid>
<content:encoded><![CDATA[
arXiv:2505.23858v1 Announce Type: cross 
Abstract: With the increasing brightness of Light sources, including the Diffraction-Limited brightness upgrade of APS and the high-repetition-rate upgrade of LCLS, the proposed experiments therein are becoming increasingly complex. For instance, experiments at LCLS-II-HE will require the X-ray beam to be within a fraction of a micron in diameter, with pointing stability of a few nanoradians, at the end of a kilometer-long electron accelerator, a hundred-meter-long undulator section, and tens of meters long X-ray optics. This enhancement of brightness will increase the data production rate to rival the largest data generators in the world. Without real-time active feedback control and an optimized pipeline to transform measurements to scientific information and insights, researchers will drown in a deluge of mostly useless data, and fail to extract the highly sophisticated insights that the recent brightness upgrades promise.
  In this article, we outline the strategy we are developing at SLAC to implement Machine Learning driven optimization, automation and real-time knowledge extraction from the electron-injector at the start of the electron accelerator, to the multidimensional X-ray optical systems, and till the experimental endstations and the high readout rate, multi-megapixel detectors at LCLS to deliver the design performance to the users. This is illustrated via examples from Accelerator, Optics and End User applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum computing and artificial intelligence: status and perspectives</title>
<link>https://arxiv.org/abs/2505.23860</link>
<guid>https://arxiv.org/abs/2505.23860</guid>
<content:encoded><![CDATA[
arXiv:2505.23860v1 Announce Type: cross 
Abstract: This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors</title>
<link>https://arxiv.org/abs/2505.23862</link>
<guid>https://arxiv.org/abs/2505.23862</guid>
<content:encoded><![CDATA[
arXiv:2505.23862v1 Announce Type: cross 
Abstract: The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at https://github.com/HudenJear/RPLoss.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gibbs randomness-compression proposition: An efficient deep learning</title>
<link>https://arxiv.org/abs/2505.23869</link>
<guid>https://arxiv.org/abs/2505.23869</guid>
<content:encoded><![CDATA[
arXiv:2505.23869v1 Announce Type: cross 
Abstract: A proposition that connects randomness and compression put forward via Gibbs entropy over set of measurement vectors associated with a compression process. The proposition states that a lossy compression process is equivalent to {\it directed randomness} that preserves information content. The proposition originated from the observed behaviour in newly proposed {\it Dual Tomographic Compression} (DTC) compress-train framework. This is akin to tomographic reconstruction of layer weight matrices via building compressed sensed projections, so called {\it weight rays}. This tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning. This novel model compress-train scheme appear in iterative fashion and act as smart neural architecture search, Experiments demonstrated utility of this dual-tomography producing state-of-the-art performance with efficient compression during training, accelerating and supporting lottery ticket hypothesis. However, random compress-train iterations having similar performance demonstrated the connection between randomness and compression from statistical physics perspective, we formulated so called {\it Gibbs randomness-compression proposition}, signifying randomness-compression relationship via Gibbs entropy. Practically, DTC framework provides a promising approach for massively energy and resource efficient deep learning training approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.23883</link>
<guid>https://arxiv.org/abs/2505.23883</guid>
<content:encoded><![CDATA[
arXiv:2505.23883v1 Announce Type: cross 
Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representational Difference Explanations</title>
<link>https://arxiv.org/abs/2505.23917</link>
<guid>https://arxiv.org/abs/2505.23917</guid>
<content:encoded><![CDATA[
arXiv:2505.23917v1 Announce Type: cross 
Abstract: We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve</title>
<link>https://arxiv.org/abs/2505.23946</link>
<guid>https://arxiv.org/abs/2505.23946</guid>
<content:encoded><![CDATA[
arXiv:2505.23946v1 Announce Type: cross 
Abstract: Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Emotion Fool Anti-spoofing?</title>
<link>https://arxiv.org/abs/2505.23962</link>
<guid>https://arxiv.org/abs/2505.23962</guid>
<content:encoded><![CDATA[
arXiv:2505.23962v1 Announce Type: cross 
Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset: https://emospoof-tts.github.io/Dataset/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic Classification of Maritime Vessels using Learnable Filterbanks</title>
<link>https://arxiv.org/abs/2505.23964</link>
<guid>https://arxiv.org/abs/2505.23964</guid>
<content:encoded><![CDATA[
arXiv:2505.23964v1 Announce Type: cross 
Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic signatures is complicated by the variability of different recording scenarios. A robust classification framework must be able to generalize across diverse acoustic environments and variable source-sensor distances. To this end, we present a deep learning model with robust performance across different recording scenarios. Using a trainable spectral front-end and temporal feature encoder to learn a Gabor filterbank, the model can dynamically emphasize different frequency components. Trained on the VTUAD hydrophone recordings from the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 % percent test accuracy across varying source-sensor distances, surpassing the previous benchmark by over 12 percentage points. We present the model, justify our architectural choices, analyze the learned Gabor filters, and perform ablation studies on sensor data fusion and attention-based pooling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention</title>
<link>https://arxiv.org/abs/2505.23968</link>
<guid>https://arxiv.org/abs/2505.23968</guid>
<content:encoded><![CDATA[
arXiv:2505.23968v1 Announce Type: cross 
Abstract: Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL</title>
<link>https://arxiv.org/abs/2505.23977</link>
<guid>https://arxiv.org/abs/2505.23977</guid>
<content:encoded><![CDATA[
arXiv:2505.23977v1 Announce Type: cross 
Abstract: Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets</title>
<link>https://arxiv.org/abs/2505.23980</link>
<guid>https://arxiv.org/abs/2505.23980</guid>
<content:encoded><![CDATA[
arXiv:2505.23980v1 Announce Type: cross 
Abstract: Understanding Greenland's subglacial topography is critical for projecting the future mass loss of the ice sheet and its contribution to global sea-level rise. However, the complex and sparse nature of observational data, particularly information about the bed topography under the ice sheet, significantly increases the uncertainty in model projections. Bed topography is traditionally measured by airborne ice-penetrating radar that measures the ice thickness directly underneath the aircraft, leaving data gap of tens of kilometers in between flight lines. This study introduces a deep learning framework, which we call as DeepTopoNet, that integrates radar-derived ice thickness observations and BedMachine Greenland data through a novel dynamic loss-balancing mechanism. Among all efforts to reconstruct bed topography, BedMachine has emerged as one of the most widely used datasets, combining mass conservation principles and ice thickness measurements to generate high-resolution bed elevation estimates. The proposed loss function adaptively adjusts the weighting between radar and BedMachine data, ensuring robustness in areas with limited radar coverage while leveraging the high spatial resolution of BedMachine predictions i.e. bed estimates. Our approach incorporates gradient-based and trend surface features to enhance model performance and utilizes a CNN architecture designed for subgrid-scale predictions. By systematically testing on the Upernavik Isstr{\o}m) region, the model achieves high accuracy, outperforming baseline methods in reconstructing subglacial terrain. This work demonstrates the potential of deep learning in bridging observational gaps, providing a scalable and efficient solution to inferring subglacial topography.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs</title>
<link>https://arxiv.org/abs/2505.23996</link>
<guid>https://arxiv.org/abs/2505.23996</guid>
<content:encoded><![CDATA[
arXiv:2505.23996v1 Announce Type: cross 
Abstract: The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study</title>
<link>https://arxiv.org/abs/2505.24006</link>
<guid>https://arxiv.org/abs/2505.24006</guid>
<content:encoded><![CDATA[
arXiv:2505.24006v1 Announce Type: cross 
Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network (A2-SBNN), a predictive spatial model designed to map coordinates to continuous fields while capturing both typical spatial patterns and extreme dependencies. By embedding the dual-tail novel Archimedean copula viz. A2 directly into the network's weight initialization, A2-SBNN naturally models complex spatial relationships, including rare co-movements in the data. The model is trained through a calibration-driven process combining Wasserstein loss, moment matching, and correlation penalties to refine predictions and manage uncertainty. Simulation results show that A2-SBNN consistently delivers high accuracy across a wide range of dependency strengths, offering a new, effective solution for spatial data modeling beyond traditional Gaussian-based approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</title>
<link>https://arxiv.org/abs/2505.24009</link>
<guid>https://arxiv.org/abs/2505.24009</guid>
<content:encoded><![CDATA[
arXiv:2505.24009v1 Announce Type: cross 
Abstract: Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System</title>
<link>https://arxiv.org/abs/2505.24016</link>
<guid>https://arxiv.org/abs/2505.24016</guid>
<content:encoded><![CDATA[
arXiv:2505.24016v1 Announce Type: cross 
Abstract: This paper discusses the construction, fine-tuning, and deployment of BeaverTalk, a cascaded system for speech-to-text translation as part of the IWSLT 2025 simultaneous translation task. The system architecture employs a VAD segmenter for breaking a speech stream into segments, Whisper Large V2 for automatic speech recognition (ASR), and Gemma 3 12B for simultaneous translation. Regarding the simultaneous translation LLM, it is fine-tuned via low-rank adaptors (LoRAs) for a conversational prompting strategy that leverages a single prior-sentence memory bank from the source language as context. The cascaded system participated in the English$\rightarrow$German and English$\rightarrow$Chinese language directions for both the low and high latency regimes. In particular, on the English$\rightarrow$German task, the system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and 3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and 3521.35, respectively.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging machine learning features for linear optical interferometer control</title>
<link>https://arxiv.org/abs/2505.24032</link>
<guid>https://arxiv.org/abs/2505.24032</guid>
<content:encoded><![CDATA[
arXiv:2505.24032v1 Announce Type: cross 
Abstract: We have developed an algorithm that constructs a model of a reconfigurable optical interferometer, independent of specific architectural constraints. The programming of unitary transformations on the interferometer's optical modes relies on either an analytical method for deriving the unitary matrix from a set of phase shifts or an optimization routine when such decomposition is not available. Our algorithm employs a supervised learning approach, aligning the interferometer model with a training set derived from the device being studied. A straightforward optimization procedure leverages this trained model to determine the phase shifts of the interferometer with a specific architecture, obtaining the required unitary transformation. This approach enables the effective tuning of interferometers without requiring a precise analytical solution, paving the way for the exploration of new interferometric circuit architectures.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Object Detection by Sequential Risk Control</title>
<link>https://arxiv.org/abs/2505.24038</link>
<guid>https://arxiv.org/abs/2505.24038</guid>
<content:encoded><![CDATA[
arXiv:2505.24038v1 Announce Type: cross 
Abstract: Recent advances in object detectors have led to their adoption for industrial uses. However, their deployment in critical applications is hindered by the inherent lack of reliability of neural networks and the complex structure of object detection models. To address these challenges, we turn to Conformal Prediction, a post-hoc procedure which offers statistical guarantees that are valid for any dataset size, without requiring prior knowledge on the model or data distribution. Our contribution is manifold: first, we formally define the problem of Conformal Object Detection (COD) and introduce a novel method, Sequential Conformal Risk Control (SeqCRC), that extends the statistical guarantees of Conformal Risk Control (CRC) to two sequential tasks with two parameters, as required in the COD setting. Then, we propose loss functions and prediction sets suited to applying CRC to different applications and certification requirements. Finally, we present a conformal toolkit, enabling replication and further exploration of our methods. Using this toolkit, we perform extensive experiments, yielding a benchmark that validates the investigated methods and emphasizes trade-offs and other practical consequences.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM</title>
<link>https://arxiv.org/abs/2505.24062</link>
<guid>https://arxiv.org/abs/2505.24062</guid>
<content:encoded><![CDATA[
arXiv:2505.24062v1 Announce Type: cross 
Abstract: Domain-wall dynamics in ferroelectric materials are strongly position-dependent since each polar interface is locked into a unique local microstructure. This necessitates spatially resolved studies of the wall-pinning using scanning-probe microscopy techniques. The pinning centers and preexisting domain walls are usually sparse within image plane, precluding the use of dense hyperspectral imaging modes and requiring time-consuming human experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$ were investigated to quantify the electric field driven dynamics of the polar-strain domain structures using ML-controlled automated Piezoresponse Force Microscopy. Analysis of 1500 switching events reveals that domain wall displacement depends not only on field parameters but also on the local ferroelectric-ferroelastic configuration. For example, twin boundaries in polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up to a certain level of bias magnitude and change only marginally as the bias increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics on the possible ferroelectric and ferroelastic wall orientations, together with the automated, high-throughput AFM workflow, can be distilled into a predictive map that links domain configurations to pulse parameters. This microstructure-specific rule set forms the foundation for designing ferroelectric memories.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models</title>
<link>https://arxiv.org/abs/2505.24065</link>
<guid>https://arxiv.org/abs/2505.24065</guid>
<content:encoded><![CDATA[
arXiv:2505.24065v1 Announce Type: cross 
Abstract: The growth and characterization of materials using empirical optimization typically requires a significant amount of expert time, experience, and resources. Several complementary characterization methods are routinely performed to determine the quality and properties of a grown sample. Machine learning (ML) can support the conventional approaches by using historical data to guide and provide speed and efficiency to the growth and characterization of materials. Specifically, ML can provide quantitative information from characterization data that is typically obtained from a different modality. In this study, we have investigated the feasibility of projecting the quantitative metric from microscopy measurements, such as atomic force microscopy (AFM), using data obtained from spectroscopy measurements, like Raman spectroscopy. Generative models were also trained to generate the full and specific features of the Raman and photoluminescence spectra from each other and the AFM images of the thin film MoS$_2$. The results are promising and have provided a foundational guide for the use of ML for the cross-modal characterization of materials for their accelerated, efficient, and cost-effective discovery.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity</title>
<link>https://arxiv.org/abs/2505.24097</link>
<guid>https://arxiv.org/abs/2505.24097</guid>
<content:encoded><![CDATA[
arXiv:2505.24097v1 Announce Type: cross 
Abstract: Calibrating blackbox machine learning models to achieve risk control is crucial to ensure reliable decision-making. A rich line of literature has been studying how to calibrate a model so that its predictions satisfy explicit finite-sample statistical guarantees under a fixed, static, and unknown data-generating distribution. However, prediction-supported decisions may influence the outcome they aim to predict, a phenomenon named performativity of predictions, which is commonly seen in social science and economics. In this paper, we introduce Performative Risk Control, a framework to calibrate models to achieve risk control under performativity with provable theoretical guarantees. Specifically, we provide an iteratively refined calibration process, where we ensure the predictions are improved and risk-controlled throughout the process. We also study different types of risk measures and choices of tail bounds. Lastly, we demonstrate the effectiveness of our framework by numerical experiments on the task of predicting credit default risk. To the best of our knowledge, this work is the first one to study statistically rigorous risk control under performativity, which will serve as an important safeguard against a wide range of strategic manipulation in decision-making processes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v1 Announce Type: cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel methodology that integrates ESNs with transfer learning, aiming to enhance predictive performance across various parameter regimes of the gKS model. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Foundation Model for GI Endoscopy Images</title>
<link>https://arxiv.org/abs/2505.24108</link>
<guid>https://arxiv.org/abs/2505.24108</guid>
<content:encoded><![CDATA[
arXiv:2505.24108v1 Announce Type: cross 
Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract abnormalities in order to detect diseases in their early stages and improve patient outcomes. Although deep learning has shown success in supporting GI diagnostics and decision-making, these models require curated datasets with labels that are expensive to acquire. Foundation models offer a promising solution by learning general-purpose representations, which can be finetuned for specific tasks, overcoming data scarcity. Developing foundation models for medical imaging holds significant potential, but the sensitive and protected nature of medical data presents unique challenges. Foundation model training typically requires extensive datasets, and while hospitals generate large volumes of data, privacy restrictions prevent direct data sharing, making foundation model training infeasible in most scenarios. In this work, we propose a FL framework for training foundation models for gastroendoscopy imaging, enabling data to remain within local hospital environments while contributing to a shared model. We explore several established FL algorithms, assessing their suitability for training foundation models without relying on task-specific labels, conducting experiments in both homogeneous and heterogeneous settings. We evaluate the trained foundation model on three critical downstream tasks--classification, detection, and segmentation--and demonstrate that it achieves improved performance across all tasks, highlighting the effectiveness of our approach in a federated, privacy-preserving setting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures</title>
<link>https://arxiv.org/abs/2505.24117</link>
<guid>https://arxiv.org/abs/2505.24117</guid>
<content:encoded><![CDATA[
arXiv:2505.24117v1 Announce Type: cross 
Abstract: Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov chain in that order (i.e., $Y \to X \to Z$), we derive upper bounds on the excess minimum risk using generalized information divergence measures. Here, $Y$ is a target vector to be estimated from an observed feature vector $X$ or its stochastically degraded version $Z$. The excess minimum risk is defined as the difference between the minimum expected loss in estimating $Y$ from $X$ and from $Z$. We present a family of bounds that generalize the mutual information based bound of Gy\"orfi et al. (2023), using the R\'enyi and $\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information. Our bounds are similar to those developed by Modak et al. (2021) and Aminian et al. (2024) for the generalization error of learning algorithms. However, unlike these works, our bounds do not require the sub-Gaussian parameter to be constant and therefore apply to a broader class of joint distributions over $Y$, $X$, and $Z$. We also provide numerical examples under both constant and non-constant sub-Gaussianity assumptions, illustrating that our generalized divergence based bounds can be tighter than the one based on mutual information for certain regimes of the parameter $\alpha$.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes</title>
<link>https://arxiv.org/abs/2505.24132</link>
<guid>https://arxiv.org/abs/2505.24132</guid>
<content:encoded><![CDATA[
arXiv:2505.24132v1 Announce Type: cross 
Abstract: We perform an information-theoretic mode decomposition for separated wakes around a wing. The current data-driven approach based on a neural network referred to as deep sigmoidal flow enables the extraction of an informative component from a given flow field snapshot with respect to a target variable at a future time stamp, thereby capturing the causality as a time-varying modal structure. We consider three examples of separated flows around a NACA0012 airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2. strong vortex-airfoil interactions, and 3. a turbulent wake in a spanwise-periodic domain. The present approach reveals informative vortical structures associated with a time-varying lift response. For the periodic shedding cases, the informative structures vary in time corresponding to the fluctuation level from their mean values. With the second example of vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in the lift response over time is identified in an interpretable manner. Furthermore, for the case of turbulent wake, the present model highlights structures near the wing and vortex cores as informative components based solely on the information metric without any prior knowledge of aerodynamics and length scales. This study provides causality-based insights into a range of unsteady aerodynamic problems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mathematical Perspective On Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.24134</link>
<guid>https://arxiv.org/abs/2505.24134</guid>
<content:encoded><![CDATA[
arXiv:2505.24134v1 Announce Type: cross 
Abstract: Multimodal contrastive learning is a methodology for linking different data modalities; the canonical example is linking image and text data. The methodology is typically framed as the identification of a set of encoders, one for each modality, that align representations within a common latent space. In this work, we focus on the bimodal setting and interpret contrastive learning as the optimization of (parameterized) encoders that define conditional probability distributions, for each modality conditioned on the other, consistent with the available data. This provides a framework for multimodal algorithms such as crossmodal retrieval, which identifies the mode of one of these conditional distributions, and crossmodal classification, which is similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This probabilistic perspective suggests two natural generalizations of contrastive learning: the introduction of novel probabilistic loss functions, and the use of alternative metrics for measuring alignment in the common latent space. We study these generalizations of the classical approach in the multivariate Gaussian setting. In this context we view the latent space identification as a low-rank matrix approximation problem. This allows us to characterize the capabilities of loss functions and alignment metrics to approximate natural statistics, such as conditional means and covariances; doing so yields novel variants on contrastive learning algorithms for specific mode-seeking and for generative tasks. The framework we introduce is also studied through numerical experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data assimilation application arising in oceanography.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction</title>
<link>https://arxiv.org/abs/2505.24136</link>
<guid>https://arxiv.org/abs/2505.24136</guid>
<content:encoded><![CDATA[
arXiv:2505.24136v1 Announce Type: cross 
Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the model's ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control</title>
<link>https://arxiv.org/abs/2505.24161</link>
<guid>https://arxiv.org/abs/2505.24161</guid>
<content:encoded><![CDATA[
arXiv:2505.24161v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making through neuromorphic hardware, making them compelling for Reinforcement Learning (RL) in resource-constrained edge devices. Recent studies in this field directly replace Artificial Neural Networks (ANNs) by SNNs in existing RL frameworks, overlooking whether the RL algorithm is suitable for SNNs. However, most RL algorithms in continuous control are designed tailored to ANNs, including the target network soft updates mechanism, which conflict with the discrete, non-differentiable dynamics of SNN spikes. We identify that this mismatch destabilizes SNN training in continuous control tasks. To bridge this gap between discrete SNN and continuous control, we propose a novel proxy target framework. The continuous and differentiable dynamics of the proxy target enable smooth updates, bypassing the incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy network operates only during training, the SNN retains its energy efficiency during deployment without inference overhead. Extensive experiments on continuous control benchmarks demonstrate that compared to vanilla SNNs, the proxy target framework enables SNNs to achieve up to 32% higher performance across different spiking neurons. Notably, we are the first to surpass ANN performance in continuous control with simple Leaky-Integrate-and-Fire (LIF) neurons. This work motivates a new class of SNN-friendly RL algorithms tailored to SNN's characteristics, paving the way for neuromorphic agents that combine high performance with low power consumption.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation</title>
<link>https://arxiv.org/abs/2505.24174</link>
<guid>https://arxiv.org/abs/2505.24174</guid>
<content:encoded><![CDATA[
arXiv:2505.24174v1 Announce Type: cross 
Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Protein Conformation Ensemble Generation with Physical Feedback</title>
<link>https://arxiv.org/abs/2505.24203</link>
<guid>https://arxiv.org/abs/2505.24203</guid>
<content:encoded><![CDATA[
arXiv:2505.24203v1 Announce Type: cross 
Abstract: Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin</title>
<link>https://arxiv.org/abs/2505.24222</link>
<guid>https://arxiv.org/abs/2505.24222</guid>
<content:encoded><![CDATA[
arXiv:2505.24222v1 Announce Type: cross 
Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of generating images via learning the noised score function of data distribution. Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level, with efforts concentrated on refining inter-level denoising strategies. While leveraging additional second-order Hessian geometry to enhance the sampling quality of Langevin is a common practice in Markov chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in high-dimensional DMs lead to quadratic-complexity computational costs, rendering them non-scalable. In this work, we introduce a novel Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion Hessian geometry in a training-free manner, drawing inspiration from the celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces two key innovations: (1) A low-rank approximation of the diffusion Hessian, leveraging the DMs' inherent structure and circumventing explicit quadratic-complexity computations; (2) A damping mechanism to stabilize the approximated Hessian. This LML approximated Hessian geometry enables the diffusion sampling to execute more accurate steps and improve the image generation quality. We further conduct a theoretical analysis to substantiate the approximation error bound of low-rank approximation and the convergence property of the damping mechanism. Extensive experiments across multiple pretrained DMs validate that the LML method significantly improves image generation quality, with negligible computational overhead.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM</title>
<link>https://arxiv.org/abs/2505.24238</link>
<guid>https://arxiv.org/abs/2505.24238</guid>
<content:encoded><![CDATA[
arXiv:2505.24238v1 Announce Type: cross 
Abstract: Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the {\dataset} benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. {\dataset} introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals that (1) the model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations; (2) current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities; and (3) question types correlate with distinct hallucination patterns, highlighting targeted challenges and potential mitigation strategies. To address these challenges, we propose {\method}, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. {\method} establishes a baseline on {\dataset}, and reduces the logical hallucinations in original base models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring</title>
<link>https://arxiv.org/abs/2505.24239</link>
<guid>https://arxiv.org/abs/2505.24239</guid>
<content:encoded><![CDATA[
arXiv:2505.24239v1 Announce Type: cross 
Abstract: While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Knockout for Unraveling Factual Information Flow</title>
<link>https://arxiv.org/abs/2505.24244</link>
<guid>https://arxiv.org/abs/2505.24244</guid>
<content:encoded><![CDATA[
arXiv:2505.24244v1 Announce Type: cross 
Abstract: This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct "features" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings</title>
<link>https://arxiv.org/abs/2505.24281</link>
<guid>https://arxiv.org/abs/2505.24281</guid>
<content:encoded><![CDATA[
arXiv:2505.24281v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) has become an essential machine learning tool for addressing multiple learning tasks simultaneously and has been effectively applied across fields such as healthcare, marketing, and biomedical research. However, to enable efficient information sharing across tasks, it is crucial to leverage both shared and heterogeneous information. Despite extensive research on MTL, various forms of heterogeneity, including distribution and posterior heterogeneity, present significant challenges. Existing methods often fail to address these forms of heterogeneity within a unified framework. In this paper, we propose a dual-encoder framework to construct a heterogeneous latent factor space for each task, incorporating a task-shared encoder to capture common information across tasks and a task-specific encoder to preserve unique task characteristics. Additionally, we explore the intrinsic similarity structure of the coefficients corresponding to learned latent factors, allowing for adaptive integration across tasks to manage posterior heterogeneity. We introduce a unified algorithm that alternately learns the task-specific and task-shared encoders and coefficients. In theory, we investigate the excess risk bound for the proposed MTL method using local Rademacher complexity and apply it to a new but related task. Through simulation studies, we demonstrate that the proposed method outperforms existing data integration methods across various settings. Furthermore, the proposed method achieves superior predictive performance for time to tumor doubling across five distinct cancer types in PDX data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Fusion for Partial Identification of Causal Effects</title>
<link>https://arxiv.org/abs/2505.24296</link>
<guid>https://arxiv.org/abs/2505.24296</guid>
<content:encoded><![CDATA[
arXiv:2505.24296v1 Announce Type: cross 
Abstract: Data fusion techniques integrate information from heterogeneous data sources to improve learning, generalization, and decision making across data sciences. In causal inference, these methods leverage rich observational data to improve causal effect estimation, while maintaining the trustworthiness of randomized controlled trials. Existing approaches often relax the strong no unobserved confounding assumption by instead assuming exchangeability of counterfactual outcomes across data sources. However, when both assumptions simultaneously fail - a common scenario in practice - current methods cannot identify or estimate causal effects. We address this limitation by proposing a novel partial identification framework that enables researchers to answer key questions such as: Is the causal effect positive or negative? and How severe must assumption violations be to overturn this conclusion? Our approach introduces interpretable sensitivity parameters that quantify assumption violations and derives corresponding causal effect bounds. We develop doubly robust estimators for these bounds and operationalize breakdown frontier analysis to understand how causal conclusions change as assumption violations increase. We apply our framework to the Project STAR study, which investigates the effect of classroom size on students' third-grade standardized test performance. Our analysis reveals that the Project STAR results are robust to simultaneous violations of key assumptions, both on average and across various subgroups of interest. This strengthens confidence in the study's conclusions despite potential unmeasured biases in the data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels</title>
<link>https://arxiv.org/abs/2505.24311</link>
<guid>https://arxiv.org/abs/2505.24311</guid>
<content:encoded><![CDATA[
arXiv:2505.24311v1 Announce Type: cross 
Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm for visualizing high-dimensional data by finding low-dimensional representations. In this paper, we study the convergence of t-SNE with generalized kernels and extend the results of Auffinger and Fletcher in 2023. Our work starts by giving a concrete formulation of generalized input and output kernels. Then we prove that under certain conditions, the t-SNE algorithm converges to an equilibrium distribution for a wide range of input and output kernels as the number of data points diverges.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising</title>
<link>https://arxiv.org/abs/2505.24327</link>
<guid>https://arxiv.org/abs/2505.24327</guid>
<content:encoded><![CDATA[
arXiv:2505.24327v1 Announce Type: cross 
Abstract: Remote sensing image (RSI) denoising is an important topic in the field of remote sensing. Despite the impressive denoising performance of RSI denoising methods, most current deep learning-based approaches function as black boxes and lack integration with physical information models, leading to limited interpretability. Additionally, many methods may struggle with insufficient attention to non-local self-similarity in RSI and require tedious tuning of regularization parameters to achieve optimal performance, particularly in conventional iterative optimization approaches. In this paper, we first propose a novel RSI denoising method named sparse tensor-aided representation network (STAR-Net), which leverages a low-rank prior to effectively capture the non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a sparse variant called STAR-Net-S to deal with the interference caused by non-Gaussian noise in original RSI for the purpose of improving robustness. Different from conventional iterative optimization, we develop an alternating direction method of multipliers (ADMM)-guided deep unrolling network, in which all regularization parameters can be automatically learned, thus inheriting the advantages of both model-based and deep learning-based approaches and successfully addressing the above-mentioned shortcomings. Comprehensive experiments on synthetic and real-world datasets demonstrate that STAR-Net and STAR-Net-S outperform state-of-the-art RSI denoising methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation</title>
<link>https://arxiv.org/abs/2505.24333</link>
<guid>https://arxiv.org/abs/2505.24333</guid>
<content:encoded><![CDATA[
arXiv:2505.24333v1 Announce Type: cross 
Abstract: Finding the right initialisation for neural networks is crucial to ensure smooth training and good performance. In transformers, the wrong initialisation can lead to one of two failure modes of self-attention layers: rank collapse, where all tokens collapse into similar representations, and entropy collapse, where highly concentrated attention scores lead to training instability. While the right initialisation has been extensively studied in feed-forward networks, an exact description of signal propagation through a full transformer block has so far been lacking. Here, we provide an analytical theory of signal propagation through vanilla transformer blocks with self-attention layers, layer normalisation, skip connections and ReLU MLP. To treat the self-attention layer, we draw on a formal parallel with the Random Energy Model from statistical physics. We identify and characterise two regimes governed by the variance of the query and key initialisations: a low-variance regime, where we recover the known rank collapse behaviour; and a previously unexplored high-variance regime, where signal is preserved but \textit{entropy collapse} occurs. In the low-variance regime, we calculate the critical strength for the residual connection to ensure signal propagation. Our theory yields trainability diagrams that identify the correct choice of initialisation hyper-parameters for a given architecture. Experiments with BERT-style models trained on TinyStories validate our predictions. Our theoretical framework gives a unified perspective on the two failure modes of self-attention and gives quantitative predictions on the scale of both weights and residual connections that guarantees smooth training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds</title>
<link>https://arxiv.org/abs/2505.24336</link>
<guid>https://arxiv.org/abs/2505.24336</guid>
<content:encoded><![CDATA[
arXiv:2505.24336v1 Announce Type: cross 
Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres. Demo samples are available at https://nc-ai.github.io/speech/publications/nonhuman-vc/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models</title>
<link>https://arxiv.org/abs/2505.24340</link>
<guid>https://arxiv.org/abs/2505.24340</guid>
<content:encoded><![CDATA[
arXiv:2505.24340v1 Announce Type: cross 
Abstract: Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Unlearning via Sparse Autoencoder Subspace Guided Projections</title>
<link>https://arxiv.org/abs/2505.24428</link>
<guid>https://arxiv.org/abs/2505.24428</guid>
<content:encoded><![CDATA[
arXiv:2505.24428v1 Announce Type: cross 
Abstract: Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an "irrelevant" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System</title>
<link>https://arxiv.org/abs/2505.24429</link>
<guid>https://arxiv.org/abs/2505.24429</guid>
<content:encoded><![CDATA[
arXiv:2505.24429v1 Announce Type: cross 
Abstract: Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers</title>
<link>https://arxiv.org/abs/2505.24443</link>
<guid>https://arxiv.org/abs/2505.24443</guid>
<content:encoded><![CDATA[
arXiv:2505.24443v1 Announce Type: cross 
Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and unlabeled data share an identical class distribution, however in practice, this assumption is easily violated, as unlabeled data often includes unknown class data, i.e., outliers. The outliers are treated as noise, considerably degrading the performance of SSL models. To address this drawback, we propose a novel framework, Diversify and Conquer (DAC), to enhance SSL robustness in the context of open-set semi-supervised learning. In particular, we note that existing open-set SSL methods rely on prediction discrepancies between inliers and outliers from a single model trained on labeled data. This approach can be easily failed when the labeled data is insufficient, leading to performance degradation that is worse than naive SSL that do not account for outliers. In contrast, our approach exploits prediction disagreements among multiple models that are differently biased towards the unlabeled distribution. By leveraging the discrepancies arising from training on unlabeled data, our method enables robust outlier detection even when the labeled data is underspecified. Our key contribution is constructing a collection of differently biased models through a single training process. By encouraging divergent heads to be differently biased towards outliers while making consistent predictions for inliers, we exploit the disagreement among these heads as a measure to identify unknown concepts. Our code is available at https://github.com/heejokong/DivCon.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed gradient methods under heavy-tailed communication noise</title>
<link>https://arxiv.org/abs/2505.24464</link>
<guid>https://arxiv.org/abs/2505.24464</guid>
<content:encoded><![CDATA[
arXiv:2505.24464v1 Announce Type: cross 
Abstract: We consider a standard distributed optimization problem in which networked nodes collaboratively minimize the sum of their locally known convex costs. For this setting, we address for the first time the fundamental problem of design and analysis of distributed methods to solve the above problem when inter-node communication is subject to \emph{heavy-tailed} noise. Heavy-tailed noise is highly relevant and frequently arises in densely deployed wireless sensor and Internet of Things (IoT) networks. Specifically, we design a distributed gradient-type method that features a carefully balanced mixed time-scale time-varying consensus and gradient contribution step sizes and a bounded nonlinear operator on the consensus update to limit the effect of heavy-tailed noise. Assuming heterogeneous strongly convex local costs with mutually different minimizers that are arbitrarily far apart, we show that the proposed method converges to a neighborhood of the network-wide problem solution in the mean squared error (MSE) sense, and we also characterize the corresponding convergence rate. We further show that the asymptotic MSE can be made arbitrarily small through consensus step-size tuning, possibly at the cost of slowing down the transient error decay. Numerical experiments corroborate our findings and demonstrate the resilience of the proposed method to heavy-tailed (and infinite variance) communication noise. They also show that existing distributed methods, designed for finite-communication-noise-variance settings, fail in the presence of infinite variance noise.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation</title>
<link>https://arxiv.org/abs/2505.24472</link>
<guid>https://arxiv.org/abs/2505.24472</guid>
<content:encoded><![CDATA[
arXiv:2505.24472v1 Announce Type: cross 
Abstract: Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Gemini in an arena for learning</title>
<link>https://arxiv.org/abs/2505.24477</link>
<guid>https://arxiv.org/abs/2505.24477</guid>
<content:encoded><![CDATA[
arXiv:2505.24477v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an "arena for learning" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Code-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2505.24480</link>
<guid>https://arxiv.org/abs/2505.24480</guid>
<content:encoded><![CDATA[
arXiv:2505.24480v1 Announce Type: cross 
Abstract: In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.24486</link>
<guid>https://arxiv.org/abs/2505.24486</guid>
<content:encoded><![CDATA[
arXiv:2505.24486v1 Announce Type: cross 
Abstract: The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don't effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953 % across five experiences. The code is available at: https://github.com/falihgoz/RAIS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Fall Prevention system for the Next-generation of Workers</title>
<link>https://arxiv.org/abs/2505.24487</link>
<guid>https://arxiv.org/abs/2505.24487</guid>
<content:encoded><![CDATA[
arXiv:2505.24487v1 Announce Type: cross 
Abstract: Developing a general-purpose wearable real-time fall-detection system is still a challenging task, especially for healthy and strong subjects, such as industrial workers that work in harsh environments. In this work, we present a hybrid approach for fall detection and prevention, which uses the dynamic model of an inverted pendulum to generate simulations of falling that are then fed to a deep learning framework. The output is a signal to activate a fall mitigation mechanism when the subject is at risk of harm. The advantage of this approach is that abstracted models can be used to efficiently generate training data for thousands of different subjects with different falling initial conditions, something that is practically impossible with real experiments. This approach is suitable for a specific type of fall, where the subjects fall without changing their initial configuration significantly, and it is the first step toward a general-purpose wearable device, with the aim of reducing fall-associated injuries in industrial environments, which can improve the safety of workers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Density Functions for Weighted Convolution in Learning Models</title>
<link>https://arxiv.org/abs/2505.24527</link>
<guid>https://arxiv.org/abs/2505.24527</guid>
<content:encoded><![CDATA[
arXiv:2505.24527v1 Announce Type: cross 
Abstract: The paper introduces the weighted convolution, a novel approach to the convolution for signals defined on regular grids (e.g., 2D images) through the application of an optimal density function to scale the contribution of neighbouring pixels based on their distance from the central pixel. This choice differs from the traditional uniform convolution, which treats all neighbouring pixels equally. Our weighted convolution can be applied to convolutional neural network problems to improve the approximation accuracy. Given a convolutional network, we define a framework to compute the optimal density function through a minimisation model. The framework separates the optimisation of the convolutional kernel weights (using stochastic gradient descent) from the optimisation of the density function (using DIRECT-L). Experimental results on a learning model for an image-to-image task (e.g., image denoising) show that the weighted convolution significantly reduces the loss (up to 53% improvement) and increases the test accuracy compared to standard convolution. While this method increases execution time by 11%, it is robust across several hyperparameters of the learning model. Future work will apply the weighted convolution to real-case 2D and 3D image convolutional learning problems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial Foundation Models to Enable Progress on Sustainable Development Goals</title>
<link>https://arxiv.org/abs/2505.24528</link>
<guid>https://arxiv.org/abs/2505.24528</guid>
<content:encoded><![CDATA[
arXiv:2505.24528v1 Announce Type: cross 
Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data</title>
<link>https://arxiv.org/abs/2505.24556</link>
<guid>https://arxiv.org/abs/2505.24556</guid>
<content:encoded><![CDATA[
arXiv:2505.24556v1 Announce Type: cross 
Abstract: Bayesian models based on Gaussian processes (GPs) offer a flexible framework to predict spatially distributed variables with uncertainty. But the use of nonstationary priors, often necessary for capturing complex spatial patterns, makes sampling from the predictive posterior distribution (PPD) computationally intractable. In this paper, we propose a two-step approach based on diffusion generative models (DGMs) to mimic PPDs associated with non-stationary GP priors: we replace the GP prior by a DGM surrogate, and leverage recent advances on training-free guidance algorithms for DGMs to sample from the desired posterior distribution. We apply our approach to a rich non-stationary GP prior from which exact posterior sampling is untractable and validate that the issuing distributions are close to their GP counterpart using several statistical metrics. We also demonstrate how one can fine-tune the trained DGMs to target specific parts of the GP prior. Finally we apply the proposed approach to solve inverse problems arising in environmental sciences, thus yielding state-of-the-art predictions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</title>
<link>https://arxiv.org/abs/2505.24593</link>
<guid>https://arxiv.org/abs/2505.24593</guid>
<content:encoded><![CDATA[
arXiv:2505.24593v1 Announce Type: cross 
Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable phenotyping of Heart Failure patients with Dutch discharge letters</title>
<link>https://arxiv.org/abs/2505.24619</link>
<guid>https://arxiv.org/abs/2505.24619</guid>
<content:encoded><![CDATA[
arXiv:2505.24619v1 Announce Type: cross 
Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes affecting treatment and prognosis. This study evaluates models for phenotyping HF patients based on left ventricular ejection fraction (LVEF) classes, using structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105 hospitalizations, 16,334 patients). Data from AMC were used for model training, and from VUmc for external validation. The dataset was unlabelled and included tabular clinical measurements and discharge letters. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for 300 patients for testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. To evaluate interpretability, two clinicians annotated 20 discharge letters by highlighting information they considered relevant for LVEF classification. These were compared to SHAP and LIME explanations from black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone, achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation), outperforming baselines. Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for phenotyping HF patients. Aug-Linear models matched black-box performance while providing clinician-aligned interpretability, supporting their use in transparent clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success</title>
<link>https://arxiv.org/abs/2505.24622</link>
<guid>https://arxiv.org/abs/2505.24622</guid>
<content:encoded><![CDATA[
arXiv:2505.24622v1 Announce Type: cross 
Abstract: Predicting startup success requires models that are both accurate and interpretable. We present a lightweight ensemble framework that combines YES/NO questions generated by large language models (LLMs), forming a transparent decision-making system. Each question acts as a weak heuristic, and by filtering, ranking, and aggregating them through a threshold-based voting mechanism, we construct a strong ensemble predictor. On a test set where 10% of startups are classified as successful, our approach achieves a precision rate of 50%, representing a 5x improvement over random selection, while remaining fully transparent. When we incorporate expert-guided heuristics into the generation process, performance improves further to 54% precision. These results highlight the value of combining LLM reasoning with human insight and demonstrate that simple, interpretable ensembles can support high-stakes decisions in domains such as venture capital (VC).
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
arXiv:2505.24650v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models</title>
<link>https://arxiv.org/abs/2505.24655</link>
<guid>https://arxiv.org/abs/2505.24655</guid>
<content:encoded><![CDATA[
arXiv:2505.24655v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) risk prediction models are essential for identifying high-risk individuals and guiding preventive actions. However, existing models struggle with the challenges of real-world clinical practice as they oversimplify patient profiles, rely on rigid input schemas, and are sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk prediction framework built on large language models extensively fine-tuned on over half a million participants from the UK Biobank. In benchmark comparisons, AdaCVD surpasses established risk scores and standard machine learning approaches, achieving state-of-the-art performance. Crucially, for the first time, it addresses key clinical challenges across three dimensions: it flexibly incorporates comprehensive yet variable patient information; it seamlessly integrates both structured data and unstructured text; and it rapidly adapts to new patient populations using minimal additional data. In stratified analyses, it demonstrates robust performance across demographic, socioeconomic, and clinical subgroups, including underrepresented cohorts. AdaCVD offers a promising path toward more flexible, AI-driven clinical decision support tools suited to the realities of heterogeneous and dynamic healthcare environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders</title>
<link>https://arxiv.org/abs/2505.24668</link>
<guid>https://arxiv.org/abs/2505.24668</guid>
<content:encoded><![CDATA[
arXiv:2505.24668v1 Announce Type: cross 
Abstract: Modern deep neural networks exhibit strong generalization even in highly overparameterized regimes. Significant progress has been made to understand this phenomenon in the context of supervised learning, but for unsupervised tasks such as denoising, several open questions remain. While some recent works have successfully characterized the test error of the linear denoising problem, they are limited to linear models (one-layer network). In this work, we focus on two-layer linear denoising autoencoders trained under gradient flow, incorporating two key ingredients of modern deep learning architectures: A low-dimensional bottleneck layer that effectively enforces a rank constraint on the learned solution, as well as the possibility of a skip connection that bypasses the bottleneck. We derive closed-form expressions for all critical points of this model under product regularization, and in particular describe its global minimizer under the minimum-norm principle. From there, we derive the test risk formula in the overparameterized regime, both for models with and without skip connections. Our analysis reveals two interesting phenomena: Firstly, the bottleneck layer introduces an additional complexity measure akin to the classical bias-variance trade-off -- increasing the bottleneck width reduces bias but introduces variance, and vice versa. Secondly, skip connection can mitigate the variance in denoising autoencoders -- especially when the model is mildly overparameterized. We further analyze the impact of skip connections in denoising autoencoder using random matrix theory and support our claims with numerical evidence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches</title>
<link>https://arxiv.org/abs/2505.24703</link>
<guid>https://arxiv.org/abs/2505.24703</guid>
<content:encoded><![CDATA[
arXiv:2505.24703v1 Announce Type: cross 
Abstract: Deep learning techniques have enabled vast improvements in computer vision technologies. Nevertheless, these models are vulnerable to adversarial patch attacks which catastrophically impair performance. The physically realizable nature of these attacks calls for certifiable defenses, which feature provable guarantees on robustness. While certifiable defenses have been successfully applied to single-label classification, limited work has been done for multi-label classification. In this work, we present PatchDEMUX, a certifiably robust framework for multi-label classifiers against adversarial patches. Our approach is a generalizable method which can extend any existing certifiable defense for single-label classification; this is done by considering the multi-label classification task as a series of isolated binary classification problems to provably guarantee robustness. Furthermore, in the scenario where an attacker is limited to a single patch we propose an additional certification procedure that can provide tighter robustness bounds. Using the current state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a backbone, we find that PatchDEMUX can achieve non-trivial robustness on the MS-COCO and PASCAL VOC datasets while maintaining high clean performance
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes</title>
<link>https://arxiv.org/abs/2505.24704</link>
<guid>https://arxiv.org/abs/2505.24704</guid>
<content:encoded><![CDATA[
arXiv:2505.24704v1 Announce Type: cross 
Abstract: Kernel method-based intensity estimators, formulated within reproducing kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs) have been among the most easy-to-implement and feasible methods for estimating the intensity functions of inhomogeneous Poisson processes. While both approaches share the term "kernel", they are founded on distinct theoretical principles, each with its own strengths and limitations. In this paper, we propose a novel regularized kernel method for Poisson processes based on the least squares loss and show that the resulting intensity estimator involves a specialized variant of the representer theorem: it has the dual coefficient of unity and coincides with classical KIEs. This result provides new theoretical insights into the connection between classical KIEs and kernel method-based intensity estimators, while enabling us to develop an efficient KIE by leveraging advanced techniques from RKHS theory. We refer to the proposed model as the kernel method-based kernel intensity estimator (K$^2$IE). Through experiments on synthetic datasets, we show that K$^2$IE achieves comparable predictive performance while significantly surpassing the state-of-the-art kernel method-based estimator in computational efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery</title>
<link>https://arxiv.org/abs/2505.24727</link>
<guid>https://arxiv.org/abs/2505.24727</guid>
<content:encoded><![CDATA[
arXiv:2505.24727v1 Announce Type: cross 
Abstract: This paper introduces a novel Knockoff-guided compressive sensing framework, referred to as \TheName{}, which enhances signal recovery by leveraging precise false discovery rate (FDR) control during the support identification phase. Unlike LASSO, which jointly performs support selection and signal estimation without explicit error control, our method guarantees FDR control in finite samples, enabling more reliable identification of the true signal support. By separating and controlling the support recovery process through statistical Knockoff filters, our framework achieves more accurate signal reconstruction, especially in challenging scenarios where traditional methods fail. We establish theoretical guarantees demonstrating how FDR control directly ensures recovery performance under weaker conditions than traditional $\ell_1$-based compressive sensing methods, while maintaining accurate signal reconstruction. Extensive numerical experiments demonstrate that our proposed Knockoff-based method consistently outperforms LASSO-based and other state-of-the-art compressive sensing techniques. In simulation studies, our method improves F1-score by up to 3.9x over baseline methods, attributed to principled false discovery rate (FDR) control and enhanced support recovery. The method also consistently yields lower reconstruction and relative errors. We further validate the framework on real-world datasets, where it achieves top downstream predictive performance across both regression and classification tasks, often narrowing or even surpassing the performance gap relative to uncompressed signals. These results establish \TheName{} as a robust and practical alternative to existing approaches, offering both theoretical guarantees and strong empirical performance through statistically grounded support selection.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v1 Announce Type: cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan. It begins by selecting genes with high Signal-to-Noise Ratio (SNR) to capture the most informative features, from which a cost matrix is constructed using cosine distances between cell-type centroids. Importantly, the MESH procedure iteratively refines the cost matrix, leading to a transport plan with significantly enhanced sparsity and interpretability of the resulting correspondence matrices. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, symmetric, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Dynamics of Linear Diffusion Models</title>
<link>https://arxiv.org/abs/2505.24769</link>
<guid>https://arxiv.org/abs/2505.24769</guid>
<content:encoded><![CDATA[
arXiv:2505.24769v1 Announce Type: cross 
Abstract: Diffusion models trained on finite datasets with $N$ samples from a target distribution exhibit a transition from memorisation, where the model reproduces training examples, to generalisation, where it produces novel samples that reflect the underlying data distribution. Understanding this transition is key to characterising the sample efficiency and reliability of generative models, but our theoretical understanding of this transition is incomplete. Here, we analytically study the memorisation-to-generalisation transition in a simple model using linear denoisers, which allow explicit computation of test errors, sampling distributions, and Kullback-Leibler divergences between samples and target distribution. Using these measures, we predict that this transition occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is smaller than the dimension of the inputs $d$, so that only a fraction of relevant directions of variation are present in the training data, we demonstrate how both regularization and early stopping help to prevent overfitting. For $N > d$, we find that the sampling distributions of linear diffusion models approach their optimum (measured by the Kullback-Leibler divergence) linearly with $d/N$, independent of the specifics of the data distribution. Our work clarifies how sample complexity governs generalisation in a simple model of diffusion-based generative models and provides insight into the training dynamics of linear denoisers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models</title>
<link>https://arxiv.org/abs/2505.24784</link>
<guid>https://arxiv.org/abs/2505.24784</guid>
<content:encoded><![CDATA[
arXiv:2505.24784v1 Announce Type: cross 
Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-Manual Joint Camera Calibration and Scene Representation</title>
<link>https://arxiv.org/abs/2505.24819</link>
<guid>https://arxiv.org/abs/2505.24819</guid>
<content:encoded><![CDATA[
arXiv:2505.24819v1 Announce Type: cross 
Abstract: Robot manipulation, especially bimanual manipulation, often requires setting up multiple cameras on multiple robot manipulators. Before robot manipulators can generate motion or even build representations of their environments, the cameras rigidly mounted to the robot need to be calibrated. Camera calibration is a cumbersome process involving collecting a set of images, with each capturing a pre-determined marker. In this work, we introduce the Bi-Manual Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables multiple robot manipulators, each with cameras mounted, to circumvent taking images of calibration markers. By leveraging 3D foundation models for dense, marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the extrinsic transformation from each camera to its end-effector, (ii) the inter-arm relative poses between manipulators, and (iii) a unified, scale-consistent 3D representation of the shared workspace, all from the same captured RGB image sets. The representation, jointly constructed from images captured by cameras on both manipulators, lives in a common coordinate frame and supports collision checking and semantic segmentation to facilitate downstream bimanual coordination tasks. We empirically evaluate the robustness of Bi-JCR on a variety of tabletop environments, and demonstrate its applicability on a variety of downstream tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck</title>
<link>https://arxiv.org/abs/2505.24840</link>
<guid>https://arxiv.org/abs/2505.24840</guid>
<content:encoded><![CDATA[
arXiv:2505.24840v1 Announce Type: cross 
Abstract: This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Recognition in the Wild</title>
<link>https://arxiv.org/abs/2505.24848</link>
<guid>https://arxiv.org/abs/2505.24848</guid>
<content:encoded><![CDATA[
arXiv:2505.24848v1 Announce Type: cross 
Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial to be able to keep a record of the user's interactions with the world, including during reading. In this paper, we introduce a new task of reading recognition to determine when the user is reading. We first introduce the first-of-its-kind large-scale multimodal Reading in the Wild dataset, containing 100 hours of reading and non-reading videos in diverse and realistic scenarios. We then identify three modalities (egocentric RGB, eye gaze, head pose) that can be used to solve the task, and present a flexible transformer model that performs the task using these modalities, either individually or combined. We show that these modalities are relevant and complementary to the task, and investigate how to efficiently and effectively encode each modality. Additionally, we show the usefulness of this dataset towards classifying types of reading, extending current reading understanding studies conducted in constrained settings to larger scale, diversity and realism. Code, model, and data will be public.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical mechanics of extensive-width Bayesian neural networks near interpolation</title>
<link>https://arxiv.org/abs/2505.24849</link>
<guid>https://arxiv.org/abs/2505.24849</guid>
<content:encoded><![CDATA[
arXiv:2505.24849v1 Announce Type: cross 
Abstract: For three decades statistical mechanics has been providing a framework to analyse neural networks. However, the theoretically tractable models, e.g., perceptrons, random features models and kernel machines, or multi-index models and committee machines with few neurons, remained simple compared to those used in applications. In this paper we help reducing the gap between practical networks and their theoretical understanding through a statistical physics analysis of the supervised learning of a two-layer fully connected network with generic weight distribution and activation function, whose hidden layer is large but remains proportional to the inputs dimension. This makes it more realistic than infinitely wide networks where no feature learning occurs, but also more expressive than narrow ones or with fixed inner weights. We focus on the Bayes-optimal learning in the teacher-student scenario, i.e., with a dataset generated by another network with the same architecture. We operate around interpolation, where the number of trainable parameters and of data are comparable and feature learning emerges. Our analysis uncovers a rich phenomenology with various learning transitions as the number of data increases. In particular, the more strongly the features (i.e., hidden neurons of the target) contribute to the observed responses, the less data is needed to learn them. Moreover, when the data is scarce, the model only learns non-linear combinations of the teacher weights, rather than "specialising" by aligning its weights with the teacher's. Specialisation occurs only when enough data becomes available, but it can be hard to find for practical training algorithms, possibly due to statistical-to-computational~gaps.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data</title>
<link>https://arxiv.org/abs/2505.24852</link>
<guid>https://arxiv.org/abs/2505.24852</guid>
<content:encoded><![CDATA[
arXiv:2505.24852v1 Announce Type: cross 
Abstract: On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2505.24853</link>
<guid>https://arxiv.org/abs/2505.24853</guid>
<content:encoded><![CDATA[
arXiv:2505.24853v1 Announce Type: cross 
Abstract: We study the problem of functional retargeting: learning dexterous manipulation policies to track object states from human hand-object demonstrations. We focus on long-horizon, bimanual tasks with articulated objects, which is challenging due to large action space, spatiotemporal discontinuities, and embodiment gap between human and robot hands. We propose DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual object controllers with decaying strength: an object is first driven automatically towards its target states, such that the policy can gradually learn to take over under motion and contact guidance. We release a simulation benchmark with a diverse set of tasks and dexterous hands, and show that DexMachina significantly outperforms baseline methods. Our algorithm and benchmark enable a functional comparison for hardware designs, and we present key findings informed by quantitative and qualitative results. With the recent surge in dexterous hand development, we hope this work will provide a useful platform for identifying desirable hardware capabilities and lower the barrier for contributing to future research. Videos and more at https://project-dexmachina.github.io/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs</title>
<link>https://arxiv.org/abs/2505.24858</link>
<guid>https://arxiv.org/abs/2505.24858</guid>
<content:encoded><![CDATA[
arXiv:2505.24858v1 Announce Type: cross 
Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of $\textit{faithful confidence calibration}$ of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that $\textit{faithfully reflect}$ their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24871</link>
<guid>https://arxiv.org/abs/2505.24871</guid>
<content:encoded><![CDATA[
arXiv:2505.24871v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[
arXiv:2505.24878v1 Announce Type: cross 
Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Connectivity through Network Gradients for Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2209.06932</link>
<guid>https://arxiv.org/abs/2209.06932</guid>
<content:encoded><![CDATA[
arXiv:2209.06932v4 Announce Type: replace 
Abstract: Leveraging sparse networks to connect successive layers in deep neural networks has recently been shown to provide benefits to large-scale state-of-the-art models. However, network connectivity also plays a significant role in the learning performance of shallow networks, such as the classic Restricted Boltzmann Machine (RBM). Efficiently finding sparse connectivity patterns that improve the learning performance of shallow networks is a fundamental problem. While recent principled approaches explicitly include network connections as model parameters that must be optimized, they often rely on explicit penalization or network sparsity as a hyperparameter. This work presents the Network Connectivity Gradients (NCG), an optimization method to find optimal connectivity patterns for RBMs. NCG leverages the idea of network gradients: given a specific connection pattern, it determines the gradient of every possible connection and uses the gradient to drive a continuous connection strength parameter that in turn is used to determine the connection pattern. Thus, learning RBM parameters and learning network connections is truly jointly performed, albeit with different learning rates, and without changes to the model's classic energy-based objective function. The proposed method is applied to the MNIST and other data sets showing that better RBM models are found for the benchmark tasks of sample generation and classification. Results also show that NCG is robust to network initialization and is capable of both adding and removing network connections while learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2303.14537</link>
<guid>https://arxiv.org/abs/2303.14537</guid>
<content:encoded><![CDATA[
arXiv:2303.14537v5 Announce Type: replace 
Abstract: Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design Fundamentals of Diffusion Models: A Survey</title>
<link>https://arxiv.org/abs/2306.04542</link>
<guid>https://arxiv.org/abs/2306.04542</guid>
<content:encoded><![CDATA[
arXiv:2306.04542v4 Announce Type: replace 
Abstract: Diffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark</title>
<link>https://arxiv.org/abs/2306.17100</link>
<guid>https://arxiv.org/abs/2306.17100</guid>
<content:encoded><![CDATA[
arXiv:2306.17100v5 Announce Type: replace 
Abstract: Combinatorial optimization (CO) is fundamental to several real-world applications, from logistics and scheduling to hardware design and resource allocation. Deep reinforcement learning (RL) has recently shown significant benefits in solving CO problems, reducing reliance on domain expertise and improving computational efficiency. However, the absence of a unified benchmarking framework leads to inconsistent evaluations, limits reproducibility, and increases engineering overhead, raising barriers to adoption for new researchers. To address these challenges, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 27 CO problem environments and 23 state-of-the-art baselines. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configurations of diverse environments, policy architectures, RL algorithms, and utilities with extensive documentation. RL4CO helps researchers build on existing successes while exploring and developing their own designs, facilitating the entire research process by decoupling science from heavy engineering. We finally provide extensive benchmark studies to inspire new insights and future work. RL4CO has already attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</title>
<link>https://arxiv.org/abs/2310.00098</link>
<guid>https://arxiv.org/abs/2310.00098</guid>
<content:encoded><![CDATA[
arXiv:2310.00098v2 Announce Type: replace 
Abstract: While federated learning (FL) and differential privacy (DP) have been extensively studied, their application to automatic speech recognition (ASR) remains largely unexplored due to the challenges in training large transformer models. Specifically, large models further exacerbate issues in FL as they are particularly susceptible to gradient heterogeneity across layers, unlike the relatively uniform gradient behavior observed in shallow models. As a result, prior works struggle to converge with standard optimization techniques, even in the absence of DP mechanisms. To the best of our knowledge, no existing work establishes a competitive, practical recipe for FL with DP in the context of ASR. To address this gap, we establish \textbf{the first benchmark for FL with DP in end-to-end ASR}. Our approach centers on per-layer clipping and layer-wise gradient normalization: theoretical analysis reveals that these techniques together mitigate clipping bias and gradient heterogeneity across layers in deeper models. Consistent with these theoretical insights, our empirical results show that FL with DP is viable under strong privacy guarantees, provided a population of at least several million users. Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5, $10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate when extrapolating to high (resp. low) population scales for FL with DP in ASR. Although our experiments focus on ASR, the underlying principles we uncover - particularly those concerning gradient heterogeneity and layer-wise gradient normalization - offer broader guidance for designing scalable, privacy-preserving FL algorithms for large models across domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCNETS: A Modular Causal Learning Framework for Pattern Recognition in Imbalanced Datasets</title>
<link>https://arxiv.org/abs/2401.04139</link>
<guid>https://arxiv.org/abs/2401.04139</guid>
<content:encoded><![CDATA[
arXiv:2401.04139v3 Announce Type: replace 
Abstract: Handling class imbalance remains a central challenge in machine learning, particularly in pattern recognition tasks where rare but critical events-such as fraudulent transactions or medical anomalies-must be identified accurately. Traditional generative models offer a potential remedy through data augmentation but often treat generation and classification as independent processes, leading to distribution mismatch and limited classifier benefit. To address these shortcomings, we propose Causal Cooperative Networks (CCNETS), a modular learning framework that integrates generation, inference, and reconstruction within a unified causal paradigm. CCNETS comprises three cooperative modules: an Explainer for latent feature abstraction, a Reasoner for label prediction, and a Producer for context-aware data generation. These components interact through a causal feedback loop, where classification results guide targeted sample synthesis. A key innovation, the Zoint mechanism, enables adaptive fusion of latent and observable features, enhancing semantic richness and enabling robust decision-making under uncertainty. We evaluate CCNETS on a real-world credit card fraud detection dataset with extreme imbalance (fraud cases < 0.2%). Across three experimental setups-including synthetic training, amplified generation, and direct classifier comparison-CCNETS outperforms baseline methods, achieving higher F1 scores, precision, and recall. Models trained on CCNETS-generated data also demonstrate superior generalization under limited data conditions. These results establish CCNETS as a scalable, interpretable, and hybrid soft computing framework. By causally aligning synthetic data with classifier objectives, CCNETS advances imbalanced pattern recognition and opens new directions for robust, modular learning in real-world applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross Entropy versus Label Smoothing: A Neural Collapse Perspective</title>
<link>https://arxiv.org/abs/2402.03979</link>
<guid>https://arxiv.org/abs/2402.03979</guid>
<content:encoded><![CDATA[
arXiv:2402.03979v3 Announce Type: replace 
Abstract: Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications</title>
<link>https://arxiv.org/abs/2403.00485</link>
<guid>https://arxiv.org/abs/2403.00485</guid>
<content:encoded><![CDATA[
arXiv:2403.00485v3 Announce Type: replace 
Abstract: Geometric graphs are a special kind of graph with geometric features, which are vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To address this issue, researchers proposed a variety of geometric GNNs equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we formalize geometric graph as the data structure, on top of which we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later research for methodology development and experimental evaluation. We also discuss the challenges and future potential directions of geometric GNNs at the end of this survey.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Extrapolation Expedites Alignment</title>
<link>https://arxiv.org/abs/2404.16792</link>
<guid>https://arxiv.org/abs/2404.16792</guid>
<content:encoded><![CDATA[
arXiv:2404.16792v5 Announce Type: replace 
Abstract: Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning</title>
<link>https://arxiv.org/abs/2405.07543</link>
<guid>https://arxiv.org/abs/2405.07543</guid>
<content:encoded><![CDATA[
arXiv:2405.07543v2 Announce Type: replace 
Abstract: Personalization is crucial for the widespread adoption of advanced driver assistance system. To match up with each user's preference, the online evolution capability is a must. However, conventional evolution methods learn from naturalistic driving data, which requires a lot computing power and cannot be applied online. To address this challenge, this paper proposes a lesson learning approach: learning from driver's takeover interventions. By leveraging online takeover data, the driving zone is generated to ensure perceived safety using Gaussian discriminant analysis. Real-time corrections to trajectory planning rewards are enacted through apprenticeship learning. Guided by the objective of optimizing rewards within the constraints of the driving zone, this approach employs model predictive control for trajectory planning. This lesson learning framework is highlighted for its faster evolution capability, adeptness at experience accumulating, assurance of perceived safety, and computational efficiency. Simulation results demonstrate that the proposed system consistently achieves a successful customization without further takeover interventions. Accumulated experience yields a 24% enhancement in evolution efficiency. The average number of learning iterations is only 13.8. The average computation time is 0.08 seconds.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispaRisk: Auditing Fairness Through Usable Information</title>
<link>https://arxiv.org/abs/2405.12372</link>
<guid>https://arxiv.org/abs/2405.12372</guid>
<content:encoded><![CDATA[
arXiv:2405.12372v3 Announce Type: replace 
Abstract: Machine Learning algorithms (ML) impact virtually every aspect of human lives and have found use across diverse sectors including healthcare, finance, and education. Often, ML algorithms have been found to exacerbate societal biases present in datasets leading to adversarial impacts on subsets/groups of individuals and in many cases on minority groups. To effectively mitigate these untoward effects, it is crucial that disparities/biases are identified early in a ML pipeline. This proactive approach facilitates timely interventions to prevent bias amplification and reduce complexity at later stages of model development. In this paper, we leverage recent advancements in usable information theory to introduce DispaRisk, a novel framework designed to proactively assess the potential risks of disparities in datasets during the initial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by benchmarking it against commonly used datasets in fairness research. Our findings demonstrate DispaRisk's capabilities to identify datasets with a high risk of discrimination, detect model families prone to biases within an ML pipeline, and enhance the explainability of these bias risks. This work contributes to the development of fairer ML systems by providing a robust tool for early bias detection and mitigation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Bayesian Filter for Bayes-faithful Data Assimilation</title>
<link>https://arxiv.org/abs/2405.18674</link>
<guid>https://arxiv.org/abs/2405.18674</guid>
<content:encoded><![CDATA[
arXiv:2405.18674v3 Announce Type: replace 
Abstract: State estimation for nonlinear state space models (SSMs) is a challenging task. Existing assimilation methodologies predominantly assume Gaussian posteriors on physical space, where true posteriors become inevitably non-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on nonlinear SSMs. DBF constructs new latent variables $h_t$ in addition to the original physical variables $z_t$ and assimilates observations $o_t$. By (i) constraining the state transition on the new latent space to be linear and (ii) learning a Gaussian inverse observation operator $r(h_t|o_t)$, posteriors remain Gaussian. Notably, the structured design of test distributions enables an analytical formula for the recursive computation, eliminating the accumulation of Monte Carlo sampling errors across time steps. DBF trains the Gaussian inverse observation operators $r(h_t|o_t)$ and other latent SSM parameters (e.g., dynamics matrix) by maximizing the evidence lower bound. Experiments demonstrate that DBF outperforms model-based approaches and latent assimilation methods in tasks where the true posterior distribution on physical space is significantly non-Gaussian.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients</title>
<link>https://arxiv.org/abs/2406.06751</link>
<guid>https://arxiv.org/abs/2406.06751</guid>
<content:encoded><![CDATA[
arXiv:2406.06751v2 Announce Type: replace 
Abstract: We propose a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery. Our work is aligned with the popular DSR framework which focuses on learning a data-specific expression generator, without relying on pretrained models or additional search or planning procedures. Despite the success of existing DSR methods, they are built on recurrent neural networks, solely guided by data fitness, and potentially meet tail barriers that can zero out the policy gradient, causing inefficient model updates. To overcome these limitations, we design a decoder-only architecture that performs attention in the frequency domain and introduce a dual-indexed position encoding to conduct layer-wise generation. Second, we propose a Bayesian information criterion (BIC)-based reward function that can automatically adjust the trade-off between expression complexity and data fitness, without the need for explicit manual tuning. Third, we develop a ranking-based weighted policy update method that eliminates the tail barriers and enhances training effectiveness. Extensive benchmarks and systematic experiments demonstrate the advantages of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening</title>
<link>https://arxiv.org/abs/2406.09291</link>
<guid>https://arxiv.org/abs/2406.09291</guid>
<content:encoded><![CDATA[
arXiv:2406.09291v4 Announce Type: replace 
Abstract: Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing.
  Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.18354</link>
<guid>https://arxiv.org/abs/2406.18354</guid>
<content:encoded><![CDATA[
arXiv:2406.18354v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) excel in learning from network-like data but often lack interpretability, making their application challenging in domains requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold Network (GKAN), a novel GNN model leveraging spline-based activation functions on edges to enhance both accuracy and interpretability. Our experiments on five benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks. In addition to the improved accuracy, GKAN's design inherently provides clear insights into the model's decision-making process, eliminating the need for post-hoc explainability techniques. This paper discusses the methodology, performance, and interpretability of GKAN, highlighting its potential for applications in domains where interpretability is crucial.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Distances from Data with Normalizing Flows and Score Matching</title>
<link>https://arxiv.org/abs/2407.09297</link>
<guid>https://arxiv.org/abs/2407.09297</guid>
<content:encoded><![CDATA[
arXiv:2407.09297v2 Announce Type: replace 
Abstract: Density-based distances (DBDs) provide a principled approach to metric learning by defining distances in terms of the underlying data distribution. By employing a Riemannian metric that increases in regions of low probability density, shortest paths naturally follow the data manifold. Fermat distances, a specific type of DBD, have attractive properties, but existing estimators based on nearest neighbor graphs suffer from poor convergence due to inaccurate density estimates. Moreover, graph-based methods scale poorly to high dimensions, as the proposed geodesics are often insufficiently smooth. We address these challenges in two key ways. First, we learn densities using normalizing flows. Second, we refine geodesics through relaxation, guided by a learned score model. Additionally, we introduce a dimension-adapted Fermat distance that scales intuitively to high dimensions and improves numerical stability. Our work paves the way for the practical use of density-based distances, especially in high-dimensional spaces.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Unlearning with Single Layer Unlearning Gradient</title>
<link>https://arxiv.org/abs/2407.11867</link>
<guid>https://arxiv.org/abs/2407.11867</guid>
<content:encoded><![CDATA[
arXiv:2407.11867v3 Announce Type: replace 
Abstract: Machine unlearning methods aim to remove sensitive or unwanted content from trained models, but typically demand extensive model updates at significant computational cost while potentially degrading model performance on both related and unrelated tasks. We propose Single Layer Unlearning Gradient (SLUG) as an efficient method to unlearn targeted information by updating a single critical layer using a one-time gradient computation. SLUG uses layer importance and gradient alignment metrics to identify the optimal layer for targeted information removal while preserving the model utility. We demonstrate the effectiveness of SLUG for CLIP, Stable Diffusion, and vision-language models (VLMs) in removing concrete (e.g., identities and objects) and abstract concepts (e.g., artistic styles). On the UnlearnCanvas benchmark, SLUG achieves comparable unlearning performance to existing methods while requiring significantly less computational resources. Our proposed approach offers a practical solution for targeted unlearning that is computationally efficient and precise. Our code is available at https://github.com/CSIPlab/SLUG.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning in a recurrent neural network that plays Sokoban</title>
<link>https://arxiv.org/abs/2407.15421</link>
<guid>https://arxiv.org/abs/2407.15421</guid>
<content:encoded><![CDATA[
arXiv:2407.15421v3 Announce Type: replace 
Abstract: Planning is essential for solving complex tasks, yet the internal mechanisms underlying planning in neural networks remain poorly understood. Building on prior work, we analyze a recurrent neural network (RNN) trained on Sokoban, a challenging puzzle requiring sequential, irreversible decisions. We find that the RNN has a causal plan representation which predicts its future actions about 50 steps in advance. The quality and length of the represented plan increases over the first few steps. We uncover a surprising behavior: the RNN "paces" in cycles to give itself extra computation at the start of a level, and show that this behavior is incentivized by training. Leveraging these insights, we extend the trained RNN to significantly larger, out-of-distribution Sokoban puzzles, demonstrating robust representations beyond the training regime. We open-source our model and code, and believe the neural network's interesting behavior makes it an excellent model organism to deepen our understanding of learned planning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks</title>
<link>https://arxiv.org/abs/2407.17963</link>
<guid>https://arxiv.org/abs/2407.17963</guid>
<content:encoded><![CDATA[
arXiv:2407.17963v2 Announce Type: replace 
Abstract: Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Friends in Unexpected Places: Enhancing Local Fairness in Federated Learning through Clustering</title>
<link>https://arxiv.org/abs/2407.19331</link>
<guid>https://arxiv.org/abs/2407.19331</guid>
<content:encoded><![CDATA[
arXiv:2407.19331v3 Announce Type: replace 
Abstract: Federated Learning (FL) has been a pivotal paradigm for collaborative training of machine learning models across distributed datasets. In heterogeneous settings, it has been observed that a single shared FL model can lead to low local accuracy, motivating personalized FL algorithms. In parallel, fair FL algorithms have been proposed to enforce group fairness on the global models. Again, in heterogeneous settings, global and local fairness do not necessarily align, motivating the recent literature on locally fair FL. In this paper, we propose new FL algorithms for heterogeneous settings, spanning the space between personalized and locally fair FL. Building on existing clustering-based personalized FL methods, we incorporate a new fairness metric into cluster assignment, enabling a tunable balance between local accuracy and fairness. Our methods match or exceed the performance of existing locally fair FL approaches, without explicit fairness intervention. We further demonstrate (numerically and analytically) that personalization alone can improve local fairness and that our methods exploit this alignment when present.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECG-FM: An Open Electrocardiogram Foundation Model</title>
<link>https://arxiv.org/abs/2408.05178</link>
<guid>https://arxiv.org/abs/2408.05178</guid>
<content:encoded><![CDATA[
arXiv:2408.05178v2 Announce Type: replace 
Abstract: Conventional task-specific electrocardiogram (ECG) analysis models require large annotated datasets to train. Foundation models mitigate this burden by leveraging self-supervised pretraining; however, the scarcity of open-weight ECG foundation models hinders adoption and cross-study comparability. We present ECG-FM, an open foundation model for ECG analysis, and conduct a study using a dataset of 1.5 million ECGs. ECG-FM is a transformer-based model pretrained using a hybrid contrastive and generative self-supervised learning approach. Our downstream tasks include predicting reduced left ventricular ejection fraction (LVEF) and ECG interpretation labels, where we release a benchmark task on the MIMIC-IV-ECG dataset. We affirm that ECG-FM is robust, label-efficient, and functionally discriminative by showcasing data scaling experiments, performing a latent space analysis, and generating saliency maps. ECG-FM markedly outperforms task-specific models in the small-to-medium-scale data regime and demonstrates cross-dataset generalizability, achieving high AUROC on many clinically salient labels such as atrial fibrillation (0.996) and LVEF<=40% (0.929). We release our code, model weights, and benchmark task at https://github.com/bowang-lab/ECG-FM/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction</title>
<link>https://arxiv.org/abs/2408.08713</link>
<guid>https://arxiv.org/abs/2408.08713</guid>
<content:encoded><![CDATA[
arXiv:2408.08713v5 Announce Type: replace 
Abstract: Modeling high-order feature interactions is crucial for click-through rate (CTR) prediction, and traditional approaches often predefine a maximum interaction order and rely on exhaustive enumeration of feature combinations up to this predefined order. This framework heavily relies on prior domain knowledge to define interaction scope and entails high computational costs from enumeration. Conventional CTR models face a trade-off between improving representation through complex high-order feature interactions and reducing computational inefficiencies associated with these processes. To address this dual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse Efficient Interaction Network (KarSein). Drawing inspiration from the learnable activation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages this mechanism to adaptively transform low-order basic features into high-order feature interactions, offering a novel approach to feature interaction modeling. KarSein extends the capabilities of KAN by introducing a more efficient architecture that significantly reduces computational costs while accommodating two-dimensional embedding vectors as feature inputs. Furthermore, it overcomes the limitation of KAN's its inability to spontaneously capture multiplicative relationships among features.
  Extensive experiments highlight the superiority of KarSein, demonstrating its ability to surpass not only the vanilla implementation of KAN in CTR predictio but also other baseline methods. Remarkably, KarSein achieves exceptional predictive accuracy while maintaining a highly compact parameter size and minimal computational overhead. As the first attempt to apply KAN in the CTR domain, this work introduces KarSein as a novel solution for modeling complex feature interactions, underscoring its transformative potential in advancing CTR prediction task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs</title>
<link>https://arxiv.org/abs/2408.08868</link>
<guid>https://arxiv.org/abs/2408.08868</guid>
<content:encoded><![CDATA[
arXiv:2408.08868v3 Announce Type: replace 
Abstract: The state-of-the-art for training on-device language models for mobile keyboard applications combines federated learning (FL) with differential privacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two variants of DP-FTRL are used in practice, tree aggregation and matrix factorization. However, tree aggregation suffers from significantly suboptimal privacy/utility tradeoffs, while matrix mechanisms require expensive optimization parameterized by hard-to-estimate-in-advance constants, and high runtime memory costs. This paper extends the recently introduced Buffered Linear Toeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRL maintains the ease-of-use advantages of tree aggregation, while essentially matching matrix factorization in terms of utility and privacy. We evaluate BLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulation benchmark, and across four on-device language model tasks in a production FL system. Our empirical results highlight the advantages of the BLT mechanism and elevate the practicality and effectiveness of DP in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.09429</link>
<guid>https://arxiv.org/abs/2408.09429</guid>
<content:encoded><![CDATA[
arXiv:2408.09429v3 Announce Type: replace 
Abstract: Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Causal Discovery without Acyclicity Constraints</title>
<link>https://arxiv.org/abs/2408.13448</link>
<guid>https://arxiv.org/abs/2408.13448</guid>
<content:encoded><![CDATA[
arXiv:2408.13448v4 Announce Type: replace 
Abstract: Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs</title>
<link>https://arxiv.org/abs/2408.13467</link>
<guid>https://arxiv.org/abs/2408.13467</guid>
<content:encoded><![CDATA[
arXiv:2408.13467v3 Announce Type: replace 
Abstract: The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is automatically improved through additional fine-tuning using extra similar data generated by the service LLM. This multi-turn process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness</title>
<link>https://arxiv.org/abs/2409.01249</link>
<guid>https://arxiv.org/abs/2409.01249</guid>
<content:encoded><![CDATA[
arXiv:2409.01249v2 Announce Type: replace 
Abstract: Recent work has proposed neural network pruning techniques to reduce the size of a network while preserving robustness against adversarial examples, i.e., well-crafted inputs inducing a misclassification. These methods, which we refer to as adversarial pruning methods, involve complex and articulated designs, making it difficult to analyze the differences and establish a fair and accurate comparison. In this work, we overcome these issues by surveying current adversarial pruning methods and proposing a novel taxonomy to categorize them based on two main dimensions: the pipeline, defining when to prune; and the specifics, defining how to prune. We then highlight the limitations of current empirical analyses and propose a novel, fair evaluation benchmark to address them. We finally conduct an empirical re-evaluation of current adversarial pruning methods and discuss the results, highlighting the shared traits of top-performing adversarial pruning methods, as well as common issues. We welcome contributions in our publicly-available benchmark at https://github.com/pralab/AdversarialPruningBenchmark
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified theoretical guarantees for stability, consistency, and convergence in neural PDE solvers from non-IID data to physics-informed networks</title>
<link>https://arxiv.org/abs/2409.05030</link>
<guid>https://arxiv.org/abs/2409.05030</guid>
<content:encoded><![CDATA[
arXiv:2409.05030v3 Announce Type: replace 
Abstract: We establish a unified theoretical framework addressing the stability, consistency, and convergence of neural networks under realistic training conditions, specifically, in the presence of non-IID data, geometric constraints, and embedded physical laws. For standard supervised learning with dependent data, we derive uniform stability bounds for gradient-based methods using mixing coefficients and dynamic learning rates. In federated learning with heterogeneous data and non-Euclidean parameter spaces, we quantify model inconsistency via curvature-aware aggregation and information-theoretic divergence. For Physics-Informed Neural Networks (PINNs), we rigorously prove perturbation stability, residual consistency, Sobolev convergence, energy stability for conservation laws, and convergence under adaptive multi-domain refinements. Each result is grounded in variational analysis, compactness arguments, and universal approximation theorems in Sobolev spaces. Our theoretical guarantees are validated across parabolic, elliptic, and hyperbolic PDEs, confirming that residual minimization aligns with physical solution accuracy. This work offers a mathematically principled basis for designing robust, generalizable, and physically coherent neural architectures across diverse learning environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying biological perturbation targets through causal differential networks</title>
<link>https://arxiv.org/abs/2410.03380</link>
<guid>https://arxiv.org/abs/2410.03380</guid>
<content:encoded><![CDATA[
arXiv:2410.03380v4 Announce Type: replace 
Abstract: Identifying variables responsible for changes to a biological system enables applications in drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation</title>
<link>https://arxiv.org/abs/2410.03782</link>
<guid>https://arxiv.org/abs/2410.03782</guid>
<content:encoded><![CDATA[
arXiv:2410.03782v4 Announce Type: replace 
Abstract: Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue that their static nature limits downstream performance while achieving efficiency. In this work, we propose DaWin, a training-free dynamic weight interpolation method that leverages the entropy of individual models over each unlabeled test sample to assess model expertise, and compute per-sample interpolation coefficients dynamically. Unlike previous works that typically rely on additional training to learn such coefficients, our approach requires no training. Then, we propose a mixture modeling approach that greatly reduces inference overhead raised by dynamic interpolation. We validate DaWin on the large-scale visual recognition benchmarks, spanning 14 tasks across robust fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and multi-task learning with eight classification tasks. Results demonstrate that DaWin achieves significant performance gain in considered settings, with minimal computational overhead. We further discuss DaWin's analytic behavior to explain its empirical success.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalization Result for Convergence in Learning-to-Optimize</title>
<link>https://arxiv.org/abs/2410.07704</link>
<guid>https://arxiv.org/abs/2410.07704</guid>
<content:encoded><![CDATA[
arXiv:2410.07704v2 Announce Type: replace 
Abstract: Learning-to-optimize leverages machine learning to accelerate optimization algorithms. While empirical results show tremendous improvements compared to classical optimization algorithms, theoretical guarantees are mostly lacking, such that the outcome cannot be reliably assured. Especially, convergence is hardly studied in learning-to-optimize, because conventional convergence guarantees in optimization are based on geometric arguments, which cannot be applied easily to learned algorithms. Thus, we develop a probabilistic framework that resembles classical optimization and allows for transferring geometric arguments into learning-to-optimize. Based on our new proof-strategy, our main theorem is a generalization result for parametric classes of potentially non-smooth, non-convex loss functions and establishes the convergence of learned optimization algorithms to critical points with high probability. This effectively generalizes the results of a worst-case analysis into a probabilistic framework, and frees the design of the learned algorithm from using safeguards.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</title>
<link>https://arxiv.org/abs/2410.16222</link>
<guid>https://arxiv.org/abs/2410.16222</guid>
<content:encoded><![CDATA[
arXiv:2410.16222v2 Announce Type: replace 
Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, nonparametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After an extensive comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent bigrams, either selecting the ones absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title>
<link>https://arxiv.org/abs/2410.22307</link>
<guid>https://arxiv.org/abs/2410.22307</guid>
<content:encoded><![CDATA[
arXiv:2410.22307v2 Announce Type: replace 
Abstract: The ever-increasing size of open-source Large Language Models (LLMs) renders local deployment impractical for individual users. Decentralized computing has emerged as a cost-effective solution, allowing individuals and small companies to perform LLM inference for users using surplus computational power. However, a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby benefiting from cost savings. We introduce SVIP, a secret-based verifiable LLM inference protocol. Unlike existing solutions based on cryptographic or game-theoretic techniques, our method is computationally effective and does not rest on strong assumptions. Our protocol requires the computing provider to return both the generated text and processed hidden representations from LLMs. We then train a proxy task on these representations, effectively transforming them into a unique model identifier. With our protocol, users can reliably verify whether the computing provider is acting honestly. A carefully integrated secret mechanism further strengthens its security. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per prompt query for verification.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2410.22366</link>
<guid>https://arxiv.org/abs/2410.22366</guid>
<content:encoded><![CDATA[
arXiv:2410.22366v3 Announce Type: replace 
Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients</title>
<link>https://arxiv.org/abs/2410.22815</link>
<guid>https://arxiv.org/abs/2410.22815</guid>
<content:encoded><![CDATA[
arXiv:2410.22815v2 Announce Type: replace 
Abstract: Federated fine-tuning for Large Language Models (LLMs) faces significant challenges due to the heavy communication overhead of transmitting large model updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a significant reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism increases robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization</title>
<link>https://arxiv.org/abs/2411.02355</link>
<guid>https://arxiv.org/abs/2411.02355</guid>
<content:encoded><![CDATA[
arXiv:2411.02355v3 Announce Type: replace 
Abstract: Quantization is a powerful tool for accelerating large language model (LLM) inference, but the accuracy-performance trade-offs across different formats remain unclear. In this paper, we conduct the most comprehensive empirical study to date, evaluating FP8, INT8, and INT4 quantization across academic benchmarks and real-world tasks on the entire Llama-3.1 model family. Through over 500,000 evaluations, our investigation yields several key findings: (1) FP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned INT8 (W8A8-INT) achieves surprisingly low (1-3\%) accuracy degradation, and (3) INT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit quantization. Further, we investigate the optimal quantization format for different deployments by analyzing inference performance through the popular vLLM framework. Our analysis provides clear deployment recommendations: W4A16 is the most cost-efficient for synchronous setups, while W8A8 dominates in asynchronous continuous batching. For mixed workloads, the optimal choice depends on the specific use case. Our findings offer practical, data-driven guidelines for deploying quantized LLMs at scale -- ensuring the best balance between speed, efficiency, and accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Convergence of Softmax Policy Mirror Ascent</title>
<link>https://arxiv.org/abs/2411.12042</link>
<guid>https://arxiv.org/abs/2411.12042</guid>
<content:encoded><![CDATA[
arXiv:2411.12042v2 Announce Type: replace 
Abstract: Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani et al. [2021] introduced a policy gradient method that corresponds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Unlike that for NPG, generalizing SPMA to the linear function approximation (FA) setting does not require compatible function approximation. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems. We prove that SPMA achieves linear convergence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical performance on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA consistently achieves similar or better performance compared to MDPO, PPO and TRPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting</title>
<link>https://arxiv.org/abs/2501.16178</link>
<guid>https://arxiv.org/abs/2501.16178</guid>
<content:encoded><![CDATA[
arXiv:2501.16178v2 Announce Type: replace 
Abstract: In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling. However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models. To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF). Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series. (ii) Achieving cross-band information fusion with a learnable filter. (iii) Using only one shared linear layer or one shallow MLP for sub-series' mapping. We conduct comprehensive experiments, and the results show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task. Moreover, it is noteworthy that the number of parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a single-layer linear model for time-domain prediction. Our code is available at https://github.com/LancelotXWX/SWIFT.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function</title>
<link>https://arxiv.org/abs/2501.19374</link>
<guid>https://arxiv.org/abs/2501.19374</guid>
<content:encoded><![CDATA[
arXiv:2501.19374v2 Announce Type: replace 
Abstract: Recent advancements in data-driven weather forecasting models have delivered deterministic models that outperform the leading operational forecast systems based on traditional, physics-based models. However, these data-driven models are typically trained with a mean squared error loss function, which causes smoothing of fine scales through a "double penalty" effect. We develop a simple, parameter-free modification to this loss function that avoids this problem by separating the loss attributable to decorrelation from the loss attributable to spectral amplitude errors. Fine-tuning the GraphCast model with this new loss function results in sharp deterministic weather forecasts, an increase of the model's effective resolution from 1,250km to 160km, improvements to ensemble spread, and improvements to predictions of tropical cyclone strength and surface wind extremes.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sundial: A Family of Highly Capable Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2502.00816</link>
<guid>https://arxiv.org/abs/2502.00816</guid>
<content:encoded><![CDATA[
arXiv:2502.00816v2 Announce Type: replace 
Abstract: We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</title>
<link>https://arxiv.org/abs/2502.03773</link>
<guid>https://arxiv.org/abs/2502.03773</guid>
<content:encoded><![CDATA[
arXiv:2502.03773v4 Announce Type: replace 
Abstract: In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand \cite{bordt2022post}. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests. Our code is publicly available at https://github.com/emlaufer/ExpProof.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Reasoning with Guidelines</title>
<link>https://arxiv.org/abs/2502.04040</link>
<guid>https://arxiv.org/abs/2502.04040</guid>
<content:encoded><![CDATA[
arXiv:2502.04040v2 Announce Type: replace 
Abstract: Training safe LLMs remains a critical challenge. The most widely used method, Refusal Training (RT), struggles to generalize against various Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced methods have been proposed to address this issue, we instead question whether OOD attacks inherently surpass the capability of vanilla RT. Evaluations using Best-of-N (BoN) reveal significant safety improvements as N increases, indicating models possess adequate latent safety knowledge but RT fails to consistently elicit it under OOD scenarios. Further domain adaptation analysis reveals that direct RT causes reliance on superficial shortcuts, resulting in non-generalizable representation mappings. Inspired by our findings, we propose training model to perform safety reasoning for each query. Specifically, we synthesize reasoning supervision aligned with specified guidelines that reflect diverse perspectives on safety knowledge. This encourages model to engage in deeper reasoning, explicitly eliciting and utilizing latent safety knowledge for each query. Extensive experiments show that our method significantly improves model generalization against OOD attacks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaferLLM: Large Language Model Inference at Wafer Scale</title>
<link>https://arxiv.org/abs/2502.04563</link>
<guid>https://arxiv.org/abs/2502.04563</guid>
<content:encoded><![CDATA[
arXiv:2502.04563v3 Announce Type: replace 
Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.
  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.
  Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and 16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces</title>
<link>https://arxiv.org/abs/2502.04959</link>
<guid>https://arxiv.org/abs/2502.04959</guid>
<content:encoded><![CDATA[
arXiv:2502.04959v2 Announce Type: replace 
Abstract: Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance on vision and language tasks across various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGenNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling</title>
<link>https://arxiv.org/abs/2502.06250</link>
<guid>https://arxiv.org/abs/2502.06250</guid>
<content:encoded><![CDATA[
arXiv:2502.06250v2 Announce Type: replace 
Abstract: Solving parametric partial differential equations (PDEs) and associated PDE-based, inverse problems is a central task in engineering and physics, yet existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large amounts of {\em labeled} training data. We propose the Deep Generative Neural Operator (DGenNO), a physics-aware framework that addresses these challenges by leveraging a deep, generative, probabilistic model in combination with a set of lower-dimensional, latent variables that simultaneously encode PDE-inputs and PDE-outputs. This formulation can make use of unlabeled data and significantly improves inverse problem-solving, particularly for discontinuous or discrete-valued input functions. DGenNO enforces physics constraints without labeled data by incorporating as virtual observables, weak-form residuals based on compactly supported radial basis functions (CSRBFs). These relax regularity constraints and eliminate higher-order derivatives from the objective function. We also introduce MultiONet, a novel neural operator architecture, which is a more expressive generalization of the popular DeepONet that significantly enhances the approximating power of the proposed model. These innovations make DGenNO particularly effective for challenging forward and inverse, PDE-based problems, such as those involving multi-phase media. Numerical experiments demonstrate that DGenNO achieves higher accuracy across multiple benchmarks while exhibiting robustness to noise and strong generalization to out-of-distribution cases. Its adaptability, and the ability to handle sparse, noisy data while providing probabilistic estimates, make DGenNO a powerful tool for scientific and engineering applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation</title>
<link>https://arxiv.org/abs/2502.06516</link>
<guid>https://arxiv.org/abs/2502.06516</guid>
<content:encoded><![CDATA[
arXiv:2502.06516v2 Announce Type: replace 
Abstract: Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations. Code is available at https://github.com/soobin-um/BnS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Samples to Question Trained Models</title>
<link>https://arxiv.org/abs/2502.06658</link>
<guid>https://arxiv.org/abs/2502.06658</guid>
<content:encoded><![CDATA[
arXiv:2502.06658v2 Announce Type: replace 
Abstract: There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Advantage-Aligned Online Reinforcement Learning with Offline Data</title>
<link>https://arxiv.org/abs/2502.07937</link>
<guid>https://arxiv.org/abs/2502.07937</guid>
<content:encoded><![CDATA[
arXiv:2502.07937v2 Announce Type: replace 
Abstract: Online reinforcement learning (RL) enhances policies through direct interactions with the environment, but faces challenges related to sample efficiency. In contrast, offline RL leverages extensive pre-collected data to learn policies, but often produces suboptimal results due to limited data coverage. Recent efforts integrate offline and online RL in order to harness the advantages of both approaches. However, effectively combining online and offline RL remains challenging due to issues that include catastrophic forgetting, lack of robustness to data quality and limited sample efficiency in data utilization. In an effort to address these challenges, we introduce A3RL, which incorporates a novel confidence aware Active Advantage Aligned (A3) sampling strategy that dynamically prioritizes data aligned with the policy's evolving needs from both online and offline sources, optimizing policy improvement. Moreover, we provide theoretical insights into the effectiveness of our active sampling strategy and conduct diverse empirical experiments and ablation studies, demonstrating that our method outperforms competing online RL techniques that leverage offline data. Our code will be publicly available at:https://github.com/xuefeng-cs/A3RL.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Force Field: Few-shot Learning of Generalized Physical Reasoning</title>
<link>https://arxiv.org/abs/2502.08987</link>
<guid>https://arxiv.org/abs/2502.08987</guid>
<content:encoded><![CDATA[
arXiv:2502.08987v3 Announce Type: replace 
Abstract: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE) to learn complex object interactions through force field representations, which can be efficiently integrated through an Ordinary Differential Equation (ODE) solver to predict object trajectories. Unlike existing approaches that rely on discrete latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in continuous explicit force fields. Experiments on three challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence</title>
<link>https://arxiv.org/abs/2502.09263</link>
<guid>https://arxiv.org/abs/2502.09263</guid>
<content:encoded><![CDATA[
arXiv:2502.09263v2 Announce Type: replace 
Abstract: Message-passing Graph Neural Networks (GNNs) are often criticized for their limited expressiveness, issues like over-smoothing and over-squashing, and challenges in capturing long-range dependencies. Conversely, Graph Transformers (GTs) are regarded as superior due to their employment of global attention mechanisms, which potentially mitigate these challenges. Literature frequently suggests that GTs outperform GNNs in graph-level tasks, especially for graph classification and regression on small molecular graphs. In this study, we explore the untapped potential of GNNs through an enhanced framework, GNN+, which integrates six widely used techniques: edge feature integration, normalization, dropout, residual connections, feed-forward networks, and positional encoding, to effectively tackle graph-level tasks. We conduct a systematic re-evaluation of three classic GNNs (GCN, GIN, and GatedGCN) enhanced by the GNN+ framework across 14 well-known graph-level datasets. Our results reveal that, contrary to prevailing beliefs, these classic GNNs consistently match or surpass the performance of GTs, securing top-three rankings across all datasets and achieving first place in eight. Furthermore, they demonstrate greater efficiency, running several times faster than GTs on many datasets. This highlights the potential of simple GNN architectures, challenging the notion that complex mechanisms in GTs are essential for superior graph-level performance. Our source code is available at https://github.com/LUOyk1999/GNNPlus.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.11147</link>
<guid>https://arxiv.org/abs/2502.11147</guid>
<content:encoded><![CDATA[
arXiv:2502.11147v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires an LLM to generate long sequences, incurring $O(N)$ time and memory complexities per token, where $N$ is the current sequence length. To reduce complexities, existing sparsity-based algorithms propose to retain Key-Value (KV) vectors, the intermediate representations of only the most critical tokens. However, these algorithms struggle with the "impossible trinity" of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \ll N$). To address the "impossible trinity", in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm RaaS that identifies milestone tokens and retains their KV vectors until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyper-SET: Designing Transformers via Hyperspherical Energy Minimization</title>
<link>https://arxiv.org/abs/2502.11646</link>
<guid>https://arxiv.org/abs/2502.11646</guid>
<content:encoded><![CDATA[
arXiv:2502.11646v3 Announce Type: replace 
Abstract: Transformer-based models have achieved remarkable success, but their core components, Transformer layers, are largely heuristics-driven and engineered from the bottom up, calling for a prototypical model with high interpretability and practical competence. To this end, we conceptualize a principled, top-down approach grounded in energy-based interpretation. Specifically, we formalize token dynamics as a joint maximum likelihood estimation on the hypersphere, featuring two properties: semantic alignment in the high-dimensional space and distributional uniformity in the low-dimensional space. By quantifying them with extended Hopfield energy functions, we instantiate this idea as a constrained energy minimization problem, which enables designs of symmetric attention and feedforward modules with RMS normalization. We further present \textit{Hyper-Spherical Energy Transformer} (Hyper-SET), a recurrent-depth alternative to vanilla Transformers naturally emerging from iterative energy optimization on the hypersphere. With shared parameters across layers, Hyper-SET can scale to arbitrary depth with fewer parameters. Theoretically grounded and compact, it achieves competitive or superior performance across diverse tasks, including Sudoku solving, image classification, and masked image modeling. We also design novel variations under the proposed general principle, such as linear attention and gated feedforward layer. Moreover, we showcase its scalability with depth-wise LoRA. Our results highlight Hyper-SET as a step toward interpretable and principled Transformer design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
arXiv:2502.12115v4 Announce Type: replace 
Abstract: We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
arXiv:2502.12929v2 Announce Type: replace 
Abstract: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic framework developed for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in LLM solutions through compressed and interpretable task representations, resulting in improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art baselines. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Going beyond tabular classification and regression, we show the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our code is open-sourced at: https://github.com/flagshippioneering/Flow-of-Options.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>